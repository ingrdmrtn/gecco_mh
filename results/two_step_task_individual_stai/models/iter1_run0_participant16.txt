def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-weighted arbitration and learned transitions.
    
    Overview
    --------
    - Learns second-stage action values (Q2) from rewards.
    - Learns first-stage model-free values (Q1_MF) via eligibility traces.
    - Learns the transition model T(a -> s) online.
    - Arbitrates between model-based (MB) and MF at stage 1 using a weight that
      decreases with transition surprise, with stronger down-weighting for higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0 or 1 within the reached state).
    reward : array-like of float in [0,1]
        Obtained rewards at the end of trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase arbitration sensitivity to surprise.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: softmax inverse temperature for both stages.
        - omega_base in [0,1]: baseline MB weight at stage 1 (when no surprise).
        - lambda_ in [0,1]: eligibility trace controlling MF credit assignment to stage 1.
        - tau in [0,1]: transition learning rate for T(a -> s).
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha, beta, omega_base, lambda_, tau = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize with instructed/common transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Action values
    q2 = np.zeros((2, 2), dtype=float)     # state x action
    q1_mf = np.zeros(2, dtype=float)       # action at stage 1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Model-based plan using current transition beliefs
        max_q2 = np.max(q2, axis=1)             # best action per state
        q1_mb_all = T @ max_q2                  # MB value for each stage-1 action

        # Arbitration weight shaped by surprise and anxiety
        a1 = int(action_1[t])
        s_obs = int(state[t])
        # Surprise given current transition belief for chosen action
        surprise = 1.0 - float(T[a1, s_obs])
        omega = omega_base * (1.0 - stai * surprise)
        omega = float(np.clip(omega, 0.0, 1.0))

        q1 = omega * q1_mb_all + (1.0 - omega) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s_obs]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning from reward
        r = float(reward[t])

        # Update Q2 at experienced state-action
        delta2 = r - q2[s_obs, a2]
        q2[s_obs, a2] += alpha * delta2

        # MF update for stage 1 with eligibility trace
        # Blend bootstrapped value and reward-based credit
        target_mf = (1.0 - lambda_) * np.max(q2[s_obs]) + lambda_ * r
        q1_mf[a1] += alpha * (target_mf - q1_mf[a1])

        # Update transition model for chosen action towards observed state (row-stochastic)
        # One-hot target for the observed state
        for s_val in (0, 1):
            target = 1.0 if s_val == s_obs else 0.0
            T[a1, s_val] += tau * (target - T[a1, s_val])
        # Numerical cleanup to keep probabilities valid
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] /= np.sum(T[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Information-bonus planner with anxiety-amplified exploration and stickiness.
    
    Overview
    --------
    - Tracks both mean and uncertainty (variance) of second-stage rewards.
    - Adds an information bonus proportional to uncertainty when evaluating actions.
    - Anxiety increases the weight on the information bonus and reduces stage-2 inverse temperature.
    - Stage-1 choices plan over uncertainty-augmented values; includes first-stage perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0 or 1).
    reward : array-like of float in [0,1]
        Obtained rewards.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values amplify the information bonus and reduce stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for both mean and variance trackers.
        - beta in [0,10]: base inverse temperature (both stages).
        - xi_base in [0,1]: base weight on the information (uncertainty) bonus.
        - kappa in [0,1]: stage-1 perseveration/stickiness for repeating previous action.
        - zeta in [0,1]: anxiety-to-info gain factor.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, xi_base, kappa, zeta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure known to participant
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Reward belief: mean and variance for each second-stage option
    q2_mean = np.zeros((2, 2), dtype=float)
    q2_var = np.ones((2, 2), dtype=float) * 0.25  # initial uncertainty around 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None

    # Anxiety modulation
    xi = float(np.clip(xi_base * (1.0 + zeta * stai), 0.0, 1.0))
    beta2 = max(0.0, beta * (1.0 - 0.4 * stai))  # more anxiety -> more stochastic stage-2

    for t in range(n_trials):
        # Uncertainty bonus
        bonus = xi * np.sqrt(np.maximum(q2_var, 1e-8))
        q2_aug = q2_mean + bonus

        # Plan at stage 1 over augmented values
        max_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_aug
        logits1 = beta * q1_mb

        # Stage-1 perseveration
        if prev_a1 is not None:
            logits1[prev_a1] += kappa

        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy over augmented values, with anxiety-reduced beta
        s = int(state[t])
        logits2 = beta2 * q2_aug[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = float(reward[t])

        # Update mean with delta-rule
        delta = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * delta
        # Update variance as moving average of squared PE
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (delta ** 2)

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor Representation (SR) first-stage controller with anxiety-tuned weighting
    and reward sensitivity.
    
    Overview
    --------
    - Maintains SR over first-stage actions to second-stage states and learns it online.
    - Combines SR-based planning with a simple MF stage-1 value via anxiety-weighted mixing.
    - Second-stage values learned with standard delta-rule but with reward sensitivity rho
      that increases with anxiety.
    - Includes second-stage perseveration within each state.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage actions (0 or 1).
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase SR weight and reward sensitivity, and reduce stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for SR and Q-values.
        - beta in [0,10]: inverse temperature (stage 1 and baseline for stage 2).
        - sr_w_base in [0,1]: baseline weight on SR controller at stage 1.
        - rho_base in [0,1]: baseline reward sensitivity.
        - psi in [0,1]: second-stage perseveration within each state.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, sr_w_base, rho_base, psi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize SR rows for each first-stage action over states (rows sum to 1)
    SR = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)

    # Second-stage values and stage-1 MF values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Anxiety-modulated weights
    sr_w = float(np.clip(sr_w_base + 0.5 * stai * (1.0 - sr_w_base), 0.0, 1.0))
    rho = float(np.clip(rho_base + 0.7 * stai * (1.0 - rho_base), 0.0, 1.0))
    beta2 = max(0.0, beta * (1.0 - 0.3 * stai))
    lam = 0.3 + 0.5 * stai  # eligibility for MF backprop to stage 1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Track previous second-stage action per state for perseveration
    prev_a2 = [-1, -1]

    for t in range(n_trials):
        # SR-based first-stage values: expected max Q2 weighted by SR occupancy
        max_q2 = np.max(q2, axis=1)
        q1_sr = SR @ max_q2

        # Combine SR and MF controllers
        q1 = sr_w * q1_sr + (1.0 - sr_w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-state perseveration
        s = int(state[t])
        logits2 = beta2 * q2[s]
        if prev_a2[s] != -1:
            logits2[prev_a2[s]] += psi
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = float(reward[t])
        r_subj = rho * r  # reward sensitivity (higher anxiety -> stronger impact)

        # Update Q2
        delta2 = r_subj - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update stage-1 MF with eligibility towards both next-state value and reward
        target_mf = (1.0 - lam) * np.max(q2[s]) + lam * r_subj
        q1_mf[a1] += alpha * (target_mf - q1_mf[a1])

        # Update SR row for chosen action towards the observed state one-hot
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            SR[a1, ss] += alpha * (0.5 + 0.5 * stai) * (target - SR[a1, ss])
        SR[a1] = np.clip(SR[a1], 1e-6, 1.0)
        SR[a1] /= np.sum(SR[a1])

        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)