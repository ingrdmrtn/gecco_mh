def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model with anxiety-shaped MB weighting, anxiety-scaled forgetfulness, and state-conditional perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically in {0,1}.
    stai : array-like of float in [0,1]
        Anxiety score (single value array). Used to modulate MB weighting, forgetfulness, and perseveration.
    model_parameters : tuple/list
        Parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate for value updates in [0,1]
        - beta: inverse temperature for softmax in [0,10]
        - w_slope: base MB mixing weight; anxiety shifts toward (1 - w_slope) in [0,1]
                   Effective w_mb = (1 - stai)*w_slope + stai*(1 - w_slope)
        - rho_forget: baseline forgetting/decay strength in [0,1]
                      Effective decay factor each trial: decay = 1 - rho_forget * stai
        - tau_stay: state-conditional perseveration strength in [0,1]
                    Bias to repeat previous second-stage action in same state scales with stai

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, w_slope, rho_forget, tau_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (commonly A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Anxiety-shaped MB weight: higher anxiety pushes weight toward (1 - w_slope)
    w_mb = (1.0 - stai) * w_slope + stai * (1.0 - w_slope)
    w_mb = 0.0 if w_mb < 0.0 else (1.0 if w_mb > 1.0 else w_mb)

    # Probability logs
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)        # stage-1 model-free
    q2 = np.zeros((2, 2))      # stage-2 action values

    # Perseveration bookkeeping
    prev_a2 = None
    prev_s = None

    # Decay factor influenced by anxiety
    decay = 1.0 - rho_forget * stai
    if decay < 0.0: decay = 0.0
    if decay > 1.0: decay = 1.0

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q2 = np.max(q2, axis=1)           # best action per state
        q1_mb = T @ max_q2                    # expected value via transition model

        # Hybrid Q for stage-1 decision
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Softmax for stage 1
        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 decision with state-conditional perseveration
        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s == s:
            bias2[prev_a2] += tau_stay * stai  # stronger stickiness under higher anxiety

        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Apply global decay (forgetting) before incorporating new information
        q1_mf *= decay
        q2 *= decay

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping off stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a2 = a2
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive MF learning with anxiety-scaled exploration and WSLS choice kernel.

    Mechanisms:
    - Utility-based (risk-sensitive) learning: negative outcomes are weighted more strongly
      via an anxiety-dependent loss-aversion term.
    - Exploration increases with anxiety by reducing effective inverse temperature.
    - Win-Stay/Lose-Switch (WSLS) choice kernel adds a bias to repeat/switch based on last outcome.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically in {0,1}.
    stai : array-like of float in [0,1]
        Anxiety score (single value array). Used to modulate loss aversion, exploration, and WSLS bias.
    model_parameters : tuple/list
        Parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate in [0,1]
        - beta: base inverse temperature in [0,10]
        - k_loss: loss-aversion weight in [0,1]; larger values amplify negative utility under higher anxiety
                  Utility transform: u = r - k_loss*stai*(1 - r)
        - e_stai: anxiety-exploration coupling in [0,1]; effective beta = beta*(1 - e_stai*stai)
        - phi_wsls: WSLS kernel strength in [0,1]; bias = phi_wsls * (reward_prev - 0.5) * stai

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, k_loss, e_stai, phi_wsls = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective temperature decreases with anxiety (more exploration)
    beta_eff = beta * (1.0 - e_stai * stai)
    if beta_eff < 0.0: beta_eff = 0.0

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice probabilities per trial
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # WSLS kernel bookkeeping
    prev_a1 = None
    prev_a2 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # WSLS biases based on previous outcome
        bias1 = np.zeros(2)
        bias2 = np.zeros(2)
        if prev_r is not None:
            ws = (prev_r - 0.5)  # positive if win, negative if loss
            bmag = phi_wsls * ws * stai
            if prev_a1 is not None:
                # Win => stay (positive bias on previous action), Loss => switch (negative on prev action)
                bias1[prev_a1] += bmag
            if prev_a2 is not None and prev_s is not None:
                # Only bias the same state as previously visited
                if prev_s == state[t]:
                    bias2[prev_a2] += bmag

        # Stage-1 softmax over MF values + WSLS bias
        z1 = (q1_mf + bias1) - np.max(q1_mf + bias1)
        exp1 = np.exp(beta_eff * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax over MF second-stage values + WSLS bias
        s = state[t]
        q2_s = q2[s].copy()
        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta_eff * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Risk-sensitive utility transform with anxiety-modulated loss aversion
        r = reward[t]
        u = r - k_loss * stai * (1.0 - r)  # if r=0, u=-k_loss*stai; if r=1, u=1

        # Stage-2 update (utility prediction error)
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping on second-stage value (post-update)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Bookkeeping
        prev_a1 = a1
        prev_a2 = a2
        prev_s = s
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-driven exploration and anxiety-weighted trust in learned transitions (adaptive MB/MF hybrid).

    Mechanisms:
    - Learns the transition function online; computes transition surprise each trial.
    - Anxiety reduces model-based reliance when recent transition surprise is high.
    - Temperature adapts to value uncertainty: higher uncertainty -> lower effective beta (more exploration),
      with strength scaled by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (spaceship A=0, U=1) for each trial.
    state : array-like of int (0 or 1)
        Second-stage state (planet X=0, Y=1) reached on each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien indices W/P=0, S/H=1) for each trial.
    reward : array-like of float
        Reward (coins) on each trial, typically in {0,1}.
    stai : array-like of float in [0,1]
        Anxiety score (single value array). Scales exploration from uncertainty and reduces MB reliance under surprise.
    model_parameters : tuple/list
        Parameters (all in [0,1] except beta in [0,10]):
        - alpha: learning rate for both value updates and transition updates in [0,1]
        - beta: base inverse temperature in [0,10]
        - u_scale: uncertainty-to-exploration coupling in [0,1]
                   beta_eff = beta / (1 + u_scale * stai * uncertainty)
        - kappa: smoothing for uncertainty/surprise traces in [0,1]
                 new_trace = (1 - kappa)*old + kappa*current_signal
        - t0: baseline trust in model-based control in [0,1]
              Effective MB weight w_mb = clip(t0 * (1 - stai * surprise_trace), 0, 1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, u_scale, kappa, t0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix with a weak prior toward common transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and uncertainty
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    v2 = np.zeros((2, 2))  # running uncertainty proxy per state-action (abs TD magnitude)

    # Probability logs
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Surprise trace (recent unexpectedness of transitions)
    surprise_trace = 0.0

    for t in range(n_trials):
        # Compute MB component using current learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Determine MB weight from anxiety and recent transition surprise
        w_mb = t0 * (1.0 - stai * surprise_trace)
        if w_mb < 0.0: w_mb = 0.0
        if w_mb > 1.0: w_mb = 1.0

        # Uncertainty-driven temperature adjustments
        # Stage-2: use state-conditional uncertainty
        s = state[t]
        unc2 = 0.5 * (v2[s, 0] + v2[s, 1])  # mean uncertainty in current state
        beta2_eff = beta / (1.0 + u_scale * stai * unc2)
        if beta2_eff < 0.0: beta2_eff = 0.0

        # Stage-1: global uncertainty (mean over all second-stage actions)
        unc1 = np.mean(v2)
        beta1_eff = beta / (1.0 + u_scale * stai * unc1)
        if beta1_eff < 0.0: beta1_eff = 0.0

        # Stage-1 decision
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb
        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta1_eff * z1)
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 decision
        q2_s = q2[s].copy()
        z2 = q2_s - np.max(q2_s)
        exp2 = np.exp(beta2_eff * z2)
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Transition learning and surprise update (based on pre-update T)
        # Surprise = 1 - predicted probability of observed transition
        pred_p = T[a1, s]
        curr_surprise = 1.0 - pred_p
        surprise_trace = (1.0 - kappa) * surprise_trace + kappa * curr_surprise

        # Update transition row toward the observed state (simple delta rule)
        T[a1, s] += alpha * (1.0 - T[a1, s])
        T[a1, 1 - s] += alpha * (0.0 - T[a1, 1 - s])

        # Stage-2 TD update and uncertainty update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        v2[s, a2] = (1.0 - kappa) * v2[s, a2] + kappa * abs(delta2)

        # Stage-1 MF update (bootstrap from second stage)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll