def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated model-based control with anxiety-dependent lapse and eligibility.
    
    Description:
    - Stage 1 action values are a hybrid of model-based (MB) and model-free (MF) values.
    - The MB weight is dynamically modulated by transition surprise (rare vs. common) and attenuated by anxiety.
      Specifically, rare transitions increase MB control by k_surprise, but the increase is scaled by (1 - stai).
    - An anxiety-dependent lapse mixes softmax choice with uniform random responding, stronger with higher stai.
    - Stage 2 follows MF learning; stage 1 MF includes an eligibility trace whose strength increases as anxiety decreases.
    - Anxiety also slightly reduces the learning rate (putatively reflecting cautious updating).
    
    Parameters (model_parameters):
    - alpha: base learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w0: baseline MB weight in [0,1]
    - k_surprise: surprise sensitivity (adds/subtracts to MB weight on rare/common) in [0,1]
    - lapse: baseline lapse rate for random choice mixing in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on observed planet
                (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w0, k_surprise, lapse)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w0, k_surprise, lapse = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    Q1_mf = np.zeros(2)      # Stage-1 MF values
    Q2 = np.zeros((2, 2))    # Stage-2 MF values: state x action

    # Anxiety effects:
    # - eligibility trace stronger for low anxiety
    lam = 1.0 - st  # in [0,1]
    # - effective learning rate reduced by anxiety (gentler updates with higher stai)
    alpha_eff = alpha * (1.0 - 0.5 * st)
    # - lapse grows with anxiety
    lapse_eff = lapse * st

    # Helper for stable logit transforms
    eps = 1e-10
    def logit(p):
        return np.log(p + eps) - np.log(1.0 - p + eps)
    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute MB stage-1 values from current Q2
        max_Q2 = np.max(Q2, axis=1)   # best value per state
        Q1_mb = T @ max_Q2            # expected value of each spaceship

        # Surprise (rare vs common) based on chosen action and observed state
        # common if (A->X) or (U->Y), else rare
        common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Signed surprise signal: +1 for rare, -1 for common
        surprise = 1.0 if common == 0 else -1.0

        # Anxiety-attenuated surprise effect on MB weight (via logit space)
        w_base_logit = logit(np.clip(w0, eps, 1 - eps))
        w_adj = w_base_logit + (1.0 - st) * k_surprise * surprise
        w = inv_logit(w_adj)
        w = min(1.0, max(0.0, w))

        # Hybrid stage-1 value
        Q1_hyb = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage-1 policy with lapse
        pref1 = Q1_hyb
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1.0 - lapse_eff) * soft1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        pref2 = Q2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1.0 - lapse_eff) * soft2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning: MF with eligibility trace on stage-1
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_eff * delta2

        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_eff * delta1
        # eligibility propagation of outcome to stage-1
        Q1_mf[a1] += alpha_eff * lam * delta2

    eps_ll = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps_ll)) + np.sum(np.log(p_choice_2 + eps_ll)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with fixed MB weight and anxiety-modulated curvature and perseveration.
    
    Description:
    - Rewards are transformed by a concave utility u(r) = r^gamma_eff to capture risk/uncertainty sensitivity.
      Higher anxiety yields more concave utility (stronger diminishing returns).
    - Stage 1 values are a fixed hybrid of model-based (MB) and model-free (MF) values with weight w_mb.
    - A single perseveration parameter biases repeating previous choices at both stages; its influence decreases with anxiety.
    - Stage 2 values are MF and learned from utility-transformed rewards.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w_mb: fixed MB weight in [0,1]
    - gamma_base: baseline utility curvature in [0,1] (smaller -> more concave)
    - pi: perseveration strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on observed planet
                (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w_mb, gamma_base, pi)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w_mb, gamma_base, pi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety effects:
    # - Utility more concave with higher anxiety
    gamma_eff = np.clip((1.0 - st) * gamma_base + st * max(1e-6, 0.5 * gamma_base + 0.25), 1e-6, 1.0)
    # - Perseveration weaker with higher anxiety
    pi_eff = pi * (1.0 - st)
    # - Eligibility trace derived from anxiety to propagate outcomes to stage 1
    lam = 0.5 * (1.0 - st)

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB stage-1 values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid stage-1
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add perseveration bias
        pref1 = Q1_hyb.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += pi_eff

        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        pref2 = Q2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += pi_eff

        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Utility transform of reward
        u = r ** gamma_eff

        # Learning
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1
        Q1_mf[a1] += alpha * lam * delta2

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Dual-temperature, dual-learning-rate hybrid with anxiety-driven exploration and forgetting.
    
    Description:
    - Separate learning rates for stage 1 and stage 2 (alpha1, alpha2).
    - Separate inverse temperatures for stage 1 and stage 2 (beta1, beta2).
      Anxiety reduces beta1 (more exploration at stage 1) and mildly reduces beta2.
    - Fixed MB weight omega combines model-based and model-free stage-1 values.
    - Stage 2 includes anxiety-driven forgetting/decay of unchosen action values within the visited state.
    - Stage 1 MF uses an eligibility trace whose strength increases with anxiety (faster credit assignment to outcomes).
    
    Parameters (model_parameters):
    - alpha1: learning rate for stage-1 MF in [0,1]
    - alpha2: learning rate for stage-2 MF in [0,1]
    - beta1: inverse temperature for stage 1 in [0,10]
    - beta2: inverse temperature for stage 2 in [0,10]
    - omega: MB weight in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on observed planet
                (0=W on X or P on Y; 1=S on X or H on Y)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha1, alpha2, beta1, beta2, omega)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha1, alpha2, beta1, beta2, omega = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety effects:
    # - exploration: reduce beta with higher anxiety
    beta1_eff = max(1e-8, beta1 * (1.0 - 0.5 * st))
    beta2_eff = max(1e-8, beta2 * (1.0 - 0.25 * st))
    # - eligibility: faster outcome credit assignment at higher anxiety
    lam = 0.2 + 0.8 * st  # in [0.2,1.0]
    # - forgetting/decay rate for unchosen action in visited state increases with anxiety
    decay = 0.1 * st  # decays toward zero

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB estimate from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid stage-1 values
        Q1_hyb = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage-1 policy
        pref1 = Q1_hyb
        exp1 = np.exp(beta1_eff * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        pref2 = Q2[s]
        exp2 = np.exp(beta2_eff * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha2 * delta2

        # Anxiety-driven decay of the unchosen action in the visited state
        other = 1 - a2
        Q2[s, other] *= (1.0 - decay)

        # Stage-1 MF updates with eligibility
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha1 * delta1
        Q1_mf[a1] += alpha1 * lam * (r - Q1_mf[a1])

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll