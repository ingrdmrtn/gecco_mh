def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Pessimistic lookahead with anxiety-modulated loss aversion and forgetting.
    Parameters (model_parameters):
      - alpha: learning rate for Q-value updates at both stages, in [0,1]
      - beta: inverse temperature for both stages, in [0,10]
      - lambda_loss: base loss-aversion coefficient, in [0,1] (effective loss-aversion increases with anxiety)
      - kappa_anx: strength of anxiety-driven pessimism in model-based lookahead, in [0,1]
      - phi_forget: forgetting rate for unchosen actions (value decay), in [0,1]
    Inputs:
      - action_1: array of first-stage choices (0=A, 1=U)
      - state: array of second-stage states (0=X, 1=Y)
      - action_2: array of second-stage choices (0 or 1; e.g., alien index on the planet)
      - reward: array of scalar rewards (can be negative or positive)
      - stai: array-like with a single anxiety score in [0,1]
      - model_parameters: list/tuple as described above
    Returns:
      - Negative log-likelihood of the observed first- and second-stage choices.
    Model summary:
      - Second-stage learning is model-free with loss-averse utility u(r).
      - First-stage decision uses a convex combination of:
          (i) model-free Q1 values bootstrapped from second-stage MF values via eligibility,
          (ii) a pessimistic model-based lookahead over the transition matrix.
        Anxiety increases loss aversion and pessimism, and also increases forgetting of unchosen actions.
    """
    alpha, beta, lambda_loss, kappa_anx, phi_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition: rows = first-stage actions (A=0, U=1); cols = states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    Q1_mf = np.zeros(2)        # model-free values for first-stage actions
    Q2 = np.zeros((2, 2))      # second-stage state-action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated components
    # Effective loss aversion: grows with anxiety (mapping lambda_eff in [0, 2*lambda_loss])
    lambda_eff = lambda_loss * (1.0 + stai)
    # Pessimistic blending within each second-stage state: xi near 1 -> optimistic (max), near 0 -> pessimistic (min)
    xi = np.clip(1.0 - kappa_anx * stai, 0.0, 1.0)
    # Forgetting rate scales with anxiety
    forget = np.clip(phi_forget * (0.5 + stai), 0.0, 1.0)

    eps = 1e-12

    for t in range(n_trials):
        # Construct pessimistic state values from Q2 for model-based lookahead
        vmax = np.max(Q2, axis=1)   # per state
        vmin = np.min(Q2, axis=1)   # per state
        V_state = xi * vmax + (1.0 - xi) * vmin  # pessimism-weighted value per state

        Q1_mb_pess = T @ V_state  # model-based action values under pessimistic evaluation

        # Anxiety-driven arbitration without adding a separate parameter:
        # weight increases with anxiety when loss aversion is low, and vice versa.
        w_eff = np.clip(0.5 + 0.5 * stai * (1.0 - lambda_loss), 0.0, 1.0)

        Q1 = w_eff * Q1_mb_pess + (1.0 - w_eff) * Q1_mf

        # Softmax for first-stage choice
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice policy
        s = int(state[t])
        q2c = Q2[s, :] - np.max(Q2[s, :])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Utility transformation with loss aversion
        r = float(reward[t])
        if r >= 0:
            u = r
        else:
            u = - (1.0 + lambda_eff) * (-r)

        # Learning updates
        # Second-stage MF update
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Forgetting for unchosen second-stage action at current state
        other_a2 = 1 - a2
        Q2[s, other_a2] *= (1.0 - forget)

        # First-stage MF bootstrapping via observed second-stage value (eligibility assumed 1 here)
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        # Forget unchosen first-stage action
        other_a1 = 1 - a1
        Q1_mf[other_a1] *= (1.0 - forget)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Successor Representation (SR) arbitration with anxiety-modulated SR weight.
    Parameters (model_parameters):
      - alpha: learning rate for value and SR updates, in [0,1]
      - beta: inverse temperature for both stages, in [0,10]
      - omega0: baseline weight on SR vs MF at stage 1, in [0,1]
      - phi: strength of anxiety modulation of SR weight, in [0,1]
      - tau: SR discount/trace parameter (controls spread of occupancy), in [0,1]
    Inputs:
      - action_1: array of first-stage choices (0=A, 1=U)
      - state: array of second-stage states (0=X, 1=Y)
      - action_2: array of second-stage choices on the planet (0 or 1)
      - reward: array of rewards
      - stai: array-like with a single anxiety score in [0,1]
      - model_parameters: list/tuple as described above
    Returns:
      - Negative log-likelihood of observed first- and second-stage choices.
    Model summary:
      - Stage-2 learning is standard MF (Q2).
      - Stage-1 combines model-free Q1_mf with a Successor Representation value Q1_sr.
        SR maps first-stage actions to expected occupancy over second-stage states; Q1_sr is M[a1] dot max(Q2).
      - Anxiety increases reliance on SR via omega = clip(omega0 + phi*(stai - 0.5), 0, 1).
    """
    alpha, beta, omega0, phi, tau = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix (for computing surprise diagnostics if needed)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    Q1_mf = np.zeros(2)       # MF values for first-stage actions
    Q2 = np.zeros((2, 2))     # Second-stage state-action values

    # SR matrix: rows = first-stage actions, cols = second-stage states (occupancy)
    M = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration
    omega = np.clip(omega0 + phi * (stai - 0.5), 0.0, 1.0)
    eps = 1e-12

    for t in range(n_trials):
        # Compute SR-based first-stage values
        V_states = np.max(Q2, axis=1)  # value per second-stage state
        Q1_sr = M @ V_states           # SR value per first-stage action
        Q1 = omega * Q1_sr + (1.0 - omega) * Q1_mf

        # First-stage policy
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = int(state[t])
        q2c = Q2[s, :] - np.max(Q2[s, :])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Stage-2 MF learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping from realized second-stage value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        # SR update for chosen first-stage action:
        # Target occupancy vector for the chosen action is one-hot for observed state plus discounted future (terminal -> 0).
        e_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        target = e_s  # terminal at stage 2, so no future occupancy beyond discount
        M[a1, :] += alpha * (target + tau * 0.0 - M[a1, :])

        # Optional small SR consolidation for the unchosen action toward known transitions (stabilizes learning)
        # The degree of consolidation is scaled by (1 - stai) to avoid redundancy with anxiety-weighted arbitration.
        unchosen = 1 - a1
        M[unchosen, :] += alpha * (1.0 - stai) * 0.1 * (T[unchosen, :] - M[unchosen, :])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-sensitive learning with anxiety-modulated exploration and choice kernels.
    Parameters (model_parameters):
      - alpha: base learning rate for Q updates, in [0,1]
      - beta: base inverse temperature, in [0,10]
      - kappa_surprise: strength of surprise-driven learning-rate and exploration modulation, in [0,1]
      - chi: choice kernel learning rate (stickiness) for both stages, in [0,1]
      - eta2: scales stage-2 inverse temperature relative to stage-1, in [0,1]
    Inputs:
      - action_1: array of first-stage choices (0=A, 1=U)
      - state: array of second-stage states (0=X, 1=Y)
      - action_2: array of second-stage choices (0 or 1)
      - reward: array of rewards
      - stai: array-like with a single anxiety score in [0,1]
      - model_parameters: list/tuple as described above
    Returns:
      - Negative log-likelihood of observed choices.
    Model summary:
      - MF learner at both stages with choice kernels (tendency to repeat chosen actions).
      - Surprise = 1 - P(observed state | chosen action) under known transitions.
      - Anxiety amplifies surprise effects:
          * alpha_eff = alpha * (1 + kappa_surprise * stai * surprise)
          * beta1_eff decreases with surprise*stai (more exploration after surprising transitions)
          * beta2_eff = beta * (1 + 2*eta2*(0.5 - stai)) bounded at [0,10] via soft scaling
    """
    alpha, beta, kappa_surprise, chi, eta2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and choice kernels
    Q1 = np.zeros(2)        # MF first-stage values
    Q2 = np.zeros((2, 2))   # MF second-stage values
    K1 = np.zeros(2)        # choice kernel for first-stage
    K2 = np.zeros((2, 2))   # choice kernel for second-stage (per state)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute current inverse temperatures with anxiety-surprise modulation
        # Stage-2 beta depends on anxiety (eta2 shrinks beta for higher stai) but stays >= 0
        beta2_eff = beta * (1.0 + 2.0 * eta2 * (0.5 - stai))
        beta2_eff = max(0.0, min(10.0, beta2_eff))

        # Stage-1 policy with choice kernel
        prefs1 = Q1 + K1
        prefs1_c = prefs1 - np.max(prefs1)
        probs_1 = np.exp(beta * prefs1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with choice kernel
        s = int(state[t])
        prefs2 = Q2[s, :] + K2[s, :]
        prefs2_c = prefs2 - np.max(prefs2)
        probs_2 = np.exp(beta2_eff * prefs2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # Surprise based on observed transition
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # common -> 0.3; rare -> 0.7

        # Anxiety-amplified effective learning rate and exploration modulation
        alpha_eff = alpha * (1.0 + kappa_surprise * stai * surprise)
        # Reduce exploitation at stage 1 when surprised under anxiety
        beta1_eff = beta / (1.0 + kappa_surprise * stai * surprise)
        beta1_eff = max(0.0, min(10.0, beta1_eff))

        # Recompute first-stage probability with beta1_eff for internal consistency of likelihood
        # Note: We adjust only the recorded probability (policy remains softmax over same prefs)
        probs_1_adj = np.exp(beta1_eff * prefs1_c)
        probs_1_adj = probs_1_adj / (np.sum(probs_1_adj) + eps)
        p_choice_1[t] = probs_1_adj[a1]

        # Learning updates (MF)
        # Second stage
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_eff * delta2

        # First stage bootstrapping
        td1 = Q2[s, a2] - Q1[a1]
        Q1[a1] += alpha_eff * td1

        # Choice kernel updates (reinforce chosen, decay others)
        # Stage 1
        K1 *= (1.0 - chi)
        K1[a1] += chi * (1.0 - K1[a1])
        # Stage 2
        K2[s, :] *= (1.0 - chi)
        K2[s, a2] += chi * (1.0 - K2[s, a2])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll