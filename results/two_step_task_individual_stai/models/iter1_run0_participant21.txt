def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated model-based weighting and dual learning rates.

    The model combines model-free (MF) and model-based (MB) action values at stage 1.
    Stage-2 values are learned with a delta rule. Stage-1 MF values receive a TD-style
    backup from stage-2. The model-based weight is reduced by higher anxiety, capturing
    a shift away from planning under anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha2 (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = alpha1 (0 to 1): learning rate for stage-1 MF Q-values
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = w_base (0 to 1): baseline weight on model-based values at stage 1
    - model_parameters[4] = anx_MB (0 to 1): strength of anxiety modulation on MB weight
        Effective MB weight: w = clip(w_base + (0.5 - stai) * anx_MB, 0, 1).
        Thus, higher anxiety (stai>0.5) reduces w; lower anxiety increases w.

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha2, alpha1, beta, w_base, anx_MB = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (rows: A,U; cols: X,Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2)          # stage-1 MF values for A/U
    q2 = np.ones((2, 2)) * 0.5   # stage-2 values per state (X,Y) and action (0,1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (fixed across trials for parsimony)
    w = w_base + (0.5 - stai_val) * anx_MB
    w = 0.0 if w < 0.0 else (1.0 if w > 1.0 else w)

    for t in range(n_trials):
        # Model-based stage-1 values via lookahead through transitions
        max_q2 = np.max(q2, axis=1)  # best action per planet
        q1_mb = transition_matrix @ max_q2

        # Hybrid stage-1 values
        q1_mix = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy
        q1c = q1_mix - np.max(q1_mix)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha2 * pe2

        # Stage-1 MF TD backup from stage-2 value at reached state
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] = q1_mf[a1] + alpha1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with anxiety-suppressed directed exploration (uncertainty bonus) and stage-2 perseveration.

    Stage-2 action values are augmented with an uncertainty bonus (UCB-style) to encourage
    directed exploration. Uncertainty is tracked via an exponential moving estimate of reward
    variance per state-action. Anxiety reduces the exploration bonus. A small perseveration
    bias at stage 2 is stronger when anxiety is higher, capturing anxious tendency to repeat.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning/smoothing rate for both mean and variance
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = kappa_base (0 to 1): baseline weight of uncertainty bonus
    - model_parameters[3] = anx_explore (0 to 1): strength of anxiety suppression of bonus
        Effective bonus weight: kappa_eff = kappa_base * (1 - anx_explore * stai)
    - model_parameters[4] = rho2 (0 to 1): baseline perseveration magnitude at stage 2
        Effective perseveration: stick_eff = rho2 * stai

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, kappa_base, anx_explore, rho2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Track mean and variance of rewards per state-action
    q2_mean = np.ones((2, 2)) * 0.5
    q2_var = np.ones((2, 2)) * 0.05  # small initial uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    kappa_eff = kappa_base * (1.0 - anx_explore * stai_val)
    if kappa_eff < 0.0:
        kappa_eff = 0.0
    stick_eff = rho2 * stai_val  # more anxious -> more perseveration at stage 2

    prev_a2 = [None, None]  # last action taken in each state (X=0,Y=1)

    for t in range(n_trials):
        # Construct UCB-augmented values
        bonus = kappa_eff * np.sqrt(np.maximum(q2_var, 1e-12))
        q2_ucb = q2_mean + bonus

        # Stage-1 model-based values via UCB-propagated expectations
        max_q2_ucb = np.max(q2_ucb, axis=1)
        q1_mb = transition_matrix @ max_q2_ucb

        # Stage-1 softmax
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 softmax with perseveration bias in reached state
        s = state[t]
        q_s = q2_ucb[s].copy()
        if prev_a2[s] is not None:
            q_s[prev_a2[s]] += stick_eff
        q2c = q_s - np.max(q_s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning updates from observed reward
        r = reward[t]
        pe = r - q2_mean[s, a2]
        # Mean update
        q2_mean[s, a2] = q2_mean[s, a2] + alpha * pe
        # Variance update (EWMA of squared error around updated mean)
        # Use one-step approximation: v <- (1-alpha)*v + alpha*(pe^2)
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (pe ** 2)

        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Planning with anxiety-accelerated transition learning and reward forgetting.

    The agent learns the transition model between stage 1 and stage 2 with a delta rule.
    Anxiety increases the transition learning rate, capturing heightened sensitivity to
    environmental changes. Stage-2 reward values are subject to forgetting toward 0.5,
    with forgetting mildly increased by anxiety. Planning uses the learned transition matrix.

    Parameters (bounds):
    - model_parameters[0] = alpha_r (0 to 1): learning rate for stage-2 rewards
    - model_parameters[1] = beta (0 to 10): inverse temperature at both stages
    - model_parameters[2] = alpha_T_base (0 to 1): baseline transition learning rate
    - model_parameters[3] = anx_T (0 to 1): strength of anxiety modulation on transitions and forgetting
        Effective transition LR: alpha_T = clip(alpha_T_base * (1 + anx_T * stai), 0, 1)
        Effective forgetting: d_eff = clip(d_base * (1 + 0.5 * anx_T * stai), 0, 1)
    - model_parameters[4] = d_base (0 to 1): baseline forgetting rate of stage-2 values toward 0.5

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha_r, beta, alpha_T_base, anx_T, d_base = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model close to canonical structure but learnable
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Stage-2 values
    q2 = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated rates
    alpha_T = alpha_T_base * (1.0 + anx_T * stai_val)
    if alpha_T > 1.0:
        alpha_T = 1.0
    d_eff = d_base * (1.0 + 0.5 * anx_T * stai_val)
    if d_eff > 1.0:
        d_eff = 1.0

    for t in range(n_trials):
        # Planning with learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2s = q2[s]
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward learning with forgetting toward 0.5 baseline
        r = reward[t]
        # Apply forgetting to all state-action values each trial
        q2 = (1.0 - d_eff) * q2 + d_eff * 0.5
        # Then update the visited state-action with reward PE
        pe2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha_r * pe2

        # Transition learning: move T[a1] toward the observed next state s
        # One-hot target distribution for observed transition
        for next_s in range(2):
            target = 1.0 if next_s == s else 0.0
            T[a1, next_s] = T[a1, next_s] + alpha_T * (target - T[a1, next_s])

        # Renormalize to avoid numeric drift
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum
        else:
            T[a1] = np.array([0.5, 0.5])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll