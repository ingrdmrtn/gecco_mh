def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-modulated arbitration.
    
    The agent learns: 
    - Model-free Q-values at both stages.
    - A transition model from first-stage actions to second-stage states.
    First-stage choice values are a convex combination of model-based and model-free values,
    with the arbitration weight modulated by the participant's anxiety (stai).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates at both stages.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - mb0: [0,1] baseline weight of model-based control (0=MF only, 1=MB only).
    - anx_slope: [0,1] strength with which anxiety shifts control; higher stai reduces mb weight.
    - eta_T: [0,1] learning rate for updating the transition model.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship each trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage planet reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet.
    - reward: array-like of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, mb0, anx_slope, eta_T].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, mb0, anx_slope, eta_T = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix; rows are actions, columns are states
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper to get a bounded MB weight as a logistic transform of mb0 shifted by anxiety
    def stable_logit(p):
        eps = 1e-6
        p = min(1 - eps, max(eps, p))
        return np.log(p) - np.log(1 - p)

    base_logit = stable_logit(mb0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Compute model-based first-stage Q via learned transitions and second-stage max values
        max_q2_by_state = np.max(q2, axis=1)  # length 2 (for states X,Y)
        q1_mb = T @ max_q2_by_state           # length 2 (for actions A,U)

        # Anxiety-modulated arbitration: higher stai shifts weight toward model-free
        # mb_weight = sigmoid(base_logit + k*(0.5 - stai)), where k âˆˆ [0,4] via anx_slope scaling
        k = 4.0 * anx_slope
        mb_weight = 1.0 / (1.0 + np.exp(-(base_logit + k * (0.5 - stai))))
        mb_weight = min(1.0, max(0.0, mb_weight))

        q1 = mb_weight * q1_mb + (1.0 - mb_weight) * q1_mf

        # First-stage policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping on reached state's chosen Q
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update learned transitions based on observed state given action
        # Move the row for the chosen action toward the observed state one-hot
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - eta_T) * T[a1, :] + eta_T * target
        # Normalize to ensure probabilities
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with directed exploration bonus (UCB-style) modulated by anxiety, plus value decay.
    
    The agent is primarily model-free. At stage 2, it uses an uncertainty-driven exploration
    bonus that decreases with experience; higher anxiety reduces the magnitude of this bonus.
    Q-values decay toward a neutral prior to capture nonstationarity.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q updates.
    - beta: [0,10] inverse temperature for softmax.
    - bonus0: [0,1] baseline exploration bonus magnitude.
    - anx_bonus: [0,1] scales how much anxiety reduces the bonus: bonus_eff = bonus0*(1 - anx_bonus*stai).
    - rho: [0,1] per-trial decay toward 0.5 for all Q-values (captures drift/nonstationarity).
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship each trial (0=A, 1=U).
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet.
    - reward: array-like of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, bonus0, anx_bonus, rho].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, bonus0, anx_bonus, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective directed exploration bonus magnitude
    bonus_eff = bonus0 * (1.0 - anx_bonus * stai)
    bonus_eff = min(1.0, max(0.0, bonus_eff))

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Visit counts for UCB-like uncertainty bonus at stage 2
    n2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 policy (pure MF)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with UCB-style bonus decreasing with visits
        bonus_vec = np.zeros(2)
        for a in range(2):
            bonus_vec[a] = bonus_eff / np.sqrt(n2[s, a] + 1.0)
        logits2 = beta * (q2[s, :] + bonus_vec)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        n2[s, a2] += 1.0

        # Stage-1 TD bootstrapping on stage-2 chosen value
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1

        # Per-trial decay toward neutral value 0.5 to capture drifting environment
        if rho > 0:
            q2 = (1.0 - rho) * q2 + rho * 0.5
            q1 = (1.0 - rho) * q1 + rho * 0.5

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-free RL with transition-dependent stickiness modulated by rarity.
    
    The agent is model-free but transforms rewards via a concavity parameter (risk sensitivity).
    Anxiety modulates risk sensitivity: higher anxiety increases concavity (more risk-averse),
    which down-weights the impact of obtained rewards. Additionally, first-stage choices
    include a transition-dependent stickiness: the tendency to repeat the previous first-stage action
    is positive after common transitions and negative after rare transitions, scaled by tau_trans.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate.
    - beta: [0,10] inverse temperature for softmax.
    - eta0: [0,1] baseline risk/utility exponent; utility u(r) = r^(eta_eff).
    - anx_risk: [0,1] how much anxiety shifts eta: eta_eff = clip(eta0 + anx_risk*(stai - 0.31), 0,1).
    - tau_trans: [0,1] magnitude of transition-dependent stickiness on first-stage logits.
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U).
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, chosen alien on the reached planet.
    - reward: array-like of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, eta0, anx_risk, tau_trans].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, eta0, anx_risk, tau_trans = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective risk exponent; centered around medium anxiety threshold (.31 from instructions)
    eta_eff = eta0 + anx_risk * (stai_val - 0.31)
    eta_eff = min(1.0, max(0.0, eta_eff))

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_common = None

    # Fixed transition structure to determine common vs rare
    # Action 0 (A) commonly -> state 0 (X); Action 1 (U) commonly -> state 1 (Y)
    def is_common(a, s):
        return (a == 0 and s == 0) or (a == 1 and s == 1)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Transition-dependent stickiness bias on first-stage logits
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_common is not None:
            # Bias toward repeating previous action if common, away if rare
            sign = 1.0 if prev_common else -1.0
            bias1[prev_a1] += tau_trans * sign

        # First-stage policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Risk-sensitive utility transform
        u = (r ** eta_eff)

        # Learning updates (pure MF)
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1

        # Update previous transition info
        prev_common = is_common(a1, s)
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll