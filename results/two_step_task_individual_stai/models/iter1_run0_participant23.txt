def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Trust-weighted model-based planning with anxiety-modulated lapse and eligibility.

    Idea
    ----
    The agent relies on a trusted transition model to plan at stage 1, but anxiety
    reduces this trust and introduces a uniform/noisy assumption over transitions.
    The same trust also serves as the model-based arbitration weight (higher trust
    -> more model-based). Choices include an anxiety-driven lapse. Stage-2 values
    are learned with TD; stage-1 model-free values are updated via an eligibility
    trace scaled by eta. All parameters are used and bounded.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int in {0,1}
        Observed second-stage state (0 = planet X, 1 = planet Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices within the visited state (0/1 = the two aliens).
    reward : array-like of float
        Reward on each trial (e.g., 0 or 1).
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha2, beta, trust0, lapse0, eta]
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - trust0 in [0,1]: baseline trust in the known transition model (reduced by anxiety).
        - lapse0 in [0,1]: baseline lapse probability (increased by anxiety).
        - eta in [0,1]: eligibility strength to update stage-1 MF from stage-2 value.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha2, beta, trust0, lapse0, eta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known common transitions: A->X, U->Y (0.7 common, 0.3 rare)
    true_T = np.array([[0.7, 0.3],
                       [0.3, 0.7]])  # rows: action A/U, cols: state X/Y

    # Anxiety reduces trust in the true transition model and increases lapse
    trust = max(0.0, min(1.0, trust0 * (1.0 - stai)))
    lapse = max(0.0, min(1.0, lapse0 * stai))

    # Effective transitions are a blend of true and uniform (agnostic) due to reduced trust
    uniform_T = np.full_like(true_T, 0.5, dtype=float)
    eff_T = trust * true_T + (1.0 - trust) * uniform_T

    # Stage values
    q1_mf = np.zeros(2)        # First-stage model-free Q
    q2_mf = np.zeros((2, 2))   # Second-stage model-free Q: state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation via effective transitions
        max_q2 = np.max(q2_mf, axis=1)     # best action per state
        q1_mb = eff_T @ max_q2             # plan over transitions

        # Arbitration: trust also sets the MB weight
        q1_hybrid = trust * q1_mb + (1.0 - trust) * q1_mf

        # First-stage policy with lapse
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Second-stage policy with lapse
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning: second stage TD
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha2 * delta2

        # Eligibility-like update for first-stage MF from realized second-stage value
        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += eta * alpha2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric affective learning with anxiety-modulated negativity bias and confirmation.

    Idea
    ----
    Second-stage learning uses separate learning rates for positive vs. negative
    prediction errors, with anxiety amplifying negative learning and attenuating
    positive learning (negativity bias). A confirmation bias further scales updates:
    when the outcome's sign matches the pre-existing choice advantage, the learning
    rate is increased; otherwise decreased. Stage 1 is model-free and backed up
    via an eligibility term proportional to psi, which also serves as reward
    sensitivity at stage 2.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices.
    state : array-like of int in {0,1}
        Second-stage state per trial.
    action_2 : array-like of int in {0,1}
        Second-stage actions per state.
    reward : array-like of float
        Trial rewards (e.g., 0/1).
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha_pos, alpha_neg, beta, psi0, conf0]
        - alpha_pos in [0,1]: base LR for positive PEs (reduced by anxiety).
        - alpha_neg in [0,1]: base LR for negative PEs (increased by anxiety).
        - beta in [0,10]: inverse temperature for both stages.
        - psi0 in [0,1]: reward sensitivity and eligibility strength (reduced by anxiety).
        - conf0 in [0,1]: baseline confirmation bias magnitude (reduced by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, psi0, conf0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety modulation
    a_pos = max(0.0, min(1.0, alpha_pos * (1.0 - 0.5 * stai)))
    a_neg = max(0.0, min(1.0, alpha_neg * (1.0 + 0.5 * stai)))
    psi = max(0.0, min(1.0, psi0 * (1.0 - stai)))
    conf = max(0.0, min(1.0, conf0 * (1.0 - stai)))

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage choice: pure MF
        logits1 = beta * q1_mf
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Second-stage choice: MF with reward sensitivity in valuation (implicit via learning)
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Effective reward scaled by psi
        r_eff = psi * r

        # Compute PE and sign of prior choice advantage
        chosen_val = q2_mf[s, a2]
        unchosen_val = q2_mf[s, 1 - a2]
        delta2 = r_eff - chosen_val

        # Confirmation factor: if PE sign matches choice advantage sign, amplify; else dampen
        advantage_sign = 0.0
        if chosen_val != unchosen_val:
            advantage_sign = np.sign(chosen_val - unchosen_val)
        pe_sign = np.sign(delta2) if delta2 != 0 else 0.0
        same_sign = (advantage_sign != 0.0) and (pe_sign == advantage_sign)

        conf_scale = (1.0 + conf) if same_sign else (1.0 - conf)

        # Asymmetric LR by PE sign
        lr2 = a_pos if delta2 >= 0.0 else a_neg
        lr2 *= conf_scale
        lr2 = max(0.0, min(1.0, lr2))

        # Update second-stage Q
        q2_mf[s, a2] += lr2 * delta2

        # Eligibility-like backup to stage 1 (scaled by psi)
        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += psi * lr2 * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transitions with UCB exploration and anxiety-modulated forgetting.

    Idea
    ----
    The agent learns the stage-1 transition probabilities from experience via
    simple Dirichlet counts. Stage-2 action selection includes an uncertainty
    bonus (UCB-style) based on visit counts, with anxiety reducing exploratory
    bonus. First-stage action values combine learned model-based values and
    model-free values; anxiety reduces the model-based blend. Q-values decay
    toward 0 each trial (forgetting), which is stronger under higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices.
    state : array-like of int in {0,1}
        Second-stage state per trial.
    action_2 : array-like of int in {0,1}
        Second-stage actions.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float in [0,1]
        Anxiety score; only stai[0] is used.
    model_parameters : list/tuple of floats
        [alpha, beta, zeta0, phi0, forget0]
        - alpha in [0,1]: learning rate for Q-values at both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - zeta0 in [0,1]: baseline exploration bonus scale (reduced by anxiety).
        - phi0 in [0,1]: baseline model-based weight at stage 1 (reduced by anxiety).
        - forget0 in [0,1]: baseline per-trial forgetting rate (increased by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, zeta0, phi0, forget0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety modulations
    zeta = max(0.0, min(1.0, zeta0 * (1.0 - stai)))            # less exploration with anxiety
    phi = max(0.0, min(1.0, phi0 * (1.0 - 0.5 * stai)))        # less MB weighting with anxiety
    forget = max(0.0, min(1.0, forget0 * (0.5 + 0.5 * stai)))  # more forgetting with anxiety

    # Initialize learned transitions with symmetric Dirichlet(1,1) priors
    trans_counts = np.ones((2, 2))  # rows: a1 in {0,1}, cols: state in {0,1}
    # Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))
    # Visit counts for UCB at stage 2 (avoid div by zero by starting at 1)
    visit_counts = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Apply forgetting toward zero
        q1_mf *= (1.0 - forget)
        q2_mf *= (1.0 - forget)

        # Compute learned transition probabilities
        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)  # shape (2,2)

        # Stage-1 MB value from learned transitions and current Q2
        max_q2 = np.max(q2_mf, axis=1)           # best per state
        q1_mb = trans_probs @ max_q2

        # Hybrid value
        q1_hybrid = phi * q1_mb + (1.0 - phi) * q1_mf

        # First-stage policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        p_choice_1[t] = probs1[a1]

        # Stage-2 UCB bonus based on inverse sqrt of visit counts
        bonus = zeta / np.sqrt(visit_counts[s] + 1e-8)
        logits2 = beta * (q2_mf[s] + bonus)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Update counts for UCB
        visit_counts[s, a2] += 1.0

        # Learning: second stage TD
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Eligibility-like backup to stage 1 MF
        backup = q2_mf[s, a2]
        delta1 = backup - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update learned transition counts with observed transition
        trans_counts[a1, s] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)