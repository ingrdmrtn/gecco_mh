def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with learned transition model and anxiety-modulated arbitration.
    
    This model learns both stage-2 action values and the stage-1 transition probabilities.
    First-stage decisions combine model-free and model-based values. The arbitration weight
    on the model-based controller increases or decreases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = alien within the current planet).
    reward : array-like of float in [0,1]
        Reward outcome on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; modulates the arbitration weight toward model-based control.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w0, eta_T, phi]
        - alpha (learning rate for Q-values; [0,1])
        - beta (inverse temperature; [0,10])
        - w0 (baseline model-based weight; [0,1])
        - eta_T (transition learning rate; [0,1])
        - phi (anxiety modulation strength on model-based weight; [0,1])
          effective weight w = sigmoid(logit(w0) + phi * (stai - 0.5))
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w0, eta_T, phi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model rows for actions 0 and 1
    # Start centered on the canonical common/rare structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize values
    q1_mf = np.zeros(2)          # model-free first-stage values
    q2 = np.zeros((2, 2))        # second-stage action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper: stable softmax
    eps = 1e-10

    # Anxiety-modulated arbitration weight
    # Map w0 from [0,1] through logit-sigmoid to keep interior and allow smooth modulation
    w0_clipped = min(max(w0, 1e-6), 1 - 1e-6)
    logit_w0 = np.log(w0_clipped) - np.log(1 - w0_clipped)
    w_eff = 1.0 / (1.0 + np.exp(-(logit_w0 + phi * (st - 0.5))))
    w_eff = min(max(w_eff, 0.0), 1.0)

    for t in range(n_trials):
        # Compute model-based first-stage values from current transition model and max second-stage values
        max_q2 = np.max(q2, axis=1)             # shape (2,)
        q1_mb = T @ max_q2                      # propagate through learned transitions

        # Hybrid first-stage preference
        q1_hybrid = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # First-stage choice probability via softmax
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy in the observed state
        s = int(state[t])
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Model-free first-stage TD update bootstrapping from updated second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Learn transition probabilities from observed (a1, s)
        # Move row T[a1] toward the one-hot vector for the observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1 - eta_T) * T[a1] + eta_T * target

        # Renormalize to ensure probabilities sum to 1 (avoid drift due to numerical precision)
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-based control with anxiety-driven pessimism and surprise-bias trace.
    
    The second-stage subjective values are risk-adjusted using a mean-variance penalty
    derived from the learned reward probabilities. Anxiety increases pessimism (greater
    weight on variance). First-stage choices are model-based using the canonical
    transition matrix and include an additional surprise-bias trace that favors the
    action that recently produced a rare transition (or disfavors if common).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the planet (0/1 = alien).
    reward : array-like of float in [0,1]
        Reward outcomes.
    stai : array-like with one float in [0,1]
        Anxiety score; increases risk aversion (pessimism) in valuation.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, psi0, zeta, kappa]
        - alpha (learning rate for reward probabilities; [0,1])
        - beta (inverse temperature; [0,10])
        - psi0 (baseline risk penalty weight; [0,1])
        - zeta (anxiety modulation of risk penalty; [0,1])
          effective psi = clip(psi0 + zeta * (stai - 0.5), 0, 1)
        - kappa (surprise-bias learning/decay rate; [0,1])
          bias trace B decays and is driven by rare vs common transitions
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, psi0, zeta, kappa = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed canonical transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Learned reward probabilities at stage 2 (MF for outcomes)
    q2 = np.full((2, 2), 0.5, dtype=float)  # start uncertain at 0.5

    # Surprise bias trace over first-stage actions
    B = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Anxiety-modulated risk penalty
    psi = psi0 + zeta * (st - 0.5)
    psi = min(max(psi, 0.0), 1.0)

    # Helper to compute subjective second-stage values with mean-variance penalty
    # For Bernoulli reward with mean p, variance p*(1-p)
    def subjective_values(q2_mat):
        p = q2_mat
        var = p * (1.0 - p)
        return p - psi * var

    # Keep track of previous transition to update surprise bias
    prev_a1 = None
    prev_s = None

    for t in range(n_trials):
        # Update surprise bias from the previous trial's transition
        if prev_a1 is not None and prev_s is not None:
            # Rarity under canonical transitions
            prob_s = T[prev_a1, prev_s]
            was_rare = 1.0 if prob_s < 0.5 else -1.0  # rare -> +1, common -> -1
            # Decay and drive bias on the action taken
            B = (1.0 - kappa) * B
            B[prev_a1] += kappa * was_rare

        # Compute subjective second-stage values
        sub_q2 = subjective_values(q2)
        # Model-based first-stage values using subjective second-stage values
        max_sub_q2 = np.max(sub_q2, axis=1)
        q1_mb = T @ max_sub_q2

        # Add surprise-bias trace to first-stage preferences
        pref1 = q1_mb + B

        # First-stage softmax
        pref1c = pref1 - np.max(pref1)
        probs1 = np.exp(beta * pref1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage softmax in realized state using subjective values
        s = int(state[t])
        v2 = sub_q2[s].copy()
        v2c = v2 - np.max(v2)
        probs2 = np.exp(beta * v2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and learning of Bernoulli mean at second stage
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Store for next trial's surprise update
        prev_a1 = a1
        prev_s = s

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-driven exploration (UCB-like) with anxiety-damped exploration and hybrid control.
    
    The agent learns second-stage values and an uncertainty trace per action.
    Choices include an exploration bonus proportional to uncertainty. Anxiety
    suppresses the exploration bonus. First-stage policies combine model-based
    (propagated value+uncertainty) and model-free values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Reward outcomes.
    stai : array-like with one float in [0,1]
        Anxiety score; reduces the exploration bonus.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, w, nu0, xi]
        - alpha (learning rate for values and uncertainty; [0,1])
        - beta (inverse temperature; [0,10])
        - w (weight on model-based vs model-free at stage 1; [0,1])
        - nu0 (baseline exploration-bonus weight; [0,1])
        - xi (anxiety suppression of exploration; [0,1])
          effective bonus weight nu = nu0 * (1 - xi * stai)
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, w, nu0, xi = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)            # model-free stage-1 values
    q2 = np.zeros((2, 2))          # stage-2 values

    # Uncertainty traces for second-stage actions (initialized high)
    u2 = np.ones((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Anxiety-damped exploration bonus weight
    nu = nu0 * (1.0 - xi * st)
    nu = min(max(nu, 0.0), 1.0)

    for t in range(n_trials):
        # Compute exploration-augmented second-stage preferences
        aug_q2 = q2 + nu * u2
        # Model-based first-stage values by propagating best augmented values per state
        max_aug_q2 = np.max(aug_q2, axis=1)
        q1_mb = T @ max_aug_q2

        # Hybrid first-stage value
        q1_hybrid = (1.0 - w) * q1_mf + w * q1_mb

        # First-stage softmax
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage softmax in current state using augmented preferences
        s = int(state[t])
        pref2 = aug_q2[s].copy()
        pref2c = pref2 - np.max(pref2)
        probs2 = np.exp(beta * pref2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage value update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty: decay toward recent unpredictability via absolute PE
        u2[s, a2] = (1.0 - alpha) * u2[s, a2] + alpha * abs(pe2)

        # Model-free first-stage update bootstrapping from value at second stage (without bonus)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik