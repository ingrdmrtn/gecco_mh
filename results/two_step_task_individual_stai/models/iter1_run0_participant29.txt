def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated model-based control and eligibility trace.
    The agent uses both model-based (MB) and model-free (MF) values at the first stage.
    Second-stage values are updated with TD learning; a one-step eligibility trace
    propagates second-stage values to the chosen first-stage action.
    Anxiety reduces the reliance on model-based control.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score (higher means more anxious).
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for MF updates at both stages
        beta: [0,10] — inverse temperature used at both stages
        w_mb0: [0,1] — baseline weight on model-based control at stage 1
        elig: [0,1] — eligibility trace weight for backing up stage-2 value to stage-1
        anx_mb_shift: [0,1] — how strongly anxiety reduces model-based weight

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, w_mb0, elig, anx_mb_shift = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # from A to [X, Y]
                  [0.3, 0.7]]) # from U to [X, Y]

    # Values
    q1_mf = np.zeros(2)       # model-free first-stage action values
    q2 = np.zeros((2, 2))     # second-stage state-action values: state in {X=0, Y=1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (higher anxiety => lower MB control)
    # Effective MB weight clipped to [0,1]
    w_mb = np.clip(w_mb0 - anx_mb_shift * stai_val, 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):

        # Compute model-based first-stage action values by planning through T and max over q2
        max_q2 = np.max(q2, axis=1)           # shape (2,)
        q1_mb = T @ max_q2                    # shape (2,)

        # Mix MF and MB for first-stage decision
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Back up to stage 1:
        # - MF TD toward realized stage-2 action value (eligibility trace controls strength)
        # - We update only the chosen first-stage action
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * elig * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with anxiety-inflated associability and exploration temperature.
    The agent plans at stage 1 via the known transition model and max second-stage values.
    Second-stage learning uses a Pearce-Hall style dynamic learning rate (associability)
    proportional to recent unsigned prediction errors. Anxiety increases associability
    (faster adaptation to changing rewards) and increases exploration (lower effective beta).

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha0: [0,1] — baseline learning rate
        beta0: [0,10] — baseline inverse temperature
        eta: [0,1] — associability gain (scales effect of unsigned PE on learning rate)
        tau0: [0,1] — exploration noise scale (reduces beta)
        anx_temp: [0,1] — anxiety coupling to both associability and exploration

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha0, beta0, eta, tau0, anx_temp = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and associability traces (per state-action)
    q2 = np.zeros((2, 2))
    assoc = np.zeros((2, 2))  # running estimate of unsigned PEs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated temperature: higher anxiety => more exploration (lower beta)
    temp_scale = 1.0 + tau0 * (1.0 + anx_temp * stai_val)
    beta_eff = beta0 / max(temp_scale, 1e-6)

    eps = 1e-12
    for t in range(n_trials):
        # Stage-1 planning via MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Dynamic learning rate with anxiety-inflated associability
        pe2 = r - q2[s, a2]
        assoc[s, a2] = 0.5 * assoc[s, a2] + 0.5 * abs(pe2)  # smooth recent unsigned PE
        lr_dyn = alpha0 + eta * assoc[s, a2]
        # Anxiety increases learning responsiveness
        lr_eff = np.clip(lr_dyn * (1.0 + anx_temp * stai_val), 0.0, 1.0)

        q2[s, a2] += lr_eff * pe2
        # Note: stage-1 has no MF component; choice remains fully MB via q1_mb

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB with anxiety-sensitive stay/switch bias.
    The agent learns the transition probabilities online and plans model-based at stage 1.
    In addition, a choice bias encourages repeating the previous first-stage action;
    this stay bias is attenuated or reversed following rare transitions in proportion
    to anxiety, capturing an anxiety-sensitive response to surprising transitions.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for second-stage action values
        beta: [0,10] — inverse temperature for both stages
        trans_lr: [0,1] — learning rate for transition probabilities
        stay_bias: [0,1] — baseline logit bonus to repeat previous first-stage choice
        anx_rare: [0,1] — strength of anxiety-driven reversal of stay bias after rare transitions

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, trans_lr, stay_bias, anx_rare = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix T_hat: rows are actions (A=0,U=1), cols are states (X=0,Y=1)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_was_rare = False  # rare relative to current T_hat at the time

    eps = 1e-12
    for t in range(n_trials):
        # Model-based first-stage values using learned transitions
        max_q2 = np.max(q2, axis=1)     # best action in each state
        q1_mb = T_hat @ max_q2

        # Construct first-stage logits with stay/switch bias
        logits1 = q1_mb.copy()

        # Apply stay bias on the previously chosen action.
        # After a rare transition, anxiety flips or attenuates the stay bias.
        if last_a1 is not None:
            # Effective bias multiplier: if last was rare, reduce or flip sign depending on anx*anx_rare
            rare_mod = 1.0 - 2.0 * (1.0 * last_was_rare) * (anx_rare * stai_val)
            # rare_mod in [1 - 2*anx_rare, 1]; at max anx_rare=1, stai=1 => rare_mod=-1 (full flip)
            bias_to_add = stay_bias * rare_mod
            logits1[last_a1] += bias_to_add

        # Softmax for stage 1
        logits1_s = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1_s)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update learned transition probabilities for the chosen first-stage action
        # One-hot observation for the realized state
        obs = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        # Delta rule toward the observed state
        T_hat[a1] = T_hat[a1] + trans_lr * (obs - T_hat[a1])
        # Renormalize to avoid numerical drift
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

        # Determine rarity of the observed transition under the pre-update T_hat (approximate):
        # We use the current T_hat after update as a proxy; for small trans_lr this is close.
        prob_to_s = T_hat[a1, s]
        last_was_rare = (prob_to_s < 0.5)
        last_a1 = a1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)