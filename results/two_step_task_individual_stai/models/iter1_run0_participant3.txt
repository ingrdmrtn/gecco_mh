def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-gated arbitration with eligibility trace and anxiety-modulated gating.
    
    Idea:
    - Stage-2 values are learned with a constant learning rate.
    - Stage-1 mixes model-free and model-based values. The arbitration weight shifts toward
      model-free when estimated volatility (unsigned RPE) is high; anxiety amplifies this shift.
    - An eligibility trace propagates stage-2 prediction errors back to stage-1 MF values.
    - Perseveration bias affects both stages.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: int array {0,1}, first-stage choices
    - state:    int array {0,1}, reached second-stage state
    - action_2: int array {0,1}, second-stage choices
    - reward:   float array [0,1], reward outcomes
    - stai:     float array [0,1] with single element, anxiety score
    - model_parameters: list/tuple of five values:
        eta    (0..1): stage-2 learning rate
        beta   (0..10): inverse temperature
        rho    (0..1): eligibility trace from stage-2 to stage-1 MF
        kappa  (0..1): perseveration strength
        chi    (0..1): anxiety sensitivity of arbitration to volatility
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    eta, beta, rho, kappa, chi = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed (true) transition model used for planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)          # model-free stage-1 action values
    q2 = np.zeros((2, 2))        # stage-2 state-action values

    # Volatility proxy (EWMA of unsigned stage-2 RPE)
    vol = 0.0
    vol_decay = 0.8  # fixed decay; arbitration sensitivity is governed by chi and anxiety

    # Perseveration memory
    prev_a1 = -1
    prev_a2 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Model-based evaluation from stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight toward model-based control.
        # When volatility is high, weight decreases; anxiety amplifies this effect via chi.
        # Base tendency is 0.7 toward MB; shrink with vol and anxiety.
        w = 0.7 - (vol * (0.5 + 0.5 * s_anx) * chi)
        w = np.clip(w, 0.0, 1.0)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Perseveration features
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # First-stage policy
        logits1 = beta * q1 + kappa * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Stage-1 MF update via direct TD and eligibility trace from stage-2 RPE
        # Direct TD to move q1_mf toward the value backing the chosen path
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * delta1
        # Eligibility: propagate a fraction of stage-2 RPE
        q1_mf[a1] += rho * eta * delta2

        # Update volatility proxy (unsigned RPE)
        vol = vol_decay * vol + (1.0 - vol_decay) * abs(delta2)

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility, transition-dependent choice bias, and lapse; anxiety modulates risk and bias.
    
    Idea:
    - Rewards are transformed by a risk-sensitive utility u(r) = r^gamma_eff.
      Higher anxiety pushes gamma_eff below 1 (more risk-averse for gains).
    - Model-free and model-based are mixed via a fixed weight determined by omega0, with
      anxiety reducing planning weight.
    - Transition-dependent bias: after rewarded common transitions, a first-stage
      stay-bias increases; after rewarded rare transitions, it decreases. Anxiety enhances
      sensitivity to rare events.
    - A small lapse mixes the softmax policy with uniform choice; lapse increases with anxiety.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: int array {0,1}
    - state:    int array {0,1}
    - action_2: int array {0,1}
    - reward:   float array [0,1]
    - stai:     float array [0,1] with one value, anxiety score
    - model_parameters: list/tuple of five values:
        alpha   (0..1): stage-2 learning rate
        beta    (0..10): inverse temperature
        gamma0  (0..1): baseline risk exponent for utility transform
        omega0  (0..1): baseline planning weight (MB vs MF)
        eps0    (0..1): baseline lapse rate
    Returns:
    - Negative log-likelihood over both stages.
    """
    alpha, beta, gamma0, omega0, eps0 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure for planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value stores
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Bias state
    prev_a1 = -1
    prev_trans_common = 0
    prev_reward = 0.0

    # Anxiety effects:
    # - Risk exponent shifts down with anxiety (more curvature for gains)
    gamma_eff = np.clip(gamma0 * (1.0 - 0.5 * s_anx), 0.0, 1.0)
    # - Planning weight reduced by anxiety
    w_plan = np.clip(omega0 * (1.0 - 0.7 * s_anx), 0.0, 1.0)
    # - Lapse increases with anxiety
    lapse = np.clip(eps0 * (0.5 + 0.5 * s_anx), 0.0, 1.0)
    eps = 1e-12

    for t in range(n_trials):
        # Current MB estimate
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Transition-dependent stay bias term for first-stage
        # If last trial was rewarded and common, bias toward staying; if rewarded and rare,
        # bias against staying. Anxiety increases the magnitude when rare.
        bias_vec = np.zeros(2)
        if prev_a1 in (0, 1):
            # Compute signed bias magnitude
            sign = 1.0 if prev_trans_common == 1 else -1.0
            # Rare events amplified by anxiety
            amp = 1.0 + s_anx if prev_trans_common == 0 else 1.0 - 0.5 * s_anx
            bias_mag = prev_reward * sign * amp
            bias_vec[prev_a1] = bias_mag  # apply to the previously chosen action

        # Combine MB and MF
        q1 = w_plan * q1_mb + (1.0 - w_plan) * q1_mf

        # First-stage softmax with bias
        logits1 = beta * q1 + bias_vec
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        # Lapse mixture with uniform
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = state[t]
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning with risk-sensitive utility
        r_raw = reward[t]
        r_util = r_raw ** (gamma_eff + eps)  # utility-transformed reward
        delta2 = r_util - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        # Stage-1 MF bootstrapping toward current stage-2 value
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update bias state for next trial
        # Determine whether the last transition was common given choice and reached state
        # Common if (A->X) or (U->Y)
        is_common = 1 if ((a1 == 0 and s_idx == 0) or (a1 == 1 and s_idx == 1)) else 0
        prev_trans_common = is_common
        prev_reward = r_raw
        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation planning with learned transitions; anxiety reduces temporal horizon and transition learning.
    
    Idea:
    - Learn an explicit estimate of action-dependent transition probabilities (p_hat).
    - Use a Successor Representation (SR) at stage 1: Q1_SR[a] = sum_s M[a,s] * V2[s],
      where M[a,s] is the (one-step) successor feature based on learned transitions and discount gamma.
    - Anxiety reduces the effective discount (shorter horizon) and slows transition learning,
      reflecting reduced confidence in learning the structure.
    - Include perseveration at both stages.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - action_1: int array {0,1}
    - state:    int array {0,1}
    - action_2: int array {0,1}
    - reward:   float array [0,1]
    - stai:     float array [0,1] with one value, anxiety score
    - model_parameters: list/tuple of five values:
        alpha_r (0..1): stage-2 reward learning rate
        beta    (0..10): inverse temperature
        alpha_T (0..1): transition learning rate (baseline)
        gamma0  (0..1): baseline SR discount factor
        psi     (0..1): anxiety sensitivity scaling for gamma and transition learning
    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_r, beta, alpha_T, gamma0, psi = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Anxiety effects:
    # - Reduce effective discount (more myopic)
    gamma_eff = np.clip(gamma0 * (1.0 - psi * s_anx), 0.0, 1.0)
    # - Reduce transition learning under anxiety
    alpha_T_eff = np.clip(alpha_T * (1.0 - 0.7 * psi * s_anx), 0.0, 1.0)

    # Initialize learned transition probabilities p_hat[action, state]
    # Start near symmetric but slightly biased toward common structure to aid identifiability
    p_hat = np.array([[0.6, 0.4],
                      [0.4, 0.6]], dtype=float)

    # Value stores
    q2 = np.zeros((2, 2))  # stage-2 state-action values

    # Likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration
    kappa = 0.2 * (0.5 + s_anx)  # small fixed perseveration scaled by anxiety (uses stai)
    prev_a1 = -1
    prev_a2 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Compute V2 from current q2
        V2 = np.max(q2, axis=1)  # shape (2,)

        # SR-like first-stage Q via learned transitions and discount
        # One-step SR for this task: M[a, s] = (1 - gamma_eff) * 0 + gamma_eff * p_hat[a, s]
        # Since there is no immediate reward at stage 1, only future value matters.
        q1_sr = gamma_eff * (p_hat @ V2)

        # Add perseveration
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * q1_sr + kappa * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update stage-2 values
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha_r * delta2

        # Learn transitions p_hat using simple delta rule toward observed state
        # For chosen action a1, move probability mass toward the observed state s_idx.
        for s2 in (0, 1):
            target = 1.0 if s2 == s_idx else 0.0
            p_hat[a1, s2] += alpha_T_eff * (target - p_hat[a1, s2])
        # Normalize to avoid drift
        row_sum = p_hat[a1, 0] + p_hat[a1, 1]
        if row_sum > 0:
            p_hat[a1, :] /= row_sum

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll