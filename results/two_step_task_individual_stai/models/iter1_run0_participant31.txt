def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free RL with learned transitions and anxiety-suppressed planning.

    The agent learns second-stage values and also learns the transition structure online.
    First-stage choice values are a convex combination of model-based (using the learned
    transition matrix) and model-free values. Anxiety (stai) reduces the weight placed on
    model-based planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1; e.g., W/S or P/H).
    reward : array-like of float
        Obtained rewards on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety down-weights planning.
    model_parameters : array-like of floats, length 5
        [alpha_mf, alpha_tr, beta, omega_base, k_anx_w]
        Bounds:
        - alpha_mf in [0,1]: learning rate for model-free Q updates.
        - alpha_tr in [0,1]: transition learning rate (for the transition matrix rows).
        - beta in [0,10]: inverse temperature for both stages.
        - omega_base in [0,1]: baseline weight on model-based versus model-free values.
        - k_anx_w in [0,1]: anxiety modulation; effective omega = omega_base * (1 - k_anx_w*stai).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_mf, alpha_tr, beta, omega_base, k_anx_w = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective model-based weight reduced by anxiety
    omega_eff = omega_base * (1.0 - k_anx_w * stai)
    if omega_eff < 0.0:
        omega_eff = 0.0
    if omega_eff > 1.0:
        omega_eff = 1.0

    # Initialize learned transition matrix as uniform and learn it
    T = np.full((2, 2), 0.5)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage action-values computed from learned transitions
        max_q2 = np.max(q2_mf, axis=1)          # best value available on each planet
        q1_mb = T @ max_q2                      # expected value of each spaceship

        # Hybrid first-stage values
        q1_hybrid = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # First-stage policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Update model-free values
        # Stage-1 MF towards the value of the chosen second-stage action
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_mf * delta1

        # Stage-2 MF towards reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_mf * delta2

        # Learn transitions for the chosen spaceship: move its row toward the observed state
        # One-hot target: 1 for observed state, 0 for the other, then renormalize (though update preserves sum)
        for sp in (0, 1):
            if sp == a1:
                # decay toward zero, then add mass to observed state
                T[sp, :] = (1.0 - alpha_tr) * T[sp, :]
                T[sp, s] += alpha_tr
                # numerical cleanup
                row_sum = T[sp, 0] + T[sp, 1]
                if row_sum <= 0:
                    T[sp, :] = 0.5
                else:
                    T[sp, :] /= row_sum
            else:
                # leave unchosen spaceship transition unchanged this trial
                pass

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free RL with anxiety-driven pessimism and a model-based credit assignment (MBCA) stay/switch bias.

    Core values are learned model-free. Anxiety increases pessimism about uncertain second-stage options
    and increases exploration. First-stage decisions are additionally biased by a single-trial
    model-based credit assignment heuristic: after a rewarded common or unrewarded rare transition,
    the model favors staying with the same first-stage action; after a rewarded rare or unrewarded common
    transition, it favors switching. The strength of this heuristic is controlled by mbca.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0].
    model_parameters : array-like of floats, length 5
        [alpha, beta, psi0, k_anx_beta, mbca]
        Bounds:
        - alpha in [0,1]: learning rate for model-free Q updates at both stages.
        - beta in [0,10]: base inverse temperature for both stages.
        - psi0 in [0,1]: baseline pessimism weight penalizing uncertain second-stage options.
        - k_anx_beta in [0,1]: anxiety modulation; higher stai reduces beta and increases pessimism.
                               beta_eff = beta * (1 - 0.8*k_anx_beta*stai), psi_eff = psi0 + k_anx_beta*stai.
        - mbca in [0,1]: strength of model-based credit assignment stay/switch bias on first-stage choices.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, psi0, k_anx_beta, mbca = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety effects: more exploration and more pessimism
    beta_eff = beta * (1.0 - 0.8 * k_anx_beta * stai)
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0
    psi_eff = psi0 + k_anx_beta * stai
    if psi_eff < 0.0:
        psi_eff = 0.0
    if psi_eff > 1.0:
        psi_eff = 1.0

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # First-stage policy with MBCA stay/switch bias from previous trial
        bias = np.zeros(2)
        if prev_a1 is not None:
            prev_common = (prev_a1 == 0 and prev_s == 0) or (prev_a1 == 1 and prev_s == 1)
            # Stay preference if (rewarded & common) or (unrewarded & rare); else switch
            stay_pref = 1 if ((prev_common and prev_r > 0.0) or ((not prev_common) and prev_r <= 0.0)) else -1
            if stay_pref == 1:
                bias[prev_a1] += mbca
            else:
                bias[1 - prev_a1] += mbca

        logits1 = beta_eff * q1 + bias
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy with pessimism penalty for uncertainty
        s = state[t]
        q2_row = q2[s].copy()
        # Uncertainty penalty: larger when values are near 0.5
        uncert = 0.5 - np.abs(q2_row - 0.5)
        q2_eff = q2_row - psi_eff * uncert
        logits2 = beta_eff * q2_eff
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Model-free updates
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_s = s
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Dual-temperature MF with anxiety-amplified choice kernels and value forgetting.

    This purely model-free account uses separate softmax temperatures for stage 1 and 2,
    adds choice kernels (perseveration) at both stages whose influence scales with anxiety,
    and applies trial-to-trial forgetting of values toward 0.5. This captures increased
    habitual stickiness and reduced reliance on precise values often associated with higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage states encountered.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety strengthens the choice-kernel influence.
    model_parameters : array-like of floats, length 5
        [alpha, beta1, beta2, k_anx_ck, f_forget]
        Bounds:
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - k_anx_ck in [0,1]: scales choice-kernel weights with anxiety; kernel weight = k_anx_ck * stai.
        - f_forget in [0,1]: forgetting rate pulling Q-values toward 0.5 each trial
                             and decaying choice kernels (higher => more forgetting/decay).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta1, beta2, k_anx_ck, f_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Value tables
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (perseveration traces)
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    # Anxiety-scaled kernel weight
    w_ck = k_anx_ck * stai
    if w_ck < 0.0:
        w_ck = 0.0
    if w_ck > 1.0:
        w_ck = 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Forgetting pulls Q toward 0.5 and decays kernels
        q1 = (1.0 - f_forget) * q1 + f_forget * 0.5
        q2 = (1.0 - f_forget) * q2 + f_forget * 0.5
        k1 *= (1.0 - f_forget)
        k2 *= (1.0 - f_forget)

        # Stage 1 policy with kernel
        logits1 = beta1 * q1 + w_ck * k1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage 2 policy with kernel
        s = state[t]
        logits2 = beta2 * q2[s] + w_ck * k2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # TD updates
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update kernels after choices
        k1[a1] += 1.0
        k2[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll