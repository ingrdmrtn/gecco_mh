def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Successor-biased hybrid with anxiety-gated temporal horizon and attention-modulated learning.
    
    Idea:
    - Second stage is learned model-free (Q-learning).
    - First-stage action values combine model-free and a successor-like, model-based expectation.
    - The successor/horizon weight is reduced as anxiety increases (shorter planning horizon).
    - Learning rates are modulated on each update by a Pearce–Hall-like attention term driven by unsigned prediction errors,
      with stronger attention when anxiety is higher.
    - Choice stickiness (perseveration) also increases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the reached planet for each trial.
    reward : array-like of float
        Obtained reward on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], where higher is more anxious.
    model_parameters : list or array-like of float
        [alpha, beta, gamma_sr, psi, chi]
        Bounds:
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: softmax inverse temperature (both stages share beta).
        - gamma_sr in [0,1]: baseline successor/temporal-horizon weight for model-based contribution at stage 1.
        - psi in [0,1]: choice stickiness strength (converted to additive bias; scaled by anxiety).
        - chi in [0,1]: attention gain for PE-driven learning-rate modulation (scaled by anxiety).
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, gamma_sr, psi, chi = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: rows are first-stage actions (A,U), cols are states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)        # model-free values for first-stage actions
    q2 = np.zeros((2, 2))      # second-stage Q-values per state x action

    # Attention traces for adaptive learning-rate (initialized to 1)
    att2 = np.ones((2, 2))
    att1 = np.ones(2)

    # Previous choices for stickiness
    prev_a1 = None
    prev_a2 = {0: None, 1: None}

    # Anxiety effects
    # Planning horizon weight declines with anxiety
    gamma_eff = np.clip(gamma_sr * (1.0 - stai), 0.0, 1.0)
    # Stickiness increases with anxiety
    stick_scale = psi * stai
    # Attention amplification with anxiety
    att_gain = 1.0 + chi * stai

    for t in range(n_trials):
        # Model-based lookahead: expected max Q at second stage
        max_q2 = np.max(q2, axis=1)                 # best action per planet
        q1_mb = transition_matrix @ max_q2          # expected value of each first-stage action

        # Combine MF and successor-like MB via anxiety-gated horizon
        q1_combined = (1.0 - gamma_eff) * q1_mf + gamma_eff * q1_mb

        # Add first-stage stickiness
        if prev_a1 is not None:
            stick_vec1 = np.array([1.0 if i == prev_a1 else 0.0 for i in range(2)])
        else:
            stick_vec1 = np.zeros(2)
        q1_policy = q1_combined + stick_scale * stick_vec1

        # First-stage choice probabilities
        q1c = q1_policy - np.max(q1_policy)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with stickiness within state
        s = state[t]
        if prev_a2[s] is not None:
            stick_vec2 = np.array([1.0 if i == prev_a2[s] else 0.0 for i in range(2)])
        else:
            stick_vec2 = np.zeros(2)
        q2_policy = q2[s] + stick_scale * stick_vec2

        q2c = q2_policy - np.max(q2_policy)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Second-stage learning with attention-modulated learning rate
        pe2 = r - q2[s, a2]
        lr2 = np.clip(alpha * att2[s, a2] * att_gain, 0.0, 1.0)
        q2[s, a2] += lr2 * pe2
        # Update attention by unsigned PE (Pearce–Hall style)
        att2[s, a2] = np.clip(0.5 * att2[s, a2] + 0.5 * abs(pe2), 0.0, 1.0)

        # First-stage MF learning bootstrapping on updated second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        lr1 = np.clip(alpha * att1[a1] * att_gain, 0.0, 1.0)
        q1_mf[a1] += lr1 * pe1
        att1[a1] = np.clip(0.5 * att1[a1] + 0.5 * abs(pe1), 0.0, 1.0)

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Transition-learning planner with stage-specific temperatures and anxiety-biased uncertainty.
    
    Idea:
    - Learns the transition matrix from experience (simple delta rule).
    - First-stage uses model-based planning through the learned transition matrix.
    - Anxiety increases a bias toward uncertain/flattened transitions by interpolating the learned
      transitions with a uniform distribution, reducing planning precision.
    - Stage-specific softmax temperatures (beta1, beta2) allow different exploration at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the reached planet for each trial.
    reward : array-like of float
        Obtained reward on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], where higher is more anxious.
    model_parameters : list or array-like of float
        [alpha, beta1, beta2, gamma, zeta]
        Bounds:
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta1 in [0,10]: softmax inverse temperature for first-stage choices.
        - beta2 in [0,10]: softmax inverse temperature for second-stage choices.
        - gamma in [0,1]: learning rate for transition probabilities.
        - zeta in [0,1]: strength of anxiety-driven uncertainty bias on transitions.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta1, beta2, gamma, zeta = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix near the canonical structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Uniform transition matrix for uncertainty bias
    Tunif = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    # Anxiety-driven interpolation weight toward uncertainty
    w_unc = np.clip(zeta * stai, 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))  # second-stage Q-values per state x action

    for t in range(n_trials):
        # Anxiety-biased effective transition matrix
        T_eff = (1.0 - w_unc) * T + w_unc * Tunif

        # First-stage model-based action values from current Q2 and T_eff
        max_q2 = np.max(q2, axis=1)          # best action per state
        q1 = T_eff @ max_q2                  # expected value of first-stage actions

        # First-stage choice
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta1 * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta2 * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward and learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Learn transition probabilities for the chosen action: move toward the observed state
        # For chosen a1, update the row to increase probability of observed s
        for dest in range(2):
            target = 1.0 if dest == s else 0.0
            T[a1, dest] += gamma * (target - T[a1, dest])
        # Ensure row normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive model-free learner with anxiety-amplified confirmation and eligibility trace.
    
    Idea:
    - Purely model-free learning across both stages with an eligibility trace that backs up second-stage PEs to first stage.
    - Rewards are transformed by a concave utility capturing risk aversion; anxiety increases effective risk aversion.
    - Confirmation bias: when the transition is common (i.e., expected), the backup to the first stage is amplified,
      and this amplification grows with anxiety (stronger confirmation under anxiety).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Second-stage states reached (0=Planet X, 1=Planet Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1) within the reached planet for each trial.
    reward : array-like of float
        Obtained reward on each trial (typically in [0,1]).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], where higher is more anxious.
    model_parameters : list or array-like of float
        [alpha, beta, lam_e, lambda_risk, kappa]
        Bounds:
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: softmax inverse temperature (both stages share beta).
        - lam_e in [0,1]: eligibility trace for backing up second-stage PE to first-stage value.
        - lambda_risk in [0,1]: baseline risk aversion (utility curvature).
        - kappa in [0,1]: strength of anxiety-amplified confirmation bias on the first-stage backup.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, lam_e, lambda_risk, kappa = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition matrix to determine whether a transition is common or rare
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)         # first-stage model-free values
    q2 = np.zeros((2, 2))    # second-stage model-free values

    # Effective risk aversion grows with anxiety
    risk_eff = np.clip(lambda_risk + (1.0 - lambda_risk) * 0.5 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Policies (pure MF at both stages)
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed reward (risk-averse concave utility; r in [0,1])
        r_raw = reward[t]
        # Avoid 0^0; clamp
        r_clamped = np.clip(r_raw, 0.0, 1.0)
        # u(r) = r^(1 - risk_eff) with risk_eff in [0,1] -> concave for risk_eff>0
        util = (r_clamped + 1e-12) ** (1.0 - risk_eff)

        # Second-stage learning
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Determine if the transition observed was common for the chosen first-stage action
        prob_common = T[a1, s]
        is_common = 1.0 if prob_common >= 0.5 else 0.0

        # Confirmation bias factor increases first-stage backup on common transitions
        conf = 1.0 + kappa * stai * is_common

        # First-stage learning: TD to Q2 plus eligibility trace of the second-stage PE
        bootstrap = q2[s, a2]
        pe1 = bootstrap - q1[a1]
        q1[a1] += alpha * (pe1 + lam_e * conf * pe2)

    eps = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik