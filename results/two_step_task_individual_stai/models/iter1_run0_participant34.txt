def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MBâ€“MF with anxiety-dampened transition learning and reliability-weighted arbitration.

    The agent learns:
      - Model-free action values at both stages (MF).
      - A per-action transition model to second-stage states (MB).
    Arbitration between MF and MB at the first stage is driven by the learned
    reliability of the transition model (how far the learned transition is from chance).
    Anxiety (stai) reduces the effective learning rate for transitions, slowing the
    acquisition of a reliable model and thus indirectly lowering MB control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha_mf, beta, tau, k_anx_trans, alpha2]
        - alpha_mf in [0,1]: MF learning rate for first-stage values.
        - beta in [0,10]: inverse temperature for both stages.
        - tau in [0,1]: base learning rate for the transition model p(state|action1).
        - k_anx_trans in [0,1]: anxiety impact on transition learning (tau_eff = tau*(1 - k_anx_trans*stai)).
        - alpha2 in [0,1]: MF learning rate for second-stage values.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_mf, beta, tau, k_anx_trans, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective transition learning rate reduced by anxiety
    tau_eff = tau * (1.0 - k_anx_trans * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q_stage1_mf = np.zeros(2)           # for actions A/U
    q_stage2 = np.zeros((2, 2))         # for states X/Y and their two aliens

    # Learned transition model: probability of going to state X given action a
    # Initialize neutral at 0.5 to allow learning
    p_to_X = np.array([0.5, 0.5], dtype=float)

    for t in range(n_trials):
        # Compute MB values using current transition model
        max_q2 = np.max(q_stage2, axis=1)  # [max at X, max at Y]
        # For action 0 (A): P(X)=p_to_X[0], P(Y)=1-p
        q1_mb_A = p_to_X[0] * max_q2[0] + (1.0 - p_to_X[0]) * max_q2[1]
        # For action 1 (U)
        q1_mb_U = p_to_X[1] * max_q2[0] + (1.0 - p_to_X[1]) * max_q2[1]
        q1_mb = np.array([q1_mb_A, q1_mb_U])

        # Reliability of transition model: distance from chance (0.5)
        r_A = 2.0 * abs(p_to_X[0] - 0.5)  # in [0,1]
        r_U = 2.0 * abs(p_to_X[1] - 0.5)
        omega = max(0.0, min(1.0, 0.5 * (r_A + r_U)))  # average reliability, clipped

        # Hybrid first-stage values
        q1_hyb = omega * q1_mb + (1.0 - omega) * q_stage1_mf

        # First-stage policy
        logits1 = q1_hyb - np.max(q1_hyb)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage MF
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Update first-stage MF toward bootstrapped value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_mf * delta1

        # Update transition model for the chosen action
        # Move p_to_X[a1] toward indicator(s==X)
        ind_X = 1.0 if s == 0 else 0.0
        p_to_X[a1] += tau_eff * (ind_X - p_to_X[a1])
        # keep within [0,1]
        p_to_X[a1] = max(0.0, min(1.0, p_to_X[a1]))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated uncertainty bonus (UCB-like) and decaying counts.

    The agent learns model-free values at both stages. Exploration at both stages
    is guided by an uncertainty bonus derived from decaying choice counts (pseudo-counts):
    actions chosen less often receive a larger bonus. Anxiety reverses or attenuates this
    bonus: low anxiety -> uncertainty seeking (positive bonus), high anxiety -> uncertainty
    aversion (negative bonus). Counts decay over trials to reflect nonstationarity.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens).
    reward : array-like of float
        Rewards.
    stai : array-like of float
        Anxiety in [0,1]; higher = more anxious. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha1, beta, k_anx_bonus, k_decay, alpha2]
        - alpha1 in [0,1]: MF learning rate for first-stage values.
        - beta in [0,10]: inverse temperature for both stages.
        - k_anx_bonus in [0,1]: scales how strongly anxiety modulates the uncertainty bonus.
        - k_decay in [0,1]: decay factor for pseudo-counts each trial (closer to 1 decays slower).
        - alpha2 in [0,1]: MF learning rate for second-stage values.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha1, beta, k_anx_bonus, k_decay, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Sign and magnitude of bonus from anxiety: in [-k_anx_bonus, +k_anx_bonus]
    # Low anxiety -> positive bonus; high anxiety -> negative
    bonus_weight = k_anx_bonus * (1.0 - 2.0 * stai_val)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1 = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Decaying pseudo-counts for uncertainty bonuses
    N1 = np.zeros(2)        # counts for first-stage actions
    N2 = np.zeros((2, 2))   # counts per state-action at second stage

    # Fixed transition structure for expected uncertainty at stage 1
    # A->X common, U->Y common
    P_common = 0.7
    trans = np.array([[P_common, 1 - P_common], [1 - P_common, P_common]])  # rows a1, cols states X,Y

    for t in range(n_trials):
        # Decay counts
        N1 *= k_decay
        N2 *= k_decay

        # Compute uncertainty bonuses
        u1 = 1.0 / np.sqrt(N1 + 1e-6)  # size-2
        u2 = 1.0 / np.sqrt(N2 + 1e-6)  # 2x2

        # Second-stage policy with bonus on current state actions
        s = state[t]
        logits2 = q_stage2[s].copy() + bonus_weight * u2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage policy with expected bonus based on transition structure
        # For each action, expected bonus approx equals expected max uncertainty at reached state
        max_u2 = np.max(u2, axis=1)  # per state
        exp_bonus_a0 = trans[0, 0] * max_u2[0] + trans[0, 1] * max_u2[1]
        exp_bonus_a1 = trans[1, 0] * max_u2[0] + trans[1, 1] * max_u2[1]
        bonus1 = np.array([exp_bonus_a0, exp_bonus_a1])
        logits1 = q_stage1.copy() + bonus_weight * bonus1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        r = reward[t]

        # Update MF values
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1[a1]
        q_stage1[a1] += alpha1 * delta1

        # Update counts after observing the choices
        N1[a1] += 1.0
        N2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like model-based control with anxiety-modulated structural reliance.

    The agent combines:
      - Model-free values at both stages (MF).
      - A learned action-to-state mapping (row-stochastic) M[a1, s], updated from experience.
      - A prior structural model M_prior reflecting common transitions (A->X, U->Y).
    Anxiety scales the reliance on the prior structure via a discount-like factor gamma:
    higher anxiety reduces gamma_eff, diminishing the influence of the prior and MB control.
    First-stage values are a convex combination of MF and MB values using gamma_eff.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens).
    reward : array-like of float
        Rewards.
    stai : array-like of float
        Anxiety in [0,1]; higher = more anxious. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, gamma_base, k_anx_gamma, alpha2]
        - alpha in [0,1]: learning rate for MF first-stage values and for M updates.
        - beta in [0,10]: inverse temperature for both stages.
        - gamma_base in [0,1]: base reliance on prior structure and MB control.
        - k_anx_gamma in [0,1]: anxiety reduces gamma (gamma_eff = gamma_base*(1 - k*stai)).
        - alpha2 in [0,1]: MF learning rate for second-stage values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, gamma_base, k_anx_gamma, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    gamma_eff = gamma_base * (1.0 - k_anx_gamma * stai_val)
    gamma_eff = max(0.0, min(1.0, gamma_eff))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q_stage1_mf = np.zeros(2)
    q_stage2 = np.zeros((2, 2))

    # Learned mapping from action to state (rows sum to ~1 via the update rule)
    M = np.full((2, 2), 0.5)  # start neutral
    # Prior structure encoding common transitions
    P_common = 0.7
    M_prior = np.array([[P_common, 1 - P_common], [1 - P_common, P_common]], dtype=float)

    for t in range(n_trials):
        # Blend learned mapping with prior according to gamma_eff
        M_eff = (1.0 - gamma_eff) * M + gamma_eff * M_prior

        # Compute MB first-stage values
        max_q2 = np.max(q_stage2, axis=1)  # per state
        q1_mb = M_eff @ max_q2  # size-2 (for actions A,U)

        # Combine with MF first-stage values using gamma_eff as MB weight
        q1_hyb = gamma_eff * q1_mb + (1.0 - gamma_eff) * q_stage1_mf

        # First-stage policy
        logits1 = q1_hyb - np.max(q1_hyb)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF)
        s = state[t]
        logits2 = q_stage2[s].copy()
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage MF
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha2 * delta2

        # Update first-stage MF toward bootstrapped value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update learned mapping M for the chosen action toward observed state
        # One-hot for observed state
        e = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        M[a1, :] += alpha * (e - M[a1, :])
        # Keep row within [0,1]
        M[a1, 0] = max(0.0, min(1.0, M[a1, 0]))
        M[a1, 1] = max(0.0, min(1.0, M[a1, 1]))
        # Optional light re-normalization (not strictly required due to symmetric update)
        row_sum = M[a1, 0] + M[a1, 1]
        if row_sum > 0:
            M[a1, :] = M[a1, :] / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)