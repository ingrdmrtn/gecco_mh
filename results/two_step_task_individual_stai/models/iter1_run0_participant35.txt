def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated hybrid with learned transitions and perseveration.

    This model learns both second-stage action values and the first-stage transition
    matrix online. First-stage decisions combine model-based action values computed
    from the learned transition model with model-free first-stage values. Anxiety
    reduces the reliance on model-based control and shifts learning rates asymmetrically
    across stages. A perseveration bias (choice stickiness) is included and increases
    with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) in the reached state
    - reward: array-like of floats in [0,1], reward outcome on each trial
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha1_base: base learning rate for first-stage MF values in [0,1]
        alpha2_base: base learning rate for second-stage MF values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        kappa: base perseveration weight in [0,1] (scaled by anxiety)
        tau: transition learning rate in [0,1] for learning the transition matrix

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.

    Anxiety usage
    - Arbitration weight: w_mb = 1 - stai (higher anxiety => less model-based).
    - Learning rates: alpha1 decreases with anxiety; alpha2 increases with anxiety.
      alpha1_eff = alpha1_base * (1 - 0.3*stai), alpha2_eff = alpha2_base * (1 + 0.5*stai), both clipped to [0,1].
    - Perseveration increases with anxiety: kappa_eff = kappa * stai.
    """
    alpha1_base, alpha2_base, beta, kappa, tau = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model; start near the instructed structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value tables
    q1_mf = np.zeros(2)          # MF values for stage-1 actions
    q2 = np.zeros((2, 2))        # MF values for stage-2 (state x action)

    # Likelihood recording
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = -1
    prev_a2 = np.array([-1, -1])  # per-state

    # Anxiety-dependent components
    w_mb = max(0.0, min(1.0, 1.0 - stai))
    alpha1_eff = min(1.0, max(0.0, alpha1_base * (1.0 - 0.3 * stai)))
    alpha2_eff = min(1.0, max(0.0, alpha2_base * (1.0 + 0.5 * stai)))
    kappa_eff = kappa * stai

    for t in range(n_trials):
        # Model-based action values from learned transition and current q2
        max_q2 = np.max(q2, axis=1)     # size 2 for states X,Y
        q1_mb = T @ max_q2              # size 2 for actions A,U

        # Arbitration between MB and MF
        q1_combined = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add perseveration bias to first-stage
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa_eff

        # First-stage policy
        centered_q1 = q1_combined + bias1 - np.max(q1_combined + bias1)
        probs1 = np.exp(beta * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with perseveration bias within the reached state
        s = int(state[t])
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2[s] in (0, 1):
            bias2[prev_a2[s]] += kappa_eff

        centered_q2 = q2_s + bias2 - np.max(q2_s + bias2)
        probs2 = np.exp(beta * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second-stage MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_eff * delta2

        # Learning: first-stage MF update towards current state's best value (bootstrap)
        target1 = q2[s, a2]  # on-policy SARSA-like using chosen a2 after its update
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_eff * delta1

        # Transition learning: move T[a1] toward observed one-hot transition to state s
        o = np.array([1.0 if si == s else 0.0 for si in range(2)])
        T[a1] = (1.0 - tau) * T[a1] + tau * o
        # Normalize to ensure valid probability vector
        T[a1] /= np.sum(T[a1])

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with anxiety-damped directed exploration (UCB) and reward stickiness.

    First-stage choices are purely model-based, planning through the known transition matrix,
    using second-stage action values augmented with an uncertainty bonus (UCB). Anxiety reduces
    the directed exploration bonus. Second-stage choices also include the bonus. A reward
    stickiness bias increases the tendency to repeat actions that were rewarded on the last
    visit to the same context (state for stage 2; previous trial for stage 1).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions
    - state: array-like of ints in {0,1}, reached second-stage state
    - action_2: array-like of ints in {0,1}, second-stage actions
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: learning rate for second-stage values in [0,1]
        beta1: inverse temperature for first-stage softmax in [0,10]
        beta2: inverse temperature for second-stage softmax in [0,10]
        xi_base: base weight for the UCB exploration bonus in [0,1]
        rho: reward stickiness strength in [0,1]

    Returns
    - Negative log-likelihood of observed choices.

    Anxiety usage
    - Directed exploration bonus weight: xi_eff = xi_base * (1 - stai), so higher anxiety
      reduces the uncertainty bonus at both stages.
    """
    alpha2, beta1, beta2, xi_base, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (commonly A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage value function and visit counts for UCB
    q2 = np.zeros((2, 2))
    n_visits = np.ones((2, 2))  # start at 1 to keep bonus finite

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Reward stickiness memory
    prev_a1 = -1
    prev_r1 = 0.0
    prev_a2 = np.array([-1, -1])
    prev_r2 = np.array([0.0, 0.0])

    # Anxiety-damped exploration bonus
    xi_eff = xi_base * (1.0 - stai)

    for t in range(n_trials):
        # Compute UCB bonus per state-action
        bonus = xi_eff / np.sqrt(n_visits)

        # Model-based first-stage Q via planning with bonus-augmented second-stage values
        max_mb = np.max(q2 + bonus, axis=1)  # best action per state including bonus
        q1_mb = T @ max_mb

        # Reward stickiness bias at stage 1: repeat previous first-stage action if last reward was high
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1) and prev_r1 > 0.0:
            bias1[prev_a1] += rho

        centered_q1 = q1_mb + bias1 - np.max(q1_mb + bias1)
        probs1 = np.exp(beta1 * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with UCB bonus and reward stickiness within the reached state
        s = int(state[t])
        q2_s = q2[s] + bonus[s]
        bias2 = np.zeros(2)
        if prev_a2[s] in (0, 1) and prev_r2[s] > 0.0:
            bias2[prev_a2[s]] += rho

        centered_q2 = q2_s + bias2 - np.max(q2_s + bias2)
        probs2 = np.exp(beta2 * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: second-stage TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update counts for UCB after observing this choice
        n_visits[s, a2] += 1.0

        # Update stickiness memories
        prev_r1 = r
        prev_a1 = a1
        prev_r2[s] = r
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning MF+MB hybrid with anxiety-shaped arbitration and noise, plus stage-1 perseveration.

    Second-stage values are learned with separate learning rates for positive vs. negative
    prediction errors. First-stage decisions blend model-based (using the known transition)
    and model-free first-stage values. The MB weight and choice stochasticity depend on
    anxiety: higher anxiety reduces MB arbitration weight and increases choice noise at stage 1.
    A stage-1 perseveration bias encourages repeating the previous first-stage action.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions
    - state: array-like of ints in {0,1}, reached second-stage state
    - action_2: array-like of ints in {0,1}, second-stage actions
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_pos: learning rate for positive TD errors in [0,1]
        alpha_neg: learning rate for negative TD errors in [0,1]
        beta1: inverse temperature base for first-stage in [0,10]
        beta2: inverse temperature for second-stage in [0,10]
        kappa: stage-1 perseveration weight in [0,1]

    Returns
    - Negative log-likelihood of observed choices.

    Anxiety usage
    - MB arbitration: w_mb = 1 - stai (higher anxiety => more MF).
    - Stage-1 noise: beta1_eff = beta1 * (1 - 0.3*stai), increasing noise with anxiety.
    - Perseveration is unaffected by anxiety here; stai already shapes arbitration and noise.
    """
    alpha_pos, alpha_neg, beta1, beta2, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure known
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)        # MF first-stage
    q2 = np.zeros((2, 2))      # second-stage

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1

    # Anxiety-shaped arbitration and noise
    w_mb = max(0.0, min(1.0, 1.0 - stai))
    beta1_eff = max(0.0, beta1 * (1.0 - 0.3 * stai))

    for t in range(n_trials):
        # Model-based component from max q2 per state
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB and MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += kappa

        centered_q1 = q1 + bias1 - np.max(q1 + bias1)
        probs1 = np.exp(beta1_eff * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2_s = q2[s]
        centered_q2 = q2_s - np.max(q2_s)
        probs2 = np.exp(beta2 * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage asymmetric learning
        pe2 = r - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * pe2

        # First-stage MF update toward the chosen second-stage value (on-policy)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        alpha1 = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += alpha1 * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll