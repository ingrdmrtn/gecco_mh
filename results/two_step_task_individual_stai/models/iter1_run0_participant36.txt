def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Asymmetric model-free SARSA(λ) with anxiety-modulated learning and perseveration.
    
    Overview
    --------
    This purely model-free controller learns second-stage action values (Q2) and propagates
    value to the first stage (Q1_mf) via an eligibility trace (lambda). Learning is asymmetric
    for positive vs. negative prediction errors. Anxiety (stai) increases the effective learning
    rate magnitude and strengthens a choice perseveration (stickiness) bias at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher stai increases learning gain and stickiness.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta, lam, phi]
        - alpha_pos in [0,1]: learning rate for positive prediction errors (PEs).
        - alpha_neg in [0,1]: learning rate for negative PEs.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace strength from Q2 to Q1_mf.
        - phi in [0,1]: baseline perseveration bias magnitude added to chosen action logits.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, lam, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Probabilities for each stage's observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    Q1_mf = np.zeros(2)          # first-stage MF values over actions A/U
    Q2 = np.zeros((2, 2))        # second-stage values, indexed by state X/Y and alien 0/1

    # Perseveration traces (previous actions), initialized as None
    prev_a1 = None
    prev_a2 = [None, None]  # one for each state

    # Anxiety modulation
    learn_gain = 0.5 + 0.5 * stai               # scales learning rates upward with anxiety
    phi_eff = phi * (0.5 + 0.5 * stai)          # stronger stickiness with higher anxiety

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage 1 policy: softmax over Q1_mf with perseveration bias
        logits1 = beta * Q1_mf.copy()
        if prev_a1 is not None:
            logits1[prev_a1] += phi_eff
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: softmax over Q2[s] with perseveration within state
        logits2 = beta * Q2[s].copy()
        if prev_a2[s] is not None:
            logits2[prev_a2[s]] += phi_eff
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 (asymmetric TD)
        pe2 = r - Q2[s, a2]
        alpha2 = (alpha_pos if pe2 >= 0.0 else alpha_neg) * learn_gain
        Q2[s, a2] += alpha2 * pe2

        # Eligibility-trace update for stage 1 (SARSA(λ)-like backup from chosen path)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        alpha1 = (alpha_pos if pe1 >= 0.0 else alpha_neg) * learn_gain
        Q1_mf[a1] += alpha1 * lam * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Transition-belief model-based planner with MF backup and decay; anxiety blunts transition confidence.
    
    Overview
    --------
    This hybrid controller forms a model-based (MB) first-stage value by planning with a
    subjective transition model whose common-transition probability is not fixed, but is
    biased toward uncertainty (0.5) as anxiety increases. It also maintains a model-free
    first-stage value (Q1_mf). Q2 updates with TD learning and undergoes forgetting/decay
    to capture nonstationarity. Arbitration between MB and MF is weighted by omega, which
    is down-weighted by anxiety (more anxious -> more MF).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher stai pushes subjective transition prob toward 0.5,
        and reduces model-based arbitration weight.
    model_parameters : list or array
        [alpha, beta, omega_base, p_common_base, rho]
        - alpha in [0,1]: TD learning rate for Q2 and MF Q1 backup.
        - beta in [0,10]: inverse temperature for choices at both stages.
        - omega_base in [0,1]: baseline MB arbitration weight (stai=0).
        - p_common_base in [0,1]: baseline belief that each ship commonly reaches its favored planet.
        - rho in [0,1]: forgetting rate applied to all Q2 entries each trial (larger -> faster decay).
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, omega_base, p_common_base, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    # Subjective common-transition probability moves toward 0.5 with anxiety
    p_common = (1.0 - stai) * p_common_base + stai * 0.5
    p_common = np.clip(p_common, 0.0, 1.0)
    # Arbitration weight reduced by anxiety
    omega = np.clip((1.0 - stai) * omega_base, 0.0, 1.0)

    # Construct subjective transition matrix based on p_common
    # Row: action (A=0, U=1), Col: state (X=0, Y=1)
    T = np.array([[p_common, 1.0 - p_common],
                  [1.0 - p_common, p_common]])

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Apply forgetting/decay to all Q2 before computing policy/updates
        Q2 = (1.0 - rho) * Q2

        # Model-based planning for stage 1
        max_Q2 = np.max(Q2, axis=1)   # best alien per planet
        Q1_mb = T @ max_Q2            # expected value per spaceship

        # Arbitration between MB and MF
        Q1 = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage 1 policy
        logits1 = beta * Q1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # TD update at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Model-free first-stage backup from realized second-stage value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Surprise-penalized model-based controller with anxiety-amplified avoidance and lapse.
    
    Overview
    --------
    A fully model-based first-stage planner computes expected values via the known common transitions.
    In addition, the model tracks a per-action 'surprise trace' that increases when a rare transition
    occurs after choosing that action. This trace penalizes the corresponding first-stage action
    (reflecting anxiety-driven avoidance of unexpected outcomes). Anxiety amplifies both the penalty
    weight and a small lapse probability at stage 1. Second-stage learning is standard TD.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher stai increases transition-surprise penalty
        and increases lapse mixing at stage 1.
    model_parameters : list or array
        [alpha, beta, eta_surprise, kappa, xi]
        - alpha in [0,1]: TD learning rate for second-stage values.
        - beta in [0,10]: inverse temperature (both stages).
        - eta_surprise in [0,1]: learning rate for updating surprise traces.
        - kappa in [0,1]: baseline weight of surprise penalty applied to first-stage logits.
        - xi in [0,1]: baseline lapse rate for stage 1 (mixed with uniform choice).
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, eta_surprise, kappa, xi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Known true transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Second-stage values
    Q2 = np.zeros((2, 2))

    # Surprise traces per first-stage action (A, U)
    z_surprise = np.zeros(2)

    # Anxiety-modulated penalty and lapse
    kappa_eff = kappa * (0.5 + 0.5 * stai)  # stronger penalty with more anxiety
    xi_eff = np.clip(xi * (1.0 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based first-stage values
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Apply surprise penalty to first-stage logits
        logits1 = beta * Q1_mb - kappa_eff * z_surprise
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        # Lapse mix at stage 1
        probs1 = (1.0 - xi_eff) * probs1 + xi_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * Q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # TD update at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update surprise trace for the chosen first-stage action based on whether transition was rare
        # Common if (A->X) or (U->Y); rare otherwise
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        surprisal = 0.0 if is_common else 1.0
        # Exponential moving average of surprisal per action
        z_surprise[a1] = (1.0 - eta_surprise) * z_surprise[a1] + eta_surprise * surprisal

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)