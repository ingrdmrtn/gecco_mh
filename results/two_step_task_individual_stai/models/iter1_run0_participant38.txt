def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MBâ€“MF with learned transitions and anxiety-dampened planning.
    
    This model learns both stage-2 values and the stage-1 transition matrix online.
    Stage-1 choices are driven by a hybrid of model-based (MB) values (computed using
    the learned transition matrix) and model-free (MF) values. Anxiety reduces reliance
    on MB planning and slows transition learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial at the visited planet.
    reward : array-like of float (e.g., 0 or 1)
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values reduce MB weight and transition learning rate.
    model_parameters : iterable of floats
        [alpha_r, alpha_t, beta, w0]
        - alpha_r: [0,1] learning rate for stage-2 rewards and stage-1 MF value.
        - alpha_t: [0,1] learning rate for updating the transition matrix.
        - beta: [0,10] inverse temperature for softmax choices (both stages).
        - w0: [0,1] baseline MB arbitration weight at stage 1 (dampened by anxiety).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_t, beta, w0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize probabilities and value functions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-1 MF values for A/U
    q1_mf = np.zeros(2)
    # Stage-2 values for each planet (X/Y) and alien (two options)
    q2 = np.zeros((2, 2))
    # Learned transition matrix T[a, s] = P(s | a); start uninformative
    T = np.full((2, 2), 0.5)

    # Anxiety-dampened MB weight and transition learning rate
    w_eff = np.clip(w0 * (1.0 - stai), 0.0, 1.0)
    alpha_t_eff = np.clip(alpha_t * (1.0 - 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based Q at stage 1 from learned transition and current stage-2 values
        max_q2 = np.max(q2, axis=1)      # value of best alien per planet
        q1_mb = T @ max_q2               # forward planning

        # Hybrid action values
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited planet
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # MF update at stage 1 toward the realized stage-2 chosen value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha_r * (target1 - q1_mf[a1])

        # Update learned transition matrix for chosen action using one-hot target for observed state
        # Move row T[a1] toward the observed state s
        T[a1, :] = (1.0 - alpha_t_eff) * T[a1, :]
        T[a1, s] += alpha_t_eff
        # Normalize for numerical stability (should already sum to 1)
        row_sum = T[a1, :].sum()
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning with anxiety-weighted negativity and value decay; hybrid control.
    
    Stage-2 learning uses outcome-valence asymmetric learning rates. Anxiety increases
    sensitivity to negative outcomes and reduces reliance on model-based planning.
    Both stage-2 and stage-1 MF values decay over time (forgetting). Stage-1 choice
    uses a hybrid of MB planning (using the known transition structure) and MF values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (two aliens per planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase negative-learning asymmetry and forgetting,
        and reduce MB arbitration weight.
    model_parameters : iterable of floats
        [alpha, beta, mb_w, asym, decay]
        - alpha: [0,1] base learning rate for TD updates.
        - beta: [0,10] inverse temperature for softmax choices (both stages).
        - mb_w: [0,1] baseline MB weight at stage 1 (reduced by anxiety).
        - asym: [0,1] strength of valence asymmetry in learning rates.
        - decay: [0,1] forgetting rate (toward neutral values) per trial.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, mb_w, asym, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (A->X and U->Y are common)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q1_mf = np.zeros(2)       # stage-1 MF Q for A/U (initialized neutral at 0)
    q2 = np.full((2, 2), 0.5) # stage-2 Q initialized neutral at 0.5

    # Anxiety-dependent MB weight and decay
    w = mb_w * (1.0 - stai)
    w = np.clip(w, 0.0, 1.0)
    # Decay strength increases with anxiety toward neutral values
    decay_eff_2 = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)
    decay_eff_1 = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # MB plan at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid value
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        exp_q2 = np.exp(beta * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Valence-asymmetric TD learning at stage 2 with anxiety-weighted negativity
        delta2 = r - q2[s, a2]
        # Positive and negative learning rates
        alpha_pos = np.clip(alpha * (1.0 + asym * (1.0 - stai)), 0.0, 1.0)
        alpha_neg = np.clip(alpha * (1.0 + asym * stai), 0.0, 1.0)
        lr = alpha_pos if delta2 >= 0 else alpha_neg
        q2[s, a2] += lr * delta2

        # Stage-1 MF learning toward realized stage-2 chosen value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])

        # Forgetting/decay after learning
        # Stage-2 values decay toward 0.5 (neutral)
        q2 = (1.0 - decay_eff_2) * q2 + decay_eff_2 * 0.5
        # Stage-1 MF values decay toward 0 (neutral baseline for advantage)
        q1_mf = (1.0 - decay_eff_1) * q1_mf

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-adaptive exploration with anxiety-modulated eligibility credit.
    
    Exploration (softmax temperature) adapts to recent reward volatility; anxiety amplifies
    this volatility-driven exploration. Stage-1 choices combine MB planning and MF values,
    with MB weight reduced by anxiety and by estimated volatility. An eligibility trace
    backs up stage-2 prediction errors to stage-1 MF values, with anxiety reducing credit
    assignment after rare transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial. 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state visited. 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Coins received on each trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase exploration under volatility and
        reduce eligibility credit after rare transitions, and reduce MB weight.
    model_parameters : iterable of floats
        [alpha, beta, vol_gain, mb_w, lambda_e]
        - alpha: [0,1] learning rate for TD updates (both stages).
        - beta: [0,10] base inverse temperature for softmax.
        - vol_gain: [0,1] strength of volatility control over temperature (scaled by anxiety).
        - mb_w: [0,1] baseline MB arbitration weight at stage 1.
        - lambda_e: [0,1] eligibility trace scaling of stage-2 PE onto stage-1 MF.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, vol_gain, mb_w, lambda_e = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Running reward volatility estimate (EWMA of absolute reward change)
    vol = 0.0
    prev_r = 0.0
    vol_alpha = 0.1  # fixed smoother

    for t in range(n_trials):
        # Compute MB plan
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Volatility-adaptive inverse temperature (more volatility -> more exploration)
        beta_eff = beta / (1.0 + vol_gain * stai * max(vol, 0.0))
        beta_eff = max(beta_eff, 1e-6)

        # Anxiety- and volatility-dependent MB weight
        w = mb_w * (1.0 - stai) * (1.0 - 0.5 * vol)
        w = np.clip(w, 0.0, 1.0)

        # Hybrid Q for stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited planet
        s = state[t]
        exp_q2 = np.exp(beta_eff * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Determine whether transition was common (A->X or U->Y)
        is_common = 1.0 if (a1 == s) else 0.0
        # Anxiety reduces credit after rare transitions
        elig_mod = 1.0 if is_common > 0.5 else (1.0 - stai)

        # Stage-1 MF update: direct bootstrap + eligibility from stage-2 PE
        target1 = q2[s, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1]) + alpha * lambda_e * elig_mod * delta2

        # Update volatility estimate from reward stream
        vol = (1.0 - vol_alpha) * vol + vol_alpha * abs(r - prev_r)
        prev_r = r

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_likelihood