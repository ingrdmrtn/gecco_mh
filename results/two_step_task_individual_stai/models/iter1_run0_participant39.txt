def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Learned-transition model-based planner with anxiety-modulated exploration bonus and MF backup.
    
    The agent learns second-stage rewards model-free and learns the first-stage
    transition model online. First-stage values combine:
      - Model-based (MB): learned transition matrix times max second-stage values
      - Directed exploration bonus: entropy of learned transitions (per action)
      - Model-free (MF) backup: cached TD(0) values for first-stage actions
    Anxiety increases directed exploration and shifts weight toward MF control by
    reducing MB weight through a stai-dependent MF-weight.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes in [0,1]
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha_r: [0,1] reward learning rate at stage 2
        alpha_T_base: [0,1] baseline transition learning rate at stage 1
        beta: [0,10] inverse temperature for both stages
        phi_base: [0,1] baseline weight on exploration bonus (transition entropy)
        omega_mf: [0,1] baseline weight on MF in the hybrid (before anxiety)
    Returns
    - Negative log-likelihood of observed choices (sum over both stages).
    """
    alpha_r, alpha_T_base, beta, phi_base, omega_mf = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition matrix rows to neutral (0.5, 0.5)
    T = np.ones((2, 2)) * 0.5  # rows: actions; cols: states
    # Stage-2 MF Q-values
    q2 = np.zeros((2, 2))
    # Stage-1 MF Q-values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    alpha_T = np.clip(alpha_T_base * (1.0 + stai0), 0.0, 1.0)  # anxious updates transitions faster
    phi = phi_base * (1.0 + stai0)  # more exploration bonus with higher anxiety
    w_mf = np.clip(omega_mf * stai0, 0.0, 1.0)                 # anxious shifts toward MF
    w_mb = 1.0 - w_mf

    eps = 1e-12
    for t in range(n_trials):
        # Model-based Q1 from learned transitions and current q2
        max_q2 = np.max(q2, axis=1)   # per state
        q1_mb = T @ max_q2            # per action

        # Directed exploration bonus: entropy of transition distribution per action
        H = np.zeros(2)
        for a in range(2):
            p = T[a]
            # entropy with clipping
            H[a] = -np.sum(p * np.log(np.clip(p, eps, 1.0)))
        q1_dir = phi * H

        # Hybrid Q1
        q1 = w_mb * q1_mb + w_mf * q1_mf + q1_dir

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        q2_s = q2[s2]
        q2c = q2_s - np.max(q2_s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update stage-2 MF values
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # Update learned transitions for chosen action using a simple delta rule toward observed state
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * onehot_s
        # Normalize for safety
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] = T[a1] / np.sum(T[a1])

        # Update stage-1 MF via TD(0) bootstrap from current stage-2 value of the chosen alien
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1  # use same reward learning rate to keep params within limit

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Kalman reward-tracking with anxiety-driven volatility and planet-switch cost.
    
    The agent tracks each second-stage action's reward probability with a
    Kalman filter (state uncertainty -> adaptive learning rate). Anxiety
    increases the assumed volatility, thus larger adaptive learning rates,
    and increases a cognitive cost for switching targeted planet between trials.
    First-stage values combine:
      - Model-based value via fixed transition structure and tracked means
      - A planet-switch aversion term (cost if chosen ship targets a different common planet than the previous observed planet)
      - A simple stage-1 MF cache updated from the chosen second-stage value
    
    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes in [0,1]
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        beta: [0,10] inverse temperature for both stages
        sigma_w_base: [0,1] baseline process noise (volatility) for Kalman filter
        sigma0: [0,1] initial variance for each option
        alpha1_mf: [0,1] learning rate for stage-1 MF cache
        c_switch_base: [0,1] baseline cost for switching targeted planet
    Returns
    - Negative log-likelihood of observed choices (sum over both stages).
    """
    beta, sigma_w_base, sigma0, alpha1_mf, c_switch_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition matrix (common 0.7)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Kalman parameters
    sigma_w = sigma_w_base * (1.0 + 2.0 * stai0)  # anxious => higher assumed volatility
    sigma_n = 0.25  # observation noise (fixed)

    # Means and variances for each state-action option
    m = np.ones((2, 2)) * 0.5
    v = np.ones((2, 2)) * sigma0

    # Stage-1 MF cache
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Planet-switch cost increases with anxiety
    c_switch = c_switch_base * (1.0 + stai0)

    # Keep track of previous observed planet
    prev_state = None

    eps = 1e-12
    for t in range(n_trials):
        # Model-based value: expected value of best alien on each planet
        max_m = np.max(m, axis=1)  # per planet
        q1_mb = T_fixed @ max_m

        # Planet-switch cost: ships target planet X for A (0), Y for U (1) in common transition
        target_planet = np.array([0, 1])
        switch_cost = np.zeros(2)
        if prev_state is not None:
            for a in range(2):
                # Cost if the targeted common planet differs from the last visited planet
                switch_cost[a] = c_switch if target_planet[a] != prev_state else 0.0

        # Combine MB and MF cache (simple average to limit parameters)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf - switch_cost

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy based on current means
        s2 = state[t]
        q2_s = m[s2]
        q2c = q2_s - np.max(q2_s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Kalman update for chosen second-stage option
        # Predict step for all options: increase variance by process noise
        v = v + sigma_w

        # Update chosen option
        pred_var = v[s2, a2]
        K = pred_var / (pred_var + sigma_n)  # adaptive learning rate in (0,1)
        m[s2, a2] = m[s2, a2] + K * (r - m[s2, a2])
        v[s2, a2] = (1.0 - K) * pred_var

        # Update stage-1 MF cache toward the current chosen second-stage mean
        target1 = m[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_mf * pe1

        prev_state = s2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Transition-sensitive win-stay/lose-switch (WSLS) blended with model-based planning.
    
    The first-stage decision blends:
      - Model-based value from the known transition structure and learned second-stage values
      - A WSLS bias that depends on prior reward and whether the prior transition was common vs rare
        (i.e., win-stay after common, win-switch after rare; reverse for losses).
    Anxiety reduces reliance on model-based planning and strengthens WSLS bias.
    Second-stage values are learned with a simple delta rule.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes in [0,1]
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        beta: [0,10] inverse temperature for both stages
        alpha2: [0,1] learning rate for second-stage Q-values
        omega_plan_base: [0,1] baseline weight on model-based component (before anxiety)
        zeta_base: [0,1] baseline weight on transition sensitivity in WSLS (0=ignores transition, 1=fully transition-dependent)
        p_ws_base: [0,1] baseline strength of WSLS bias
    Returns
    - Negative log-likelihood of observed choices (sum over both stages).
    """
    beta, alpha2, omega_plan_base, zeta_base, p_ws_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition matrix (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Second-stage Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    omega_plan = np.clip(omega_plan_base * (1.0 - stai0), 0.0, 1.0)   # anxious -> less planning
    zeta = np.clip(zeta_base * (1.0 + stai0), 0.0, 1.0)               # anxious -> more transition sensitivity
    p_ws = p_ws_base * (1.0 + stai0)                                  # stronger WSLS with anxiety

    prev_a1 = None
    prev_r = None
    prev_common = None

    eps = 1e-12
    for t in range(n_trials):
        # Model-based value via expected max second-stage value
        max_q2 = np.max(q2, axis=1)  # per planet
        q1_mb = T @ max_q2

        # WSLS bias vector
        wsls_bias = np.zeros(2)
        if prev_a1 is not None and prev_r is not None and prev_common is not None:
            win = 1.0 if prev_r > 0.0 else 0.0
            # Transition-insensitive WSLS tendency: +1 for win (stay), -1 for loss (switch)
            t_ins = 1.0 if win == 1.0 else -1.0
            # Transition-sensitive adjustment: common -> stay (+1), rare -> switch (-1)
            t_tr = 1.0 if prev_common else -1.0
            # Interpolate by zeta
            t_eff = (1.0 - zeta) * t_ins + zeta * (1.0 if (win == 1.0) else -1.0) * t_tr
            # Apply as a centered bias over actions
            wsls_bias[prev_a1] += p_ws * t_eff
            wsls_bias[1 - prev_a1] -= p_ws * t_eff

        # Combine MB and WSLS
        q1 = omega_plan * q1_mb + (1.0 - omega_plan) * wsls_bias

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        q2_s = q2[s2]
        q2c = q2_s - np.max(q2_s)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn second-stage values
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha2 * pe2

        # Track whether the last transition was common (given the known structure)
        # Common if (A->X) or (U->Y)
        prev_common = ((a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1))
        prev_a1 = a1
        prev_r = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll