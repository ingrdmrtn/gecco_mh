def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-surprise arbitration with anxiety-modulated stickiness.
    
    Overview
    - Stage 1 action values are a trial-wise arbitration between model-based (MB)
      planning and model-free (MF) values. The arbitration weight increases with
      transition surprise and participant anxiety.
    - Stage 2 uses standard Rescorla–Wagner learning.
    - An anxiety-scaled perseveration (stickiness) bias is added at both stages.
    - Model-free credit assignment from stage 2 to stage 1 uses an eligibility
      trace implicitly set by the participant’s anxiety (higher anxiety -> longer trace).
    
    Parameters (all used; total = 5)
    - alpha: [0,1]          Learning rate for value updates.
    - beta: [0,10]          Inverse temperature for softmax choice at both stages.
    - kappa_trans: [0,1]    Sensitivity of MB arbitration to transition surprise.
    - gamma_anx: [0,1]      Gain mapping anxiety (stai) into baseline MB arbitration.
    - zeta_stick: [0,1]     Base perseveration strength; scaled by stai.
    
    Inputs
    - action_1: int array of shape (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state: int array of shape (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array of shape (n_trials,), chosen alien per trial (0/1).
    - reward: float array of shape (n_trials,), coins obtained per trial in [0,1].
    - stai: float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, kappa_trans, gamma_anx, zeta_stick].
    
    Returns
    - Negative log-likelihood (float) of the observed stage-1 and stage-2 choices.
    """
    alpha, beta, kappa_trans, gamma_anx, zeta_stick = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)
    
    # Transition structure: rows = action (A,U), cols = state (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    
    # Value functions
    q1_mf = np.zeros(2)      # model-free Q at stage 1 (spaceships A/U)
    q2 = np.zeros((2, 2))    # stage-2 Q for each planet (X/Y) and alien (0/1)
    
    # Choice probabilities per trial
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    
    # Stickiness (perseveration) memory
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # per planet
    
    # Anxiety-scaled stickiness and eligibility
    kappa_eff = zeta_stick * stai
    lam_eff = stai  # eligibility trace: higher anxiety -> more MF backprop
    
    for t in range(n_trials):
        s = state[t]
        
        # Model-based Q at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)          # shape (2,)
        q1_mb = T @ max_q2                   # shape (2,)
        
        # Trial-wise arbitration weight w_t in [0,1]
        # Surprise = -log p(transition); common=0.7, rare=0.3
        a1_obs = action_1[t]
        p_trans = T[a1_obs, s]
        surprise = -np.log(max(1e-8, p_trans))
        z = (gamma_anx * stai) + (kappa_trans * (surprise - 0.5))  # center ~0.5 nats
        w_t = 1.0 / (1.0 + np.exp(-5.0 * z))  # sharpened logistic to keep in (0,1)
        
        # Hybrid Q at stage 1
        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb
        
        # Stickiness biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_eff
        
        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1_obs]
        
        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2_obs = action_2[t]
        p2[t] = probs2[a2_obs]
        
        # Learning updates
        r = reward[t]
        delta2 = r - q2[s, a2_obs]
        q2[s, a2_obs] += alpha * delta2
        # MF back-propagation to chosen stage-1 action
        q1_mf[a1_obs] += alpha * lam_eff * delta2
        
        # Update stickiness memory
        prev_a1 = a1_obs
        prev_a2[s] = a2_obs
    
    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-adaptive learning with anxiety-inflated volatility and stage-2 stickiness.
    
    Overview
    - Stage 2 learning rate adapts online to estimated reward volatility (delta^2) with
      anxiety inflating perceived volatility, thus increasing learning rate under anxiety.
    - Stage 1 is purely model-based (no fixed MB/MF weight), using current stage-2 values.
    - A stage-2 perseveration bias scales with anxiety.
    
    Parameters (all used; total = 5)
    - alpha0: [0,1]        Base learning rate (minimum) for stage-2 Q.
    - beta: [0,10]         Inverse temperature for softmax at both stages.
    - k_vol: [0,1]         Sensitivity of learning rate to estimated volatility.
    - psi_anx: [0,1]       How strongly anxiety increases effective volatility.
    - kappa2: [0,1]        Base stage-2 stickiness, scaled by anxiety.
    
    Inputs
    - action_1: int array (n_trials,), chosen spaceship (0/1).
    - state: int array (n_trials,), reached planet (0/1).
    - action_2: int array (n_trials,), chosen alien (0/1).
    - reward: float array (n_trials,), coins obtained [0,1].
    - stai: float array length-1, anxiety score in [0,1].
    - model_parameters: [alpha0, beta, k_vol, psi_anx, kappa2].
    
    Returns
    - Negative log-likelihood (float) of observed choices at both stages.
    """
    alpha0, beta, k_vol, psi_anx, kappa2 = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)
    
    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    
    # Stage-2 Q-values and volatility estimates per state-action
    q2 = np.zeros((2, 2))
    vol = np.zeros((2, 2))  # running estimate of variance of prediction error
    
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    
    # Stickiness memory for stage-2 only
    prev_a2 = np.array([None, None], dtype=object)
    kappa2_eff = kappa2 * stai
    
    for t in range(n_trials):
        s = state[t]
        
        # Stage-1 purely model-based from current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        
        # Softmax for stage 1
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1_obs = action_1[t]
        p1[t] = probs1[a1_obs]
        
        # Stage-2 softmax with stickiness
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += kappa2_eff
        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2_obs = action_2[t]
        p2[t] = probs2[a2_obs]
        
        # Outcome and adaptive learning
        r = reward[t]
        delta2 = r - q2[s, a2_obs]
        
        # Update volatility estimate with parameterized decay
        # decay in [0.5,1.0] ensures smoother vol when k_vol small
        decay = 1.0 - 0.5 * k_vol
        vol[s, a2_obs] = decay * vol[s, a2_obs] + (1.0 - decay) * (delta2 ** 2)
        
        # Anxiety-inflated effective volatility
        vol_eff = vol[s, a2_obs] * (1.0 + psi_anx * stai)
        # Map to effective learning rate in [0,1]
        alpha_eff = alpha0 + k_vol * vol_eff
        if alpha_eff > 1.0:
            alpha_eff = 1.0
        
        # Update stage-2 Q
        q2[s, a2_obs] += alpha_eff * delta2
        
        # Update stickiness memory
        prev_a2[s] = a2_obs
    
    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation-inspired planning with anxiety-shortened horizon and common-transition preference.
    
    Overview
    - Stage 1 values blend a model-based component with a successor-representation (SR)
      approximation: SR reduces to a mixture between MF bootstrapping and one-step MB,
      controlled by a discount gamma that shortens with anxiety.
    - A preference bias toward the spaceship that commonly reaches the currently
      better planet is modulated by anxiety, capturing structure-seeking tendencies.
    - Stage 2 uses standard Rescorla–Wagner learning.
    
    Parameters (all used; total = 5)
    - alpha: [0,1]          Learning rate for value updates.
    - beta: [0,10]          Inverse temperature for softmax at both stages.
    - gamma_sr: [0,1]       Base SR discount controlling weight on one-step MB within SR.
    - w_sr: [0,1]           Weight of SR-computed values relative to pure MB at stage 1.
    - pref_common: [0,1]    Strength of bias toward the spaceship with the better common destination.
    
    Inputs
    - action_1: int array (n_trials,), chosen spaceship (0=A, 1=U).
    - state: int array (n_trials,), reached planet (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien (0/1) within planet.
    - reward: float array (n_trials,), coins obtained [0,1].
    - stai: float array length-1, anxiety score in [0,1].
    - model_parameters: [alpha, beta, gamma_sr, w_sr, pref_common].
    
    Returns
    - Negative log-likelihood (float) of observed choices.
    """
    alpha, beta, gamma_sr, w_sr, pref_common = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)
    
    # Transition matrix (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    
    # Values
    q1_mf = np.zeros(2)   # MF at stage 1
    q2 = np.zeros((2, 2)) # stage-2 values
    
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    
    # Anxiety-shortened planning horizon: higher stai -> smaller gamma_eff
    gamma_eff = gamma_sr * (1.0 - 0.5 * stai)  # stays within [0,1]
    
    for t in range(n_trials):
        s = state[t]
        
        # Compute standard MB value from current stage-2 values
        max_q2 = np.max(q2, axis=1)   # best alien per planet
        q1_mb = T @ max_q2
        
        # SR-approx value: interpolate MF and one-step MB via gamma_eff
        # q1_sr = (1 - gamma) * MF + gamma * (one-step MB)
        q1_sr = (1.0 - gamma_eff) * q1_mf + gamma_eff * q1_mb
        
        # Final stage-1 value: blend SR with pure MB using w_sr
        q1 = (1.0 - w_sr) * q1_mb + w_sr * q1_sr
        
        # Common-transition preference bias toward currently better planet
        # If planet X is better, bias spaceship A (index 0); else bias spaceship U (index 1).
        better_is_X = (max_q2[0] >= max_q2[1])
        bias1 = np.zeros(2)
        pref_eff = pref_common * stai
        if better_is_X:
            bias1[0] += pref_eff
        else:
            bias1[1] += pref_eff
        
        # Stage-1 policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1_obs = action_1[t]
        p1[t] = probs1[a1_obs]
        
        # Stage-2 policy (no stickiness here to keep parameters within budget)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2_obs = action_2[t]
        p2[t] = probs2[a2_obs]
        
        # Learning from outcome
        r = reward[t]
        delta2 = r - q2[s, a2_obs]
        q2[s, a2_obs] += alpha * delta2
        
        # MF update at stage 1 via bootstrapped TD from stage 2
        q1_mf[a1_obs] += alpha * delta2
    
    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)