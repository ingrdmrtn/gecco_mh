def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety- and surprise-modulated control and two-stage stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, omega0, eta, phi)
        - alpha in [0,1]: learning rate for model-free values at both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - omega0 in [0,1]: baseline weight on model-based (MB) control.
        - eta in [0,1]: surprise sensitivity; after rare transitions MB weight is reduced
          by eta * stai.
        - phi in [0,1]: stickiness strength at stage 2 (bias to repeat the last alien
          chosen in the same planet).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Transition structure is assumed known (common: 0.7).
    - Effective MB weight on each trial is w_t = clip(omega0 - eta * stai * I_rare, 0, 1).
      Thus, higher anxiety amplifies MB down-weighting after surprising (rare) transitions.
    - Stage-1 also has a simple MF component learned by TD from the observed stage-2 value.
    - Stickiness at stage 2 is state-dependent; a bias is added to the previously chosen
      alien in that planet.
    """
    alpha, beta, omega0, eta, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: rows = actions (A,U), cols = states (X,Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free values
    q_stage1_mf = np.zeros(2)         # Q^MF at stage 1 over actions (A,U)
    q_stage2_mf = np.zeros((2, 2))    # Q^MF at stage 2: Q[state, action]

    # For stickiness at stage 2 (state-dependent)
    prev_a2 = [None, None]
    prev_a1 = None

    for t in range(n_trials):
        # MB projection using current MF estimates of second stage
        max_q_stage2 = np.max(q_stage2_mf, axis=1)       # best alien on each planet
        q_stage1_mb = transition_matrix @ max_q_stage2    # MB action values at stage 1

        # Combine MB and MF with dynamic, anxiety- and surprise-modulated weight
        # Surprise indicator (rare transition detection uses previous trial's choice/state after they occur)
        a1 = int(action_1[t])
        s2 = int(state[t])

        # Compute choice probabilities for stage 1 using last trial's w (before seeing s2)
        # We need the policy before seeing s2, so use a provisional w based on expected rarity.
        # Approximate with baseline omega0 attenuated by average surprise probability weighted by anxiety.
        w_prov = np.clip(omega0 - eta * stai * 0.3, 0.0, 1.0)
        q1_combined = w_prov * q_stage1_mb + (1.0 - w_prov) * q_stage1_mf

        # Add first-stage stickiness toward repeating last action (use phi/2 as mild, symmetric bias)
        stick1 = np.zeros(2)
        if prev_a1 is not None:
            stick1[prev_a1] = phi / 2.0
        logits1 = beta * q1_combined + stick1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with state-dependent stickiness
        a2 = int(action_2[t])
        bias2 = np.zeros(2)
        if prev_a2[s2] is not None:
            bias2[prev_a2[s2]] = phi
        logits2 = beta * q_stage2_mf[s2] + bias2
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Observe transition rarity for this trial and adjust weight for learning update
        # Rare if A->Y or U->X
        is_rare = 1 if ((a1 == 0 and s2 == 1) or (a1 == 1 and s2 == 0)) else 0
        w_t = np.clip(omega0 - eta * stai * is_rare, 0.0, 1.0)

        # Learning updates
        r = reward[t]

        # Stage-2 MF update
        delta2 = r - q_stage2_mf[s2, a2]
        q_stage2_mf[s2, a2] += alpha * delta2

        # Stage-1 MF update bootstrapping from updated stage-2 value
        target1 = q_stage2_mf[s2, a2]
        delta1 = target1 - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Update memory for stickiness
        prev_a2[s2] = a2
        prev_a1 = a1

        # Optionally, we could use w_t for a post-hoc blended update; here w_t influences only control,
        # which we already applied in policy via w_prov and through future trials.

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based control with learned transitions; anxiety dampens transition learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, tau, kappa1)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - tau in [0,1]: transition learning rate for updating P(state | action).
        - kappa1 in [0,1]: first-stage stickiness strength (bias to repeat previous spaceship).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - The agent learns the transition matrix T (rows = actions, cols = states) online.
    - Anxiety reduces transition learning: tau_eff = tau * (1 - stai).
    - Stage-1 policy is fully model-based using the learned T and current stage-2 values.
    - Stage-2 values are learned via simple TD with learning rate alpha2.
    """
    alpha2, beta, tau, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize transition beliefs to uniform (uncertain)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage values
    Q2 = np.zeros((2, 2))

    prev_a1 = None

    # Effective transition learning rate reduced by anxiety
    tau_eff = tau * (1.0 - stai_val)

    for t in range(n_trials):
        # Compute MB Q for stage 1: expected max Q2 under learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # First-stage stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = kappa1

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]

        # Update T row for chosen action toward observed state (EMA ensuring row-stochastic)
        if tau_eff > 0.0:
            # Decay entire row
            T[a1, :] = (1.0 - tau_eff) * T[a1, :]
            # Add mass to observed state
            T[a1, s2] += tau_eff
            # Numerical renormalization (guard)
            row_sum = np.sum(T[a1, :])
            if row_sum > 0:
                T[a1, :] /= row_sum

        # Update second-stage Q values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based with uncertainty-driven exploration bonus modulated by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, kappa, alpha_u)
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - kappa in [0,1]: exploration bonus weight applied to uncertainty.
        - alpha_u in [0,1]: learning rate for uncertainty estimates.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Fixed, known transitions (common: 0.7).
    - The agent tracks per-alien uncertainty u[state, action] and adds a bonus
      b = kappa * (1 - stai) * u to encourage exploration; higher anxiety reduces
      exploration bonus.
    - Uncertainty u is updated via an error-driven rule toward |reward prediction error|.
    - Stage-1 choice is purely model-based using the bonus-augmented stage-2 values.
    """
    alpha, beta, kappa, alpha_u = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 values and uncertainty
    Q2 = np.zeros((2, 2))
    U2 = np.ones((2, 2))  # initialize high uncertainty

    bonus_scale = kappa * (1.0 - stai_val)

    for t in range(n_trials):
        # Build bonus-augmented values for each state
        Q2_plus = Q2 + bonus_scale * U2

        # Model-based projection to stage 1 using augmented values
        max_Q2_plus = np.max(Q2_plus, axis=1)
        Q1_MB = transition_matrix @ max_Q2_plus

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited state using augmented values
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2_plus[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # TD error on raw value (without bonus) for learning Q2
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Uncertainty update toward absolute prediction error |delta2|
        U2[s2, a2] += alpha_u * (abs(delta2) - U2[s2, a2])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll