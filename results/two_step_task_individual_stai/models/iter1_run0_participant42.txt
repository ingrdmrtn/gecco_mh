def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """MF two-step learner with transition-signed credit assignment and anxiety-boosted perseveration.
    
    This model is purely model-free but allows the transition structure to shape how second-stage
    prediction errors are credited back to the first-stage action. Additionally, an anxiety-modulated
    choice perseveration term biases repetition of recent actions at both stages.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int
        Second-stage choices within state (0/1; e.g., aliens W/S on X, P/H on Y).
    reward : array-like of float
        Scalar reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher values indicate higher anxiety. Used to scale perseveration.
    model_parameters : sequence of floats
        [alpha_pos, alpha_neg, beta, kappa0, phi]
        Bounds:
        - alpha_pos in [0, 1]: learning rate for positive prediction errors.
        - alpha_neg in [0, 1]: learning rate for negative prediction errors.
        - beta in [0, 10]: inverse temperature for both stages.
        - kappa0 in [0, 1]: baseline perseveration weight; effective kappa = kappa0 * (1 + stai).
        - phi in [0, 1]: weight of transition-signed credit assignment to stage-1 (common vs rare).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, kappa0, phi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Common transition structure: A->X, U->Y
    # We'll use this only to determine common vs rare on each trial.
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    q1 = np.zeros(2)          # first-stage MF values for A/U
    q2 = np.zeros((2, 2))     # second-stage MF values per state and alien choice

    # Choice kernels (perseveration): stage 1 (2), stage 2 (2x2)
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))

    kappa = kappa0 * (1.0 + stai_val)  # higher anxiety -> stronger perseveration
    # Softmax helper with perseveration
    eps = 1e-12

    for t in range(n_trials):
        # Stage 1 policy
        logits1 = beta * q1 + kappa * ck1
        z1 = np.max(logits1)
        probs1 = np.exp(logits1 - z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (conditioned on state)
        s = state[t]
        logits2 = beta * q2[s] + kappa * ck2[s]
        z2 = np.max(logits2)
        probs2 = np.exp(logits2 - z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += alpha2 * pe2

        # Transition-signed credit assignment to stage 1
        # Common transitions: reinforce congruent outcome; rare: invert (negative interaction)
        sign = 1.0 if is_common(a1, s) else -1.0
        # TD error from bootstrapping onto updated second-stage value
        pe1_td = q2[s, a2] - q1[a1]
        # Transition-outcome interaction channel
        pe1_interact = sign * pe2
        delta1 = pe1_td + phi * pe1_interact
        alpha1 = alpha_pos if delta1 >= 0 else alpha_neg
        q1[a1] += alpha1 * delta1

        # Update choice kernels with mild decay
        ck1 *= 0.8
        ck2 *= 0.8
        ck1[a1] += 1.0
        ck2[s, a2] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus model with anxiety-modulated exploration and model-based lookahead.
    
    Second-stage action values carry an exploration bonus proportional to a learned uncertainty
    signal per alien. First-stage values are computed via model-based lookahead using the transition
    structure and the bonus-augmented second-stage values. Anxiety reduces the effective inverse
    temperature (i.e., increases exploration) via a scaling factor.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int
        Second-stage choices within state (0/1; e.g., aliens W/S on X, P/H on Y).
    reward : array-like of float
        Scalar reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher values indicate higher anxiety. Used to reduce beta.
    model_parameters : sequence of floats
        [alpha, beta_base, xi, tau_u, c_anx]
        Bounds:
        - alpha in [0, 1]: learning rate for Q-updates at second stage.
        - beta_base in [0, 10]: baseline inverse temperature.
        - xi in [0, 1]: weight of the uncertainty bonus added to Q-values.
        - tau_u in [0, 1]: learning rate for the uncertainty signal update.
        - c_anx in [0, 1]: strength by which anxiety reduces inverse temperature,
          beta_eff = beta_base * (1 - c_anx * stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta_base, xi, tau_u, c_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Storage for probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values and uncertainty per alien
    q2 = np.zeros((2, 2))   # per state and alien
    u2 = np.ones((2, 2)) * 0.5  # start moderately uncertain

    # First-stage MF cache not used; we compute MB values each trial
    eps = 1e-12

    # Anxiety-modulated temperature
    beta_eff = beta_base * (1.0 - c_anx * stai_val)
    beta_eff = max(0.0, min(beta_eff, 10.0))

    for t in range(n_trials):
        # Construct bonus-augmented second-stage values
        q2_aug = q2 + xi * u2

        # Model-based first-stage values: expected max over each planet
        max_q2_aug = np.max(q2_aug, axis=1)     # shape (2,)
        q1_mb = T @ max_q2_aug                  # shape (2,)

        # Stage 1 policy
        logits1 = beta_eff * q1_mb
        z1 = np.max(logits1)
        probs1 = np.exp(logits1 - z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta_eff * q2_aug[s]
        z2 = np.max(logits2)
        probs2 = np.exp(logits2 - z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update uncertainty signal first (volatility-like PE magnitude)
        pe2_pred = r - q2[s, a2]
        u2[s, a2] = (1.0 - tau_u) * u2[s, a2] + tau_u * abs(pe2_pred)

        # Update second-stage Q-values
        q2[s, a2] += alpha * pe2_pred

        # No explicit first-stage Q since we use model-based lookahead each trial

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with learned transition model, Q-value decay, and anxiety-biased transition beliefs.
    
    The agent learns both the second-stage values and the first-stage transition matrix from experience.
    Anxiety biases transition beliefs toward uncontrollable (uniform) dynamics, reducing the confidence in
    learned transitions. Additionally, Q-values decay toward zero over trials to capture forgetting.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices (0: spaceship A, 1: spaceship U) per trial.
    state : array-like of int
        Second-stage state encountered (0: planet X, 1: planet Y) per trial.
    action_2 : array-like of int
        Second-stage choices within state (0/1; e.g., aliens W/S on X, P/H on Y).
    reward : array-like of float
        Scalar reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher values indicate higher anxiety. Increases transition-uniformity bias.
    model_parameters : sequence of floats
        [alpha, beta1, beta2, decay, bias_base]
        Bounds:
        - alpha in [0, 1]: learning rate for both transition model and Q-updates.
        - beta1 in [0, 10]: inverse temperature for first-stage softmax.
        - beta2 in [0, 10]: inverse temperature for second-stage softmax.
        - decay in [0, 1]: per-trial decay rate for all Q-values toward zero.
        - bias_base in [0, 1]: magnitude of anxiety-induced bias that pulls transitions toward uniform.
          Effective bias = bias_base * stai.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta1, beta2, decay, bias_base = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize transition estimates with known prior (common/rare)
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Second-stage Q-values and first-stage MF (for bootstrapping/regularization)
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-biased transition prior strength
    bias = max(0.0, min(1.0, bias_base * stai_val))

    for t in range(n_trials):
        # Apply decay to values (forgetting)
        q2 *= (1.0 - decay)
        q1_mf *= (1.0 - decay)

        # Bias transition estimates toward uniform according to anxiety
        T_eff = (1.0 - bias) * T_est + bias * 0.5

        # Model-based stage-1 values from T_eff and current q2
        max_q2 = np.max(q2, axis=1)          # best value on each planet
        q1_mb = T_eff @ max_q2               # expected value per spaceship

        # Combine MB and a small MF regularizer (implicit via q1_mf)
        q1_comb = 0.8 * q1_mb + 0.2 * q1_mf  # fixed mixture to stabilize; both are learned/updated

        # Stage 1 policy
        logits1 = beta1 * q1_comb
        z1 = np.max(logits1)
        probs1 = np.exp(logits1 - z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta2 * q2[s]
        z2 = np.max(logits2)
        probs2 = np.exp(logits2 - z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model for the chosen first-stage action using alpha
        one_hot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - alpha) * T_est[a1] + alpha * one_hot

        # Second-stage learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage MF bootstrap from obtained second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll