def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MBâ€“MF with anxiety-modulated transition learning.

    Idea:
    - Stage-2 values are learned model-free from reward.
    - Stage-1 uses a hybrid of model-free Q1 and model-based evaluation using a learned transition model.
    - The transition model T(s'|a) is learned online; anxiety reduces transition learning, making the agent rely
      more on prior/common-transition beliefs (i.e., slower to update surprises).
    
    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha_v (0..1), value learning rate for stage-2 MF and stage-1 TD backup
    - model_parameters[1]: beta (0..10), inverse temperature used for both stages
    - model_parameters[2]: alpha_tr (0..1), base learning rate for transitions T(s'|a)
    - model_parameters[3]: omega (0..1), weight of model-based vs model-free at stage-1 (Q = (1-omega)*MF + omega*MB)
    - model_parameters[4]: k_anx_tr (0..1), scales reduction of transition learning with STAI
         alpha_tr_eff = alpha_tr * (1 - k_anx_tr * stai)
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on the reached planet
    - reward: array of floats (e.g., 0.0 or 1.0)
    - stai: array-like with a single float in [0,1], STAI score
    - model_parameters: list/array of parameter values as above
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha_v, beta, alpha_tr, omega, k_anx_tr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize choice probability storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 MF values: Q2[state, action]
    q2 = np.zeros((2, 2))
    # Stage-1 MF values: Q1[action]
    q1_mf = np.zeros(2)

    # Learned transition model T(s'|a)
    # Start with a prior reflecting the task's common transitions (0.7/0.3)
    # Implemented via pseudo-counts; these set the initial probabilities.
    prior_strength = 2.0  # fixed constant prior strength
    T = np.array([
        [0.7, 0.3],  # P(X|A), P(Y|A)
        [0.3, 0.7],  # P(X|U), P(Y|U)
    ], dtype=float)
    # Represent T via soft counts to allow simple incremental updates
    counts = T * prior_strength

    eps = 1e-10

    for t in range(n_trials):
        # Compute model-based Q at stage-1 from current T and max Q2 per state
        max_q2 = np.max(q2, axis=1)  # size 2: [max on X, max on Y]
        q1_mb = T @ max_q2  # size 2: expected value of each spaceship

        # Hybrid Q at stage-1
        q1 = (1.0 - omega) * q1_mf + omega * q1_mb

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Observe transition
        s = int(state[t])

        # Stage-2 policy in reached state
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update stage-2 MF values
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha_v * delta2

        # TD backup to stage-1 MF from the realized state-action value (eligibility via one-step backup)
        # Use the chosen stage-2 action value as the outcome for stage-1 MF
        backup_val = q2[s, a2]
        q1_mf[a1] = q1_mf[a1] + alpha_v * (backup_val - q1_mf[a1])

        # Update transition model counts and probabilities; anxiety reduces the step size
        alpha_tr_eff = alpha_tr * (1.0 - k_anx_tr * stai_val)
        if alpha_tr_eff < 0.0:
            alpha_tr_eff = 0.0
        # Convert counts to probabilities for the chosen action via an incremental update
        # First, form the current probs for chosen action
        curr_probs = counts[a1] / (np.sum(counts[a1]) + eps)
        # Target one-hot for observed next state
        target = np.zeros(2)
        target[s] = 1.0
        # Incremental probability update toward target
        new_probs = (1 - alpha_tr_eff) * curr_probs + alpha_tr_eff * target
        # Map back to counts by preserving total mass
        total = np.sum(counts[a1])
        counts[a1] = new_probs * total
        # Update T from counts for both actions
        T = (counts.T / (np.sum(counts, axis=1) + eps)).T

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive model-based planner with anxiety-inflated risk aversion.

    Idea:
    - Track for each alien the mean and variance of reward using exponential recency weighting.
    - Use a risk-sensitive utility: U = mean - phi * sqrt(variance), where phi increases with STAI.
    - Stage-1 choices are model-based: compute expected utility of each spaceship via the known transition matrix.
    - Stage-2 choices use the utilities directly.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha_val (0..1), learning rate for reward mean per alien
    - model_parameters[1]: beta (0..10), inverse temperature for softmax at both stages
    - model_parameters[2]: alpha_var (0..1), learning rate for reward variance per alien
    - model_parameters[3]: rho_base (0..1), baseline risk aversion weight
    - model_parameters[4]: k_anx_risk (0..1), increase of risk aversion with STAI
         phi = rho_base + k_anx_risk * stai

    Inputs:
    - action_1: array of ints in {0,1} (0=A, 1=U)
    - state: array of ints in {0,1} (0=X, 1=Y)
    - action_2: array of ints in {0,1} (alien index within planet)
    - reward: array of floats (e.g., 0/1)
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of the 5 parameters above

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha_val, beta, alpha_var, rho_base, k_anx_risk = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common transitions)
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Mean and variance trackers for each alien within each planet
    m = np.zeros((2, 2))
    v = np.ones((2, 2)) * 0.25  # initial variance guess for Bernoulli outcome

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # Risk aversion coefficient
    phi = rho_base + k_anx_risk * stai_val
    if phi < 0.0:
        phi = 0.0
    if phi > 1.0:
        phi = 1.0

    for t in range(n_trials):
        # Stage-2 utilities (risk-adjusted)
        U2 = m - phi * np.sqrt(np.maximum(v, 1e-8))  # shape (2,2)

        # Stage-1 model-based value = expected max utility over reached states
        max_u2 = np.max(U2, axis=1)  # per state
        q1_mb = T @ max_u2

        # Stage-1 policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = int(state[t])
        logits2 = beta * U2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Update mean for the chosen alien
        m_prev = m[s, a2]
        m[s, a2] = m_prev + alpha_val * (r - m_prev)

        # Update variance using exponential smoothing of squared deviation
        # v <- v + alpha_var * ( (r - m_prev)^2 - v ), use pre-update m_prev for stability
        sq_err = (r - m_prev) ** 2
        v[s, a2] = v[s, a2] + alpha_var * (sq_err - v[s, a2])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    MF learner with anxiety-modulated lapse and directed exploration bonus.

    Idea:
    - Pure model-free TD learning at stage-2; stage-1 receives a one-step TD backup from stage-2.
    - Add a directed exploration bonus at stage-2 based on visit-count uncertainty (eta / sqrt(n+1)).
    - Incorporate an anxiety-modulated lapse probability epsilon:
        final choice prob = (1 - epsilon) * softmax + epsilon * uniform.
      Higher anxiety increases lapses.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), learning rate for MF value updates
    - model_parameters[1]: beta (0..10), inverse temperature used for both stages
    - model_parameters[2]: epsilon_base (0..1), baseline lapse probability
    - model_parameters[3]: k_anx_lapse (0..1), slope by which STAI increases lapse
         epsilon = clip(epsilon_base + k_anx_lapse * stai, 0, 1)
    - model_parameters[4]: eta (0..1), strength of directed exploration bonus at stage-2

    Inputs:
    - action_1: array of ints in {0,1} (spaceship)
    - state: array of ints in {0,1} (planet)
    - action_2: array of ints in {0,1} (alien on planet)
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha, beta, epsilon_base, k_anx_lapse, eta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated lapse
    epsilon = epsilon_base + k_anx_lapse * stai_val
    if epsilon < 0.0:
        epsilon = 0.0
    if epsilon > 1.0:
        epsilon = 1.0

    # Model-free values
    q1 = np.zeros(2)        # stage-1 MF
    q2 = np.zeros((2, 2))   # stage-2 MF

    # Visit counts for directed exploration bonus at stage-2
    n_visits = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        # Stage-1 policy (with lapse)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 = soft1 / (np.sum(soft1) + eps)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s = int(state[t])

        # Stage-2 policy: add directed exploration bonus
        bonus = eta / np.sqrt(n_visits[s] + 1.0)
        logits2 = beta * (q2[s] + bonus)
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 = soft2 / (np.sum(soft2) + eps)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Update stage-2 MF
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha * delta2

        # Increment visit count after choice for exploration statistics
        n_visits[s, a2] += 1.0

        # One-step TD backup to stage-1
        q1[a1] = q1[a1] + alpha * (q2[s, a2] - q1[a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll