def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-weighted arbitration, perseveration, and rarity aversion.
    
    Idea:
    - Learn the stage-1 transition model online; compute model-based (MB) action values from learned transitions.
    - Arbitration between model-based and model-free (MF) values is state/action-specific and depends on transition uncertainty.
      Anxiety increases MF reliance by down-weighting MB when uncertainty is present.
    - Perseveration is amplified by anxiety.
    - "Rarity aversion" bias favors actions expected to lead to common transitions; anxiety amplifies this bias.
    
    Parameters (bounds):
    - alpha in [0,1]: learning rate for Q updates (both stages).
    - beta in [0,10]: inverse temperature for both stages.
    - eta_T in [0,1]: transition learning rate for updating the stage-1 transition model.
    - xi in [0,1]: perseveration strength (anxiety amplifies it).
    - phi in [0,1]: rarity-aversion strength (anxiety amplifies it).
    
    Inputs:
    - action_1: array-like ints {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints {0,1} reached planet (0=X, 1=Y).
    - action_2: array-like ints {0,1} second-stage choices (aliens per planet).
    - reward: array-like floats, typically {0.0,1.0}.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, eta_T, xi, phi].
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, eta_T, xi, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Qs
    q1_mf = np.zeros(2)           # stage-1 MF values for spaceships
    q2_mf = np.zeros((2, 2))      # stage-2 MF values for aliens on each planet

    # Anxiety-modulated coefficients
    xi_eff = xi * (0.5 + 0.5 * stai)          # more anxiety -> more perseveration
    phi_eff = phi * stai                      # more anxiety -> stronger rarity aversion

    prev_a1 = None
    prev_a2 = None

    for t in range(n_trials):
        # Compute model-based Q1 from learned transitions
        max_q2 = np.max(q2_mf, axis=1)          # [best on X, best on Y]
        q1_mb = T @ max_q2                      # MB Q for each spaceship

        # Arbitration weight per action based on transition uncertainty (entropy), reduced by anxiety
        # For each action a, use w_a = (1 - stai) * (1 - H(T[a]))
        # Entropy H in [0,1] after normalization by log(2)
        w = np.zeros(2)
        for a in range(2):
            p = T[a, :]
            # avoid extreme zeros
            p_clip = np.clip(p, 1e-8, 1 - 1e-8)
            H = -np.sum(p_clip * np.log(p_clip)) / np.log(2.0)  # normalized entropy in [0,1]
            w[a] = (1.0 - stai) * (1.0 - H)
        q1_combined = (1.0 - w) * q1_mf + w * q1_mb

        # Rarity aversion bias toward actions with higher "commonness" (max transition prob)
        rarity_bias = np.array([np.max(T[0, :]), np.max(T[1, :])]) - 0.5
        rarity_bias *= phi_eff

        # perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += xi_eff

        # First-stage choice probabilities
        logits1 = beta * q1_combined + bias1 + rarity_bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probabilities with perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += xi_eff
        logits2 = beta * q2_mf[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Transition learning update for T given chosen a1 and observed s
        # Move row a1 toward one-hot of observed state
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += eta_T * (target - T[a1, sp])
        # Renormalize row for safety
        T[a1, :] = np.clip(T[a1, :], 1e-8, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

        # Model-free TD updates
        # Stage-1 MF bootstraps from chosen second-stage value
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update from reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_ll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric MF with anxiety-modulated stay/switch bias from transition-reward interaction and stage-2 stickiness.
    
    Idea:
    - Pure model-free learning with separate learning rates for positive vs negative prediction errors.
    - Anxiety amplifies a dynamic "stay" bias that depends on whether last trial's transition was common/rare and rewarded:
      replicate the classical two-step interaction as a bias on repeating the previous first-stage action.
    - Stage-2 perseveration included and amplified by anxiety.
    
    Parameters (bounds):
    - alpha_pos in [0,1]: learning rate for positive TD errors.
    - alpha_neg in [0,1]: learning rate for negative TD errors.
    - beta in [0,10]: inverse temperature for both stages.
    - psi in [0,1]: strength of the stay/switch bias from transition-reward interaction (scaled by anxiety).
    - theta in [0,1]: stage-2 perseveration strength (scaled by anxiety).
    
    Inputs:
    - action_1: array-like ints {0,1} first-stage choices.
    - state: array-like ints {0,1} reached planet.
    - action_2: array-like ints {0,1} second-stage choices.
    - reward: array-like floats {0,1}.
    - stai: array-like with a single float in [0,1].
    - model_parameters: [alpha_pos, alpha_neg, beta, psi, theta]
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, psi, theta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Qs
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Keep track of previous trial to shape the dynamic bias
    prev_a1 = None
    prev_s = None
    prev_r = None
    prev_transition_common = None
    prev_a2 = None

    # Common transition structure assumed known for bias computation only (0->X common, 1->Y common)
    # This is not used for MB planning, only to compute the stay/switch bias term.
    common_map = np.array([0, 1])  # action 0 commonly -> state 0; action 1 commonly -> state 1

    theta_eff = theta * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        # Dynamic stay/switch bias at stage 1 based on previous trial
        # If last transition was common and rewarded OR rare and unrewarded -> positive bias to repeat.
        # Otherwise -> negative bias to repeat.
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            sign = 0.0
            if prev_transition_common is not None and prev_r is not None:
                if (prev_transition_common and prev_r > 0.0) or ((not prev_transition_common) and prev_r <= 0.0):
                    sign = 1.0
                else:
                    sign = -1.0
            bias_strength = psi * stai * sign
            bias1[prev_a1] += bias_strength

        # First-stage policy: softmax over MF Q with dynamic bias
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += theta_eff
        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]
        r = reward[t]

        # MF updates with valence-asymmetric learning rates
        delta2 = r - q2[s, a2]
        alpha2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2 * delta2

        # Stage-1 bootstraps from chosen second-stage value
        delta1 = q2[s, a2] - q1[a1]
        alpha1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1[a1] += alpha1 * delta1

        # Update prev info for next trial's bias
        prev_transition_common = (s == common_map[a1])
        prev_a1 = a1
        prev_a2 = a2
        prev_s = s
        prev_r = r

    eps = 1e-12
    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_ll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Utility-distortion MF with anxiety-modulated probability weighting, eligibility trace, and temperature scaling.
    
    Idea:
    - Rewards are transformed by a probability-weighting-like distortion that increases with anxiety
      (overweighting/underweighting; here implemented as a concave/convex mapping).
    - An eligibility trace backs up second-stage outcomes to first-stage MF values.
    - Decision temperature increases with anxiety (noisy choices under higher anxiety).
    
    Parameters (bounds):
    - alpha in [0,1]: learning rate for Q updates.
    - beta in [0,10]: baseline inverse temperature.
    - rho_elig in [0,1]: eligibility trace strength from stage 2 to stage 1.
    - gamma_pw in [0,1]: strength of reward probability weighting (distortion) applied to TD error.
    - tau_decay in [0,1]: decay of perseveration influence across stages within a trial (used for mild stage-2 stickiness).
    
    Inputs:
    - action_1: array-like ints {0,1} first-stage choices.
    - state: array-like ints {0,1} reached planet.
    - action_2: array-like ints {0,1} second-stage choices.
    - reward: array-like floats {0,1}.
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, rho_elig, gamma_pw, tau_decay]
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, rho_elig, gamma_pw, tau_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Anxiety-modulated temperature: higher anxiety -> lower inverse temperature (more noise)
    beta_eff1 = beta * (1.0 - 0.5 * stai)
    beta_eff2 = beta * (1.0 - 0.5 * stai)

    # Mild perseveration that decays from stage 1 to stage 2 within a trial
    prev_a1 = None
    prev_a2 = None

    # Reward distortion function (Prelec-like but simplified in [0,1])
    # r in [0,1], gamma_eff in [0,1]. Higher stai -> stronger distortion toward extremes.
    gamma_eff = gamma_pw * (0.5 + 0.5 * stai)

    def distort_reward(r_scalar):
        # Map reward to [0,1] via s-shaped transform controlled by gamma_eff.
        # For gamma_eff=0 -> identity; for larger gamma -> more curvature.
        p = np.clip(r_scalar, 0.0, 1.0)
        if gamma_eff <= 1e-8:
            return p
        # Use logistic map around 0.5 to create overweighting of rare positives when high anxiety
        k = 8.0 * gamma_eff  # slope controlled by gamma
        return 1.0 / (1.0 + np.exp(-k * (p - 0.5)))

    for t in range(n_trials):
        # Stage-1 policy with mild perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += tau_decay  # reuse tau_decay as small stickiness at stage 1
        logits1 = beta_eff1 * q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with weaker stickiness (decayed)
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += tau_decay * 0.5
        logits2 = beta_eff2 * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        r_tilde = distort_reward(r)

        # TD updates with distorted reward and eligibility trace
        delta2 = r_tilde - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 update from q2 plus eligibility trace on the reward prediction error
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1 + alpha * rho_elig * delta2

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_ll