def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-weighted hybrid learner with stickiness and transition-surprise gating.
    
    Core ideas:
    - Stage 1 uses a hybrid of model-free (MF) and model-based (MB) values.
    - Anxiety down-weights MB arbitration weight and increases choice stickiness (perseveration).
    - Rare transitions gate credit assignment to stage-1 MF values more strongly when anxiety is high.
    - Stage 2 uses standard Q-learning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = alien within the visited planet).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher values:
        - reduce the model-based weight at stage 1,
        - increase choice stickiness,
        - increase gating (down-weight) of MF credit assignment after rare transitions.
    model_parameters : array-like of float
        [alpha, beta, w_base, kappa]
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: inverse temperature for both stages.
        - w_base in [0,1]: baseline MB weight (reduced by anxiety).
        - kappa in [0,1]: baseline stickiness; anxiety scales it up.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = action at stage 1, cols = next state
    # A commonly -> X, U commonly -> Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)        # stage-1 model-free
    q2 = 0.5 * np.ones((2, 2)) # stage-2 action values (both aliens on each planet)

    # Choice stickiness biases (applied as additive bias to the chosen action from previous trial)
    bias1 = np.zeros(2)
    bias2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    w_eff = np.clip(w_base * (1.0 - 0.7 * stai), 0.0, 1.0)
    kappa_eff = kappa * (0.2 + 0.8 * stai)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    prev_a1 = None
    prev_s = None
    prev_a2 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute MB value at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid stage-1 values with stickiness
        q1_hybrid = w_eff * q1_mb + (1.0 - w_eff) * q1_mf + bias1

        # Stage-1 policy
        exp1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-state stickiness
        q2_with_bias = q2[s] + bias2[s]
        exp2 = np.exp(beta * (q2_with_bias - np.max(q2_with_bias)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Identify whether the transition was common vs rare for gating
        # Common if (a1=0 -> s=0) or (a1=1 -> s=1)
        is_common = (a1 == s)
        # Gate MF credit assignment at stage 1 more strongly on rare transitions when anxiety is high
        gate_mf = 1.0 if is_common else (1.0 - 0.7 * stai)
        gate_mf = np.clip(gate_mf, 0.0, 1.0)

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF learning (bootstrapping from obtained stage-2 value)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha * gate_mf) * pe1

        # Update stickiness biases for next trial
        bias1[:] = 0.0
        bias1[a1] += kappa_eff

        bias2[:] = 0.0
        bias2[s, a2] += kappa_eff

        prev_a1, prev_s, prev_a2 = a1, s, a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Directed exploration with anxiety-modulated bonus and valence-dependent learning rates.

    Core ideas:
    - Stage 2 action selection includes an uncertainty bonus derived from recent outcome volatility.
    - Anxiety reduces directed exploration (smaller bonus) but increases learning rates.
    - Learning uses separate rates for positive vs negative prediction errors.
    - Stage 1 is model-based (planning via transition matrix) from current stage-2 values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - reduces directed exploration bonus,
        - increases effective learning rate magnitude,
        - indirectly affects stage-1 policy via updated Q-values.
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, bonus0, decay]
        - alpha_pos in [0,1]: base learning rate for positive PEs.
        - alpha_neg in [0,1]: base learning rate for negative PEs.
        - beta in [0,10]: inverse temperature for both stages.
        - bonus0 in [0,1]: base scale of uncertainty bonus at stage 2.
        - decay in [0,1]: volatility tracking rate (EMA of squared PEs).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, bonus0, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = 0.5 * np.ones((2, 2))  # stage-2 action values
    var2 = np.zeros((2, 2))     # volatility proxy: EMA of squared PEs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration bonus scale
    bonus_scale = np.clip(bonus0 * (1.0 - stai), 0.0, 1.0)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage 1 model-based planning from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy (pure MB)
        exp1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus based on volatility
        bonus = bonus_scale * np.sqrt(np.maximum(var2[s], 0.0))
        q2_bonus = q2[s] + bonus
        exp2 = np.exp(beta * (q2_bonus - np.max(q2_bonus)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 with valence-dependent, anxiety-amplified rates
        pe2 = r - q2[s, a2]
        alpha_val = alpha_pos if pe2 >= 0.0 else alpha_neg
        # Anxiety increases responsiveness to outcomes
        alpha_eff = np.clip(alpha_val * (0.5 + 0.5 * stai), 0.0, 1.0)

        # Update Q and volatility proxy
        q2[s, a2] += alpha_eff * pe2
        var2[s, a2] = (1.0 - decay) * var2[s, a2] + decay * (pe2 ** 2)

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive hybrid with anxiety-modulated lapse and inverse temperature.

    Core ideas:
    - Stage 2 uses risk-sensitive utility: action values are penalized by recent outcome volatility.
    - Lapse rate increases with anxiety; inverse temperature decreases with anxiety.
    - Stage 1 is a hybrid of MB and MF values, with MB weight decreasing with anxiety.
    - Credit assignment from stage 2 to stage 1 uses an eligibility that increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 = aliens).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases lapse rate,
        - lowers inverse temperature (more noise),
        - increases risk sensitivity (stronger variance penalty),
        - increases credit assignment from stage 2 to stage 1 (eligibility).
    model_parameters : array-like of float
        [alpha, beta0, lapse0, rho]
        - alpha in [0,1]: learning rate for Q updates.
        - beta0 in [0,10]: baseline inverse temperature.
        - lapse0 in [0,1]: baseline lapse rate (scaled by anxiety).
        - rho in [0,1]: baseline risk-sensitivity scaling for variance penalty.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, lapse0, rho = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values and volatility trackers
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))
    var2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    w_mb = np.clip(1.0 - stai, 0.0, 1.0)                # arbitration: more anxiety -> less MB
    beta_eff = np.clip(beta0 * (1.0 - 0.5 * stai), 0.0, 10.0)  # more anxiety -> lower beta
    lapse_eff = np.clip(lapse0 * stai, 0.0, 1.0)        # more anxiety -> more lapses
    risk_eff = np.clip(rho * (0.5 + 0.5 * stai), 0.0, 1.0)     # more anxiety -> stronger variance penalty
    elig = np.clip(0.3 + 0.7 * stai, 0.0, 1.0)          # credit assignment from stage 2 to stage 1

    eps = 1e-12
    decay_var = 0.2  # fixed EMA rate for volatility; not a parameter

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based planning at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Hybrid stage-1 values
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy with lapse
        exp1 = np.exp(beta_eff * (q1_hybrid - np.max(q1_hybrid)))
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - lapse_eff) * soft1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 risk-sensitive policy: penalize by volatility (uncertainty)
        util2 = q2[s] - risk_eff * np.maximum(var2[s], 0.0)
        exp2 = np.exp(beta_eff * (util2 - np.max(util2)))
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - lapse_eff) * soft2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        var2[s, a2] = (1.0 - decay_var) * var2[s, a2] + decay_var * (pe2 ** 2)

        # Stage-1 MF update via eligibility-weighted bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha * elig) * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)