def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Model-based uncertainty bonus (UCB) with anxiety-modulated exploration and perseveration.

    Concept
    -------
    - Uses a model-based planner at stage 1 that looks ahead to the best action on each planet.
    - Adds an uncertainty bonus (UCB) to second-stage action values to encourage exploration.
    - Uncertainty is learned via an error-driven proxy that increases when prediction errors are large.
    - Anxiety reduces directed exploration (lower UCB weight) and increases perseveration (stickiness).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage planets (0=X, 1=Y) actually reached.
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices (0 or 1).
    reward : array-like of float
        Received coins (e.g., -1, 0, 1).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha_q, alpha_unc, beta, w_ucb, stickiness]
        - alpha_q in [0,1]: learning rate for second-stage Q-values.
        - alpha_unc in [0,1]: learning rate for uncertainty proxy (driven by absolute prediction error).
        - beta in [0,10]: inverse temperature used for both stages.
        - w_ucb in [0,1]: base weight on uncertainty bonus (directed exploration).
        - stickiness in [0,1]: base perseveration strength at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, alpha_unc, beta, w_ucb, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Second-stage value and uncertainty estimates
    q2 = np.zeros((2, 2))
    unc2 = np.ones((2, 2))  # start maximally uncertain

    # Perseveration bias at stage 1
    last_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations:
    # - Lower exploration when anxiety is higher
    w_ucb_eff = np.clip(w_ucb * (1.0 - stai), 0.0, 1.0)
    # - Higher perseveration when anxiety is higher
    stick_eff = np.clip(stickiness * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Compute model-based values for stage 1 using UCB-augmented stage 2
        bonus = w_ucb_eff * np.sqrt(np.maximum(unc2, 0.0))
        q2_ucb = q2 + bonus
        max_q2_ucb = np.max(q2_ucb, axis=1)
        q1_mb = T @ max_q2_ucb  # expected best value on each planet

        # Add perseveration bias at stage 1
        bias = np.zeros(2)
        if last_a1 is not None:
            bias[last_a1] = stick_eff

        # Stage 1 policy
        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state with UCB-augmented values
        s = int(state[t])
        logits2 = beta * q2_ucb[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Outcome and updates
        r = reward[t]
        pe = r - q2[s, a2]
        # Update value
        q2[s, a2] += alpha_q * pe
        # Update uncertainty proxy by tracking absolute prediction error
        unc2[s, a2] = (1.0 - alpha_unc) * unc2[s, a2] + alpha_unc * abs(pe)

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Transition-dependent credit assignment (MF-SARSA with eligibility) and lapse noise.

    Concept
    -------
    - Purely model-free SARSA with an eligibility trace that propagates reward to the first-stage action.
    - Credit assignment at stage 1 is modulated by transition type:
      common transitions strengthen credit; rare transitions partially invert it (capturing MB-like signatures).
    - Includes lapse noise that mixes the softmax policy with a uniform distribution.
    - Anxiety increases sensitivity to transition surprise (stronger inversion on rare transitions)
      and increases lapse noise slightly.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage planets (0=X, 1=Y) actually reached.
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices (0 or 1).
    reward : array-like of float
        Received coins (e.g., -1, 0, 1).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, lambda_tr, kappa_surprise, lapse]
        - alpha in [0,1]: learning rate for MF updates.
        - beta in [0,10]: inverse temperature for both stages.
        - lambda_tr in [0,1]: eligibility trace factor controlling how much reward backs up to stage 1.
        - kappa_surprise in [0,1]: base strength of surprise-based inversion on rare transitions.
        - lapse in [0,1]: base lapse probability; mixes softmax with uniform.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, lambda_tr, kappa_surprise, lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    # Common if (a1==0 and s==0) or (a1==1 and s==1)
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations
    kappa_eff = np.clip(kappa_surprise * (0.5 + 0.5 * stai), 0.0, 1.0)
    lapse_eff = np.clip(lapse * (0.5 + stai), 0.0, 1.0)  # more lapse with higher anxiety

    for t in range(n_trials):
        # Stage 1 policy with lapse
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs_1 = (1.0 - lapse_eff) * soft1 + lapse_eff * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with lapse
        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs_2 = (1.0 - lapse_eff) * soft2 + lapse_eff * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Outcomes and MF SARSA updates
        r = reward[t]

        # Stage 2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Transition-dependent credit to stage 1
        common = is_common(a1, s)
        # mod = +1 for common, -(kappa-dependent) for rare transitions
        mod = 1.0 if common else (1.0 - 2.0 * kappa_eff)  # equals - (2*kappa-1), in [-1,1]
        # Eligibility-weighted backup of outcome to first-stage chosen action
        delta1 = (r - q1[a1]) * lambda_tr * mod
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Risk-sensitive hybrid with anxiety-modulated loss curvature and forgetting.

    Concept
    -------
    - Stage 2 uses a risk-sensitive utility transform with asymmetric curvature for gains vs losses.
    - Anxiety increases loss curvature (more sensitivity to losses) and increases forgetting of unchosen actions.
    - Stage 1 combines model-based (MB) and model-free (MF) values with an anxiety-dampened MB weight.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage planets (0=X, 1=Y) actually reached.
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices (0 or 1).
    reward : array-like of float
        Received coins (e.g., -1, 0, 1).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, omega_mb, rho, decay]
        - alpha in [0,1]: learning rate for value updates.
        - beta in [0,10]: inverse temperature for both stages.
        - omega_mb in [0,1]: base weight on model-based values at stage 1.
        - rho in [0,1]: base curvature parameter for utility (lower = more risk-averse curvature).
        - decay in [0,1]: forgetting rate applied to unchosen second-stage actions.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega_mb, rho, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations
    # - Reduce MB reliance with higher anxiety
    omega_eff = np.clip(omega_mb * (1.0 - 0.4 * stai), 0.0, 1.0)
    # - Increase forgetting with higher anxiety
    decay_eff = np.clip(decay * (0.5 + 0.5 * stai), 0.0, 1.0)
    # - Sharpen loss curvature with anxiety (greater sensitivity to losses)
    rho_pos = np.clip(rho * (1.0 - 0.3 * stai), 0.0, 1.0)
    rho_neg = np.clip(min(1.0, rho * (1.0 + 0.7 * stai)), 0.0, 1.0)

    def utility(x):
        if x >= 0.0:
            return x ** (rho_pos if rho_pos > 0 else 1.0)
        else:
            return - (abs(x) ** (rho_neg if rho_neg > 0 else 1.0))

    for t in range(n_trials):
        # MB estimate from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid stage 1 values
        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1)
        probs_1 /= np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy (risk-sensitive values used implicitly through learned q2)
        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2)
        probs_2 /= np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Outcome and updates
        r = reward[t]
        u = utility(r)

        # Update chosen second-stage value with risk-sensitive utility
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Forget unchosen second-stage action in the reached state
        other_a2 = 1 - a2
        q2[s, other_a2] *= (1.0 - decay_eff)

        # MF backup to stage 1 (toward realized second-stage value)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)