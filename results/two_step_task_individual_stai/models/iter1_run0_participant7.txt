def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-based learner with anxiety-modulated loss aversion and asymmetric learning.
    
    Idea:
    - The agent is fully model-based at stage 1: it computes the expected value of each spaceship
      using the known transition structure and the current second-stage values.
    - Outcomes are evaluated through a loss-averse utility function. Anxiety increases sensitivity
      to losses relative to gains.
    - Learning at stage 2 is asymmetric: separate learning rates for positive vs. negative utility
      prediction errors. Stage 1 uses the model-based projection of second-stage values.
    
    Parameters (model_parameters):
    - alpha_pos: [0,1] learning rate used when the second-stage utility prediction error >= 0
    - alpha_neg: [0,1] learning rate used when the second-stage utility prediction error < 0
    - beta: [0,10] inverse temperature for softmax at both stages
    - loss_base: [0,1] baseline loss aversion factor (transformed internally to [1,5])
    - k_loss: [0,1] anxiety modulation strength for loss aversion (higher stai -> higher loss aversion)
    
    Inputs:
    - action_1: int array in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: int array in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: int array in {0,1}, chosen alien per planet per trial
    - reward: float array, obtained coins per trial (can be negative/zero/positive)
    - stai: array-like with single float in [0,1], participant anxiety score
    - model_parameters: tuple/list of 5 parameters in the order above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, loss_base, k_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common=0.7)
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Second-stage value (in utility space)
    q_stage2 = np.zeros((2, 2))  # Q(s, a)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion; map [0,1] -> [1,5]
    loss_aversion = 1.0 + 4.0 * max(0.0, min(1.0, loss_base + k_loss * stai))

    for t in range(n_trials):
        # Stage-1 model-based values from current second-stage estimates
        max_q_stage2 = np.max(q_stage2, axis=1)  # best alien per planet in utility space
        q1_mb = transition_matrix @ max_q_stage2

        # Stage-1 policy
        q1_center = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1_center)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        q2 = q_stage2[s].copy()
        q2_center = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2_center)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome utility with anxiety-modulated loss aversion
        r = reward[t]
        util = r if r >= 0 else -loss_aversion * (-r)

        # Stage-2 learning with asymmetric learning rate in utility space
        delta2 = util - q_stage2[s, a2]
        alpha_use = alpha_pos if delta2 >= 0 else alpha_neg
        q_stage2[s, a2] += alpha_use * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based learner with anxiety-modulated directed exploration via uncertainty bonus (UCB-like).
    
    Idea:
    - The agent maintains second-stage value estimates and a simple uncertainty proxy per alien
      based on recent absolute prediction errors (volatility-like).
    - An exploration bonus proportional to this uncertainty is added to action values during choice.
    - Anxiety reduces the exploration bonus, modeling reduced directed exploration under higher anxiety.
    - Stage 1 choices are model-based, projecting the uncertainty-bonus-augmented second-stage values
      through the known transition structure.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for second-stage value updates
    - beta: [0,10] inverse temperature for softmax at both stages
    - alpha_u: [0,1] learning rate for updating the uncertainty proxy (EMA of |PE|)
    - b_base: [0,1] baseline strength of exploration bonus
    - k_b: [0,1] anxiety modulation of exploration bonus (effective bonus b decreases with stai)
    
    Inputs:
    - action_1: int array in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: int array in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: int array in {0,1}, chosen alien per planet per trial
    - reward: float array, obtained coins per trial
    - stai: array-like with single float in [0,1], participant anxiety score
    - model_parameters: tuple/list of 5 parameters in the order above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, alpha_u, b_base, k_b = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common=0.7)
    p_common = 0.7
    transition_matrix = np.array([[p_common, 1 - p_common],
                                  [1 - p_common, p_common]])

    # Second-stage values and uncertainty proxy
    q_stage2 = np.zeros((2, 2))  # Q(s, a)
    u_stage2 = np.zeros((2, 2))  # uncertainty proxy (EMA of |prediction error|)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration bonus strength
    b_eff = max(0.0, min(1.0, b_base * (1.0 - k_b * stai)))

    for t in range(n_trials):
        # Stage-2 decision values include an uncertainty bonus
        q2_aug = q_stage2 + b_eff * u_stage2

        # Stage-1 model-based values computed from augmented second-stage values
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = transition_matrix @ max_q2_aug

        # Stage-1 policy
        q1_center = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1_center)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy in reached state using augmented values
        s = state[t]
        q2_s_aug = q2_aug[s].copy()
        q2_center = q2_s_aug - np.max(q2_s_aug)
        probs_2 = np.exp(beta * q2_center)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning from outcome (update base values and uncertainty proxy)
        r = reward[t]
        delta2 = r - q_stage2[s, a2]
        # Value update
        q_stage2[s, a2] += alpha * delta2
        # Uncertainty proxy update (EMA of absolute PE)
        u_stage2[s, a2] = (1.0 - alpha_u) * u_stage2[s, a2] + alpha_u * abs(delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid arbitration with learned transitions and anxiety-modulated planning reliance.
    
    Idea:
    - The agent learns transition probabilities online and uses them to compute model-based (MB)
      first-stage values. Model-free (MF) first-stage values are learned via bootstrapping from
      second-stage values (TD).
    - Arbitration weight on MB control increases with transition certainty but is reduced by anxiety.
      This yields dynamic reliance on planning that reflects how well the transition model is learned
      and the participant's anxiety.
    
    Parameters (model_parameters):
    - alpha_r: [0,1] learning rate for reward/value updates at stage 2 and MF stage 1
    - beta: [0,10] inverse temperature for softmax at both stages
    - alpha_T_base: [0,1] baseline learning rate for updating transition probabilities
    - k_Tstai: [0,1] anxiety modulation of transition learning rate (higher stai -> slower T learning)
    - w_bias: [0,1] baseline arbitration weight scale favoring MB when transitions are certain
    
    Inputs:
    - action_1: int array in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: int array in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: int array in {0,1}, chosen alien per planet per trial
    - reward: float array, obtained coins per trial
    - stai: array-like with single float in [0,1], participant anxiety score
    - model_parameters: tuple/list of 5 parameters in the order above
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, alpha_T_base, k_Tstai, w_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T[a, s], rows sum to 1
    T = np.full((2, 2), 0.5)

    # Value functions
    q_stage2 = np.zeros((2, 2))  # Q(s, a) at stage 2
    q_stage1_mf = np.zeros(2)    # MF first-stage Q(a)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_T = max(0.0, min(1.0, alpha_T_base * (1.0 - k_Tstai * stai)))

    for t in range(n_trials):
        # Model-based first-stage values computed from learned transitions
        max_q2 = np.max(q_stage2, axis=1)
        q1_mb = T @ max_q2

        # Transition certainty in [0,1]: 0.0 when uniform (0.5), 1.0 when deterministic (0 or 1)
        certainty = np.mean(np.abs(T[:, 0] - 0.5)) * 2.0
        certainty = max(0.0, min(1.0, certainty))

        # Anxiety-reduced MB weight; higher stai lowers reliance on MB
        w = w_bias * certainty * (1.0 - stai)
        w = max(0.0, min(1.0, w))

        # Arbitration between MB and MF at stage 1
        q1 = w * q1_mb + (1.0 - w) * q_stage1_mf

        # Stage-1 policy
        q1_center = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_center)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2 = q_stage2[s].copy()
        q2_center = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2_center)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update values
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha_r * delta2

        # Stage-1 MF TD update bootstrapping from updated second-stage value
        boot = q_stage2[s, a2]
        delta1 = boot - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha_r * delta1

        # Update learned transition matrix row for chosen action based on observed next state
        # One-hot target for transition
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1, :] = (1.0 - alpha_T) * T[a1, :] + alpha_T * target
        # Ensure numerical stability and row normalization
        row_sum = T[a1, :].sum()
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll