def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated stickiness and transition-contingent stay/switch bias.
    
    Summary
    -------
    - Stage 2: model-free Q-learning of alien values.
    - Stage 1: hybrid value = w * model-based + (1 - w) * model-free.
      Model-based uses fixed transition matrix.
    - Perseveration (choice stickiness) and a stay/switch bias depend on the previous trialâ€™s
      transition (common vs rare). Anxiety amplifies both biases and reduces the MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha2, beta, w0, kappa_stick, xi_trans]
        - alpha2: learning rate for stage-2 Q-learning, used also for stage-1 MF bootstrapping [0,1]
        - beta: inverse temperature for both stages [0,10]
        - w0: baseline MB weight at stage 1 before anxiety modulation [0,1]
        - kappa_stick: strength of perseveration (repeat previous a1) bias [0,1]
        - xi_trans: strength of transition-contingent stay/switch bias [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha2, beta, w0, kappa_stick, xi_trans = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition matrix: rows = action1 (A,U), cols = planet (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (planet x alien)
    q2 = np.zeros((2, 2)) + 0.5  # neutral initialization

    # Stage-1 MF Q-values for choosing A/U
    q1_mf = np.zeros(2) + 0.0

    # Bias trackers
    prev_a1 = None
    prev_trans_common = None

    # Anxiety-modulated parameters
    # Higher anxiety reduces MB weight and increases perseveration and transition-driven bias
    w_eff_base = np.clip(w0 * (1.0 - 0.4 * st), 0.0, 1.0)
    kappa_eff = kappa_stick * (0.5 + 0.5 * st)
    xi_eff = xi_trans * (0.5 + 0.5 * st)

    for t in range(n_trials):
        # Model-based evaluation: expected max Q2 under transitions
        max_q2 = np.max(q2, axis=1)  # size 2 for planets X,Y
        q1_mb = T @ max_q2           # size 2 for actions A,U

        # Combine MB and MF
        q1 = w_eff_base * q1_mb + (1.0 - w_eff_base) * q1_mf

        # Add perseveration bias (stay with previous a1)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += kappa_eff

        # Add transition-contingent stay/switch bias on the first-stage policy:
        # - If previous transition was common: bias to stay.
        # - If previous transition was rare: bias to switch.
        if (prev_a1 is not None) and (prev_trans_common is not None):
            if prev_trans_common:
                bias[prev_a1] += xi_eff
            else:
                bias[1 - prev_a1] += xi_eff

        # First-stage action probabilities
        logits1 = q1 + bias
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage action probabilities (within reached state)
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]

        # Stage 2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage 1 MF bootstrapping toward the experienced stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update bias memory and transition type
        # Determine whether the experienced transition was common given a1->s
        prob_to_s = T[a1, s]
        prev_trans_common = (prob_to_s >= 0.5)
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman-uncertainty planner with anxiety-boosted uncertainty exploration.
    
    Summary
    -------
    - Stage 2: Each alien's value is tracked by a simple Kalman filter (mean and variance).
      Process noise scales with an anxiety-modulated volatility parameter.
    - Choice at stage 2 uses a softmax over mean + exploration bonus proportional to uncertainty.
      Anxiety increases the exploration bonus.
    - Stage 1: Model-based evaluation from fixed transitions over the current
      uncertainty-bonused alien values, mixed by an MB weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [w_mb, beta, kappa_bonus, alpha_vol, sigma0]
        - w_mb: weight of MB evaluation at stage 1 [0,1]
        - beta: inverse temperature for both stages [0,10]
        - kappa_bonus: scale of uncertainty exploration bonus [0,1]
        - alpha_vol: process noise scale (effective volatility) [0,1]
        - sigma0: initial uncertainty (variance) of each alien value [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    w_mb, beta, kappa_bonus, alpha_vol, sigma0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kalman means and variances per planet x alien
    m = np.zeros((2, 2)) + 0.5
    v = np.zeros((2, 2)) + sigma0

    # Anxiety effects:
    # - Increase process noise (volatility) with anxiety.
    # - Increase exploration bonus with anxiety.
    q_process = alpha_vol * (0.1 + 0.9 * st)  # in [0,1]
    r_meas = 0.25  # observation noise (fixed)
    bonus_scale = kappa_bonus * (0.5 + 0.5 * st)

    for t in range(n_trials):
        # Build exploration-bonused values for both planets
        bonus = bonus_scale * np.sqrt(np.maximum(v, 1e-8))
        q2_eff = m + bonus

        # Stage 1 MB value is expected max over aliens on each planet
        max_q2_eff = np.max(q2_eff, axis=1)
        q1_mb = T @ max_q2_eff

        # No explicit MF here; use MB only, scaled by w_mb and centered for stability
        q1 = w_mb * q1_mb

        # First-stage policy
        logits1 = q1 - np.max(q1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy within the reached planet
        s = state[t]
        logits2 = q2_eff[s] - np.max(q2_eff[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Kalman update for visited alien
        r = reward[t]
        # Time update (add process noise to all aliens; simple diffusion)
        v = v + q_process

        # Measurement update only for visited (s, a2)
        pred_var = v[s, a2]
        K = pred_var / (pred_var + r_meas + 1e-10)  # Kalman gain
        m[s, a2] = m[s, a2] + K * (r - m[s, a2])
        v[s, a2] = (1.0 - K) * pred_var

        # Clamp for numerical stability
        m[s, a2] = np.clip(m[s, a2], 0.0, 1.0)
        v[s, a2] = np.clip(v[s, a2], 1e-8, 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive planner with learned transitions and anxiety-weighted loss aversion.
    
    Summary
    -------
    - Learns transition probabilities T(a1 -> planet) over time via simple delta-rule.
    - Learns second-stage Q-values via delta-rule.
    - Utility is risk-sensitive: u(r) = r for gains and = -lambda*r for losses, where
      lambda (loss aversion) increases with anxiety.
    - Stage 1 is purely model-based using the learned transitions and the current stage-2 Qs.
      A small perseveration term at stage 1 captures inertia and is reduced by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta_r, beta, alpha_T, lambda0, gamma_prev]
        - eta_r: reward learning rate at stage 2 [0,1]
        - beta: inverse temperature for both stages [0,10]
        - alpha_T: transition learning rate [0,1]
        - lambda0: baseline loss aversion (multiplier on negative utility) [0,1]
        - gamma_prev: perseveration strength to repeat previous first-stage action [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta_r, beta, alpha_T, lambda0, gamma_prev = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transitions close to uniform to allow learning
    # T[a, s] rows sum to 1
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (planet x alien)
    q2 = np.zeros((2, 2)) + 0.5

    prev_a1 = None

    # Anxiety-modulated parameters:
    # - Loss aversion increases with anxiety.
    # - Perseveration decreases with anxiety (more anxious -> less inertia).
    lambda_loss = lambda0 * (0.5 + 1.0 * st)  # can exceed 1 if lambda0 near 1 and st high (still within reason)
    persev = gamma_prev * (1.0 - 0.6 * st)

    for t in range(n_trials):
        # Stage-1 model-based evaluation using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add perseveration bias toward previous a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev

        # First-stage choice
        logits1 = q1_mb + bias
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice within reached planet
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and utility transformation (risk-sensitive)
        r = reward[t]
        # Utility: if r >= current expectation -> gain; else treated as loss relative to 0
        # Since rewards are 0/1, we define utility as:
        # u(1) = 1, u(0) = -lambda_loss
        u = 1.0 if r >= 0.5 else (-lambda_loss)

        # Stage-2 update toward utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += eta_r * pe2
        q2[s, a2] = np.clip(q2[s, a2], -lambda_loss, 1.0)

        # Transition learning: update the chosen row of T toward the observed state
        # One-hot target for reached state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * target
        # Ensure normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll