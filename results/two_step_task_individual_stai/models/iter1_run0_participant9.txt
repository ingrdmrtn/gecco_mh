def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model with learned transitions and anxiety-weighted model-based arbitration plus risk-sensitive utility.

    Core idea:
    - Learn second-stage values model-free.
    - Learn first-stage transition probabilities online.
    - Use a hybrid action value at stage 1: w * model-based + (1-w) * model-free.
    - The arbitration weight w increases when (a) the participant is less anxious and (b) transition uncertainty (entropy) is low.
    - Rewards are transformed by a risk-sensitivity parameter before learning.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Reached second-stage state on each trial: 0=planet X, 1=planet Y.
    action_2 : array-like of int {0,1}
        Second-stage choices: 0 or 1.
    reward : array-like of float
        Received reward in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score. Lower anxiety boosts model-based control especially when transitions are certain.
    model_parameters : list/tuple of 5 floats
        [alpha2, beta, w_mb0, tau_tr, rho]
        - alpha2: [0,1] learning rate for second-stage MF values; also scales first-stage MF update
        - beta: [0,10] inverse temperature for both stages
        - w_mb0: [0,1] baseline model-based weight when transitions are perfectly known and stai=0
        - tau_tr: [0,1] transition learning rate (how fast P(s|a) updates)
        - rho: [0,1] reward utility curvature; effective utility is r**rho (rho<1 -> risk-seeking)
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """
    alpha2, beta, w_mb0, tau_tr, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Second-stage MF Q-values: Q2[state, action2]
    q2 = np.zeros((2, 2))
    # First-stage MF Q-values (learned via bootstrapping from Q2)
    q1_mf = np.zeros(2)

    # Learn transitions P(s|a) online; initialize as uncertain (0.5/0.5)
    trans_p = np.ones((2, 2)) * 0.5  # rows: a in {0,1}; cols: s in {0,1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper for entropy in bits normalized to [0,1] by dividing by log(2)
    def norm_entropy(pvec):
        p = np.clip(pvec, 1e-8, 1 - 1e-8)
        h = -np.sum(p * np.log(p))
        return float(h / np.log(2.0))  # 1.0 when p=[0.5,0.5], 0.0 when deterministic

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based Q at stage 1 using current transition estimates
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = trans_p @ max_q2     # shape (2,)

        # Arbitration weight w depends on anxiety and global transition certainty.
        # Use mean normalized entropy across actions to measure uncertainty.
        H_mean = 0.5 * (norm_entropy(trans_p[0]) + norm_entropy(trans_p[1]))
        # Higher certainty (1 - H_mean) -> larger MB weight; lower anxiety -> larger MB weight
        w = w_mb0 * (1.0 - stai_val) * (1.0 - H_mean)
        w = max(0.0, min(1.0, w))

        # Hybrid action values at stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF)
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learn transition model for the chosen action a1
        # Move row trans_p[a1] toward one-hot of observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        trans_p[a1] = trans_p[a1] + tau_tr * (target - trans_p[a1])
        # Keep rows normalized (they remain so by construction)

        # Risk-sensitive utility transform
        u = (np.clip(r, 0.0, 1.0)) ** max(1e-6, rho)

        # Stage 2 MF update
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage 1 MF update towards the realized second-stage value (SARSA(0))
        alpha1_eff = alpha2 * (1.0 - 0.5 * stai_val)  # less update when anxiety is high
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += max(0.0, min(1.0, alpha1_eff)) * pe1

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free controller with anxiety-modulated forgetting and WSLS bias that depends on reward and transition type.

    Core idea:
    - Pure MF Q-learning at both stages with trial-by-trial forgetting (decay).
    - A first-stage win-stay/lose-shift (WSLS) bias influences action selection:
        - After rewarded-common or unrewarded-rare transitions, bias to repeat is stronger when anxiety is low.
        - After unrewarded-common or rewarded-rare transitions, bias to switch is stronger when anxiety is high.
    - Anxiety also increases forgetting (higher decay when stai is high).

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Reached second-stage state: 0=X, 1=Y.
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; higher -> more forgetting and stronger lose-shift after certain outcomes.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, decay, wsls_gain, xi]
        - alpha: [0,1] learning rate for both stages
        - beta: [0,10] inverse temperature (both stages)
        - decay: [0,1] baseline forgetting rate applied each trial to all Q-values
        - wsls_gain: [0,1] magnitude of WSLS bias on first-stage logits
        - xi: [0,1] mixes reward vs transition contributions to the WSLS signal
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay, wsls_gain, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    q1 = np.zeros(2)        # first-stage MF values
    q2 = np.zeros((2, 2))   # second-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Keep track of previous trial info for WSLS bias
    last_a1 = None
    last_reward = 0.0
    last_is_common = 0  # 1 common, 0 rare

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply forgetting (decay) stronger for higher anxiety
        decay_eff = decay * (0.5 + 0.5 * stai_val)
        q1 *= (1.0 - decay_eff)
        q2 *= (1.0 - decay_eff)

        # WSLS bias for first-stage logits
        bias1 = np.zeros(2)
        if last_a1 is not None:
            # Signal that combines reward and transition type
            # reward_term: +1 if rewarded, -1 if not
            reward_term = (2.0 * np.clip(last_reward, 0.0, 1.0)) - 1.0
            # transition_term: +1 if common, -1 if rare
            trans_term = 1.0 if last_is_common == 1 else -1.0

            # Blend terms; xi controls weighting; anxiety flips emphasis toward losing/rare
            # Effective signed bias to repeat last action:
            # - Low anxiety emphasizes reward-term on common transitions (classic WSLS).
            # - High anxiety emphasizes switching after losses and rare transitions.
            base_signal = xi * reward_term + (1.0 - xi) * trans_term
            anxiety_gain = (0.5 + 0.5 * stai_val)  # scales switching on adverse signals
            signed_signal = base_signal * (1.0 - stai_val) - reward_term * (anxiety_gain - (1.0 - stai_val))

            # Convert to bias on last chosen action's logit
            bias_strength = wsls_gain * signed_signal
            # Positive -> repeat bias; negative -> switch bias implemented by negative bias on last action
            bias1[last_a1] += bias_strength

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF)
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Compute whether current transition is common given the task structure:
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        # MF learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update WSLS bookkeeping
        last_a1 = a1
        last_reward = r
        last_is_common = is_common

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with planet-specific learning rates and anxiety-modulated exploration and perseveration.

    Core idea:
    - First-stage choice uses a fixed-transition model-based computation with planning:
        Q1_MB = T @ max_a2 Q2[state]
    - Second-stage values are learned model-free with planet-specific learning rates (alpha_x vs alpha_y).
    - Exploration temperature beta is reduced with higher anxiety (i.e., more randomness for high stai).
    - Perseveration bias applies at both stages but is shifted by anxiety:
        - At stage 1, low anxiety -> more perseveration (exploit habits).
        - At stage 2, high anxiety -> more perseveration (stick within the reached state).

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices: 0=A, 1=U.
    state : array-like of int {0,1}
        Reached second-stage state: 0=X, 1=Y.
    action_2 : array-like of int {0,1}
        Second-stage choices: 0 or 1.
    reward : array-like of float
        Rewards in [0,1].
    stai : array-like with single float in [0,1]
        Anxiety score; higher -> lower beta (more randomness) and stronger perseveration at stage 2.
    model_parameters : list/tuple of 5 floats
        [alpha_x, alpha_y, beta0, beta_stai, pi]
        - alpha_x: [0,1] learning rate for planet X (state=0)
        - alpha_y: [0,1] learning rate for planet Y (state=1)
        - beta0: [0,10] baseline inverse temperature when stai=0
        - beta_stai: [0,1] scales how much stai reduces beta (effective beta = beta0 * (1 - beta_stai*stai))
        - pi: [0,1] base perseveration strength (added to last-chosen action's logit; anxiety shifts it across stages)
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_x, alpha_y, beta0, beta_stai, pi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    # Perseveration bookkeeping
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated temperatures
    beta_eff = beta0 * (1.0 - beta_stai * stai_val)
    beta_eff = max(0.0, min(10.0, beta_eff))

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q1 from planning over Q2 with fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2  # shape (2,)

        # Perseveration: stage 1 stronger when anxiety is low; stage 2 stronger when anxiety is high
        pi1 = pi * (1.0 - stai_val)
        pi2 = pi * stai_val

        # Stage 1 policy with perseveration
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += pi1
        logits1 = beta_eff * (q1_mb - np.max(q1_mb)) + bias1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perseveration
        bias2 = np.zeros(2)
        la2 = last_a2_by_state[s]
        if la2 is not None:
            bias2[la2] += pi2
        logits2 = beta_eff * (q2[s] - np.max(q2[s])) + bias2
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning with planet-specific learning rates
        alpha_s = alpha_x if s == 0 else alpha_y
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s * pe2

        # Update perseveration memory
        last_a1 = a1
        last_a2_by_state[s] = a2

    eps = 1e-12
    return -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))