def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety- and uncertainty-gated arbitration with shared learning and adaptive temperature.

    Idea:
    - Stage 2 learned with a single learning rate (alpha).
    - Stage 1 uses a mixture of model-free (MF) and model-based (MB) values.
    - The arbitration weight (w_eff) is reduced by both anxiety and current uncertainty
      about stage-2 values.
    - Inverse temperature beta is reduced by anxiety (capturing more exploration).

    Parameters (bounds):
    - alpha: [0,1] learning rate for stage-2 Q and MF backprop to stage 1
    - omega0: [0,1] baseline MB arbitration weight at stage 1
    - beta0: [0,10] baseline inverse temperature
    - anx_temp: [0,1] how strongly anxiety reduces beta (beta_eff = beta0*(1 - anx_temp*stai))
    - zeta_unc: [0,1] how strongly current uncertainty reduces MB weight

    Anxiety usage:
    - Arbitration: w_eff = clip(omega0 * (1 - 0.5*stai) * (1 - zeta_unc * U), 0, 1)
      where U is a running uncertainty index derived from stage-2 value dispersion.
    - Temperature: beta_eff = beta0 * (1 - anx_temp * stai)

    Inputs:
    - action_1: int array of length T in {0,1} (spaceship choices A/U)
    - state: int array of length T in {0,1} (planet X/Y reached)
    - action_2: int array of length T in {0,1} (alien choices W/S on X, P/H on Y)
    - reward: float array of length T (received coins)
    - stai: array-like with one element in [0,1] (anxiety score)
    - model_parameters: [alpha, omega0, beta0, anx_temp, zeta_unc]

    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, omega0, beta0, anx_temp, zeta_unc = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed transition structure: A->X and U->Y are common (0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2)           # MF values for stage-1 actions (A, U)
    q2 = np.zeros((2, 2))         # Q-values at stage 2 for each state (X,Y) and actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Uncertainty proxy U_t: average entropy-like measure across states
    # We maintain a running value updated from q2.
    U = 1.0

    # Effective temperature (same at both stages)
    beta_eff = max(1e-6, beta0 * (1.0 - anx_temp * stai_score))

    for t in range(n_trials):
        # Model-based Q at stage 1: expected max value on each planet given transitions
        max_q2 = np.max(q2, axis=1)  # size 2: best action per planet
        q1_mb = transition_matrix @ max_q2

        # Compute current uncertainty from q2 dispersion (softmax entropy across states)
        # For each state, compute softmax over q2 with a modest temperature
        temp_unc = 2.0  # fixed internal temperature for uncertainty readout
        entropies = []
        for s_idx in range(2):
            logits = temp_unc * (q2[s_idx] - np.max(q2[s_idx]))
            probs = np.exp(logits)
            probs /= np.sum(probs) if np.sum(probs) > 0 else 1.0
            # Binary entropy
            p = probs[0]
            h = - (p * np.log(p + 1e-12) + (1 - p) * np.log(1 - p + 1e-12))
            # Normalize max entropy for binary is ln(2)
            entropies.append(h / np.log(2.0))
        U = 0.5 * (entropies[0] + entropies[1])

        # Arbitration: reduce MB with anxiety and uncertainty
        w_eff = omega0 * (1.0 - 0.5 * stai_score) * (1.0 - zeta_unc * U)
        w_eff = float(np.clip(w_eff, 0.0, 1.0))

        # Mixed policy at stage 1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1_sum = np.sum(probs1)
        if probs1_sum <= 0:
            probs1 = np.array([0.5, 0.5])
        else:
            probs1 /= probs1_sum

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy at observed state
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2_sum = np.sum(probs2)
        if probs2_sum <= 0:
            probs2 = np.array([0.5, 0.5])
        else:
            probs2 /= probs2_sum

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping from the actually visited state's chosen action
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Common-rare sensitive arbitration with anxiety-amplified perseveration at stage 1.

    Idea:
    - Stage 2 learned with single learning rate (alpha).
    - Stage 1 arbitration weight depends on anxiety and whether the previous trial
      was a common or rare transition (capturing reliance on MB after common vs. rare).
    - Stage 1 has a perseveration term that increases with anxiety (habit-like repeat).
    - Model-based values come from known transition matrix (no transition learning).

    Parameters (bounds):
    - alpha: [0,1] learning rate for stage-2 Q and MF backprop
    - w0: [0,1] baseline MB arbitration weight
    - phi_common: [0,1] added MB weight after a common transition, reduced after rare
    - pers1: [0,1] baseline stage-1 perseveration strength
    - beta: [0,10] inverse temperature (both stages)

    Anxiety usage:
    - Arbitration: w_eff_t = clip(w0 * (1 - stai) + phi_common * I_common_prev - phi_common * stai * I_rare_prev, 0, 1)
      where I_common_prev/I_rare_prev indicate whether previous transition was common or rare.
      This captures reduced MB reliance after rare, amplified by anxiety.
    - Perseveration: pers_eff = pers1 * (1 + stai)  # more stickiness with anxiety

    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, w0, phi_common, pers1, beta]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, w0, phi_common, pers1, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    last_a1 = -1  # for perseveration
    was_common_prev = 0  # indicator for previous trial

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    pers_eff = pers1 * (1.0 + stai_score)
    beta_eff = max(1e-6, beta)

    for t in range(n_trials):
        # MB computation from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Determine current arbitration weight based on previous trial's transition type
        I_common_prev = 1.0 if was_common_prev == 1 else 0.0
        I_rare_prev = 1.0 if was_common_prev == -1 else 0.0
        w_eff = w0 * (1.0 - stai_score) + phi_common * I_common_prev - phi_common * stai_score * I_rare_prev
        w_eff = float(np.clip(w_eff, 0.0, 1.0))

        # Stage-1 choice with perseveration bonus toward last chosen action
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        if last_a1 in [0, 1]:
            q1[last_a1] += pers_eff

        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        sum1 = np.sum(probs1)
        probs1 = probs1 / sum1 if sum1 > 0 else np.array([0.5, 0.5])

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice at reached state
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        sum2 = np.sum(probs2)
        probs2 = probs2 / sum2 if sum2 > 0 else np.array([0.5, 0.5])

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # MF backprop to stage 1
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update markers for next trial
        # Common if A->X or U->Y, rare otherwise
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        was_common_prev = 1 if is_common else -1
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Pearce-Hall associability with anxiety-scaled learning, MF eligibility, and stage-2 stickiness.

    Idea:
    - Stage 2 uses a volatility-adaptive learning rate via Pearce-Hall associability A_t.
    - Anxiety increases the gain on associability, speeding learning when surprised.
    - Stage 1 uses purely MF values updated with an eligibility-like trace.
    - Stage 2 includes action stickiness (per-state) to capture habitual repetition.

    Parameters (bounds):
    - alpha0: [0,1] base learning rate
    - kappa_ph: [0,1] associability gain factor (scaled by anxiety)
    - trace_tau: [0,1] MF eligibility factor for stage 1 update magnitude
    - stick2: [0,1] stage-2 stickiness strength for repeating last action in a state
    - beta: [0,10] inverse temperature (both stages)

    Anxiety usage:
    - Effective learning rate: alpha_t = clip(alpha0 + kappa_ph * stai * |PE2_{t-1}|, 0, 1)
      operationalized via an exponentially smoothed associability A_t driven by |PE|.
    - Stickiness: stick2_eff = stick2 * (1 + 0.5*stai)  # more repetition with anxiety

    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha0, kappa_ph, trace_tau, stick2, beta]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha0, kappa_ph, trace_tau, stick2, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Transition matrix used only to compute a model-based preview for stage 1 if desired,
    # but in this model we use pure MF at stage 1 to highlight associability effects.
    # We still compute MB preview with zeros; effectively stage 1 choice is governed by MF only.
    # (Leaving transition structure available if extensions needed.)
    # transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Pearce-Hall associability A initialized moderately
    A = 0.5
    beta_eff = max(1e-6, beta)
    stick2_eff = stick2 * (1.0 + 0.5 * stai_score)

    prev_abs_pe2 = 0.0

    for t in range(n_trials):
        # Stage-1 policy (pure MF with optional soft normalization)
        logits1 = beta_eff * (q1_mf - np.max(q1_mf))
        probs1 = np.exp(logits1)
        sum1 = np.sum(probs1)
        probs1 = probs1 / sum1 if sum1 > 0 else np.array([0.5, 0.5])

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        s = state[t]
        q2_s = q2[s].copy()
        if last_a2[s] != -1:
            q2_s[last_a2[s]] += stick2_eff

        logits2 = beta_eff * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        sum2 = np.sum(probs2)
        probs2 = probs2 / sum2 if sum2 > 0 else np.array([0.5, 0.5])

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Update associability from previous absolute PE with anxiety gain
        # A_t = (1 - trace_tau)*A_{t-1} + trace_tau*|PE2_{t-1}|; alpha_t = clip(alpha0 + kappa_ph*stai*A_t, 0, 1)
        A = (1.0 - trace_tau) * A + trace_tau * prev_abs_pe2
        alpha_t = alpha0 + kappa_ph * stai_score * A
        alpha_t = float(np.clip(alpha_t, 0.0, 1.0))

        # Stage-2 TD update with alpha_t
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * pe2

        # Stage-1 MF update with eligibility-like factor toward updated stage-2 value
        # q1_mf[a1] moves toward q2[s, a2] with step size alpha_t * trace_tau
        q1_mf[a1] += (alpha_t * trace_tau) * (q2[s, a2] - q1_mf[a1])

        # Bookkeeping
        last_a2[s] = a2
        prev_abs_pe2 = abs(pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll