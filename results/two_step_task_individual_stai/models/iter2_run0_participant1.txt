def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF RL with learned transitions, eligibility trace, and anxiety-gated arbitration.
    
    This model learns second-stage rewards and first-stage transition probabilities. The first-stage policy
    arbitrates between model-based (MB) and model-free (MF) values. The arbitration weight is reduced by
    uncertainty about transitions and further down-weighted by anxiety, making anxious participants rely
    more on MF under uncertain transitions. An eligibility trace propagates stage-2 prediction errors to
    stage-1 MF values. A small lapse handles occasional random responding.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha: learning rate for reward values and MF updates (0..1).
    - alphaT: learning rate for transition probabilities (0..1).
    - beta: inverse temperature for both stages (0..10).
    - w0: baseline arbitration weight (MB share at stage 1; 0..1).
    - xi_stai: strength by which anxiety gates the arbitration under uncertainty (0..1).
    - lam: eligibility trace from stage 2 PE to stage 1 MF (0..1).
    - epsilon: lapse rate for both stages (0..1).

    Args:
        action_1: 1D array of first-stage actions, 0/1.
        state: 1D array of second-stage states reached, 0/1.
        action_2: 1D array of second-stage actions in the reached state, 0/1.
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element: participant's anxiety score in [0,1].
        model_parameters: list/tuple of parameters in the order specified above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha, alphaT, beta, w0, xi_stai, lam, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities (rows: actions A,U; cols: states X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q2 = np.zeros((2, 2))        # second-stage state-action values
    Q1_mf = np.zeros(2)          # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1  # for optional stickiness if needed; not used here beyond MF trace

    eps = 1e-12
    ln2 = np.log(2.0)

    for t in range(n_trials):
        # Compute MB first-stage values from learned transitions and current Q2
        max_q2 = np.max(Q2, axis=1)      # max per second-stage state
        Q1_mb = T @ max_q2

        # Uncertainty over transitions: average row entropy normalized to [0,1]
        ent_rows = []
        for a in range(2):
            p = T[a].clip(eps, 1.0)
            p = p / np.sum(p)
            ent_rows.append(-(p * np.log(p)).sum())
        uncert = float(np.mean(ent_rows) / ln2)  # 0..1

        # Anxiety-gated arbitration: higher STAI and higher uncertainty push toward MF
        w_eff = w0 * (1.0 - xi_stai * stai * uncert)
        w_eff = min(max(w_eff, 0.0), 1.0)

        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # First-stage choice probability with lapse
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability with lapse
        s = state[t]
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]

        # Stage-2 TD update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Eligibility trace to stage-1 MF: use the max next value difference as TD target
        # Here we leverage the experienced second-stage value update pe2 to update Q1_mf
        Q1_mf[a1] += alpha * lam * pe2

        # Learn transitions from experienced state (Dirichlet-like exponential averaging)
        oh = np.array([1.0 if j == s else 0.0 for j in range(2)])
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive MF with win-stay/lose-shift heuristic modulated by anxiety.
    
    Stage 2 uses risk-sensitive utility and asymmetric learning rates for positive vs negative outcomes.
    A win-stay/lose-shift (WSLS) heuristic contributes to second-stage choices; anxiety strengthens
    reliance on this heuristic. Stage 1 combines a bootstrapped MF value from learned Q2 via the fixed
    transition structure with standard softmax choice. Perseveration at stage 2 is included.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha_pos: learning rate for positive outcomes at stage 2 (0..1).
    - alpha_neg: learning rate for negative outcomes at stage 2 (0..1).
    - beta: inverse temperature for both stages (0..10).
    - phi: risk/utility curvature for rewards, u(r)=r^phi (0..1, with smaller -> more concave).
    - zeta0: baseline weight of WSLS heuristic at stage 2 (0..1, translates to additive bias).
    - xi_stai: anxiety modulation of heuristic weight; zeta_eff = zeta0*(1 + xi_stai*stai) (0..1).
    - kappa2: second-stage perseveration bias for repeating last action in the same state (0..1).

    Args:
        action_1: 1D array of first-stage actions, 0/1.
        state: 1D array of second-stage states reached, 0/1.
        action_2: 1D array of second-stage actions in the reached state, 0/1.
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element: participant's anxiety score in [0,1].
        model_parameters: list/tuple of parameters in the order specified above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """
    alpha_pos, alpha_neg, beta, phi, zeta0, xi_stai, kappa2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure (common 0.7)
    T_fix = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))  # second-stage values
    Q1 = np.zeros(2)       # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For WSLS and perseveration
    prev_a2 = np.array([-1, -1])  # last action taken in each state
    prev_r2 = np.array([-1.0, -1.0])  # last reward observed in each state (scalar per state: for WSLS rule)

    eps = 1e-12

    for t in range(n_trials):
        # Compute bootstrapped stage-1 MF values from Q2 via fixed transitions
        max_q2 = np.max(Q2, axis=1)
        Q1 = T_fix @ max_q2

        # First-stage choice
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice: value + WSLS heuristic + perseveration
        s = state[t]

        # Base value logits from Q2
        logits2_val = beta * Q2[s]

        # WSLS heuristic: prefer repeating if previous reward in this state was 1, else switch
        zeta_eff = zeta0 * (1.0 + xi_stai * stai)
        heuristic_bias = np.zeros(2)
        if prev_a2[s] != -1 and prev_r2[s] != -1:
            if prev_r2[s] >= 0.5:
                # win-stay: add bias to previous action
                heuristic_bias[prev_a2[s]] += zeta_eff
            else:
                # lose-shift: add bias to the alternate action
                heuristic_bias[1 - prev_a2[s]] += zeta_eff

        # Perseveration bias to repeat last action in the same state
        persev_bias = np.zeros(2)
        if prev_a2[s] != -1:
            persev_bias[prev_a2[s]] += kappa2

        logits2 = logits2_val + heuristic_bias + persev_bias
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]
        # Risk-sensitive utility transform
        u = (r ** max(phi, eps)) if r >= 0.0 else -(((-r) ** max(phi, eps)))
        # Asymmetric learning rate
        alpha_eff = alpha_pos if u >= Q2[s, a2] else alpha_neg
        pe2 = u - Q2[s, a2]
        Q2[s, a2] += alpha_eff * pe2

        # Update WSLS memory
        prev_a2[s] = a2
        prev_r2[s] = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Subjective-transition MB with anxiety-driven dynamic lapse and value forgetting.
    
    The agent uses a subjective common-transition probability to form model-based first-stage values.
    Second-stage values are learned with exponential forgetting toward a neutral prior. A volatility
    signal (unsigned PE) drives a dynamic lapse rate; anxiety amplifies this volatility-to-lapse mapping,
    causing more random choices during volatile periods for anxious participants. First-stage perseveration
    is included.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage rewards (0..1).
    - beta: inverse temperature for both stages (0..10).
    - tau: forgetting rate toward 0.5 for Q2 on each trial (0..1).
    - p_common: subjective probability that A->X and U->Y are common (0..1).
    - delta_anx: anxiety gain mapping volatility to lapse (0..1).
    - epsilon0: baseline lapse rate (0..1).
    - kappa1: first-stage perseveration bias for repeating same spaceship (0..1).

    Args:
        action_1: 1D array of first-stage actions, 0/1.
        state: 1D array of second-stage states reached, 0/1.
        action_2: 1D array of second-stage actions, 0/1.
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element: participant's anxiety score in [0,1].
        model_parameters: list/tuple of parameters in the order specified above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """
    alphaR, beta, tau, p_common, delta_anx, epsilon0, kappa1 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Subjective transition matrix based on p_common
    T_subj = np.array([[p_common, 1.0 - p_common],
                       [1.0 - p_common, p_common]], dtype=float)

    Q2 = np.zeros((2, 2))  # second-stage values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    # Volatility estimate from unsigned PE
    v = 0.0

    eps = 1e-12

    for t in range(n_trials):
        # Dynamic lapse: baseline + anxiety-amplified volatility
        epsilon_t = epsilon0 + delta_anx * stai * v
        epsilon_t = min(max(epsilon_t, 0.0), 1.0)

        # MB first-stage values using subjective transitions
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T_subj @ max_q2

        # First-stage perseveration
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        logits1 = beta * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon_t) * probs1 + epsilon_t * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice with the same dynamic lapse
        s = state[t]
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon_t) * probs2 + epsilon_t * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Exponential forgetting toward 0.5 baseline
        Q2 = (1.0 - tau) * Q2 + tau * 0.5

        # TD learning at the encountered state-action
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Update volatility estimate v from unsigned PE (bounded in [0,1])
        v = (1.0 - tau) * v + tau * abs(pe2)
        v = min(max(v, 0.0), 1.0)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll