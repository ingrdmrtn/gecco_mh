def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free arbitration with anxiety-modulated model-based weight and stage-1 stickiness.

    This model combines a model-based (MB) planner with a model-free (MF) learner at stage 1.
    Stage 2 is learned model-free. The arbitration weight between MB and MF at stage 1 is shifted
    by the participant's anxiety (STAI). Additionally, there is a stage-1 choice perseveration
    (stickiness) term whose strength is also scaled by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1 in [0,1]. Used to modulate arbitration and stickiness.
    model_parameters : list or array
        [alpha, beta, w_base, psi_stai, kappa]
        Bounds:
          alpha in [0,1]     : MF learning rate for both stages
          beta in [0,10]     : inverse temperature for softmax at both stages
          w_base in [0,1]    : baseline model-based weight at stage 1
          psi_stai in [0,1]  : how strongly STAI shifts the MB weight (0=no effect)
          kappa in [0,1]     : baseline stage-1 perseveration strength

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, w_base, psi_stai, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common (0.7/0.3)
    transition_matrix = np.array([[0.7, 0.3],  # from A to X/Y
                                  [0.3, 0.7]]) # from U to X/Y

    # Values
    q1_mf = np.zeros(2)          # model-free values at stage 1
    q2_mf = np.zeros((2, 2))     # model-free values at stage 2: states x actions

    # Likelihood tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration and stickiness
    stai_signed = 2.0 * stai - 1.0
    w_eff = np.clip(w_base + psi_stai * stai_signed, 0.0, 1.0)  # MB weight
    # Stickiness strength increases or decreases with anxiety
    stick_strength = np.clip(kappa * (1.0 + 0.5 * stai_signed), 0.0, 1.0)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based stage-1 Q via planning through learned stage-2 values
        max_q2 = np.max(q2_mf, axis=1)  # for each state, best alien
        q1_mb = transition_matrix @ max_q2

        # Combine MB and MF; add perseveration bias for previous stage-1 choice
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            q1 = q1 + stick_strength * stick

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        p1 = np.exp(beta * q1c)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (model-free)
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        p2 = np.exp(beta * q2c)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning (MF)
        r = reward[t]
        # Stage 2 update
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage 1 MF update toward the experienced stage-2 value (simple eligibility)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated exploration (beta) and lapse (epsilon) with MF learning and eligibility.

    This purely model-free controller learns second-stage values and backs them up to stage 1
    through an eligibility trace. Anxiety jointly lowers inverse temperature (more exploration)
    and increases a soft lapse (epsilon-greedy mixture).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1 in [0,1]. Used to modulate beta and epsilon.
    model_parameters : list or array
        [alpha, beta_base, epsilon_base, chi_stai, trace]
        Bounds:
          alpha in [0,1]        : learning rate
          beta_base in [0,10]   : baseline inverse temperature
          epsilon_base in [0,1] : baseline lapse/epsilon (mixed with uniform policy)
          chi_stai in [0,1]     : sensitivity of beta/epsilon to STAI (0=no effect)
          trace in [0,1]        : eligibility strength for backing up from stage 2 to stage 1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta_base, epsilon_base, chi_stai, trace = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Signed anxiety factor in [-1,1]
    z = 2.0 * stai - 1.0
    # Anxiety reduces beta and increases epsilon
    beta = np.clip(beta_base * (1.0 - 0.5 * chi_stai * z), 0.0, 10.0)
    epsilon = np.clip(epsilon_base * (1.0 + chi_stai * z), 0.0, 1.0)

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage 1 softmax
        q1c = q1 - np.max(q1)
        p1_soft = np.exp(beta * q1c)
        p1_soft /= np.sum(p1_soft)
        # Epsilon-greedy mixture
        p1 = (1.0 - epsilon) * p1_soft + epsilon * 0.5
        p_choice_1[t] = p1[a1]

        # Stage 2 softmax
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        p2_soft = np.exp(beta * q2c)
        p2_soft /= np.sum(p2_soft)
        p2 = (1.0 - epsilon) * p2_soft + epsilon * 0.5
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility backup to stage 1
        q1[a1] += trace * alpha * pe2

        # Optional direct TD correction of stage-1 toward current stage-2 value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += (1.0 - trace) * alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-enhanced negativity and rare-transition credit control.

    This model is model-free at both stages but includes:
      - Separate learning rates for positive vs negative prediction errors at stage 2.
      - Anxiety increases negativity bias (higher learning from negative outcomes).
      - Transition-contingent credit assignment: eligibility to update stage-1 is reduced
        after rare transitions, capturing miscrediting; this reduction itself is scaled by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1 in [0,1]. Modulates negativity bias and rare-crediting.
    model_parameters : list or array
        [alpha_pos, beta, alpha_neg_base, rare_bias, eta_stai]
        Bounds:
          alpha_pos in [0,1]     : learning rate for non-negative PEs at stage 2
          beta in   [0,10]       : inverse temperature at both stages
          alpha_neg_base in [0,1]: baseline learning rate for negative PEs at stage 2
          rare_bias in [0,1]     : reduction of eligibility after rare transitions (0=no reduction, 1=full)
          eta_stai in [0,1]      : strength of anxiety effects on alpha_neg and eligibility

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, beta, alpha_neg_base, rare_bias, eta_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X and U->Y common (0.7)
    # We'll use this only to classify transitions as common vs rare
    def is_common(a1, s2):
        return (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    # Anxiety shaping
    z = 2.0 * stai - 1.0
    alpha_neg = np.clip(alpha_neg_base * (1.0 + eta_stai * z), 0.0, 1.0)
    # Eligibility reduction after rare transitions; anxiety strengthens this reduction
    rare_reduction = np.clip(rare_bias * (1.0 + 0.5 * eta_stai * z), 0.0, 1.0)

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Policies
        q1c = q1 - np.max(q1)
        p1 = np.exp(beta * q1c)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        p2 = np.exp(beta * q2c)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # Transition-contingent eligibility to update stage-1
        common = is_common(a1, s)
        if common:
            elig = 1.0
        else:
            elig = 1.0 - rare_reduction
        elig = np.clip(elig, 0.0, 1.0)

        # Back up to stage 1 using the chosen stage-2 value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += elig * lr2 * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll