def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety- and uncertainty-weighted arbitration.
    
    This model blends model-based (MB) and model-free (MF) values at stage 1
    using a trial-by-trial arbitration weight w_t that increases with:
      - the current uncertainty (entropy) over stage-2 action values on the 
        candidate next planets, and
      - participant anxiety (stai).
    Stage 2 uses MF Q-learning. Stage 1 also keeps an MF value via an eligibility
    trace bootstrapping from the realized stage-2 action value.
    
    Parameters (model_parameters):
    - alpha: learning rate for MF Q updates (stage 2 and stage-1 eligibility), in [0,1]
    - beta: inverse temperature for softmax at both stages, in [0,10]
    - a0: baseline arbitration weight intercept, in [0,1]
    - a_unc: gain on entropy-driven arbitration, in [0,1]
    - a_stai: gain of stai onto arbitration, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of scalar rewards per trial (e.g., 0/1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, a0, a_unc, a_stai = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transitions: rows spaceships [A,U], cols planets [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))      # planet x alien
    q1_mf = np.zeros(2)        # MF value per spaceship

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper to compute normalized entropy of a 2-action softmax from q-values
    def entropy_from_q(q_row, temperature):
        logits = temperature * (q_row - np.max(q_row))
        exps = np.exp(logits)
        probs = exps / (np.sum(exps) + 1e-12)
        p = np.clip(probs, 1e-12, 1.0)
        H = -np.sum(p * np.log(p))
        # Max entropy for 2 actions is ln 2; normalize to [0,1]
        return H / np.log(2.0)

    for t in range(n_trials):
        # Model-based Q1 from expected best alien on each planet
        max_q2 = np.max(q2, axis=1)   # planet-wise best alien
        q1_mb = T @ max_q2            # expected value per spaceship

        # Uncertainty signal: average entropy across both reachable planets
        H_X = entropy_from_q(q2[0], beta)
        H_Y = entropy_from_q(q2[1], beta)
        H_bar = 0.5 * (H_X + H_Y)     # in [0,1]

        # Arbitration weight combining baseline, uncertainty, and anxiety
        w = a0 + a_unc * H_bar + a_stai * st
        w = np.clip(w, 0.0, 1.0)

        # Hybrid Q for stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given reached planet
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Rewards and learning
        r = reward[t]

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF eligibility update toward realized stage-2 chosen value
        # Treat the chosen spaceship's MF value as bootstrapping from the
        # reached state's chosen alien value (advantage-style)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-sensitive learning and anxiety-amplified transition vigilance.
    
    This model uses a fixed transition structure (common=0.7, rare=0.3) but
    modulates the stage-2 learning rate on each trial based on transition
    surprise: alpha_eff = alpha * (1 + g_surprise * stai * surprise_t),
    where surprise_t = 1 - P(planet | chosen spaceship).
    
    Stage 1 combines MB and MF via a constant mixing weight omega (no anxiety
    in the mixing to avoid overlap with model1), and includes a perseveration
    bias toward repeating the previous spaceship.
    
    Parameters (model_parameters):
    - alpha: base learning rate for MF Q updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - g_surprise: gain on surprise- and anxiety-driven learning modulation, in [0,1]
    - omega: MB weight in stage-1 hybrid value, in [0,1]
    - stick1: perseveration strength at stage 1 (added to previous choice logit), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, g_surprise, omega, stick1 = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # MB Q1 from expected best alien values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick1

        # Stage-1 policy
        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Surprise based on fixed transition structure
        p_common = T[a1, s]
        surprise = 1.0 - p_common  # 0.3 for common, 0.7 for rare
        # Anxiety-amplified surprise increases the effective learning rate
        alpha_eff = alpha * (1.0 + g_surprise * st * surprise)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * delta2

        # Stage-1 MF update by eligibility from stage 2 observed value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-adaptive MF learning with MB guidance and anxiety-scaling.
    
    This model tracks reward prediction error volatility at stage 2 and adapts
    the learning rate accordingly. A running estimate of squared PE is kept:
      v_t = (1 - rho) * v_{t-1} + rho * delta2_t^2
    The effective learning rate is:
      alpha_eff = clip(alpha0 + k_vol * stai * v_t, 0, 1)
    capturing the idea that higher anxiety increases sensitivity to volatility.
    
    Stage 1 uses a hybrid of MB and MF values with a fixed mixing weight omega.
    Stage 2 uses the volatility-adaptive alpha_eff.
    
    Parameters (model_parameters):
    - alpha0: base learning rate (lower bound), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - k_vol: gain from volatility (and anxiety) to learning rate, in [0,1]
    - rho: volatility tracker update rate (EWMA), in [0,1]
    - omega: MB weight in stage-1 hybrid value, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha0, beta, k_vol, rho, omega = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0  # initial volatility estimate of delta2^2

    for t in range(n_trials):
        # MB Q1 from expected best alien values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Compute PE then update volatility estimate
        delta2 = r - q2[s, a2]
        v = (1.0 - rho) * v + rho * (delta2 ** 2)

        # Anxiety-scaled volatility-adaptive learning rate
        alpha_eff = alpha0 + k_vol * st * v
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Update stage-2 values
        q2[s, a2] += alpha_eff * delta2

        # Stage-1 MF update toward realized stage-2 chosen value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha0 * delta1  # keep base rate for MF eligibility

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll