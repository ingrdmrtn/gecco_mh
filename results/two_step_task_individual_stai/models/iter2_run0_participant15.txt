def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-weighted arbitration and learned transitions.
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for values (both stages) and MF Q1.
    - beta: [0,10] Inverse temperature for both stages.
    - w_base: [0,1] Arbitration weight toward model-based control at low anxiety.
    - w_anx: [0,1] Arbitration weight toward model-based control at high anxiety.
    - eta_t: [0,1] Transition learning rate base, amplified by anxiety.
    
    Anxiety use:
    - Arbitration weight: w = (1 - stai) * w_base + stai * w_anx (interpolates between low- and high-anxiety MB weights).
    - Transition learning: alpha_t = eta_t * (0.5 + 0.5 * stai) (higher anxiety speeds up learning of transitions).
    
    Model summary:
    - Learns state-action values at Stage-2 (Q2).
    - Learns MF Stage-1 values (Q1_mf) by bootstrapping from realized Stage-2 values.
    - Learns the transition matrix T(a -> s) online.
    - Stage-1 action values are a weighted sum of MB plan (T @ max(Q2)) and MF Q1.
    - Returns negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, w_base, w_anx, eta_t = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Start with agnostic transitions (rows sum to 1)
    T = np.full((2, 2), 0.5)

    # Values
    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    # Anxiety-weighted arbitration and transition learning rate
    w = (1.0 - stai) * w_base + stai * w_anx
    w = min(1.0, max(0.0, w))
    alpha_t = eta_t * (0.5 + 0.5 * stai)
    alpha_t = min(1.0, max(0.0, alpha_t))

    for t in range(n_trials):
        s = state[t]

        # Model-based Q at Stage-1 using current T and Q2
        mb_q1 = T @ np.max(Q2, axis=1)
        # Hybrid Q1
        q1 = w * mb_q1 + (1.0 - w) * Q1_mf

        # Policy Stage-1
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Policy Stage-2
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transitions based on observed state given chosen a1
        obs = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_t) * T[a1] + alpha_t * obs
        # Numerical guard to keep rows normalized
        T[a1] = T[a1] / max(1e-12, np.sum(T[a1]))

        # Update Q2 with reward prediction error
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update MF Q1 by bootstrapping from the realized Stage-2 value
        target_q1 = Q2[s, a2]
        pe1 = target_q1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MF with eligibility traces, anxiety-amplified outcome sensitivity, and anxious stickiness.
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for both stages.
    - beta: [0,10] Inverse temperature for both stages.
    - lam: [0,1] Eligibility trace parameter for credit assignment from Stage-2 to Stage-1.
    - phi: [0,1] Base stickiness strength on Stage-1; expressed more strongly with anxiety.
    - c: [0,1] Outcome sensitivity scaling factor; increases effective reward with anxiety.
    
    Anxiety use:
    - Effective reward scaling: r_eff = r * (1 + c * stai).
    - Stickiness bias on previous Stage-1 choice: stick = phi * stai.
    
    Model summary:
    - Pure model-free control: Q2 updated from reward; Q1 updated via eligibility trace from Q2's prediction error.
    - Stage-1 policy includes a choice stickiness bias that grows with anxiety.
    - Returns negative log-likelihood of observed choices.
    """
    alpha, beta, lam, phi, c = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = 0.5 * np.ones((2, 2))
    Q1 = np.zeros(2)

    prev_a1 = None
    stick = phi * stai

    for t in range(n_trials):
        s = state[t]

        # Stage-1 policy with stickiness bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick

        z1 = beta * (Q1 + bias - np.max(Q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (pure MF)
        z2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Anxiety-amplified outcome sensitivity
        r_eff = reward[t] * (1.0 + c * stai)

        # MF updates with eligibility trace
        pe2 = r_eff - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Credit assignment to Stage-1 choice
        Q1[a1] += alpha * lam * pe2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration with uncertainty bonus (reduced by anxiety) and MB/MF mix.
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for Q2 and MF Q1.
    - beta: [0,10] Inverse temperature for both stages.
    - b: [0,1] Directed exploration bonus strength at Stage-2 (dampened by anxiety).
    - w_mix: [0,1] Mix between MB (1) and MF (0) at Stage-1.
    - psi: [0,1] Perseveration strength on Stage-1, reduced by anxiety.
    
    Anxiety use:
    - Directed exploration bonus is scaled by (1 - stai): b_eff = b * (1 - stai).
    - Perseveration is reduced with anxiety: stick = psi * (1 - stai).
    
    Model summary:
    - Tracks uncertainty for each alien via visit counts; bonus ~ 1/sqrt(n+1).
    - Stage-2 policy adds an uncertainty bonus to Q2 before softmax (does not bias learning target).
    - Stage-1 values are a mixture of MB plan (fixed transitions) and MF Q1, plus perseveration.
    - Returns negative log-likelihood of observed choices.
    """
    alpha, beta, b, w_mix, psi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed known transitions (common-rare structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    # Uncertainty via visit counts
    N2 = np.zeros((2, 2))

    b_eff = b * (1.0 - stai)
    stick_strength = psi * (1.0 - stai)

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Model-based plan at Stage-1
        mb_q1 = T @ np.max(Q2, axis=1)
        # MF component from cached Q1
        q1_base = w_mix * mb_q1 + (1.0 - w_mix) * Q1_mf

        # Add perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_strength

        z1 = beta * (q1_base + bias - np.max(q1_base + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 directed exploration: bonus decreases with visits
        u = 1.0 / np.sqrt(N2[s] + 1.0)
        q2_bonus = Q2[s] + b_eff * u

        z2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update counts and Q2 learning (no bonus in the target)
        N2[s, a2] += 1.0
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update MF Q1 toward realized Stage-2 value
        target_q1 = Q2[s, a2]
        pe1 = target_q1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll