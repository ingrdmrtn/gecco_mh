def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated arbitration between model-based and model-free control
    with eligibility trace credit assignment.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state after first-stage choice (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Received rewards at the end of the trial.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values shift arbitration toward model-free control
        and increase eligibility trace strength.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-values.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2.
        - w_mb_base in [0,1]: baseline model-based weight at stage 1.
        - lambda_base in [0,1]: baseline eligibility trace strength.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta1, beta2, w_mb_base, lambda_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed, known transition structure (common 0.7/rare 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)         # model-free cached values for first-stage actions
    q2 = np.zeros((2, 2))       # second-stage Q-values per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety attenuates model-based weight and increases eligibility trace
    w_mb = np.clip(w_mb_base * (1.0 - 0.6 * stai), 0.0, 1.0)
    lam = np.clip(lambda_base + 0.5 * stai * (1.0 - lambda_base), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based action values from planning over second-stage max values
        v2 = np.max(q2, axis=1)             # best achievable value per state
        q1_mb = T @ v2                      # plan by forward model

        # Arbitration between MB and MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Eligibility-based credit to stage-1 MF value (anxiety increases lam)
        # Use the immediate TD error from stage 2 as the teaching signal.
        q1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty- and surprise-driven exploration with anxiety amplification.
    Rewards are learned model-free; exploration bonuses stem from reward
    uncertainty and transition surprise. Anxiety increases exploration bonuses
    and reduces determinism, especially after surprising transitions.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state after first-stage choice (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Received rewards at the end of the trial.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values increase uncertainty and surprise bonuses
        and reduce softmax inverse temperature.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q-values (means).
        - beta_base in [0,10]: baseline inverse temperature for both stages.
        - eta_base in [0,1]: baseline uncertainty bonus weight.
        - phi_base in [0,1]: baseline surprise modulation of perseveration at stage 1.
        - kappa in [0,1]: first-stage perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta_base, eta_base, phi_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition kernel (known to participant)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Reward means (Q2) and visit counts for uncertainty estimation
    q2 = np.zeros((2, 2))
    n_visits = np.zeros((2, 2))  # for beta-binomial variance proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety scaling
    # Lower determinism with anxiety
    beta = max(0.0, beta_base * (1.0 - 0.5 * stai))
    # Higher uncertainty-driven exploration with anxiety
    eta = np.clip(eta_base + 0.8 * stai * (1.0 - eta_base), 0.0, 1.0)
    # Surprise sensitivity shaping perseveration attenuation
    phi = np.clip(phi_base + 0.8 * stai * (1.0 - phi_base), 0.0, 1.0)

    prev_a1 = None
    prev_surprise = 0.0  # computed from previous trial transition

    for t in range(n_trials):
        # Reward-uncertainty bonus per state-action via beta-binomial variance proxy
        # var ~ mean*(1-mean)/(n+1); use sqrt for bonus scale
        var_sa = q2 * (1.0 - q2) / (n_visits + 1.0)
        bonus_sa = eta * np.sqrt(np.maximum(var_sa, 1e-12))

        # State values as optimistic estimates: best action including bonus
        v_state = np.max(q2 + bonus_sa, axis=1)

        # First-stage exploration bonus from state uncertainty (propagated via T)
        # Use the state's maximal action variance as its uncertainty summary.
        u_state = np.max(var_sa, axis=1)
        U_a = T @ np.sqrt(np.maximum(u_state, 1e-12))  # uncertainty routed through transitions
        U_a *= eta

        # Stage-1 perseveration, attenuated after surprising transitions (more surprise -> less stickiness)
        stick = np.zeros(2)
        if prev_a1 is not None:
            kappa_eff = kappa * (1.0 - phi * prev_surprise)
            stick[prev_a1] += kappa_eff

        # Stage 1 policy: value + exploration bonus + stickiness
        q1 = T @ v_state
        logits1 = beta * q1 + U_a + stick
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy: value + local uncertainty bonus
        s = state[t]
        logits2 = beta * (q2[s] + bonus_sa[s])
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning from reward
        r = reward[t]
        n_visits[s, a2] += 1.0
        q2[s, a2] += alpha * (r - q2[s, a2])

        # Update surprise for next trial based on current transition
        prev_surprise = 1.0 - T[a1, s]  # large when a rare transition occurred
        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Pessimistic temporal-difference learning with anxiety-enhanced forgetting.
    Stage-1 plans over pessimistic state values; stage-2 includes global
    value decay toward 0.5 that increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state after first-stage choice (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Received rewards at the end of the trial.
    stai : array-like with single float in [0,1]
        Anxiety score. Higher values increase pessimism and forgetting, and reduce
        second-stage determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for second-stage Q-values.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2_base in [0,10]: baseline inverse temperature at stage 2.
        - psi_base in [0,1]: baseline pessimism weight (0=max-only, 1=min-only).
        - omega in [0,1]: forgetting strength toward 0.5 (scaled by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta1, beta2_base, psi_base, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition kernel
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations
    psi = np.clip(psi_base + 0.7 * stai * (1.0 - psi_base), 0.0, 1.0)  # more pessimism with anxiety
    beta2 = max(0.0, beta2_base * (1.0 - 0.6 * stai))
    # Forgetting rate toward 0.5 increases with anxiety
    forget_rate = np.clip(omega * stai * 0.5, 0.0, 1.0)

    for t in range(n_trials):
        # Pessimistic state values: convex combination of max and min
        v_max = np.max(q2, axis=1)
        v_min = np.min(q2, axis=1)
        v_pess = (1.0 - psi) * v_max + psi * v_min

        # Stage 1 policy via planning over pessimistic state values
        q1 = T @ v_pess
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy with anxiety-reduced determinism
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning with pessimistic TD and forgetting toward 0.5 baseline
        r = reward[t]
        q2[s, a2] += alpha * (r - q2[s, a2])

        # Global forgetting toward 0.5, stronger with anxiety
        if forget_rate > 0.0:
            q2 = (1.0 - forget_rate) * q2 + forget_rate * 0.5

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)