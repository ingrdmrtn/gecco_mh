def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition hybrid with anxiety-modulated arbitration, temperature, and perseveration.
    
    Description:
    - The agent learns the first-stage transition structure T_hat online with a delta rule.
    - Stage-1 action values are a hybrid of model-based planning using T_hat and model-free Q1.
    - Arbitration weight, softmax temperature, and perseveration are modulated by anxiety (stai).
      Higher anxiety reduces MB weight, increases exploration (lower beta), and reduces perseveration.
    - Stage-2 values are learned model-free. Stage-1 MF uses a TD and an eligibility propagation from stage-2.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature base in [0,10]
    - kappa_T: transition learning rate in [0,1]
    - w0_mb: base MB weight in [0,1]
    - perseveration: action repetition strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, kappa_T, w0_mb, perseveration)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, kappa_T, w0_mb, perseveration = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize transition estimate (rows sum to 1); start agnostic at 0.5/0.5
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)       # model-free values for stage-1 actions
    Q2 = np.zeros((2, 2))     # stage-2 MF values: state x action

    # Anxiety-modulated arbitration and temperature
    w_mb = w0_mb * (1.0 - st)                    # higher anxiety reduces MB weighting
    w_mb = min(1.0, max(0.0, w_mb))
    beta_eff = beta * (1.0 - 0.5 * st)           # higher anxiety -> more exploration (lower beta)
    beta_eff = max(1e-6, beta_eff)

    # Perseveration scaled down by anxiety
    pers_eff = perseveration * (1.0 - st)

    prev_a1 = None
    prev_a2 = [None, None]

    lambda_elig = 0.5  # fixed eligibility trace for propagating stage-2 PE to stage-1

    for t in range(n_trials):
        s = int(state[t])

        # Model-based Q at stage-1 via current transition estimate
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_hat @ max_Q2

        # Hybrid Q
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Add perseveration bias at stage 1
        pref1 = Q1_hyb.copy()
        if prev_a1 is not None:
            pref1[prev_a1] += pers_eff

        # Softmax for stage 1
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta_eff * pref1_centered)
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration within state
        pref2 = Q2[s].copy()
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += pers_eff

        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta_eff * pref2_centered)
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update transition estimate for the chosen first-stage action based on observed state
        # Delta rule toward one-hot outcome with row sum preserved
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] += kappa_T * (target - T_hat[a1, sp])
        # Ensure numerical stability of row
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

        # TD learning
        # Stage-1 MF TD toward the observed stage-2 value (bootstrapped)
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Stage-2 MF TD toward reward
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Eligibility propagation from stage-2 to stage-1 for the taken a1
        Q1_mf[a1] += alpha * lambda_elig * delta2

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric MF learning with MB planning and anxiety-shaped asymmetry and policy bias.
    
    Description:
    - Stage-1 action values are a weighted combination of MB (fixed transition) and MF values.
    - MF learning uses valence-asymmetric learning rates: alpha_pos for positive PEs and
      alpha_neg = alpha_pos * (1 + k_neg * f(stai)), where f(stai) increases with anxiety.
      Thus, higher anxiety increases sensitivity to worse-than-expected outcomes.
    - A state-contingent choice bias favors the spaceship whose common destination matches the
      last rewarded planet; the bias is dampened by anxiety.
    - Stage-2 is MF with softmax policy.
    
    Parameters (model_parameters):
    - alpha: base learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - k_neg: multiplier (0-1) controlling how much larger alpha_neg is vs alpha_pos, [0,1]
    - omega0: base MB weight in [0,1]
    - bias: state-contingent first-stage bias strength in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}
    - state: array-like of ints in {0,1}
    - action_2: array-like of ints in {0,1}
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]
    - model_parameters: tuple/list (alpha, beta, k_neg, omega0, bias)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, k_neg, omega0, bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed true transition structure (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated MB weight: higher anxiety reduces MB reliance
    omega = omega0 * (1.0 - st)
    omega = min(1.0, max(0.0, omega))

    # Valence asymmetry as a function of anxiety
    # Higher anxiety -> larger alpha_neg relative to alpha_pos
    alpha_pos = alpha
    alpha_neg = alpha * (1.0 + k_neg * st)
    alpha_neg = min(1.0, max(0.0, alpha_neg))

    # State-contingent bias toward spaceship commonly leading to last rewarded planet
    last_rewarded_state = None
    bias_eff = bias * (1.0 - st)

    for t in range(n_trials):
        s = int(state[t])

        # MB at stage-1 from stage-2 values
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid Q
        Q1_hyb = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Apply state-contingent bias if last rewarded state is known:
        # Favor action whose common destination is that state
        pref1 = Q1_hyb.copy()
        if last_rewarded_state is not None:
            # Common destinations: action 0->state 0, action 1->state 1
            common_dest = [0, 1]
            for a in range(2):
                if common_dest[a] == last_rewarded_state:
                    pref1[a] += bias_eff

        # Softmax at stage-1
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax (pure MF)
        pref2 = Q2[s].copy()
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD updates with valence asymmetry
        # Stage-1 MF bootstraps from stage-2 chosen action value
        delta1 = Q2[s, a2] - Q1_mf[a1]
        lr1 = alpha_pos if delta1 >= 0 else alpha_neg
        Q1_mf[a1] += lr1 * delta1

        # Stage-2 MF toward reward
        delta2 = r - Q2[s, a2]
        lr2 = alpha_pos if delta2 >= 0 else alpha_neg
        Q2[s, a2] += lr2 * delta2

        # Update last rewarded state when reward is obtained (threshold at 0.5)
        if r >= 0.5:
            last_rewarded_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration bonus at stage-2 with anxiety damping and eligibility-trace MF + MB mix.
    
    Description:
    - Stage-2 incorporates a count-based uncertainty bonus: b_eff / sqrt(N[state, action] + 1).
      Anxiety reduces directed exploration by scaling b_eff = bonus * (1 - stai).
    - Stage-1 uses a hybrid MB/MF value; MB weight is reduced by anxiety and further reduced
      after rare transitions on the previous trial (more MF after surprising events).
    - MF learning uses an eligibility trace (trace parameter) to propagate reward back to stage-1.
    
    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - bonus: base directed-exploration strength in [0,1]
    - trace: eligibility trace lambda in [0,1]
    - mix: base MB weight at stage-1 in [0,1]
    
    Inputs:
    - action_1: array-like of ints in {0,1}
    - state: array-like of ints in {0,1}
    - action_2: array-like of ints in {0,1}
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]
    - model_parameters: tuple/list (alpha, beta, bonus, trace, mix)
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, bonus, trace, mix = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Visit counts for stage-2 actions to compute bonus
    N2 = np.zeros((2, 2), dtype=float)

    # Anxiety effects
    b_eff = bonus * (1.0 - st)  # higher anxiety -> smaller directed exploration
    w_base = mix * (1.0 - st)   # higher anxiety -> lower MB weight at baseline
    w_base = min(1.0, max(0.0, w_base))

    prev_a1 = None
    prev_s = None

    for t in range(n_trials):
        s = int(state[t])

        # Compute model-based Q1 from current Q2 and known transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Adapt MB weight further based on whether previous transition was rare
        w_mb = w_base
        if prev_a1 is not None and prev_s is not None:
            # Determine if previous observed state was a rare outcome for prev_a1
            prob_prev = T[prev_a1, prev_s]
            was_rare = 1.0 if prob_prev < 0.5 else 0.0
            # After rare transitions, rely more on MF (reduce MB), amplified by anxiety
            w_mb = w_base * (1.0 - st * was_rare)
        w_mb = min(1.0, max(0.0, w_mb))

        # Hybrid stage-1 Q
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        pref1 = Q1_hyb.copy()
        pref1_centered = pref1 - np.max(pref1)
        exp1 = np.exp(beta * pref1_centered)
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus
        # Add UCB-like bonus
        bonus_vec = b_eff / np.sqrt(N2[s] + 1.0)
        pref2 = Q2[s] + bonus_vec
        pref2_centered = pref2 - np.max(pref2)
        exp2 = np.exp(beta * pref2_centered)
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update counts after observing the chosen action at stage-2
        N2[s, a2] += 1.0

        # TD updates
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Eligibility trace to propagate reward PE back to stage-1 choice
        Q1_mf[a1] += alpha * trace * delta2

        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll