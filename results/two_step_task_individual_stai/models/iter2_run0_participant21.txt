def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and stage-1 perseveration.

    Overview:
    - Stage-2 action values are learned model-free from rewards with a single learning rate.
    - Stage-1 action values combine model-based planning (through the known transitions)
      and a learned model-free Q1 via a soft arbitration weight.
    - The arbitration weight is modulated by anxiety: higher/lower anxiety shifts the balance
      by moving a baseline weight (omega0) toward model-based or model-free depending on phi_anx.
    - A stage-1 perseveration bias is included and scaled by anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0..1): learning rate for stage-2 and stage-1-MF updates
    - model_parameters[1] = beta (0..10): inverse temperature for softmax at both stages
    - model_parameters[2] = omega0 (0..1): baseline weight on model-based value at stage 1
    - model_parameters[3] = phi_anx (0..1): direction/strength of anxiety effect on arbitration
      (phi_anx < 0.5 biases anxious agents toward MF; phi_anx > 0.5 toward MB)
    - model_parameters[4] = kappa1 (0..1): baseline perseveration magnitude at stage 1

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on reached planet
    - reward: array of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score for this participant
    - model_parameters: list/array of 5 floats within the bounds above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, omega0, phi_anx, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value storage
    q2 = np.zeros((2, 2))      # stage-2 Q-values per planet x alien
    q1_mf = np.zeros(2)        # stage-1 model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper to get smooth mapping from omega0 and anxiety to effective omega
    # Map omega0 in (0,1) to logit, shift by anxiety-weighted term, then sigmoid back
    eps = 1e-12
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def logit(p):
        p = np.clip(p, eps, 1 - eps)
        return np.log(p) - np.log(1 - p)

    # Anxiety effect direction: map phi_anx in [0,1] -> signed magnitude in [-1,1]
    # stai is centered at 0.5 to avoid edge dominance.
    anx_dir = 2.0 * phi_anx - 1.0
    omega_shift = anx_dir * (stai_val - 0.5) * 4.0  # scale factor controls sensitivity
    omega_eff = sigmoid(logit(omega0) + omega_shift)  # final arbitration weight in (0,1)

    prev_a1 = None

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 estimates
        max_q2 = np.max(q2, axis=1)           # best alien per planet
        q1_mb = transition_matrix @ max_q2    # expected value per spaceship

        # Stage-1 perseveration bias, scaled by anxiety (less bias when anxious)
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias_strength = kappa1 * (1.0 - stai_val)
            bias[prev_a1] = bias_strength

        # Hybrid value at stage 1
        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf + bias

        # Stage-1 policy and likelihood
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (on reached planet)
        s = state[t]
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Rewards and learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 model-free update towards the executed stage-2 value (SARSA(0)-like)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1

    eps_ll = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps_ll)) + np.sum(np.log(p_choice_2 + eps_ll)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Bayesian reward tracking with uncertainty bonus tempered by anxiety.

    Overview:
    - Each planet-alien option has a Beta-Bernoulli posterior over coin probability.
    - The decision value is the posterior mean plus an uncertainty bonus proportional
      to posterior standard deviation. Anxiety suppresses this bonus (less uncertainty seeking).
    - Stage-1 planning is fully model-based through the known transitions using these stage-2 values.

    Parameters (bounds):
    - model_parameters[0] = beta (0..10): inverse temperature for softmax at both stages
    - model_parameters[1] = tau_unc (0..1): base weight on uncertainty bonus (exploration)
    - model_parameters[2] = m0 (0..1): baseline prior mean of coin probability
    - model_parameters[3] = s0 (0..1): baseline prior strength (mapped to pseudo-counts)
    - model_parameters[4] = anx_pess (0..1): anxiety-driven shift of prior mean (higher anxiety -> more pessimistic if >0)

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on reached planet
    - reward: array of floats (0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters within the bounds above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    beta, tau_unc, m0, s0, anx_pess = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transitions
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Map prior settings to Beta pseudo-counts
    # Prior strength in [1, 21], to be neither too weak nor too strong
    strength = 1.0 + 20.0 * s0
    # Anxiety shifts prior mean: higher anxiety reduces mean if anx_pess > 0.5, increases if < 0.5
    signed = 2.0 * anx_pess - 1.0  # in [-1,1]
    m_eff = np.clip(m0 - 0.3 * signed * (stai_val - 0.5), 1e-3, 1 - 1e-3)

    a_counts = np.ones((2, 2)) * (strength * m_eff)
    b_counts = np.ones((2, 2)) * (strength * (1.0 - m_eff))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Posterior means and uncertainties at stage 2
        mean_p = a_counts / (a_counts + b_counts)
        var_p = (a_counts * b_counts) / (((a_counts + b_counts) ** 2) * (a_counts + b_counts + 1.0) + eps)
        std_p = np.sqrt(var_p)

        # Anxiety-tempered uncertainty bonus: less bonus when anxiety is higher
        bonus_scale = tau_unc * (1.0 - 0.8 * stai_val)
        q2 = mean_p + bonus_scale * std_p

        # Stage-1 MB planning
        max_q2 = np.max(q2, axis=1)
        q1 = transition_matrix @ max_q2

        # Stage-1 choice likelihood
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice likelihood on reached planet
        s = state[t]
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update Beta counts
        r = reward[t]
        # Constrain r to [0,1] in case of noise
        r_clipped = 1.0 if r >= 0.5 else 0.0
        a_counts[s, a2] += r_clipped
        b_counts[s, a2] += 1.0 - r_clipped

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learning transitions and values with anxiety-modulated surprise and eligibility.

    Overview:
    - Learns both stage-2 rewards (Q2) and stage-1 model-free values (Q1_MF).
    - Maintains a learned transition model T_hat; planning at stage 1 is model-based via T_hat.
    - Arbitration between MB and MF at stage 1 depends on transition uncertainty (entropy),
      with anxiety amplifying the impact of uncertainty (more anxiety -> rely less on MB when uncertain).
    - A surprise bonus (based on recent transition surprise per action) biases stage-1 choices,
      scaled positively by anxiety (more anxiety -> stronger surprise-driven bias).
    - An eligibility trace couples stage-2 prediction errors back to stage-1 MF values,
      with the trace strength reduced by anxiety (assuming anxious participants credit-assign less broadly).

    Parameters (bounds):
    - model_parameters[0] = lr (0..1): learning rate for stage-2 Q and stage-1 MF updates
    - model_parameters[1] = beta (0..10): inverse temperature for both stages
    - model_parameters[2] = chi_surpr (0..1): scale of surprise bonus at stage 1
    - model_parameters[3] = anx_gain (0..1): strength of anxiety effects on arbitration and surprise
    - model_parameters[4] = lam (0..1): baseline eligibility trace strength from stage 2 to stage 1

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on reached planet
    - reward: array of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters within the bounds above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    lr, beta, chi_surpr, anx_gain, lam = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model near the canonical structure
    T_hat = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    # Surprise memory per first-stage action (for biasing policy)
    last_surprise = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    def row_entropy(p_row):
        p_row = np.clip(p_row, eps, 1 - eps)
        return -np.sum(p_row * np.log(p_row))

    for t in range(n_trials):
        # Stage-1 model-based value from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Arbitration weight: more transition uncertainty reduces MB weight,
        # and anxiety amplifies this reduction.
        H0 = row_entropy(T_hat[0])
        H1 = row_entropy(T_hat[1])
        H_mean = 0.5 * (H0 + H1)  # in [~0, ~0.69]
        # Map entropy and anxiety to a [0,1] weight on MB
        # When H_mean is high and anxiety high, w_mb decreases.
        uncert_factor = (H_mean / np.log(2.0))  # normalize to [0,1]
        w_mb = 1.0 - uncert_factor ** (1.0 + anx_gain * stai_val)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Surprise-driven bias: use last surprise for each action, scaled by anxiety
        bias = chi_surpr * (0.5 + 0.5 * stai_val) * last_surprise

        # Hybrid stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias

        # Stage-1 choice likelihood
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice likelihood
        s = state[t]
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr * delta2

        # Stage-1 MF update with eligibility trace influenced by anxiety (less when anxious)
        lam_eff = lam * (1.0 - 0.7 * stai_val)
        td_to_s1 = (q2[s, a2] - q1_mf[a1]) + lam_eff * delta2
        q1_mf[a1] += lr * td_to_s1

        # Update learned transition model for the chosen first-stage action using simple delta rule
        # Prediction of reaching s given a1 is T_hat[a1, s]
        pred = T_hat[a1, s]
        # Surprise is absolute prediction error per action
        surprise = np.abs(1.0 - pred)
        # Update chosen element toward 1 and the other toward 0 to maintain stochasticity
        # Learning rate for transitions increases with anxiety
        lr_T = lr * (0.5 + 0.5 * anx_gain * stai_val)
        T_hat[a1, s] = pred + lr_T * (1.0 - pred)
        other = 1 - s
        T_hat[a1, other] = 1.0 - T_hat[a1, s]

        # Store surprise for biasing next trial
        last_surprise[a1] = surprise
        # Decay unchosen action's surprise slightly
        last_surprise[1 - a1] *= (1.0 - 0.2 * lr)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll