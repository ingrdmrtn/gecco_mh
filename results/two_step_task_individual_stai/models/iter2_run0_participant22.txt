def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transition model and anxiety-modulated MB weight.
    
    This model learns:
      - Stage-2 model-free Q-values for each alien on each planet.
      - A transition model T[a, s] for each spaceship a to each planet s.
    First-stage decisions use a hybrid of model-free and model-based values, with the
    model-based weight increased or decreased by anxiety (stai).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q-value updates at both stages.
    - beta: [0,10] inverse temperature for softmax choice at both stages.
    - w0: [0,1] baseline weight on model-based control at stage 1.
    - eta_trans: [0,1] learning rate for updating the transition model T.
    - anx_mb: [0,1] strength by which anxiety shifts the MB weight:
               w_eff = clip(w0 + anx_mb*(stai - 0.5), 0, 1).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U).
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y).
    - action_2: array of ints in {0,1}, chosen alien per trial (0/1 on each planet).
    - reward: array of floats (e.g., 0/1), coins obtained per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, w0, eta_trans, anx_mb].
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, w0, eta_trans, anx_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize values
    q1_mf = np.zeros(2)        # model-free Q at stage 1 (spaceships)
    q2 = np.zeros((2, 2))      # model-free Q at stage 2 (planets x aliens)

    # Initialize transition model T[a, s]; start from the instructed structure.
    T = np.array([[0.7, 0.3],  # A -> X common, Y rare
                  [0.3, 0.7]]) # U -> X rare,  Y common

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight (fixed across trials for parsimony)
    w_eff = w0 + anx_mb * (stai - 0.5)
    w_eff = max(0.0, min(1.0, w_eff))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based Q for stage 1 from current transition model and stage-2 values
        max_q2 = np.max(q2, axis=1)                 # best alien per planet
        q1_mb = T @ max_q2                          # expected value per spaceship

        # Hybrid Q for policy at stage 1
        q1_hyb = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 policy (softmax)
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax on q2 for reached state)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning: stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: stage 1 MF update (bootstrapping from reached state/action)
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update transition model T for the chosen spaceship using the observed state
        # Simple exponential recency-weighted update toward one-hot observation
        # Decay current row
        T[a1, :] = (1.0 - eta_trans) * T[a1, :]
        # Increment observed transition
        T[a1, s] += eta_trans
        # Renormalize to ensure it's a proper probability vector
        T[a1, :] /= (np.sum(T[a1, :]) + 1e-16)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-based planning with anxiety-modulated risk aversion and forgetting.
    
    This model:
      - Learns stage-2 reward probabilities (means) model-free.
      - Uses a risk-sensitive utility u = q - rho_eff * q*(1-q) that penalizes uncertainty.
      - Plans at stage 1 using the fixed transition structure and the risk-sensitive utilities
        of the best alien on each planet.
      - Applies uniform forgetting to all stage-2 Q-values each trial.
      - Anxiety (stai) increases risk aversion: rho_eff = clip(risk0 + anx_risk*stai, 0,1).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 Q updates.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - k_forget: [0,1] forgetting/decay rate applied to all stage-2 Q-values each trial.
    - risk0: [0,1] baseline risk aversion.
    - anx_risk: [0,1] anxiety leverage on risk aversion.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial.
    - state: array of ints in {0,1}, reached planet per trial.
    - action_2: array of ints in {0,1}, chosen alien per trial.
    - reward: array of floats (e.g., 0/1), coins obtained per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, k_forget, risk0, anx_risk].
    
    Returns:
    - Negative log-likelihood of observed choices (both stages).
    """
    alpha, beta, k_forget, risk0, anx_risk = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix per task instructions
    T = np.array([[0.7, 0.3],  # A -> X common, Y rare
                  [0.3, 0.7]]) # U -> X rare,  Y common

    # Stage-2 mean reward estimates (interpreted as coin probabilities)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated risk aversion
    rho_eff = risk0 + anx_risk * stai
    rho_eff = max(0.0, min(1.0, rho_eff))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Apply uniform forgetting to stage-2 Qs
        q2 *= (1.0 - k_forget)

        # Compute risk-sensitive utilities for each planet's aliens
        # For Bernoulli, variance estimate ~ q*(1-q)
        var2 = q2 * (1.0 - q2)
        u2 = q2 - rho_eff * var2

        # Stage-2 policy uses risk-sensitive utilities
        logits2 = beta * u2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Model-based planning at stage 1 uses the best utility on each planet
        best_u_per_planet = np.max(u2, axis=1)
        q1_mb = T @ best_u_per_planet

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Learning: update mean reward at stage 2 with the actual reward
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Reliability-based arbitration between MB and MF with anxiety-modulated arbitration
    sensitivity and choice stickiness at both stages.
    
    Core ideas:
      - Stage-2 MF learning of alien values.
      - Stage-1 uses a dynamic weight w_t to mix MB and MF values:
            Q1 = (1 - w_t)*Q1_MF + w_t*Q1_MB.
      - Arbitration weight w_t depends on relative reliabilities of MB vs MF on the previous trial:
            rel_MB_t  = 0.7 if transition was common, 0.3 if rare (inverse of surprise).
            rel_MF_t  = 1 - |delta2_t| where delta2_t is the stage-2 prediction error.
            w_{t+1}   = sigmoid(kappa_eff * (rel_MB_t - rel_MF_t)).
      - Anxiety increases arbitration sensitivity: kappa_eff = kappa0*(1 + anx_arbit*stai).
      - A stickiness term biases repeating the previous action at each stage.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF updates.
    - beta: [0,10] inverse temperature for softmax at both stages.
    - kappa0: [0,1] baseline arbitration sensitivity.
    - anx_arbit: [0,1] anxiety leverage on arbitration sensitivity.
    - stick: [0,1] stickiness added to the previously chosen action's logit (per stage).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial.
    - state: array of ints in {0,1}, reached planet per trial.
    - action_2: array of ints in {0,1}, chosen alien per trial.
    - reward: array of floats, coins obtained per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array of [alpha, beta, kappa0, anx_arbit, stick].
    
    Returns:
    - Negative log-likelihood of observed choices (both stages).
    """
    alpha, beta, kappa0, anx_arbit, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Arbitration sensitivity with anxiety
    kappa_eff = kappa0 * (1.0 + anx_arbit * stai)
    # Map to a reasonable range for sigmoid sensitivity by scaling to [0,10] softly
    kappa_eff = max(0.0, min(10.0, 10.0 * kappa_eff))

    # Initialize arbitration weight using neutral prior
    w_prev = 0.5

    # Stickiness trackers
    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 MB values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q with previous trial's arbitration weight
        q1_hyb = (1.0 - w_prev) * q1_mf + w_prev * q1_mb

        # Stage-1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick
        logits1 = beta * q1_hyb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick
        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning at stage 1 (MF)
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Compute reliabilities to update arbitration weight for next trial
        # Transition common if reached planet equals chosen spaceship index
        is_common = 1 if s == a1 else 0
        rel_mb = 0.7 if is_common == 1 else 0.3
        rel_mf = 1.0 - min(1.0, abs(delta2))

        # Update arbitration weight for next trial
        diff_rel = rel_mb - rel_mf
        w_prev = 1.0 / (1.0 + np.exp(-kappa_eff * diff_rel))
        w_prev = max(0.0, min(1.0, w_prev))

        # Update stickiness trackers
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll