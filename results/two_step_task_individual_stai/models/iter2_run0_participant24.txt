def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Model-based planning with learned transition dynamics and anxiety-dampened exploration bonus.
    
    Overview:
    - Learns second-stage Q-values from reward.
    - Learns first-stage transition probabilities from observed transitions (separately per spaceship).
    - First-stage policy is model-based: Q_MB(a1) = sum_s P(s|a1) * max_a2 Q2[s, a2].
    - Adds an uncertainty-directed, count-based exploration bonus to first-stage preferences that decreases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_q, beta, omega_bonus, alpha_T]
        - alpha_q in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega_bonus in [0,1]: scale of the first-stage exploration bonus.
        - alpha_T in [0,1]: transition learning rate for P(state | action_1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, beta, omega_bonus, alpha_T = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize learned transition probabilities P(state | action1)
    # Rows: action1 in {0,1}; Cols: state in {0,1}; rows sum to 1.
    T = np.full((2, 2), 0.5)

    # Second-stage action values
    q2 = np.zeros((2, 2))

    # Count-based exploration at stage 1
    N1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage values using current transition estimates
        max_q2_by_state = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_by_state          # shape (2,)

        # Anxiety-dampened UCB-style exploration bonus for less tried first-stage actions
        # Bonus decays with sqrt of counts; higher anxiety reduces the bonus amplitude.
        bonus_scale = omega_bonus * (1.0 - s)
        bonus1 = bonus_scale / np.sqrt(1.0 + N1)

        # First-stage policy
        prefs1 = q1_mb + bonus1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (pure MF)
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]

        # Update second-stage Q-values (MF)
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_q * pe2

        # Update transition model for the chosen first-stage action
        # Move the probability mass toward the observed state using alpha_T
        oh = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        T[a1] += alpha_T * (oh - T[a1])

        # Update counts for exploration bonus
        N1[a1] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Hybrid MB/MF with anxiety- and surprise-gated model-based control.
    
    Overview:
    - Combines model-free (MF) and model-based (MB) action values at the first stage.
    - MB uses the known transition structure (common = 0.7, rare = 0.3).
    - The MB weight is reduced by (a) higher anxiety and (b) surprising (rare) transitions.
    - Second-stage policy is MF; MF values are updated via SARSA(0)-style backup from stage 2 to stage 1.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, omega_base, zeta_surprise]
        - alpha in [0,1]: learning rate for MF Q-values (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega_base in [0,1]: baseline MB weight when anxiety and surprise are zero.
        - zeta_surprise in [0,1]: how much rare transitions suppress MB control.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega_base, zeta_surprise = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T_known = np.array([[0.7, 0.3],  # from A
                        [0.3, 0.7]]) # from U

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB values computed from known transitions and current q2
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2_by_state

        # Determine surprise (rare transition) for current trial
        a1 = action_1[t]
        st = state[t]
        # common if (A and X) or (U and Y), rare otherwise
        is_common = (a1 == 0 and st == 0) or (a1 == 1 and st == 1)
        surprise = 0.0 if is_common else 1.0

        # Anxiety- and surprise-gated MB weight
        # Higher anxiety and surprise reduce omega_t multiplicatively.
        omega_t = omega_base * (1.0 - s) * (1.0 - zeta_surprise * surprise)
        omega_t = max(0.0, min(1.0, omega_t))

        # First-stage policy from hybrid values
        q1_hybrid = omega_t * q1_mb + (1.0 - omega_t) * q1_mf
        exp1 = np.exp(beta * (q1_hybrid - np.max(q1_hybrid)))
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF)
        a2 = action_2[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Stage-1 MF update via SARSA(0) style using obtained second-stage value
        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Adaptive (Pearceâ€“Hall) learning rate and anxiety-modulated exploration temperature.
    
    Overview:
    - Second-stage learning uses an adaptive learning rate that increases with unsigned prediction error.
    - Anxiety increases sensitivity to surprise during learning (boosts the adaptive component)
      and reduces inverse temperature (more random choices).
    - First-stage policy is model-based using the known transition matrix.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices at the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha0, phi0, beta, nu_anx]
        - alpha0 in [0,1]: baseline learning rate for second-stage values.
        - phi0 in [0,1]: gain on the adaptive (unsigned-PE-based) increment to the learning rate.
        - beta in [0,10]: baseline inverse temperature for softmax.
        - nu_anx in [0,1]: how strongly anxiety reduces the effective beta.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha0, phi0, beta, nu_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3],  # from A
                        [0.3, 0.7]]) # from U

    # Second-stage values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration (lower beta -> more random)
    beta_eff = beta * (1.0 - nu_anx * s)
    if beta_eff < 0.0:
        beta_eff = 0.0

    for t in range(n_trials):
        # First-stage MB values: expected best second-stage value per action
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2_by_state

        # First-stage policy
        exp1 = np.exp(beta_eff * (q1_mb - np.max(q1_mb)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (MF with same beta_eff)
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta_eff * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with adaptive rate at stage 2
        r = reward[t]
        pe2 = r - q2[st, a2]
        # Anxiety increases the impact of surprise on learning rate
        alpha_t = alpha0 + (1.0 + s) * phi0 * abs(pe2)
        # clip to [0,1]
        if alpha_t < 0.0:
            alpha_t = 0.0
        if alpha_t > 1.0:
            alpha_t = 1.0
        q2[st, a2] += alpha_t * pe2

        # No separate MF update at stage 1; first-stage control is purely MB in this model.

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll