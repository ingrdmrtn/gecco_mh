def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-gated arbitration.

    A hybrid controller combines a model-free (MF) TD learner with a model-based (MB)
    planner that uses a learned transition model. Arbitration between MB and MF
    depends on the current uncertainty of the learned transition model, and anxiety
    up- or down-regulates the MB weight in proportion to that uncertainty.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states observed (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 for the two aliens on that planet).
    reward : array-like of float in [0,1]
        Obtained reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; modulates arbitration based on transition uncertainty.
    model_parameters : list or array-like of 5 floats
        [alpha_mf, alpha_T, beta, zeta, psi]
        - alpha_mf (0..1): learning rate for MF Q-values (both stages).
        - alpha_T  (0..1): learning rate for transition probabilities T(a->s).
        - beta     (0..10): inverse temperature for softmax (both stages).
        - zeta     (0..1): baseline weight on MB action values.
        - psi      (0..1): anxiety modulation strength of MB weight w.r.t.
                           transition uncertainty. Effective MB weight
                           w_eff = clip(zeta + psi * (stai-0.5) * U, 0, 1),
                           where U is normalized transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_mf, alpha_T, beta, zeta, psi = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model T[action, state]; start near task structure but learnable
    # Prior slightly informative toward common transitions: A->X, U->Y.
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    # MF Q-values
    Q1_mf = np.zeros(2)         # stage-1 MF action values
    Q2 = np.zeros((2, 2))       # stage-2 MF action values: Q2[state, action]

    p1 = np.zeros(n)
    p2 = np.zeros(n)
    eps = 1e-10

    for t in range(n):
        # Model-based Q1 from learned transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)            # value of each planet
        Q1_mb = T @ max_Q2                     # MB estimate over states

        # Arbitration weight: scale baseline zeta by transition uncertainty and anxiety.
        # Compute mean normalized entropy over actions as uncertainty U in [0,1].
        # Entropy H(p) normalized by log(2): Hn = -sum p log p / log2
        # Avoid log(0) with small epsilon.
        Hs = []
        for a in range(2):
            pa = T[a]
            pa = np.clip(pa, 1e-6, 1.0)
            Ha = -(pa[0]*np.log(pa[0]) + pa[1]*np.log(pa[1])) / np.log(2.0)
            Hs.append(Ha)
        U = max(0.0, min(1.0, 0.5*(Hs[0] + Hs[1])))

        w_eff = zeta + psi * (st - 0.5) * U
        w_eff = max(0.0, min(1.0, w_eff))

        # Hybrid Q for stage 1
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # First-stage policy
        pref1 = Q1 - np.max(Q1)
        probs1 = np.exp(beta * pref1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Second-stage policy (pure MF at stage 2)
        s = int(state[t])
        pref2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta * pref2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = float(reward[t])

        # Update transition model T using observed transition (a1 -> s)
        # Simple delta rule toward 1 for observed state and 0 for the other, with normalization.
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T * (target - T[a1, sp])
        # Ensure row-stochastic within numerical tolerance
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Stage-2 MF update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_mf * pe2

        # Stage-1 MF update (bootstrapped from the realized second-stage value)
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_mf * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return neg_ll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning via volatility with anxiety-coupled gain and stickiness.

    A model-based planner selects first-stage actions using the known task structure,
    while second-stage values are learned by a volatility-adaptive TD rule. The learning
    rate at stage 2 increases with estimated volatility (tracked from squared PE), and
    anxiety scales the volatility gain. Anxiety also modulates a mild second-stage
    perseveration bias.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; scales volatility sensitivity and stickiness.
    model_parameters : list or array-like of 5 floats
        [alpha0, kappa_vol, beta, tau, chi]
        - alpha0 (0..1): base learning rate for Q updates.
        - kappa_vol (0..1): volatility update rate from squared PEs.
        - beta (0..10): inverse temperature for both stages.
        - tau (0..1): baseline second-stage perseveration strength.
        - chi (0..1): anxiety modulation strength applied to
                      both volatility gain and perseveration.
                      Effective learning rate: alpha_t = clip(alpha0 + chi*(stai-0.5)*sqrt(v)),
                      Effective stickiness: rho = tau * (1 + chi*(stai-0.5)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, kappa_vol, beta, tau, chi = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Known task transition structure for MB planning
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)  # will be used to propagate MF prediction to stage 1 (optional bootstrapping)
    v = np.zeros((2, 2))  # volatility proxy per state-action (squared PE EWMA)

    p1 = np.zeros(n)
    p2 = np.zeros(n)
    eps = 1e-10

    # Perseveration memory for stage 2 per state
    last_a2 = np.array([None, None], dtype=object)
    rho = tau * (1.0 + chi * (st - 0.5))

    for t in range(n):
        # Model-based Q1 from known transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_known @ max_Q2

        # First-stage softmax
        pref1 = Q1_mb - np.max(Q1_mb)
        probs1 = np.exp(beta * pref1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Second-stage softmax with stickiness
        s = int(state[t])
        pref2 = Q2[s].copy()
        if last_a2[s] is not None:
            stick_vec = np.array([0.0, 0.0])
            stick_vec[int(last_a2[s])] = 1.0
            pref2 = pref2 + rho * stick_vec
        pref2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * pref2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        # Outcomes
        r = float(reward[t])

        # Volatility-adaptive learning rate at stage 2
        pe2 = r - Q2[s, a2]
        # Update volatility proxy (EWMA of squared PEs)
        v[s, a2] = (1.0 - kappa_vol) * v[s, a2] + kappa_vol * (pe2 * pe2)
        # Anxiety scales learning gain via sqrt volatility to keep in [0,1] range
        alpha_t = alpha0 + chi * (st - 0.5) * np.sqrt(max(0.0, v[s, a2]))
        alpha_t = max(0.0, min(1.0, alpha_t))

        # Update Q2
        Q2[s, a2] += alpha_t * pe2

        # Optional MF bootstrapping at stage 1 to capture some MF variance in choices
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha0 * pe1  # uses base rate; MB policy dominates at stage 1

        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return neg_ll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MF with outcome-by-transition bias at stage 1 and anxiety-tuned temperatures.

    A model-free learner updates action values at both stages. Stage-1 choices additionally
    include a model-based-like bias that depends on whether the previous transition was
    common vs rare and whether it was rewarded (classic stay/switch interaction).
    Anxiety tunes the strength of this bias and also modulates separate inverse temperatures
    at stage 1 and stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Second-stage states (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Reward on each trial.
    stai : array-like with one float in [0,1]
        Anxiety score; scales bias strength and exploration temperatures.
    model_parameters : list or array-like of 5 floats
        [alpha, beta1, beta2, theta, mu]
        - alpha (0..1): learning rate for MF Q-values at both stages.
        - beta1 (0..10): base inverse temperature at stage 1.
        - beta2 (0..10): base inverse temperature at stage 2.
        - theta (0..1): baseline strength of the outcome-by-transition stay/switch bias.
        - mu (0..1): anxiety modulation strength.
          Effective bias: b = theta * (1 + mu*(stai-0.5)).
          Effective temperatures:
            beta1_eff = beta1 * (1 + mu*(stai-0.5))
            beta2_eff = beta2 * (1 - mu*(stai-0.5))

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, theta, mu = model_parameters
    n = len(action_1)
    st = float(stai[0])

    beta1_eff = beta1 * (1.0 + mu * (st - 0.5))
    beta2_eff = beta2 * (1.0 - mu * (st - 0.5))
    # Clip to valid ranges to be safe
    beta1_eff = max(0.0, min(10.0, beta1_eff))
    beta2_eff = max(0.0, min(10.0, beta2_eff))

    b_eff = theta * (1.0 + mu * (st - 0.5))

    # MF Q-values
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p1 = np.zeros(n)
    p2 = np.zeros(n)
    eps = 1e-10

    # Memory of previous trial to compute bias
    last_a1 = None
    last_state = None
    last_reward = None

    for t in range(n):
        # Stage-1 preferences from MF plus outcome-by-transition bias toward stay/switch
        pref1 = Q1.copy()

        if last_a1 is not None:
            # Determine whether previous transition was common vs rare based on task structure:
            # A commonly -> X; U commonly -> Y.
            was_common = ((last_a1 == 0 and last_state == 0) or (last_a1 == 1 and last_state == 1))
            c = 1.0 if was_common else -1.0
            rsgn = 1.0 if float(last_reward) > 0.5 else -1.0
            bias_val = b_eff * c * rsgn
            # Apply symmetric bias to prefer staying vs switching
            bias_vec = np.array([0.0, 0.0])
            bias_vec[int(last_a1)] += bias_val
            bias_vec[1 - int(last_a1)] -= bias_val
            pref1 = pref1 + bias_vec

        # Stage-1 policy
        pref1_c = pref1 - np.max(pref1)
        probs1 = np.exp(beta1_eff * pref1_c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Stage-2 policy (MF) without additional bias
        s = int(state[t])
        pref2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta2_eff * pref2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = float(reward[t])

        # MF updates
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        pe1 = Q2[s, a2] - Q1[a1]
        Q1[a1] += alpha * pe1

        # Update memory for next-trial bias
        last_a1 = a1
        last_state = s
        last_reward = r

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return neg_ll