def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MF/MB with anxiety-modulated uncertainty arbitration and directed exploration.
    The agent learns second-stage action values model-free and uses a learned transition
    model to compute a model-based (MB) first-stage value. Arbitration between model-free
    (MF) and MB depends on the transition uncertainty of the chosen spaceship, and
    anxiety increases the impact of uncertainty on arbitration. Additionally, a directed
    exploration bonus at stage 2 depends on visit counts and is amplified by anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for both Q2 and transition estimates
        beta: [0,10] — inverse temperature for choices at both stages
        w_base: [0,1] — baseline weight on model-based values at stage 1
        k_unc: [0,1] — how strongly transition uncertainty shifts weight toward MB
        bonus0: [0,1] — base directed-exploration bonus at stage 2
        
    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, w_base, k_unc, bonus0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model T[a, s], start neutral (uninformative)
    T = np.full((2, 2), 0.5, dtype=float)
    # Second-stage model-free Q-values
    Q2 = np.zeros((2, 2), dtype=float)
    # First-stage model-free Q-values (SARSA-style bootstrapping from Q2)
    Q1_mf = np.zeros(2, dtype=float)
    # Visit counts for directed exploration bonus
    n_visits = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    def entropy_row(p_row):
        p = np.clip(p_row, eps, 1.0)
        return -np.sum(p * np.log(p))

    for t in range(n_trials):
        # Compute MB first-stage Q via current transition estimates and stage-2 values (greedy within state)
        max_Q2 = np.max(Q2, axis=1)  # V(s) = max_a Q2[s,a]
        Q1_mb = T @ max_Q2

        # Uncertainty of the spaceship to be chosen: use both to compute policy; arbitration uses each action's row entropy
        unc_a0 = entropy_row(T[0])
        unc_a1 = entropy_row(T[1])

        # Convert uncertainty to per-action MB weights (bounded in [0,1])
        w0 = np.clip(w_base + k_unc * (1.0 + stai_val) * (unc_a0 / np.log(2.0)), 0.0, 1.0)
        w1 = np.clip(w_base + k_unc * (1.0 + stai_val) * (unc_a1 / np.log(2.0)), 0.0, 1.0)
        # Hybrid first-stage Q-values
        Q1_hybrid = np.array([ (1 - w0) * Q1_mf[0] + w0 * Q1_mb[0],
                               (1 - w1) * Q1_mf[1] + w1 * Q1_mb[1] ])

        # First-stage choice probabilities
        logits1 = Q1_hybrid - np.max(Q1_hybrid)
        probs1 = np.exp(beta * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage with directed exploration bonus
        s = state[t]
        # Count-based bonus scaled by anxiety (more anxious => larger directed exploration)
        bonus_scale = bonus0 * (1.0 + stai_val)
        bonus = bonus_scale / np.sqrt(n_visits[s] + 1.0)
        logits2 = (Q2[s] + bonus)
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage 2 MF update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        n_visits[s, a2] += 1.0

        # Stage 1 MF update (bootstraps from realized second-stage action value)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Transition model update toward observed state for chosen action
        # Move row T[a1] toward one-hot of observed state s
        target_T = np.array([0.0, 0.0])
        target_T[s] = 1.0
        T[a1] = (1.0 - alpha) * T[a1] + alpha * target_T

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Utility-based model with anxiety-amplified loss aversion, pruned model-based planning, and Pavlovian approach/avoid.
    The agent learns second-stage utilities with asymmetric valuation (loss aversion).
    First-stage choices are purely model-based but use a pruning transform that
    overweights common transitions; pruning grows with anxiety. Pavlovian approach/avoid
    biases at stage 2 push toward options with higher expected utility and away from
    lower ones, scaled by anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        eta: [0,1] — learning rate for second-stage utilities
        beta: [0,10] — inverse temperature for both stages
        lambda_loss: [0,1] — base loss aversion (transformed internally)
        pav_bias: [0,1] — strength of Pavlovian approach/avoid bias at stage 2
        anx_gain: [0,1] — how strongly anxiety increases loss aversion and pruning

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    eta, beta, lambda_loss, pav_bias, anx_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (common 0.7, rare 0.3)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Transform loss aversion to >1 range and modulate by anxiety
    lam = 1.0 + 4.0 * lambda_loss
    lam_eff = lam * (1.0 + anx_gain * stai_val)

    # Second-stage expected utilities
    U2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # First-stage model-based planning with pruning (overweight common transitions)
        prune = np.clip(anx_gain * stai_val, 0.0, 1.0)  # 0..1
        power = 1.0 + 2.0 * prune  # raise probs to >1 to accentuate common
        # State values as max over second-stage utilities
        V = np.max(U2, axis=1)
        Q1_mb = np.zeros(2, dtype=float)
        for a in range(2):
            p = T_fixed[a].copy()
            p_trans = p**power
            p_trans /= (np.sum(p_trans) + eps)
            Q1_mb[a] = np.dot(p_trans, V)

        logits1 = Q1_mb - np.max(Q1_mb)
        probs1 = np.exp(beta * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice with Pavlovian bias toward higher-utility option
        s = state[t]
        centered = U2[s] - np.mean(U2[s])
        pav_strength = pav_bias * (1.0 + anx_gain * stai_val)
        logits2 = U2[s] + pav_strength * centered
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome utility with loss aversion
        r = reward[t]
        # Treat r=1 as gain 1, r=0 as loss magnitude lam_eff (relative to 0 baseline)
        u = 1.0 if r >= 0.5 else -lam_eff

        # Update second-stage utilities (simple delta rule)
        pe2 = u - U2[s, a2]
        U2[s, a2] += eta * pe2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated volatility-sensitive temperature, forgetting, and transition-sensitive stickiness.
    The agent is model-free at both stages. A running estimate of reward volatility
    (absolute prediction error) adjusts the softmax temperature: higher volatility
    and anxiety reduce beta (more exploration). Values decay toward 0.5 via forgetting,
    and a stage-1 stickiness bias encourages repeating the previous first-stage choice,
    but this stickiness is weakened after surprising (rare) transitions, especially
    under higher anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for Q-values and volatility running average
        beta_mu: [0,10] — baseline inverse temperature
        rho_forget: [0,1] — forgetting rate toward 0.5 each trial
        stick0: [0,1] — baseline first-stage stickiness strength
        anx_vol: [0,1] — how strongly anxiety and volatility reduce beta and stickiness

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta_mu, rho_forget, stick0, anx_vol = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transitions for surprise computation
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Model-free Q-values
    Q1 = np.zeros(2, dtype=float)
    Q2 = np.zeros((2, 2), dtype=float)

    # Volatility estimate (running abs PE at stage 2)
    vol = 0.0

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    last_a1 = None
    last_surprise = 0.0  # from previous trial's transition

    eps = 1e-12

    for t in range(n_trials):
        # Temperature adjusted by volatility and anxiety
        beta_t = beta_mu / (1.0 + anx_vol * stai_val * vol)
        beta_t = max(beta_t, eps)

        # Stage-1 stickiness modulated by previous transition surprise and anxiety
        stick_strength = stick0 * (1.0 - anx_vol * stai_val * last_surprise)
        stick_strength = np.clip(stick_strength, 0.0, 1.0)

        logits1 = Q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += stick_strength

        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta_t * logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        s = state[t]
        logits2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta_t * logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        # Update volatility estimate with absolute PE
        vol = (1.0 - alpha) * vol + alpha * abs(pe2)

        # Stage-1 update (bootstraps from realized Q2)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1[a1]
        Q1[a1] += alpha * pe1

        # Forgetting toward 0.5
        Q1 = (1.0 - rho_forget) * Q1 + rho_forget * 0.5
        Q2 = (1.0 - rho_forget) * Q2 + rho_forget * 0.5

        # Update for next trial: last choice and surprise based on transition rarity
        # Surprise = 1 - P(s | a1) under fixed transition matrix
        last_a1 = a1
        last_surprise = 1.0 - T_fixed[a1, s]

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)