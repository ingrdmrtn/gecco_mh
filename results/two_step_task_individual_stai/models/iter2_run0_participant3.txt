def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-weighted predictability preference.
    
    Core ideas:
    - Learn the first-stage transition matrix online (simple delta rule).
    - Compute model-based (MB) first-stage values from learned transitions and current stage-2 values.
    - Mix MB and model-free (MF) values at stage 1 with a fixed arbitration weight.
    - Anxiety increases preference for predictable actions at stage 1 by penalizing actions
      whose transition distributions are less peaked (higher expected surprise).
    
    Parameters (all arrays are 1D of equal length n_trials):
    - action_1: int array of {0,1}, first-stage choices (spaceships A=0, U=1).
    - state:    int array of {0,1}, reached second-stage state (planets X=0, Y=1).
    - action_2: int array of {0,1}, second-stage choices (aliens W/P=0, S/H=1 depending on planet).
    - reward:   float array in [0,1], reward outcome (gold coins).
    - stai:     float array with a single element in [0,1], anxiety score.
    - model_parameters: 5-tuple/list of:
        eta  in [0,1]: learning rate for stage-2 Q updates (MF) and MF stage-1 bootstrap
        beta in [0,10]: inverse temperature for both stages
        pi   in [0,1]: arbitration weight on MB values at stage 1 (1=MB, 0=MF)
        phi  in [0,1]: learning rate for transition matrix updates
        nu   in [0,1]: strength of anxiety-driven predictability preference at stage 1
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    eta, beta, pi, phi, nu = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition matrix; start with weak prior near the task structure
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    # Value tables
    q2 = np.zeros((2, 2), dtype=float)   # stage-2 Q(s,a)
    q1_mf = np.zeros(2, dtype=float)     # stage-1 MF values

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage values from learned transitions and current q2
        max_q2 = np.max(q2, axis=1)  # best second-stage action per state
        q1_mb = T @ max_q2

        # Predictability (lower surprise) bonus: penalize actions with flatter transition distributions
        # Expected surprise per action = 1 - max(T[a])
        expected_surprise = 1.0 - np.max(T, axis=1)
        predictability_penalty = nu * s_anx * expected_surprise  # higher anxiety -> stronger penalty

        # Arbitration between MB and MF for stage 1
        q1 = pi * q1_mb + (1.0 - pi) * q1_mf
        logits1 = beta * q1 - predictability_penalty  # subtract penalty
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s_idx = int(state[t])
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 MF update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Stage-1 MF bootstrap toward realized second-stage action value
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * delta1

        # Transition learning update for the chosen action toward the observed state
        # Move T[a1, :] toward one-hot of s_idx
        for s in (0, 1):
            targ = 1.0 if s == s_idx else 0.0
            T[a1, s] += phi * (targ - T[a1, s])

        # Renormalize to guard against numerical drift
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-based control with anxiety-weighted uncertainty aversion.
    
    Core ideas:
    - Track both mean and uncertainty (EWMA of squared prediction errors) for each stage-2 option.
    - Transform second-stage values into a risk-sensitive utility: U = mean - rho * sqrt(uncertainty),
      where rho increases with anxiety.
    - Plan model-based at stage 1 using the fixed transition structure and the current utilities.
    - Softmax policies at both stages use utilities (stage 2) and MB-computed values (stage 1).
    
    Parameters:
    - action_1: int array {0,1}, first-stage choices.
    - state:    int array {0,1}, reached second-stage state.
    - action_2: int array {0,1}, second-stage choices.
    - reward:   float array [0,1], outcomes.
    - stai:     float array with single element in [0,1], anxiety score.
    - model_parameters: 5-tuple/list of:
        alpha_m in [0,1]: learning rate for updating means at stage 2
        alpha_v in [0,1]: learning rate for updating uncertainty (variance proxy) at stage 2
        beta    in [0,10]: inverse temperature for both stages
        zeta    in [0,1]: baseline uncertainty aversion (risk penalty weight)
        kappa_r in [0,1]: strength of anxiety modulation of uncertainty aversion
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    alpha_m, alpha_v, beta, zeta, kappa_r = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed task transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Track mean and variance proxy for each second-stage option
    m = np.zeros((2, 2), dtype=float)   # means
    v = np.zeros((2, 2), dtype=float)   # variance proxy (EWMA of squared PEs)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    def utility(m_s, v_s, rho):
        # Risk-sensitive utility: mean minus rho * sqrt(uncertainty)
        # Ensure non-negative v for sqrt; add tiny floor
        return m_s - rho * np.sqrt(np.maximum(v_s, 0.0) + 1e-12)

    for t in range(n_trials):
        # Anxiety-weighted uncertainty aversion
        rho = zeta + kappa_r * s_anx
        # Compute utilities at stage 2
        U = np.zeros((2, 2), dtype=float)
        for s in (0, 1):
            U[s] = utility(m[s], v[s], rho)

        # Stage-1 MB evaluation using transition matrix and max utility in each state
        maxU = np.max(U, axis=1)
        q1_mb = T @ maxU

        # First-stage policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy based on utilities in the reached state
        s_idx = int(state[t])
        logits2 = beta * U[s_idx]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update mean and uncertainty for the chosen second-stage action
        pe = r - m[s_idx, a2]
        m[s_idx, a2] += alpha_m * pe
        v[s_idx, a2] += alpha_v * (pe**2 - v[s_idx, a2])

        # Optional: small leakage to keep v finite for unchosen actions (stability)
        # Here we do not add more parameters; let unchosen entries evolve only when visited.

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Arbitrated MF/MB with anxiety-amplified forgetting and rare-transition credit.
    
    Core ideas:
    - Stage-2 MF learning with forgetting toward a neutral prior for unvisited/unchosen options.
    - Stage-1 arbitration between MB and MF values; anxiety reduces the MB weight
      and increases forgetting.
    - After rare transitions, propagate a larger portion of the stage-2 PE to stage-1 MF,
      scaled by anxiety (biased credit assignment).
    
    Parameters:
    - action_1: int array {0,1}, first-stage choices.
    - state:    int array {0,1}, reached second-stage state.
    - action_2: int array {0,1}, second-stage choices.
    - reward:   float array [0,1], outcomes.
    - stai:     float array with single element in [0,1], anxiety score.
    - model_parameters: 5-tuple/list of:
        eta  in [0,1]: learning rate for stage-2 MF and stage-1 MF bootstrap
        beta in [0,10]: inverse temperature for both stages
        pi   in [0,1]: baseline MB weight at stage 1 (before anxiety modulation)
        f0   in [0,1]: baseline forgetting rate toward 0.5 for unchosen/uncharted Q-values
        b    in [0,1]: anxiety coupling factor for both forgetting and rare-transition credit
    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    eta, beta, pi, f0, b = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed task transitions (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.full((2, 2), 0.5, dtype=float)  # initialize around neutral prior
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Stage-1 MB evaluation from fixed transitions and current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Anxiety reduces MB weight
        w = pi * (1.0 - 0.5 * b * s_anx)
        w = np.clip(w, 0.0, 1.0)
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # First-stage policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s_idx = int(state[t])
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Apply anxiety-amplified forgetting toward 0.5 for all other entries
        f_eff = np.clip(f0 * (1.0 + b * s_anx), 0.0, 1.0)
        # Decay unchosen action in the visited state
        other_a = 1 - a2
        q2[s_idx, other_a] = (1.0 - f_eff) * q2[s_idx, other_a] + f_eff * 0.5
        # Decay both actions in the unvisited state
        other_s = 1 - s_idx
        q2[other_s, 0] = (1.0 - f_eff) * q2[other_s, 0] + f_eff * 0.5
        q2[other_s, 1] = (1.0 - f_eff) * q2[other_s, 1] + f_eff * 0.5

        # Stage-1 MF update with rare-transition credit amplification
        # Compute rarity of the observed transition given chosen action
        p_trans = T[a1, s_idx]  # probability of observed transition under fixed structure
        rarity = 1.0 - p_trans  # 0.3 for rare, 0.0 for impossible (not applicable), 0.3 rare, 0.7 common -> rarity small
        # Bootstrap update
        target1 = q2[s_idx, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * delta1
        # Additional credit proportional to stage-2 PE, rarity, and anxiety
        q1_mf[a1] += (b * s_anx) * rarity * eta * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll