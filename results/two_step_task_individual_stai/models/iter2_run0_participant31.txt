def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-modulated model-based control and transition volatility.

    This model learns second-stage Q-values and also learns the first-stage transition probabilities online.
    The first-stage policy is a mixture of model-free and model-based values, with the mixing weight
    increased or decreased by the participant's anxiety (stai). Anxiety also increases the effective
    transition learning rate (perceived environmental volatility), making transition beliefs update faster.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices on the encountered planet (0/1; W/S on X, P/H on Y).
    reward : array-like of float
        Rewards obtained on each trial, typically in [0,1].
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values reduce MB control and increase transition volatility.
    model_parameters : array-like of floats, length 5
        [alpha, beta, omega0, k_anx_omega, eta_tr]
        - alpha in [0,1]: learning rate for Q-values (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline weight of model-based control at stage 1.
        - k_anx_omega in [0,1]: strength by which anxiety modulates omega (MB weight).
                                 omega_eff = clip(omega0 + k_anx_omega*(stai - 0.51), 0, 1).
        - eta_tr in [0,1]: baseline learning rate for transition probabilities T(a->s).
                           Anxiety increases it: eta_eff = clip(eta_tr * (1 + (stai - 0.51)), 0, 1).

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, omega0, k_anx_omega, eta_tr = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities T[a, s]
    # Start from the canonical common/rare pattern but allow learning.
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)       # model-free first-stage Q
    q2 = np.zeros((2, 2))     # second-stage Q for each state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    omega_eff = omega0 + k_anx_omega * (stai - 0.51)
    omega_eff = 0.0 if omega_eff < 0.0 else (1.0 if omega_eff > 1.0 else omega_eff)

    eta_eff = eta_tr * (1.0 + (stai - 0.51))
    eta_eff = 0.0 if eta_eff < 0.0 else (1.0 if eta_eff > 1.0 else eta_eff)

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for t in range(n_trials):
        # Compute model-based Q at stage 1 using current learned transitions
        max_q2 = np.max(q2, axis=1)          # value of each second-stage state
        q1_mb = T @ max_q2                   # plan via transitions

        # Mixture of MB and MF values
        q1_mix = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # First-stage policy
        logits1 = beta_eff * q1_mix
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy conditional on observed state
        s = state[t]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # TD updates
        # Stage-1 MF update bootstraps on the chosen stage-2 action value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Stage-2 update from reward
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learn transition probabilities T[a1] toward the observed state 's'
        # Simple exponential recency-weighted update toward one-hot(s)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - eta_eff) * T[a1] + eta_eff * target
        # Keep rows normalized (they should remain normalized by construction, but renormalize for safety)
        T[a1] = T[a1] / (np.sum(T[a1]) + 1e-12)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-free SARSA with second-stage perseveration; anxiety increases outcome curvature.

    Rewards are transformed through a risk-/loss-sensitive utility function centered at 0.5.
    Higher anxiety increases the curvature (greater sensitivity near losses), which changes learning signals.
    First-stage values are updated by propagating the stage-2 utility prediction error (one-step SARSA back-up).
    A perseveration bias at the second stage captures stickiness of the alien choice within each planet.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1).
    reward : array-like of float
        Obtained rewards in [0,1].
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher stai increases outcome curvature phi.
    model_parameters : array-like of floats, length 5
        [alpha, beta, phi0, k_anx_phi, pi2]
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - phi0 in [0,1]: baseline curvature of utility transformation.
        - k_anx_phi in [0,1]: how strongly anxiety increases curvature.
                              phi_eff = clip(phi0 + k_anx_phi * stai, 0, 1).
        - pi2 in [0,1]: second-stage perseveration strength (stickiness) per state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, phi0, k_anx_phi, pi2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective parameters
    phi_eff = phi0 + k_anx_phi * stai
    phi_eff = 0.0 if phi_eff < 0.0 else (1.0 if phi_eff > 1.0 else phi_eff)

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Second-stage perseveration: maintain last chosen action per state
    last_a2 = [None, None]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage policy (no perseveration here)
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy with within-state perseveration bias
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] = pi2
        logits2 = beta_eff * q2[s] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Risk-/loss-sensitive utility transformation centered at 0.5
        r = reward[t]
        x = r - 0.5
        # Curvature: more curvature (higher phi) down-weights large magnitudes,
        # signs preserve gain vs loss relative to 0.5.
        util = np.sign(x) * (np.abs(x) ** (1.0 + phi_eff))

        # TD updates with utility as outcome
        delta2 = util - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Back up to stage-1 value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with first-stage choice kernel and anxiety-weighted surprise gating of MB control.

    The first-stage policy mixes model-based and model-free values, plus a choice kernel (habitual
    propensity independent of value). On trials with surprising transitions (rare given the fixed
    0.7/0.3 structure), model-based control is transiently down-weighted. This down-weighting is stronger
    for higher anxiety, capturing reduced reliance on planning under surprise.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases surprise-based down-weighting of MB control.
    model_parameters : array-like of floats, length 5
        [alpha, beta, kappa, k_anx_surprise, omega]
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa in [0,1]: learning rate for first-stage choice kernel (propensities).
        - k_anx_surprise in [0,1]: scales how much anxiety gates MB weight after surprising transitions.
                                   omega_t = omega * (1 - k_anx_surprise * stai * surprise_t).
        - omega in [0,1]: baseline MB weight in the value mixture at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, kappa, k_anx_surprise, omega = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for surprise detection (common=0.7)
    # A -> X common, U -> Y common
    common_prob = 0.7

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernel (tendency to repeat first-stage actions)
    K = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Determine whether the observed transition was rare (surprising)
        # A(0) common->X(0); U(1) common->Y(1)
        if a1 == 0:
            p_trans = common_prob if s == 0 else (1 - common_prob)
        else:
            p_trans = common_prob if s == 1 else (1 - common_prob)
        surprise = 1.0 if p_trans < 0.5 else 0.0  # rare when probability < 0.5

        # Compute MB Q from fixed transition model
        max_q2 = np.max(q2, axis=1)
        q1_mb = np.array([
            common_prob * max_q2[0] + (1 - common_prob) * max_q2[1],  # A
            (1 - common_prob) * max_q2[0] + common_prob * max_q2[1]   # U
        ])

        # Anxiety-weighted gating of MB contribution on surprising trials
        omega_t = omega * (1.0 - k_anx_surprise * stai * surprise)
        omega_t = 0.0 if omega_t < 0.0 else (1.0 if omega_t > 1.0 else omega_t)

        # Mixed first-stage value plus choice kernel bias
        q1_mix = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        logits1 = beta_eff * q1_mix + K
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        p_choice_1[t] = p1[a1]

        # Second-stage choice policy
        a2 = action_2[t]
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # TD updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update choice kernel toward the chosen first-stage action
        K = (1.0 - kappa) * K
        K[a1] += kappa

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll