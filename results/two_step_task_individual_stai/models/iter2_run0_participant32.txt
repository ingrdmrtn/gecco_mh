def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 1: Anxiety-dampened structure use with lapse and perseveration (pure model-based planning).
    
    Idea
    ----
    The agent plans using the known two-step structure, but anxiety (stai) reduces reliance
    on the common/rare structure by shrinking the assumed common-transition probability toward 0.5.
    Additionally, there is a lapse rate (random choice mixture) and choice perseveration at both
    stages that increases with anxiety.
    
    Parameters (all used)
    ---------------------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=Planet X, 1=Planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices for the reached state (0 or 1) per trial.
    reward : array-like of float
        Obtained reward on each trial, typically in [0,1].
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list-like of float
        [alpha, beta, delta, phi, eps]
        Bounds:
        - alpha in [0,1]: learning rate for second-stage Q-values
        - beta in [0,10]: softmax inverse temperature
        - delta in [0,1]: anxiety sensitivity that flattens the transition structure
        - phi in [0,1]: choice perseveration strength
        - eps in [0,1]: lapse rate (mix with uniform choice)
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, delta, phi, eps = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Anxiety-modulated transition matrix: common prob moves toward 0.5 as st increases
    p_common = 0.7 * (1.0 - delta * st) + 0.5 * (delta * st)
    p_common = float(np.clip(p_common, 0.5, 0.7))
    transition_matrix = np.array([[p_common, 1.0 - p_common],
                                  [1.0 - p_common, p_common]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage Q-values for each planet (rows) and action (cols)
    q2 = np.zeros((2, 2))

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Model-based values for stage 1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Perseveration terms (increase with anxiety)
        stick1 = phi * st
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0

        q1_policy = q1_mb + stick1 * bias1

        # Softmax with lapse
        q1c = q1_policy - np.max(q1_policy)
        probs_1_soft = np.exp(beta * q1c)
        probs_1_soft = probs_1_soft / np.sum(probs_1_soft)
        probs_1 = (1.0 - eps) * probs_1_soft + eps * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in reached state with perseveration
        s = state[t]
        q2_state = q2[s].copy()
        stick2 = phi * st
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] = 1.0

        q2_policy = q2_state + stick2 * bias2
        q2c = q2_policy - np.max(q2_policy)
        probs_2_soft = np.exp(beta * q2c)
        probs_2_soft = probs_2_soft / np.sum(probs_2_soft)
        probs_2 = (1.0 - eps) * probs_2_soft + eps * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps_num = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return float(neg_log_lik)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 2: Directed exploration via uncertainty bonus with leaky counts and MF fusion.
    
    Idea
    ----
    The agent maintains leaky Beta-Bernoulli counts for each second-stage action to estimate
    mean reward and its uncertainty, and uses an Upper-Confidence-Bound-like bonus to explore.
    Anxiety reduces directed exploration. A model-free (MF) value is also learned and fused
    with the uncertainty-augmented estimate. First-stage choices are model-based by propagating
    second-stage values through the fixed transition structure. Perseveration is included.
    
    Parameters (all used)
    ---------------------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=Planet X, 1=Planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choice per trial.
    reward : array-like of float
        Reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list-like of float
        [alpha, beta, c_bonus, pers, decay]
        Bounds:
        - alpha in [0,1]: mixing weight between MF and uncertainty-based estimates (also MF learning rate)
        - beta in [0,10]: inverse temperature
        - c_bonus in [0,1]: strength of directed exploration bonus
        - pers in [0,1]: choice perseveration strength
        - decay in [0,1]: leak factor for Beta counts (higher -> more forgetting); interacts with stai
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, c_bonus, pers, decay = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Beta-Bernoulli pseudo-counts for each state-action: successes a, failures b
    a_counts = np.ones((2, 2))  # prior a=1
    b_counts = np.ones((2, 2))  # prior b=1

    # Model-free Q for second-stage
    q2_mf = np.zeros((2, 2))

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Compute mean and uncertainty (variance) for each state-action
        n_sa = a_counts + b_counts
        mu = a_counts / n_sa
        var = (mu * (1.0 - mu)) / (n_sa + 1.0)

        # Anxiety-dampened exploration bonus
        bonus_scale = c_bonus * (1.0 - st)
        q2_unc = mu + bonus_scale * np.sqrt(var)

        # Fuse uncertainty-based estimate with MF using alpha as mixing weight
        q2_fused = (1.0 - alpha) * q2_unc + alpha * q2_mf

        # First-stage MB values via transition
        max_q2 = np.max(q2_fused, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Perseveration terms scale with anxiety
        stick1 = pers * (0.5 + 0.5 * st)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        q1_policy = q1_mb + stick1 * bias1

        q1c = q1_policy - np.max(q1_policy)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second stage policy in reached state with perseveration
        s = state[t]
        q2_state = q2_fused[s].copy()
        stick2 = pers * (0.5 + 0.5 * st)
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] = 1.0
        q2_policy = q2_state + stick2 * bias2

        q2c = q2_policy - np.max(q2_policy)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update leaky counts and MF Q
        r = reward[t]

        # Leaky counts: decay toward prior with anxiety-dependent forgetting
        leak = 1.0 - decay * (0.5 + 0.5 * st)
        a_counts *= leak
        b_counts *= leak
        # Add new observation to reached state-action
        a_counts[s, a2] += r
        b_counts[s, a2] += (1.0 - r)

        # MF update
        q2_mf[s, a2] += alpha * (r - q2_mf[s, a2])

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps_num = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return float(neg_log_lik)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model 3: Surprise-gated learning and arbitration with anxiety modulation and forgetting.
    
    Idea
    ----
    Learning rates adapt to transition surprise: rare transitions boost learning.
    Anxiety amplifies this surprise gating (more reactive to surprising transitions).
    Stage-1 policy blends model-based and model-free values with an anxiety-sensitive weight.
    Additionally, there is anxiety-scaled forgetting of unchosen values.
    
    Parameters (all used)
    ---------------------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int (0 or 1)
        Second-stage state reached (0=Planet X, 1=Planet Y) per trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial.
    reward : array-like of float
        Reward outcome on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list-like of float
        [alpha_base, beta, eta_surprise, rho_bias, kappa_forget]
        Bounds:
        - alpha_base in [0,1]: baseline learning rate
        - beta in [0,10]: inverse temperature
        - eta_surprise in [0,1]: strength of surprise-gated learning-rate boost
        - rho_bias in [0,1]: modulates MB arbitration and transition-credit asymmetry
        - kappa_forget in [0,1]: forgetting strength for unchosen values (scales with anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha_base, beta, eta_surprise, rho_bias, kappa_forget = model_parameters
    n_trials = len(action_1)
    st = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free first-stage and second-stage values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Compute MB first-stage value from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Anxiety-weighted arbitration toward MB
        w_mb = 0.5 + 0.5 * (1.0 - st) * rho_bias
        q1_val = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        q1c = q1_val - np.max(q1_val)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state
        s = state[t]
        q2_state = q2[s].copy()
        q2c = q2_state - np.max(q2_state)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Determine transition surprise (relative to chosen a1 and reached s)
        # Compute probability of reached state given chosen action under fixed transitions
        p_to_s = transition_matrix[a1, s]
        surprise = 1.0 - p_to_s  # 0.3 for rare, 0.7 for common -> larger for rarer transitions

        # Surprise-gated learning rate (amplified by anxiety)
        alpha_t = alpha_base + eta_surprise * surprise * (0.5 + 0.5 * st)
        alpha_t = float(np.clip(alpha_t, 0.0, 1.0))

        # Stage 2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        # Stage 1 MF update with eligibility-like and transition-asymmetry component
        # TD toward updated second-stage value
        td_to_q2 = q2[s, a2] - q1_mf[a1]

        # Transition asymmetry term: credit assignment differs for common vs rare
        is_common = 1.0 if p_to_s >= 0.5 else 0.0
        trans_factor = (1.0 if is_common > 0.5 else -1.0)  # +1 common, -1 rare
        asym_term = alpha_t * rho_bias * (1.0 - st) * trans_factor * delta2

        q1_mf[a1] += alpha_t * td_to_q2 + asym_term

        # Anxiety-scaled forgetting of unchosen values
        forget_rate = kappa_forget * (0.5 + 0.5 * st)
        if forget_rate > 0.0:
            # Decay all Q-values slightly toward zero, except the updated entries (to avoid double hit)
            # Stage 1: decay the unchosen action
            other_a1 = 1 - a1
            q1_mf[other_a1] *= (1.0 - 0.1 * forget_rate)
            # Stage 2: decay all except the updated state-action
            for ss in (0, 1):
                for aa in (0, 1):
                    if not (ss == s and aa == a2):
                        q2[ss, aa] *= (1.0 - 0.1 * forget_rate)

    eps_num = 1e-10
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return float(neg_log_lik)