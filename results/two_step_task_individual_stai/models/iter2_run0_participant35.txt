def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated hybrid with eligibility trace and lapse.

    This model blends model-based (MB) and model-free (MF) control at the first stage,
    uses an eligibility trace to propagate second-stage prediction errors back to the
    first stage, and includes an anxiety-dependent lapse rate and temperature.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) in the reached state
    - reward: array-like of floats in [0,1], reward outcome on each trial
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_r: base learning rate for second-stage MF values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        omega_base: base model-based weight for stage-1 in [0,1]
        lam_base: base eligibility trace from stage-2 to stage-1 in [0,1]
        eps_base: base lapse rate (random choice) in [0,1]

    Bounds
    - alpha_r, omega_base, lam_base, eps_base in [0,1]
    - beta in [0,10]

    Anxiety usage
    - MB weight decreases with anxiety: omega_eff = clip(omega_base * (1 - 0.6*stai))
    - Eligibility increases with anxiety: lambda_eff = clip(lam_base * (1 + 0.5*stai))
    - Lapse increases with anxiety: eps_eff = clip(eps_base * stai)
    - Temperature decreases with anxiety: beta_eff = beta * (1 - 0.5*stai)
    - Learning rate at stage-2 increases with anxiety: alpha2_eff = clip(alpha_r * (1 + 0.3*stai))
    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, omega_base, lam_base, eps_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)        # stage-1 MF values for actions A/U
    q2 = np.zeros((2, 2))      # stage-2 MF values for states X/Y and actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    omega_eff = min(1.0, max(0.0, omega_base * (1.0 - 0.6 * stai)))
    lambda_eff = min(1.0, max(0.0, lam_base * (1.0 + 0.5 * stai)))
    eps_eff = min(1.0, max(0.0, eps_base * stai))
    beta_eff = max(0.0, beta * (1.0 - 0.5 * stai))
    alpha2_eff = min(1.0, max(0.0, alpha_r * (1.0 + 0.3 * stai)))

    for t in range(n_trials):

        # Model-based first-stage action values from current second-stage values
        max_q2 = np.max(q2, axis=1)    # best value in each state
        q1_mb = T @ max_q2             # action values: expect next-state value via transition

        # Combine MB and MF for stage-1 decision
        q1_comb = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Stage-1 policy with softmax and lapse
        centered_q1 = q1_comb - np.max(q1_comb)
        logits1 = beta_eff * centered_q1
        exp1 = np.exp(logits1)
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1.0 - eps_eff) * soft1 + eps_eff * 0.5

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with softmax and lapse in reached state
        s = int(state[t])
        q2_s = q2[s].copy()
        centered_q2 = q2_s - np.max(q2_s)
        logits2 = beta_eff * centered_q2
        exp2 = np.exp(logits2)
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1.0 - eps_eff) * soft2 + eps_eff * 0.5

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2_eff * delta2

        # Propagate second-stage PE back to stage-1 via eligibility trace
        q1_mf[a1] += alpha2_eff * lambda_eff * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-driven first-stage bias with anxiety-amplified transition sensitivity.

    This model is primarily model-free for values but augments first-stage choice
    with a dynamic surprise bonus that tracks how surprising recent transitions were.
    Rare transitions produce a positive surprise signal; common transitions produce
    a negative signal. The bonus is action-specific and decays over time.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: learning rate for second-stage MF values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        eta_surprise: base magnitude of surprise bonus update in [0,1]
        gamma_decay: decay rate for surprise bonus in [0,1]
        xi_stick: base first-stage perseveration weight in [0,1]

    Bounds
    - alpha2, eta_surprise, gamma_decay, xi_stick in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Surprise sensitivity increases with anxiety: eta_eff = clip(eta_surprise * (1 + stai))
    - Perseveration increases with anxiety: xi_eff = xi_stick * (1 + 0.5*stai)
    - Temperature decreases with anxiety at stage-1: beta1 = beta * (1 - 0.4*stai); stage-2 uses beta unchanged
    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha2, beta, eta_surprise, gamma_decay, xi_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values (MF)
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Surprise bonus per first-stage action
    bonus = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eta_eff = min(1.0, max(0.0, eta_surprise * (1.0 + stai)))
    xi_eff = xi_stick * (1.0 + 0.5 * stai)
    beta1 = max(0.0, beta * (1.0 - 0.4 * stai))
    beta2 = beta

    prev_a1 = -1

    for t in range(n_trials):

        # First-stage policy: MF values + surprise bonus + perseveration
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += xi_eff

        q1_decision = q1_mf + bonus + bias1
        centered_q1 = q1_decision - np.max(q1_decision)
        probs1 = np.exp(beta1 * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: MF softmax
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta2 * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward learning at stage-2
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update first-stage MF towards second-stage chosen value (bootstrapped)
        target1 = q2[s, a2]
        q1_mf[a1] += alpha2 * (target1 - q1_mf[a1])

        # Transition surprise update for the chosen action
        # Determine whether the observed transition was rare (prob < 0.5) or common
        p_to_s = T[a1, s]
        is_rare = 1.0 if p_to_s < 0.5 else 0.0
        # Surprise signal: +1 for rare, -1 for common
        surprise_signal = 2.0 * is_rare - 1.0

        # Decay bonus for both actions
        bonus = (1.0 - gamma_decay) * bonus
        # Update bonus for chosen action toward surprise signal
        bonus[a1] += gamma_decay * eta_eff * surprise_signal

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning with anxiety-modulated loss aversion and planet preference.

    This model uses asymmetric learning rates for positive vs. negative prediction errors
    at the second stage (prospect-like), propagates value to the first stage via MF bootstrapping,
    and blends in a small model-based component whose weight is reduced by anxiety. An anxiety-
    modulated planet preference bias pushes choices toward the spaceship that is more likely
    to reach the currently preferred planet.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_base: base learning rate in [0,1] (split into pos/neg by anxiety and zeta)
        beta: inverse temperature for both stages in [0,10]
        zeta_loss: scales how much anxiety increases loss-weighted learning in [0,1]
        chi_pref: base magnitude of planet preference bias in [0,1]
        rho_mb: base model-based mixture weight in [0,1]

    Bounds
    - alpha_base, zeta_loss, chi_pref, rho_mb in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Asymmetric learning:
        alpha_pos = clip(alpha_base * (1 - 0.3*stai))
        alpha_neg = clip(alpha_base * (1 + zeta_loss*stai))
      Negative PEs are learned faster as anxiety and zeta_loss increase.
    - Model-based weight decreases with anxiety: w_mb = clip(rho_mb * (1 - 0.5*stai))
    - Planet preference bias increases with anxiety and targets a planet:
        pref_strength = chi_pref * (2*stai - 1)  (negative favors Y when stai<0.5, positive favors X when stai>0.5)
        state_desirability = [pref_strength, -pref_strength] for [X, Y]
        action_bias = T @ state_desirability added to stage-1 decision values.
    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_base, beta, zeta_loss, chi_pref, rho_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    alpha_pos = min(1.0, max(0.0, alpha_base * (1.0 - 0.3 * stai)))
    alpha_neg = min(1.0, max(0.0, alpha_base * (1.0 + zeta_loss * stai)))
    w_mb = min(1.0, max(0.0, rho_mb * (1.0 - 0.5 * stai)))

    # Planet preference bias mapped through transitions into action space
    pref_strength = chi_pref * (2.0 * stai - 1.0)   # [-chi_pref, +chi_pref]
    desirability = np.array([pref_strength, -pref_strength])  # [X, Y]
    action_pref_bias = T @ desirability  # bias for actions A/U

    for t in range(n_trials):

        # Model-based first-stage values from second-stage
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MF and MB and add planet preference bias
        q1_decision = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + action_pref_bias

        # Stage-1 policy
        centered_q1 = q1_decision - np.max(q1_decision)
        probs1 = np.exp(beta * centered_q1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * centered_q2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning with asymmetric rates at stage-2
        r = reward[t]
        pe2 = r - q2[s, a2]
        alpha2_eff = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha2_eff * pe2

        # Propagate updated second-stage value to stage-1 MF
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        alpha1_eff = alpha_pos if pe1 >= 0.0 else alpha_neg
        q1_mf[a1] += alpha1_eff * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll