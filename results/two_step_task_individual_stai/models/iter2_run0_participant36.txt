def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Pure model-based controller with learned transitions, anxiety-weighted exploration, and lapse.
    
    Overview
    --------
    This model learns second-stage action values (Q2) from reward and learns the state transition
    probabilities T(s | a1). First-stage choices are purely model-based: they value each spaceship
    by the expected value of the best alien on the planet it leads to under the learned transitions.
    Anxiety increases exploration (reduces effective beta) and increases lapse probability.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher scores increase exploration and lapses.
    model_parameters : list or array
        [alpha_r, alpha_T, beta, gamma_anx, lapse]
        - alpha_r in [0,1]: learning rate for second-stage reward values (Q2).
        - alpha_T in [0,1]: learning rate for transition probabilities T(s | a1).
        - beta in [0,10]: inverse temperature.
        - gamma_anx in [0,1]: scales how strongly anxiety reduces beta (exploration gain).
        - lapse in [0,1]: base lapse probability mixed with uniform choice; increased by anxiety.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, alpha_T, beta, gamma_anx, lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T(s | a1). Start at symmetric prior (0.5/0.5).
    T = np.ones((2, 2)) * 0.5  # rows: action (A,U), cols: next state (X,Y)
    # Initialize Q-values at second stage
    Q2 = np.zeros((2, 2))  # state x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    beta_eff = beta * (1.0 - gamma_anx * stai)  # higher anxiety -> lower beta (more exploration)
    beta_eff = max(beta_eff, 1e-6)              # avoid exactly zero
    lapse_eff = lapse * (0.5 + 0.5 * stai)      # higher anxiety -> more lapses
    alpha_T_eff = alpha_T * (0.5 + 0.5 * stai)  # anxiety accelerates learning of transitions

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # First-stage policy: model-based evaluation using learned transitions
        max_Q2 = np.max(Q2, axis=1)  # value of best alien in each state: [V(X), V(Y)]
        Q1_mb = np.array([
            T[0, 0] * max_Q2[0] + T[0, 1] * max_Q2[1],  # value of spaceship A
            T[1, 0] * max_Q2[0] + T[1, 1] * max_Q2[1],  # value of spaceship U
        ])
        logits1 = beta_eff * (Q1_mb - np.max(Q1_mb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        # Apply lapse mixture with uniform choice over 2 actions
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: softmax over Q2 in the visited state
        logits2 = beta_eff * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Learn transitions T(s | a1) by simple exponential moving average (one-hot target)
        # Target vector: 1 for observed s, 0 for other
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_T_eff * (target - T[a1, sp])
        # Keep rows normalized (they will remain normalized under this update, but guard numerical drift)
        T[a1] = T[a1] / np.sum(T[a1])

        # Learn second-stage Q-values from reward
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * pe2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Hybrid MB-MF with counterfactual learning, anxiety-reduced model-based weight, and forgetting.
    
    Overview
    --------
    The model learns:
      - Q2(s, a2) from reward for the chosen alien (chosen learning) and also updates the unchosen alien
        in the same state via counterfactual learning.
      - A model-free first-stage value Q1_mf via a TD backup from Q2(s, a2).
      - A fixed transition model (common A->X, U->Y with 0.7/0.3) for model-based evaluation.
    First-stage decisions combine model-based and model-free values with weight w. Anxiety reduces
    the effective model-based weight and increases forgetting (decay to a neutral prior).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher scores reduce MB weight and increase forgetting.
    model_parameters : list or array
        [alpha_chosen, alpha_unchosen, beta, w_hybrid, f_forget]
        - alpha_chosen in [0,1]: learning rate for chosen second-stage action.
        - alpha_unchosen in [0,1]: counterfactual learning rate for unchosen second-stage action (same state).
        - beta in [0,10]: inverse temperature for both stages.
        - w_hybrid in [0,1]: baseline weight on model-based values at stage 1.
        - f_forget in [0,1]: forgetting rate toward neutral 0.5 for Q2 and toward 0 for Q1_mf.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_c, alpha_u, beta, w_hybrid, f_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common=0.7)
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    Q2 = np.ones((2, 2)) * 0.5  # start at neutral 0.5 to align with forgetting anchor
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects: reduce MB weight; increase forgetting; reduce counterfactual learning
    w_eff = np.clip(w_hybrid * (1.0 - 0.5 * stai), 0.0, 1.0)
    f_eff = f_forget * (0.5 + 0.5 * stai)
    alpha_u_eff = alpha_u * (1.0 - 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Forgetting toward anchors before updates
        Q2 = (1.0 - f_eff) * Q2 + f_eff * 0.5
        Q1_mf = (1.0 - f_eff) * Q1_mf  # forget toward 0

        # Model-based evaluation for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2  # expected value under fixed transitions

        # Hybrid Q for stage 1
        Q1 = w_eff * Q1_mb + (1.0 - w_eff) * Q1_mf

        # Stage 1 policy
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Q2 chosen update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_c * pe2

        # Q2 unchosen counterfactual update within same state, using same outcome
        a2_unchosen = 1 - a2
        pe2_cf = r - Q2[s, a2_unchosen]
        Q2[s, a2_unchosen] += alpha_u_eff * pe2_cf

        # Model-free Q1 update via TD backup from observed Q2
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_c * pe1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Rare-transition sensitivity with anxiety-modulated noise, internal transition belief, and repetition bias.
    
    Overview
    --------
    This model combines:
      - A simple model-free learner at both stages (Q2 learned from reward; Q1_mf backed up from Q2).
      - A model-based planner for stage 1 using an internal belief about common transition probability t_common.
      - A repetition (stickiness) bias that promotes repeating the last action, scaled by anxiety.
      - Rare-transition sensitivity: after rare transitions, the second-stage choice policy becomes noisier
        with strength set by zeta_rare and amplified by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1; alien index within planet).
    reward : array-like of float
        Obtained rewards on each trial.
    stai : array-like of float
        Anxiety score(s); stai[0] in [0,1]. Higher scores intensify rare-transition noise and stickiness.
    model_parameters : list or array
        [alpha, beta, zeta_rare, t_common, rep]
        - alpha in [0,1]: learning rate for Q updates (both stages).
        - beta in [0,10]: base inverse temperature.
        - zeta_rare in [0,1]: strength of beta reduction after rare transitions.
        - t_common in [0,1]: internal belief that chosen spaceship makes a common transition (A->X, U->Y).
        - rep in [0,1]: baseline repetition bias added to the last chosen action's logit.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, zeta_rare, t_common, rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_a2 = [None, None]  # track per-state repetition

    # Anxiety-scaled repetition bias
    rep_eff = rep * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q1 using internal transition belief t_common
        # Belief: A commonly -> X, U commonly -> Y with prob t_common
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = np.array([
            t_common * max_Q2[0] + (1.0 - t_common) * max_Q2[1],  # spaceship A
            (1.0 - t_common) * max_Q2[0] + t_common * max_Q2[1],  # spaceship U
        ])
        # Hybrid with simple MF Q1
        Q1 = 0.5 * (Q1_mb + Q1_mf)  # fixed 0.5-0.5 mixture to keep parameter budget; MF updated below.

        # Stage 1 policy with repetition bias
        logits1 = beta * Q1
        if last_a1 is not None:
            logits1[last_a1] += rep_eff
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Determine if current transition was common or rare under task structure
        was_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Rare-transition sensitivity: reduce beta at stage 2 when rare occurs
        beta2 = beta * (1.0 - zeta_rare * stai * (1 - was_common))
        beta2 = max(beta2, 1e-6)

        # Stage 2 policy with per-state repetition bias
        logits2 = beta2 * Q2[s]
        if last_a2[s] is not None:
            logits2[last_a2[s]] += rep_eff
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Update repetition trackers
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)