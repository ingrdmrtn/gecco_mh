def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition hybrid RL with anxiety-modulated planning and transition learning.

    Overview:
    - Learns second-stage (planet-alien) values model-free with forgetting.
    - Learns the first-stage transition model (spaceship -> planet) from experience.
    - Forms model-based first-stage values from the learned transition model and combines
      them with model-free first-stage values via a hybrid weight.
    - Anxiety (stai) decreases planning weight and increases transition learning and forgetting.

    Parameters (bounds):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage state indices (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; e.g., aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), trait/state anxiety in [0,1]
    - model_parameters: iterable of 5 values:
        eta_v        [0,1]   base learning rate for second-stage Q-values
        beta         [0,10]  inverse temperature for both stages
        psi_T_base   [0,1]   base learning rate for transition model (spaceship->planet)
        mix_mb_base  [0,1]   baseline weight on model-based control at stage 1
        phi_decay    [0,1]   forgetting rate for second-stage values per trial

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    eta_v, beta, psi_T_base, mix_mb_base, phi_decay = model_parameters
    n_trials = len(action_1)

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Learned transition model (rows: spaceship A/U; cols: planet X/Y)
    T = np.full((2, 2), 0.5)

    # Model-free values
    q1_mf = np.zeros(2)        # stage-1 MF values for A/U
    q2_mf = np.zeros((2, 2))   # stage-2 MF values per planet and alien

    eps = 1e-10

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        stai_t = float(stai[min(t, len(stai) - 1)])

        # Anxiety-modulated parameters
        # Higher anxiety -> less planning, faster transition learning, stronger forgetting
        w_mb = np.clip(mix_mb_base * (1.0 - stai_t), 0.0, 1.0)
        psi_T = np.clip(psi_T_base * (1.0 + 0.5 * stai_t), 0.0, 1.0)
        phi_eff = np.clip(phi_decay * (1.0 + 0.5 * stai_t), 0.0, 1.0)

        # Decay second-stage values slightly each trial to track drifting rewards
        q2_mf = (1.0 - phi_eff) * q2_mf

        # Model-based first-stage values from learned transitions
        max_q2 = np.max(q2_mf, axis=1)         # size 2: best alien on each planet
        q1_mb = T @ max_q2                     # expected value of spaceships

        # Hybrid value for first-stage choice
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy
        q1_center = q1 - np.max(q1)
        p1 = np.exp(beta * q1_center)
        p1 /= (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        q2 = q2_mf[st]
        q2_center = q2 - np.max(q2)
        p2 = np.exp(beta * q2_center)
        p2 /= (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Transition learning: update row of chosen spaceship toward observed planet
        T[a1] = (1.0 - psi_T) * T[a1]
        T[a1, st] += psi_T
        # Re-normalize row (numerical safety)
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Second-stage MF update
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += eta_v * pe2

        # Stage-1 MF TD backup with the realized second-stage value
        td_target1 = q2_mf[st, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += eta_v * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric SARSA with anxiety-modulated loss aversion and stage-2 noise.

    Overview:
    - Purely model-free SARSA-style learning at both stages.
    - Separate learning rates for positive vs negative prediction errors.
    - Utility is asymmetric: losses (omissions) are weighted more heavily than gains,
      with loss aversion increasing with anxiety.
    - Anxiety also increases decision noise specifically at stage 2.

    Parameters (bounds):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 5 values:
        a_pos       [0,1]   learning rate when prediction error is positive
        a_neg       [0,1]   learning rate when prediction error is negative
        beta        [0,10]  baseline inverse temperature
        pav_base    [0,1]   Pavlovian approach bias toward repeating rewarded a2 in same state
        temp2_scale [0,1]   anxiety scaling that reduces stage-2 beta

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    a_pos, a_neg, beta, pav_base, temp2_scale = model_parameters
    n_trials = len(action_1)

    # Model-free Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Pavlovian memory per state: last action and whether it was rewarded
    last_a2 = np.full(2, -1, dtype=int)
    last_rew = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        stai_t = float(stai[min(t, len(stai) - 1)])

        # Anxiety-modulated stage-2 temperature (higher anxiety -> lower beta2)
        beta2 = beta * max(0.0, 1.0 - temp2_scale * stai_t)

        # Stage-1 policy (no explicit MB; purely MF)
        q1_c = q1 - np.max(q1)
        p1 = np.exp(beta * q1_c)
        p1 /= (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 Pavlovian approach bias: repeat rewarded action in this state
        pav_eff = pav_base * (1.0 + stai_t)  # anxiety increases approach bias to recent reward
        pav_bias = np.zeros(2)
        if last_a2[s] != -1:
            sign = 1.0 if last_rew[s] > 0.5 else -1.0
            pav_bias[last_a2[s]] = pav_eff * sign

        q2_pref = q2[s] + pav_bias
        q2_c = q2_pref - np.max(q2_pref)
        p2 = np.exp(beta2 * q2_c)
        p2 /= (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Utility with anxiety-modulated loss aversion
        # Treat r in {0,1}; transform via piecewise utility
        k_loss = 1.0 + stai_t  # higher anxiety -> stronger loss weighting
        u = r if r >= q2[s, a2] else -k_loss * (q2[s, a2] - r)

        # Second-stage update (valence-asymmetric)
        pe2 = u - q2[s, a2]
        lr2 = a_pos if pe2 >= 0.0 else a_neg
        q2[s, a2] += lr2 * pe2

        # Stage-1 SARSA backup using current second-stage estimate (MF only)
        td_target1 = q2[s, a2]
        pe1 = td_target1 - q1[a1]
        lr1 = a_pos if pe1 >= 0.0 else a_neg
        q1[a1] += lr1 * pe1

        # Update Pavlovian memory for this state
        last_a2[s] = a2
        last_rew[s] = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-weighted learning (adaptive step size) with anxiety-scaled exploration.

    Overview:
    - Second-stage learning uses an adaptive learning rate proportional to estimated
      outcome uncertainty per (state, action). Uncertainty is updated from recent
      prediction errors and decays over time.
    - First-stage values are a hybrid of model-based (fixed transition 0.7/0.3)
      and model-free backup, both updated using the same adaptive learning rate
      derived from the realized second-stage (state, action).
    - Anxiety increases the gain on uncertainty (larger step sizes) and reduces
      stage-2 inverse temperature (more exploration under anxiety).

    Parameters (bounds):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety in [0,1]
    - model_parameters: iterable of 5 values:
        beta          [0,10] baseline inverse temperature (used at stage 1)
        xi2_base      [0,1]  scales reduction of stage-2 beta with anxiety
        kappa_u_base  [0,1]  gain mapping uncertainty to learning rate
        nu_init       [0,1]  initial uncertainty per (state, action)
        omega_decay   [0,1]  decay rate for uncertainty; higher -> tracks recent variability

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    beta, xi2_base, kappa_u_base, nu_init, omega_decay = model_parameters
    n_trials = len(action_1)

    # Fixed transition model as in task description
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and uncertainty
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    U = np.full((2, 2), nu_init)  # uncertainty per (state, action), in [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]
        stai_t = float(stai[min(t, len(stai) - 1)])

        # Stage-1 model-based component from fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid first-stage value: here weight is implicit via MF update; selection uses MF+MB sum
        q1 = q1_mb + q1_mf

        # First-stage softmax with baseline beta
        q1_c = q1 - np.max(q1)
        p1 = np.exp(beta * q1_c)
        p1 /= (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax with anxiety-reduced beta
        beta2 = beta * max(0.0, 1.0 - xi2_base * stai_t)
        q2_c = q2[s] - np.max(q2[s])
        p2 = np.exp(beta2 * q2_c)
        p2 /= (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Adaptive learning rate from uncertainty
        kappa_eff = kappa_u_base * (1.0 + stai_t)          # anxiety increases sensitivity to uncertainty
        alpha_adapt = np.clip(kappa_eff * U[s, a2], 0.0, 1.0)

        # Second-stage update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_adapt * pe2

        # Update uncertainty from absolute PE with decay
        U[s, a2] = (1.0 - omega_decay) * U[s, a2] + omega_decay * min(1.0, abs(pe2))

        # First-stage MF backup using same adaptive step size
        td_target1 = q2[s, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_adapt * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll