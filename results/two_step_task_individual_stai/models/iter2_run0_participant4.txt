def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated lapse, transition-contingent repetition bias, and value forgetting.

    Overview
    - Stage-2 values learned via Rescorla–Wagner, with valence asymmetry induced by anxiety.
      Positive prediction errors are amplified when anxiety is high; negative ones dampened, and vice versa.
    - Stage-1 choice uses model-free values shaped by an anxiety-scaled, transition-contingent repetition bias:
      after rewarded common transitions the agent tends to repeat; after rewarded rare transitions the agent tends to switch.
    - A small lapse probability grows with anxiety, mixing softmax with uniform choice.
    - Values at both stages undergo forgetting toward 0.5 each trial.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]          Base learning rate for value updates.
    - beta:  [0,10]         Inverse temperature for softmax at both stages.
    - omega_trans: [0,1]    Strength of transition-contingent repetition bias at stage 1.
    - phi_lapse: [0,1]      Scales lapse probability with anxiety.
    - xi_forget: [0,1]      Per-trial forgetting rate toward 0.5.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, omega_trans, phi_lapse, xi_forget].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, omega_trans, phi_lapse, xi_forget = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)

    # Fixed transition structure (rows: action A/U; cols: state X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2) + 0.5           # Stage-1 MF values for A/U
    q2 = np.zeros((2, 2)) + 0.5         # Stage-2 values for states X/Y and actions 0/1

    # Track likelihoods
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Transition-contingent repetition bias (based on previous trial)
    prev_a1 = None
    prev_common = None
    prev_rew = None

    # Lapse grows with anxiety
    lapse = min(0.25, phi_lapse * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Forgetting toward 0.5
        q2 = (1.0 - xi_forget) * q2 + xi_forget * 0.5
        q1_mf = (1.0 - xi_forget) * q1_mf + xi_forget * 0.5

        # Stage-1 bias term: transition-contingent repetition effect
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_common is not None and prev_rew is not None:
            # Repeat bias after rewarded common; switch bias after rewarded rare
            # Signed effect: (+) to repeat when common&rewarded, (-) when rare&rewarded; zero when no reward
            sign = (1.0 if prev_common else -1.0) * (2.0 * prev_rew - 1.0)  # in {-1, 0, +1}
            bias1[prev_a1] += omega_trans * stai * sign

        # Stage-1 policy (MF baseline + bias; we do not compute MB values here to keep mechanism distinct)
        logits1 = beta * q1_mf + bias1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy (standard softmax from current state's Q)
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-2 update with valence asymmetry modulated by anxiety
        pe2 = r - q2[s, a2]
        # Anxiety-induced asymmetry: amplify learning more for the valence that aligns with anxiety
        # Map stai in [0,1] to asymmetry factor in [0.5, 1.5] around 1.0
        asym = 1.0 + (stai - 0.5)  # in [0.5, 1.5]
        a_pos = alpha * asym
        a_neg = alpha * (2.0 - asym)  # mirrors so average remains ~alpha
        eff_alpha2 = a_pos if pe2 >= 0 else a_neg
        q2[s, a2] += eff_alpha2 * pe2

        # Stage-1 MF credit assignment from stage-2 PE (eligibility is implicit via MF only)
        q1_mf[a1] += alpha * pe2

        # Prepare previous trial markers for next trial's bias
        prev_a1 = a1
        prev_rew = r
        prev_common = T[a1, s] >= 0.5

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-gated exploration and choice-kernel biases, with anxiety scaling exploration.

    Overview
    - Stage-2 learning via Rescorla–Wagner.
    - A running estimate of outcome volatility (squared PE) modulates exploration:
      higher volatility reduces effective beta, and this effect scales with anxiety.
    - Choice kernels (perseveration) for both stages bias toward previously chosen options.
    - Uses a small lapse that also scales with volatility and anxiety.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]          Learning rate for Q updates.
    - beta:  [0,10]         Base inverse temperature.
    - eta_explore: [0,1]    Strength of volatility-driven exploration (reduces beta).
    - kappa_ck: [0,1]       Choice-kernel learning rate/strength for perseveration.
    - alpha_var: [0,1]      Learning rate for updating volatility estimate from squared PE.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, eta_explore, kappa_ck, alpha_var].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, eta_explore, kappa_ck, alpha_var = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)

    # Transition matrix (used only to define common/rare if needed for kernels – here not used)
    # Kept for completeness; model policy does not plan over transitions.
    # T = np.array([[0.7, 0.3],
    #               [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5

    # Choice kernels for perseveration (stage 1 and per state at stage 2)
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))

    # Volatility estimate (scalar), initialized small
    vol = 0.05

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Effective beta reduced by volatility and anxiety
        # vol in [0, ~1]; clamp for safety
        v = max(0.0, min(1.0, vol))
        beta_t = beta * (1.0 - eta_explore * stai * v)
        beta_t = max(1e-3, beta_t)

        # Lapse also grows with volatility and anxiety (small)
        lapse = min(0.2, 0.1 * stai * v)

        # Stage-1 policy: MF values + choice kernel bias
        logits1 = beta_t * q1_mf + ck1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy: state-specific values + choice kernel
        logits2 = beta_t * q2[s] + ck2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Backpropagate to stage-1 MF value of chosen action
        q1_mf[a1] += alpha * pe2

        # Update volatility estimate from squared PE
        vol = (1.0 - alpha_var) * vol + alpha_var * (pe2 * pe2)

        # Update choice kernels (exponential recency)
        ck1 = (1.0 - kappa_ck) * ck1
        ck1[a1] += kappa_ck

        ck2[s] = (1.0 - kappa_ck) * ck2[s]
        ck2[s, a2] += kappa_ck

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like transition learning with anxiety-modulated discount and reward curvature.

    Overview
    - Learns a simple successor representation (SR) from stage-1 actions to stage-2 states
      by updating the action->state occupancy with TD(lambda_sr).
    - Model-based action values at stage 1 are computed as SR @ max(Q_stage2).
    - Anxiety decreases decisiveness (lower beta) and alters reward curvature (risk-like utility).
    - Small lapse grows with anxiety to capture occasional random choices.

    Parameters (all used; bounds in [0,1] except beta0 in [0,10])
    - alpha: [0,1]          Learning rate for both Q and SR updates.
    - beta0: [0,10]         Baseline inverse temperature.
    - lambda_sr: [0,1]      Trace parameter controlling how fast SR tracks transitions.
    - rho_anxMB: [0,1]      Strength by which anxiety reduces beta: beta = beta0 * exp(-rho*stai).
    - zeta_noise: [0,1]     Controls reward curvature and lapse scaling with anxiety.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta0, lambda_sr, rho_anxMB, zeta_noise].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta0, lambda_sr, rho_anxMB, zeta_noise = model_parameters
    stai = float(stai[0])
    n_trials = len(action_1)

    # Initialize SR from actions (rows) to states (cols)
    # Start at the known nominal transition but allow learning to adapt
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 Q-values
    q2 = np.zeros((2, 2)) + 0.5

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-modulated decisiveness and lapse
    def effective_beta():
        return max(1e-3, beta0 * np.exp(-rho_anxMB * stai))

    def transform_reward(r):
        # Curvature: r^(1 - zeta*stai); when stai high and zeta>0, utility is more concave
        power = 1.0 - zeta_noise * stai
        power = max(0.2, min(1.8, power))  # keep numerically reasonable
        return r ** power

    lapse_base = 0.02 + 0.08 * zeta_noise * stai

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r_raw = reward[t]
        r = transform_reward(r_raw)

        # Stage-1 MB values via SR
        max_q2 = np.max(q2, axis=1)   # per state
        q1_mb = M @ max_q2

        beta_t = effective_beta()
        lapse = min(0.25, lapse_base)

        # Stage-1 policy
        logits1 = beta_t * q1_mb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta_t * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-2 TD update with transformed reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # SR update for observed transition (from chosen action a1 to observed state s)
        # Target occupancy vector: one-hot for reached state (single-step SR)
        target = np.zeros(2)
        target[s] = 1.0
        # TD(lambda): move M[a1] toward target, with a small persistence via lambda_sr
        # Effective update: M[a1] <- M[a1] + alpha * (target - M[a1])
        # plus a trace that slightly diffuses mass toward the nominal structure
        M[a1] += alpha * (target - M[a1])
        # Trace-like smoothing to keep rows normalized and non-negative
        M[a1] = (1.0 - lambda_sr) * M[a1] + lambda_sr * (M[a1] / max(1e-8, np.sum(M[a1])))

        # Keep SR row normalized
        row_sum = np.sum(M[a1])
        if row_sum > 0:
            M[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)