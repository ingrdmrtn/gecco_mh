def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and first-stage stickiness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on visited planet).
    reward : array-like of float
        Reward (gold coins), can be negative, zero, or positive.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, omega0, xi, kappa1)
        - alpha2 in [0,1]: learning rate for second-stage Q-values and MF bootstrapping to stage 1.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline MB weight in first-stage arbitration.
        - xi in [0,1]: strength with which anxiety shifts MB weight (higher anxiety reduces MB if xi>0).
                        Effective weight: w_eff = clip(omega0 + xi*(0.5 - stai), 0, 1).
        - kappa1 in [0,1]: first-stage choice stickiness; bias to repeat previous spaceship.
                           Stickiness is dampened by anxiety: bias = kappa1 * (1 - stai).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.

    Notes
    -----
    - Uses fixed transition structure (common = 0.7): A->X, U->Y.
    - Stage-1 value is a convex combination of model-based (MB) and model-free (MF) values.
    - Anxiety modulates the arbitration weight (w_eff) and reduces stickiness bias.
    """
    alpha2, beta, omega0, xi, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix: rows=actions(A,U), cols=states(X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values: Q2[state, action]
    Q2 = np.zeros((2, 2))
    # Stage-1 model-free Q-values
    Q1_MF = np.zeros(2)

    prev_a1 = None

    # Anxiety-modulated arbitration weight and stickiness scale
    w_eff = omega0 + xi * (0.5 - stai_val)
    w_eff = 0.0 if w_eff < 0.0 else (1.0 if w_eff > 1.0 else w_eff)
    stickiness_scale = 1.0 - stai_val  # higher anxiety -> less stickiness

    for t in range(n_trials):
        # MB component: expected max value after transition
        max_Q2 = np.max(Q2, axis=1)  # per state
        Q1_MB = T @ max_Q2

        # Combine MB and MF
        Q1 = w_eff * Q1_MB + (1.0 - w_eff) * Q1_MF

        # Stickiness bias for stage-1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = kappa1 * stickiness_scale

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 MF bootstrapping from experienced second-stage action value
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus exploration with anxiety-modulated bonus strength (both stages).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, phi, z)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - phi in [0,1]: base strength of an optimism/uncertainty bonus.
        - z in [0,1]: anxiety modulation of bonus; effective scale:
                      scale = clip(1 + z*(0.5 - stai), 0, 2), bonus = phi * scale / sqrt(N + 1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.

    Notes
    -----
    - Uses fixed transitions (common = 0.7).
    - Maintains visit counts N[state, action]; rarely visited options carry a larger bonus.
    - Bonus augments action values at stage 2 and propagates to stage 1 via the MB backup.
    - Higher anxiety (stai > 0.5) reduces the bonus when z > 0; lower anxiety increases it.
    """
    alpha2, beta, phi, z = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))
    N = np.zeros((2, 2))  # visit counts for uncertainty bonus

    # Anxiety-modulated bonus scale
    scale = 1.0 + z * (0.5 - stai_val)
    if scale < 0.0:
        scale = 0.0
    if scale > 2.0:
        scale = 2.0
    phi_eff = phi * scale

    for t in range(n_trials):
        # Compute per-option bonus from visit counts for both states
        bonus_all = np.zeros((2, 2))
        for s in range(2):
            for a in range(2):
                bonus_all[s, a] = phi_eff / np.sqrt(N[s, a] + 1.0)

        # Stage-1 MB values use augmented second-stage utilities (Q2 + bonus)
        max_aug = np.max(Q2 + bonus_all, axis=1)  # per state
        Q1_MB = T @ max_aug

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with bonus
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * (Q2[s2] + bonus_all[s2])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        N[s2, a2] += 1.0
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like transition learning with anxiety-modulated discount and learning rate.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state per trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (alien on visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, gamma0, zeta, eta_sr)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - gamma0 in [0,1]: baseline discount controlling transition sharpening.
        - zeta in [0,1]: anxiety modulation of discount:
                         gamma_eff = clip(gamma0 * (1 - zeta * stai), 0, 1).
                         Lower gamma_eff (e.g., high anxiety with zeta>0) sharpens reliance
                         on the most likely transition.
        - eta_sr in [0,1]: learning rate for the action-to-state transition row (SR-like update).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.

    Notes
    -----
    - Learns a per-action transition row T_sr[a,:] online (initialized uniform).
    - First-stage MB value uses a sharpened transition distribution:
          T_used[a, s] âˆ T_sr[a, s] ** k, where k = 1 + 4 * (1 - gamma_eff).
      Thus, smaller gamma_eff (e.g., higher anxiety if zeta>0) increases k and
      emphasizes the modal transition.
    - Stage-2 values are learned with TD.
    """
    alpha2, beta, gamma0, zeta, eta_sr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transitions per action as uniform
    T_sr = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))

    # Anxiety-modulated discount and sharpening exponent
    gamma_eff = gamma0 * (1.0 - zeta * stai_val)
    if gamma_eff < 0.0:
        gamma_eff = 0.0
    if gamma_eff > 1.0:
        gamma_eff = 1.0
    k = 1.0 + 4.0 * (1.0 - gamma_eff)

    for t in range(n_trials):
        # Build sharpened transition matrix for decision
        T_used = np.zeros_like(T_sr)
        for a in range(2):
            row = T_sr[a] ** k
            ssum = np.sum(row)
            if ssum > 0:
                row = row / ssum
            else:
                row = np.array([0.5, 0.5])
            T_used[a] = row

        # MB value from learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T_used @ max_Q2

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(logits2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Update transition row for chosen action toward observed state (SR-like)
        # Move probability mass toward the observed state while keeping normalization
        T_sr[a1, :] = (1.0 - eta_sr) * T_sr[a1, :]
        T_sr[a1, s2] += eta_sr
        # Normalize for numerical stability
        row_sum = np.sum(T_sr[a1, :])
        if row_sum > 0:
            T_sr[a1, :] /= row_sum

        # Update stage-2 values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll