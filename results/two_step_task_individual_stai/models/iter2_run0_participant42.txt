def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition hybrid with anxiety-modulated model-based weight and choice stickiness.

    This model learns the first-stage transition matrix online and arbitrates between
    a model-based (MB) planner and a model-free (MF) first-stage value. The arbitration
    weight decreases with both anxiety and transition uncertainty. A choice-stickiness
    bias (increasing with anxiety) captures perseverative tendencies often linked to anxiety.
    Second-stage values are learned with a simple delta rule.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int
        Reached second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state (0/1; X: W/S, Y: P/H).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]; higher means higher anxiety.
    model_parameters : sequence of floats
        [alpha, beta, k_stick, eta_T, w_base]
        Bounds:
        - alpha in [0,1]: learning rate for Q-updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - k_stick in [0,1]: baseline stickiness strength; effective stickiness scales with anxiety.
        - eta_T in [0,1]: learning rate for the transition matrix.
        - w_base in [0,1]: baseline MB arbitration weight; effective weight reduced by anxiety and transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, k_stick, eta_T, w_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix with non-informative prior (uniform rows).
    T = np.full((2, 2), 0.5)

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood trackers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Previous first-stage action for stickiness
    prev_a1 = None

    for t in range(n_trials):
        # Compute transition uncertainty via row entropies (0=low, 1=high)
        # Normalize entropy of a binary distribution by dividing by log(2)=1
        epsH = 1e-12
        ent = -np.sum(T * np.log(T + epsH), axis=1)  # in nats
        ent = ent / np.log(2.0)  # normalize to [0,1]
        trans_uncert = float(np.mean(ent))

        # MB values: expectation over learned transitions of max second-stage Q
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2           # shape (2,)

        # Arbitration weight w: lower with higher anxiety and higher transition uncertainty
        w = w_base * (1.0 - stai) * (1.0 - trans_uncert)
        w = min(max(w, 0.0), 1.0)

        # Stickiness bias increases with anxiety
        stick_eff = k_stick * stai

        # Combine MB and MF
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Add stickiness as an additive bias to the chosen-last action
        pref1 = beta * q1
        if prev_a1 is not None:
            pref1[prev_a1] += stick_eff

        # First-stage choice probability
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (softmax on q2 at reached state)
        s = state[t]
        pref2 = beta * q2[s].copy()
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning
        # Update second-stage Q
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update first-stage MF toward realized second-stage value (bootstrapped)
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update learned transitions for taken first-stage action using observed state
        # One-hot target distribution for observed transition
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - eta_T) * T[a1, :] + eta_T * target
        # Normalize for numerical safety
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric model-free SARSA with anxiety-modulated lapse and endogenous eligibility.

    Purely model-free learning with separate learning rates for gains and losses at the
    second stage. A TD(λ) credit assignment to the first stage uses λ = 1 - stai, so higher
    anxiety shortens the eligibility span. An anxiety-modulated lapse (random choice mixing)
    is applied at both stages.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int
        Reached second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float
        Obtained reward per trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher means higher anxiety.
    model_parameters : sequence of floats
        [alpha_gain, alpha_loss, beta1, beta2, lapse_base]
        Bounds:
        - alpha_gain in [0,1]: learning rate when second-stage PE is positive.
        - alpha_loss in [0,1]: learning rate when second-stage PE is negative or zero.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: inverse temperature for second-stage softmax.
        - lapse_base in [0,1]: base lapse mixture; effective lapse increases with anxiety:
          lapse_eff = min(0.5, lapse_base * stai).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_g, alpha_l, beta1, beta2, lapse_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective eligibility and lapse from anxiety
    lam = 1.0 - stai
    lam = min(max(lam, 0.0), 1.0)
    lapse = min(0.5, max(0.0, lapse_base * stai))

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage policy with lapse
        pref1 = beta1 * q1
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with lapse
        s = state[t]
        pref2 = beta2 * q2[s]
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning rates depend on the sign of the second-stage PE
        pe2 = r - q2[s, a2]
        alpha2 = alpha_g if pe2 > 0.0 else alpha_l
        q2[s, a2] += alpha2 * pe2

        # Stage-1 update with TD(lambda): mixture of bootstrapped difference and second-stage PE
        pe1 = q2[s, a2] - q1[a1]
        # Use same asymmetric rate for credit assignment
        q1[a1] += alpha2 * (pe1 + lam * pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk/uncertainty-driven exploration with anxiety, MB planning, MF mixing, and Q decay.

    This model uses a directed exploration bonus based on visit counts (UCB-style) at the
    second stage. Anxiety reduces the exploration bonus magnitude. First-stage action values
    are computed via model-based planning over a fixed transition structure, using the
    uncertainty-bonused second-stage action values. A small model-free first-stage value is
    mixed in, with mixing weight increasing with anxiety. Second-stage Q-values also decay
    over time, capturing non-stationarity.

    Parameters
    ----------
    action_1 : array-like of int
        First-stage choices per trial (0: spaceship A, 1: spaceship U).
    state : array-like of int
        Reached second-stage state per trial (0: planet X, 1: planet Y).
    action_2 : array-like of int
        Second-stage choices per trial within the reached state (0/1).
    reward : array-like of float
        Obtained reward per trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher means higher anxiety.
    model_parameters : sequence of floats
        [alpha, beta, c_base, w_mix, k_forget]
        Bounds:
        - alpha in [0,1]: learning rate for Q-updates.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - c_base in [0,1]: baseline coefficient for directed exploration bonus; effective
          bonus is c_eff = c_base * (1 - stai).
        - w_mix in [0,1]: baseline weight for MF contribution at stage 1; effective
          mixing is gamma = w_mix * stai (more MF with anxiety).
        - k_forget in [0,1]: per-trial decay applied to second-stage Q-values (captures drift).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, c_base, w_mix, k_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed, commonly-known transitions: A->X (0.7), U->Y (0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and counts
    q2 = np.zeros((2, 2))
    n2 = np.zeros((2, 2))  # visit counts for exploration bonus
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    c_eff = max(0.0, min(1.0, c_base * (1.0 - stai)))
    gamma = max(0.0, min(1.0, w_mix * stai))

    for t in range(n_trials):
        # Apply decay to second-stage Q to reflect non-stationarity
        q2 *= (1.0 - k_forget)

        # Directed exploration bonus for each state-action
        bonus2 = c_eff / np.sqrt(n2 + 1.0)

        # First-stage MB values computed from bonused second-stage values
        max_bonused_q2 = np.max(q2 + bonus2, axis=1)
        q1_mb = T @ max_bonused_q2

        # Mix in MF first-stage values, more so with anxiety (gamma)
        q1 = (1.0 - gamma) * q1_mb + gamma * q1_mf

        # First-stage choice
        pref1 = beta * q1
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        probs_1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice using bonused values for exploration
        s = state[t]
        q2_bonused = q2[s] + bonus2[s]
        pref2 = beta * q2_bonused
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        probs_2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update counts for exploration statistics
        n2[s, a2] += 1.0

        # Learning at second stage
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update MF first-stage value toward realized second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll