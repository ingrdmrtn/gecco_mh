def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid model-based/model-free learner with anxiety-gated arbitration and action stickiness.

    Idea:
    - Stage-2: standard model-free TD learning of alien values.
    - Stage-1: combines model-free values with model-based evaluation via the transition matrix.
    - The arbitration weight omega between MF and MB at stage-1 is gated by STAI:
        omega(stai) = omega_low + (omega_high - omega_low) * stai
      Higher STAI can push the agent toward more MB or more MF depending on parameters.
    - Action stickiness (same parameter) applies to both stages to capture perseveration.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), TD learning rate for value updates
    - model_parameters[1]: beta (0..10), inverse temperature for softmax at both stages
    - model_parameters[2]: omega_low (0..1), arbitration weight at STAI=0
    - model_parameters[3]: omega_high (0..1), arbitration weight at STAI=1
    - model_parameters[4]: zeta (0..1), strength of action stickiness bias

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (0=A, 1=U)
    - state: array of ints in {0,1}, observed planet (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien on the planet (0 or 1)
    - reward: array of floats, obtained coins
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    alpha, beta, omega_low, omega_high, zeta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Transition structure: rows are spaceships (A=0, U=1), cols are planets (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)            # stage-1 model-free
    q2 = np.zeros((2, 2))          # stage-2 model-free (state x action)

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Previous choices for stickiness
    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-10

    # Compute arbitration weight from STAI
    omega = omega_low + (omega_high - omega_low) * stai_val
    if omega < 0.0:
        omega = 0.0
    if omega > 1.0:
        omega = 1.0

    for t in range(n_trials):
        # Model-based evaluation at stage-1 from current q2
        max_q2 = np.max(q2, axis=1)                 # best alien per planet
        q1_mb = transition_matrix @ max_q2          # expected values per spaceship

        # Hybrid action values at stage-1
        q1_hybrid = (1.0 - omega) * q1_mf + omega * q1_mb

        # Stickiness bias for stage-1
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1):
            bias1[prev_a1] += 1.0  # favor repeating last stage-1 choice

        # Softmax for stage-1
        logits1 = beta * (q1_hybrid + zeta * bias1)
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state
        s = int(state[t])
        bias2 = np.zeros(2)
        if prev_a2 in (0, 1):
            bias2[prev_a2] += 1.0  # favor repeating last stage-2 choice (global stickiness)
        logits2 = beta * (q2[s] + zeta * bias2)
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update (model-free)
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha * delta2

        # Stage-1 MF update: bootstrapped toward realized stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] = q1_mf[a1] + alpha * delta1

        # Update stickiness memory
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Model-free learner with anxiety-modulated learning rate and temperature,
    plus a transition-dependent choice kernel at stage-1.

    Idea:
    - Pure model-free TD value learning at both stages.
    - STAI increases/decreases learning rate and choice noise:
        alpha_eff = clip(alpha_base + k_anx_alpha * stai, 0, 1)
        beta_eff  = beta_base / (1 + 9 * k_anx_temp * stai)  (higher STAI -> more noise if k_anx_temp>0)
    - A transition-dependent kernel (gamma_tr) biases repeating the previous stage-1 choice:
        After reward: common transition encourages repetition; rare discourages it.
        After no reward: rare encourages repetition; common discourages it.
      This captures a model-based-like transition-outcome interaction via a simple bias term.

    Parameters (all in [0,1] except beta_base in [0,10]):
    - model_parameters[0]: alpha_base (0..1), baseline learning rate
    - model_parameters[1]: k_anx_alpha (0..1), slope of STAI effect on learning rate
    - model_parameters[2]: beta_base (0..10), baseline inverse temperature
    - model_parameters[3]: k_anx_temp (0..1), slope of STAI effect on temperature
    - model_parameters[4]: gamma_tr (0..1), strength of transition-dependent bias

    Inputs:
    - action_1: array of ints in {0,1}
    - state: array of ints in {0,1}
    - action_2: array of ints in {0,1}
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_base, k_anx_alpha, beta_base, k_anx_temp, gamma_tr = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated parameters
    alpha_eff = alpha_base + k_anx_alpha * stai_val
    if alpha_eff < 0.0:
        alpha_eff = 0.0
    if alpha_eff > 1.0:
        alpha_eff = 1.0

    beta_eff = beta_base / (1.0 + 9.0 * (k_anx_temp * stai_val))
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Values
    q1 = np.zeros(2)          # stage-1 model-free
    q2 = np.zeros((2, 2))     # stage-2 model-free

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    # For transition-dependent kernel, track previous trial info
    have_prev = False
    prev_a1 = 0
    prev_state = 0
    prev_reward = 0.0

    for t in range(n_trials):
        # Transition-dependent bias for stage-1 logits based on previous trial
        bias1 = np.zeros(2)
        if have_prev:
            # Determine if previous transition was common or rare
            # Common: A->X or U->Y; Rare otherwise
            was_common = ((prev_a1 == 0 and prev_state == 0) or
                          (prev_a1 == 1 and prev_state == 1))

            # Define signed signal for repeating previous action
            # After reward: +1 if common, -1 if rare
            # After no reward: -1 if common, +1 if rare
            rewarded = prev_reward > 0.0
            if rewarded:
                sign = 1.0 if was_common else -1.0
            else:
                sign = -1.0 if was_common else 1.0

            # Apply bias symmetrically toward repeating previous choice
            b = gamma_tr * sign
            if prev_a1 == 0:
                bias1[0] += b
                bias1[1] -= b
            else:
                bias1[1] += b
                bias1[0] -= b

        # Stage-1 policy
        logits1 = beta_eff * (q1 + bias1)
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at observed state
        s = int(state[t])
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] = q2[s, a2] + alpha_eff * delta2

        # Stage-1 TD update toward realized stage-2 value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] = q1[a1] + alpha_eff * delta1

        # Save current trial info for next trial's bias
        have_prev = True
        prev_a1 = a1
        prev_state = s
        prev_reward = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive model-free learner with anxiety-modulated risk aversion and value decay.

    Idea:
    - Stage-2 maintains exponentially weighted estimates of both mean and second moment for each alien.
      Utility is mean - rho * std, where std is derived from the EW second moment.
    - Stage-1 is purely model-free, bootstrapping toward the realized stage-2 utility.
    - STAI modulates risk aversion rho:
        rho = clip(rho_base + k_anx_rho * stai, 0, 1)
      Higher STAI can increase (or decrease) risk aversion depending on k_anx_rho.
    - A decay parameter shrinks unchosen values toward zero, capturing forgetting/instability.

    Parameters (all in [0,1] except beta in [0,10]):
    - model_parameters[0]: alpha (0..1), EW update rate for mean and second moment
    - model_parameters[1]: beta (0..10), inverse temperature at both stages
    - model_parameters[2]: rho_base (0..1), baseline risk aversion
    - model_parameters[3]: k_anx_rho (0..1), slope of STAI effect on risk aversion
    - model_parameters[4]: decay (0..1), per-trial decay applied to unchosen values

    Inputs:
    - action_1: array of ints in {0,1}
    - state: array of ints in {0,1}
    - action_2: array of ints in {0,1}
    - reward: array of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of 5 parameters as above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, rho_base, k_anx_rho, decay = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated risk aversion
    rho = rho_base + k_anx_rho * stai_val
    if rho < 0.0:
        rho = 0.0
    if rho > 1.0:
        rho = 1.0

    # Stage-2 statistics: EW means and second moments per (state,action)
    m = np.zeros((2, 2))     # EW mean reward
    m2 = np.zeros((2, 2))    # EW mean of squared reward

    # Stage-1 MF values
    q1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        # Compute risk-sensitive utility for each state-action
        var = np.maximum(m2 - m**2, 0.0)
        std = np.sqrt(var + 1e-12)
        u = m - rho * std  # utility

        # Stage-1 policy: use current MF q1
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        probs1 = soft1 / (np.sum(soft1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state using utility
        s = int(state[t])
        logits2 = beta * u[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        probs2 = soft2 / (np.sum(soft2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # Learning at stage-2: EW updates for mean and second moment
        # Decay unchosen alien at this state and also decay other state's means slightly
        other_a2 = 1 - a2
        m[s, other_a2] = (1.0 - decay) * m[s, other_a2]
        m2[s, other_a2] = (1.0 - decay) * m2[s, other_a2]
        other_state = 1 - s
        m[other_state, 0] = (1.0 - decay) * m[other_state, 0]
        m[other_state, 1] = (1.0 - decay) * m[other_state, 1]
        m2[other_state, 0] = (1.0 - decay) * m2[other_state, 0]
        m2[other_state, 1] = (1.0 - decay) * m2[other_state, 1]

        # Update chosen alien stats at observed state
        m[s, a2] = m[s, a2] + alpha * (r - m[s, a2])
        m2[s, a2] = m2[s, a2] + alpha * (r*r - m2[s, a2])

        # Recompute utility for the realized (s, a2) after the update
        var_sa = max(m2[s, a2] - m[s, a2]**2, 0.0)
        u_sa = m[s, a2] - rho * np.sqrt(var_sa + 1e-12)

        # Stage-1 learning: MF TD toward risk-sensitive utility
        # Decay unchosen stage-1 action
        q1[1 - a1] = (1.0 - decay) * q1[1 - a1]
        delta1 = u_sa - q1[a1]
        q1[a1] = q1[a1] + alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll