def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated transition learning, MF/MB arbitration, forgetting, and temperature shift.

    Idea:
    - The agent learns the transition matrix online and uses it for model-based (MB) evaluation.
    - Anxiety increases transition learning rate (hypervigilance) and increases forgetting of Q values.
    - Anxiety decreases inverse temperature (more exploration).
    - First-stage policy arbitrates between MB and model-free (MF) values based on current transition uncertainty and anxiety.
    - MF credit assignment from stage 2 to stage 1 uses an eligibility trace equal to the participant's anxiety (no extra parameter).

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for second-stage Q updates; also used for first-stage MF TD.
    - beta in [0,10]: base inverse temperature.
    - phi_trans in [0,1]: base transition learning rate; anxiety scales it upward.
    - eta_forget in [0,1]: base forgetting rate; anxiety scales it upward (more forgetting under anxiety).
    - xi_anxTemp in [0,1]: strength of anxiety-driven temperature reduction.

    Inputs:
    - action_1: array-like ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints in {0,1} for reached planet (0=X, 1=Y).
    - action_2: array-like ints in {0,1} for second-stage choices (0/1 for the two aliens on that planet).
    - reward: array-like floats (typically 0/1).
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, phi_trans, eta_forget, xi_anxTemp].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, phi_trans, eta_forget, xi_anxTemp = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize transition matrix estimate; start with common=0.7 prior
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Effective parameters modulated by anxiety
    beta_eff = max(1e-3, beta * (1.0 - xi_anxTemp * stai))  # higher anxiety -> lower beta
    phi_eff = np.clip(phi_trans * (1.0 + 0.5 * stai), 0.0, 1.0)  # higher anxiety -> faster transition learning
    forget_eff = np.clip(eta_forget * stai, 0.0, 1.0)  # higher anxiety -> more forgetting
    lambda_anx = np.clip(stai, 0.0, 1.0)  # eligibility trace equals anxiety

    # Value tables
    q1_mf = np.zeros(2)          # MF Q for A/U
    q2 = np.zeros((2, 2))        # Q for aliens on each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute MB values from current transition estimate
        max_q2 = np.max(q2, axis=1)  # best value on each planet
        q1_mb = T @ max_q2

        # Arbitration weight: less MB when anxiety high and when transitions are uncertain (high entropy)
        # Measure average transition uncertainty (mean row entropy, normalized to [0,1])
        row_ent = []
        for r in range(2):
            p = T[r]
            # entropy base 2, max entropy = 1 for binary
            h = 0.0
            for x in p:
                if x > 0:
                    h -= x * (np.log(x) / np.log(2))
            row_ent.append(h)  # in [0,1] for binary
        ent_mean = 0.5 * (row_ent[0] + row_ent[1])  # 0=certainty, 1=max uncertainty
        w_mb = np.clip((1.0 - stai) * (1.0 - ent_mean), 0.0, 1.0)

        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # First-stage policy
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Second-stage policy (no additional bias)
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD updates
        # Forgetting applied to all Qs (global drift), stronger under anxiety
        q1_mf = (1.0 - forget_eff) * q1_mf
        q2 = (1.0 - forget_eff) * q2

        # Stage-1 MF TD toward realized second-stage action value (SARSA-style)
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Stage-2 TD with received reward
        td2 = r - q2[s, a2]
        q2[s, a2] += alpha * td2

        # Eligibility trace: propagate outcome back to stage 1, scaled by anxiety
        q1_mf[a1] += alpha * lambda_anx * td2

        # Transition learning: update only the chosen first-stage action's row toward observed state
        # One-hot for observed transition
        obs = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - phi_eff) * T[a1, :] + phi_eff * obs

        # Keep rows normalized and numerically safe
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with transition-outcome interaction and anxiety-shaped heuristics.

    Idea:
    - Stage-2 learning uses separate learning rates for positive vs negative outcomes.
    - First-stage choice is driven by MF values plus a model-based heuristic that implements
      the classic transition-by-outcome interaction (win-stay for common and lose-shift for rare),
      whose strength is reduced by anxiety.
    - Perseveration bias at both stages increases with anxiety.
    - Eligibility trace from stage 2 to stage 1 equals anxiety (no extra parameter).

    Parameters (with bounds):
    - alpha_pos in [0,1]: learning rate when reward > current Q (positive TD error).
    - alpha_neg in [0,1]: learning rate when reward < current Q (negative TD error).
    - beta in [0,10]: inverse temperature for both stages.
    - z_mb in [0,1]: strength of the transition-outcome interaction heuristic at stage 1.
    - pi_pers in [0,1]: base perseveration strength (anxiety scales it up).

    Inputs:
    - action_1: array-like ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints in {0,1} for reached planet (0=X, 1=Y).
    - action_2: array-like ints in {0,1} for second-stage choices (0/1 on that planet).
    - reward: array-like floats (0/1).
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha_pos, alpha_neg, beta, z_mb, pi_pers].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha_pos, alpha_neg, beta, z_mb, pi_pers = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # MF values
    q1_mf = np.zeros(2)       # for A/U
    q2 = np.zeros((2, 2))     # for aliens

    # Perseveration parameters (increase with anxiety)
    pers_eff = pi_pers * (0.5 + 0.5 * stai)

    # Transition structure for common vs rare heuristic (fixed 0.7/0.3)
    common_map = {0: 0, 1: 1}  # A->X (0), U->Y (1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = None

    eps = 1e-12
    lambda_anx = np.clip(stai, 0.0, 1.0)

    # Variables to hold previous trial info for the heuristic
    prev_a1_t = None
    prev_common = None
    prev_reward = None

    for t in range(n_trials):
        # Heuristic bonus for each first-stage action based on previous trial
        bonus1 = np.zeros(2)
        if prev_a1_t is not None and prev_common is not None and prev_reward is not None:
            # Win-stay for common, lose-shift for rare:
            # If previous transition was common:
            #   after reward: add to previous action; after no reward: subtract
            # If previous was rare:
            #   after reward: subtract from previous action; after no reward: add
            sign = 1.0 if prev_common else -1.0
            mag = (1.0 if prev_reward > 0.0 else -1.0) * sign
            z_eff = z_mb * (1.0 - stai)  # anxiety reduces reliance on this MB heuristic
            bonus1[prev_a1_t] += z_eff * (1.0 if mag > 0 else -1.0)

        # Add perseveration bias
        if prev_a1 is not None:
            bonus1[prev_a1] += pers_eff

        logits1 = beta * q1_mf + bonus1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Second-stage policy with perseveration
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += pers_eff

        logits2 = beta * q2[s, :] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning rates by valence at stage 2
        td2 = r - q2[s, a2]
        alpha2 = alpha_pos if td2 >= 0 else alpha_neg
        q2[s, a2] += alpha2 * td2

        # Stage-1 MF TD and eligibility trace
        td1 = q2[s, a2] - q1_mf[a1]
        # Use symmetric average of pos/neg for stage-1 TD base
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * td1
        q1_mf[a1] += alpha1 * lambda_anx * td2

        # Update previous-trial variables for heuristic
        prev_a1 = a1
        prev_a2 = a2
        prev_a1_t = a1
        prev_common = (s == common_map[a1])  # True if common transition
        prev_reward = r

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with anxiety-weighted transition surprise cost and lapse.

    Idea:
    - Rewards are transformed by a concave utility u(r) = r^(gamma_eff), with anxiety increasing concavity (risk aversion).
    - First-stage MB values subtract an "effort/surprise cost" proportional to transition entropy; anxiety amplifies this cost.
    - Choices include a uniform lapse component; anxiety can still influence behavior via the cost and utility.
    - Standard MF updates with the transformed utility; MF and MB are combined by taking MB for planning and MF for bootstrapping via TD to stage 1.

    Parameters (with bounds):
    - alpha in [0,1]: learning rate for Q updates.
    - beta in [0,10]: inverse temperature for softmax.
    - gamma in [0,1]: risk sensitivity for utility transformation.
    - kappa_c in [0,1]: base weight on transition surprise cost in stage-1 MB values.
    - epsilon in [0,1]: lapse rate; mixture with uniform choice.

    Inputs:
    - action_1: array-like ints in {0,1} for first-stage choices (0=A, 1=U).
    - state: array-like ints in {0,1} for reached planet (0=X, 1=Y).
    - action_2: array-like ints in {0,1} for second-stage choices on that planet.
    - reward: array-like floats (0/1).
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: [alpha, beta, gamma, kappa_c, epsilon].

    Returns:
    - Negative log-likelihood of the observed sequence of choices under the model.
    """
    alpha, beta, gamma, kappa_c, epsilon = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Effective parameters
    gamma_eff = np.clip(gamma * (1.0 - 0.5 * stai), 1e-3, 1.0)  # anxiety -> more concave utility
    kappa_eff = kappa_c * stai  # anxiety increases perceived surprise cost
    eps = 1e-12

    # Value tables
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Precompute row entropies (base 2; max is 1 for binary)
    row_entropy = np.zeros(2)
    for r in range(2):
        h = 0.0
        for x in T[r, :]:
            if x > 0:
                h -= x * (np.log(x) / np.log(2))
        row_entropy[r] = h  # in [~0,1]

    for t in range(n_trials):
        # Model-based expected value at first stage
        max_q2 = np.max(q2, axis=1)
        q1_mb_raw = T @ max_q2

        # Subtract surprise/effort cost for each action based on its transition entropy
        cost = kappa_eff * row_entropy
        q1_mb = q1_mb_raw - cost

        # Combine MB plan with MF bias by simple addition of MF baseline (no extra param)
        q1 = q1_mb + q1_mf

        # First-stage policy with lapse
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 /= np.sum(soft1)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Second-stage policy with lapse
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 /= np.sum(soft2)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        # Utility transformation
        u = (r ** gamma_eff)

        # TD updates
        td2 = u - q2[s, a2]
        q2[s, a2] += alpha * td2

        # Stage-1 MF bootstrapping toward second-stage value (SARSA-style)
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik