def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Transition-learning hybrid with anxiety-weighted arbitration and uncertainty bonus.

    Core ideas:
    - Learns the state-transition function online (rather than assuming it's fixed).
    - Stage 1 uses a hybrid of model-based (via learned transitions) and model-free values.
    - Anxiety increases the transition learning rate and reduces MB arbitration weight.
    - Adds a directed exploration bonus at stage 1 proportional to transition uncertainty, scaled by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet (0 or 1).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases transition learning rate,
        - reduces reliance on model-based arbitration at stage 1,
        - increases directed exploration bonus driven by transition uncertainty.
    model_parameters : array-like of float
        [alpha_q, beta, alpha_T, w_mb0, phi_unc]
        Bounds:
        - alpha_q in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - alpha_T in [0,1]: learning rate for transition probability learning.
        - w_mb0 in [0,1]: baseline MB arbitration weight at stage 1.
        - phi_unc in [0,1]: scaling of uncertainty (entropy) bonus at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_q, beta, alpha_T, w_mb0, phi_unc = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities T[a, s'] as rows that sum to 1
    # Start with a mild prior favoring common transitions (0.6/0.4)
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)          # model-free values at stage 1
    q2 = 0.5 * np.ones((2, 2))   # stage-2 action values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    alpha_T_eff = np.clip(alpha_T * (0.5 + 0.5 * stai), 0.0, 1.0)  # higher stai -> faster transition learning
    w_mb = np.clip(w_mb0 * (1.0 - 0.5 * stai), 0.0, 1.0)           # higher stai -> less MB reliance
    phi_eff = phi_unc * stai                                        # higher stai -> stronger uncertainty bonus

    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute MB action values at stage 1 from learned transitions and max stage-2 values
        max_q2 = np.max(q2, axis=1)      # value of each planet
        q1_mb = T @ max_q2               # expected value of choosing each spaceship

        # Add directed exploration bonus from transition uncertainty (entropy of each row)
        # Entropy H(p) = -sum p log p, max at uniform (uncertain transitions)
        row_ent = -(T * (np.log(T + eps))).sum(axis=1)
        q1_aug = q1_mb + phi_eff * row_ent

        # Hybrid arbitration
        q1 = w_mb * q1_aug + (1.0 - w_mb) * q1_mf

        # Stage-1 choice probabilities
        x1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(x1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probabilities at reached state
        q2_s = q2[s]
        x2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(x2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Transition learning: update row for chosen action toward observed state (one-hot target)
        target = np.zeros(2)
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T_eff) * T[a1] + alpha_T_eff * target
        # Renormalize to avoid drift
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Stage-1 MF update toward obtained stage-2 action value (SARSA(0)-style backup)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-gated eligibility traces with surprise amplification and perseveration.

    Core ideas:
    - Stage-2 uses standard TD learning.
    - Stage-1 model-free values are updated via eligibility traces that persist across trials.
    - Anxiety increases the eligibility trace persistence (lambda) and amplifies credit assignment after rare transitions.
    - Includes choice stickiness at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet (0 or 1).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases eligibility persistence (lambda),
        - increases the amplification of credit assignment following rare transitions,
        - increases choice stickiness.
    model_parameters : array-like of float
        [alpha, beta, lambda0, xi, stick0]
        Bounds:
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - lambda0 in [0,1]: baseline eligibility trace persistence.
        - xi in [0,1]: magnitude of surprise amplification on eligibility after rare transitions.
        - stick0 in [0,1]: baseline stickiness; scaled up by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, lambda0, xi, stick0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for common vs. rare classification
    # Common if a1 == state, rare otherwise (given task mapping).
    # We'll use this to compute surprise.
    q1 = np.zeros(2)           # stage-1 MF values
    q2 = 0.5 * np.ones((2, 2)) # stage-2 values

    # Eligibility trace over stage-1 actions
    e1 = np.zeros(2)

    # Stickiness biases
    bias1 = np.zeros(2)
    bias2 = np.zeros((2, 2))
    stick_eff = stick0 * (0.3 + 0.7 * stai)  # higher anxiety -> more stickiness

    # Anxiety-gated lambda
    lam_eff = np.clip(lambda0 * (0.3 + 0.7 * stai), 0.0, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    prev_a1 = None
    prev_s = None
    prev_a2 = None

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy with stickiness
        logits1 = q1 + bias1
        x1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(x1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        logits2 = q2[s] + bias2[s]
        x2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(x2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Compute surprise: 0 common, 1 rare
        is_common = (a1 == s)
        surprise = 0.0 if is_common else 1.0

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update eligibility trace for stage-1 actions
        # Decay and add current chosen action; amplify by surprise scaled by anxiety
        e1 *= lam_eff
        e1[a1] += 1.0 * (1.0 + xi * stai * surprise)
        # Normalize e1 to avoid blow-up (optional but stabilizing)
        norm_e = max(1.0, np.sum(np.abs(e1)))
        e1 = e1 / norm_e

        # Back up stage-1 values toward realized second-stage value via eligibility
        target1 = q2[s, a2]
        pe1_vec = target1 - q1
        q1 += alpha * e1 * pe1_vec

        # Update stickiness biases
        bias1[:] = 0.0
        bias1[a1] += stick_eff
        bias2[:] = 0.0
        bias2[s, a2] += stick_eff

        prev_a1, prev_s, prev_a2 = a1, s, a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-weighted negative-outcome sensitivity with value forgetting and pure MB planning.

    Core ideas:
    - Stage 2 learns action values with asymmetric sensitivity to negative outcomes.
      Outcomes below a neutral baseline (0.5) are up-weighted by anxiety.
    - Q-values undergo forgetting toward a neutral prior (0.5), stronger with anxiety.
    - Stage 1 uses model-based planning from a fixed transition model; no free MB weight parameter.
      Anxiety also reduces effective choice precision (softmax temperature scaling).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0 = planet X, 1 = planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited planet (0 or 1).
    reward : array-like of float
        Received coins (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score in [0,1]. Higher anxiety:
        - increases sensitivity to negative outcomes (below 0.5),
        - increases forgetting toward neutral value at stage 2,
        - reduces choice precision (lower effective beta).
    model_parameters : array-like of float
        [alpha, beta, gamma0, zeta0]
        Bounds:
        - alpha in [0,1]: learning rate for Q-value updates.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - gamma0 in [0,1]: baseline forgetting rate toward 0.5 for stage-2 values.
        - zeta0 in [0,1]: baseline amplification of negative-outcome sensitivity.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, gamma0, zeta0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 values initialized at neutral prior 0.5
    q2 = 0.5 * np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Effective forgetting toward 0.5 increases with anxiety
    gamma_eff = np.clip(gamma0 * (0.3 + 0.7 * stai), 0.0, 1.0)
    # Negative-outcome sensitivity increases with anxiety
    zeta_eff = zeta0 * (0.5 + 0.5 * stai)
    # Choice precision reduced with anxiety
    beta_eff = beta * (0.6 + 0.4 * (1.0 - stai))

    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based stage-1 values from fixed transitions and current q2
        max_q2 = np.max(q2, axis=1)  # planet values
        q1_mb = T @ max_q2

        # Stage-1 softmax
        x1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(x1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        x2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(x2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        p_choice_2[t] = probs2[a2]

        # Stage-2 forgetting toward neutral prior
        q2[s] = (1.0 - gamma_eff) * q2[s] + gamma_eff * 0.5

        # Stage-2 update with anxiety-weighted negative-outcome sensitivity around baseline 0.5
        outcome = r - 0.5
        if outcome >= 0.0:
            adj_outcome = outcome  # positive or neutral
        else:
            adj_outcome = outcome * (1.0 + zeta_eff)  # amplify negative outcome

        pe2 = (0.5 + adj_outcome) - q2[s, a2]
        q2[s, a2] += alpha * pe2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)