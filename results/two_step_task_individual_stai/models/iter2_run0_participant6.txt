def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Partially planned TD model with anxiety-modulated foresight and transition confidence.
    
    Idea:
    - Stage-2 values are learned with simple TD.
    - Stage-1 uses a partially model-based backup that blends max and mean action values at stage 2.
    - Anxiety reduces foresight (mix toward mean) and lowers confidence in the transition structure.
    - An eligibility-like backpropagation (eta) updates a stage-1 MF value from the realized stage-2 value.
    
    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins (can be negative or positive).
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, mix, eta, conf]
        - alpha in [0,1]: learning rate for Q-values (both stages)
        - beta in [0,10]: inverse temperature for both stages
        - mix in [0,1]: base weight on max vs mean for constructing stage-2 state values
                         V2 = mix*max(Q2) + (1-mix)*mean(Q2)
        - eta in [0,1]: eligibility-like backprop weight for updating stage-1 MF from realized stage-2 value
        - conf in [0,1]: confidence in known transition structure (1 uses nominal 0.7/0.3; 0 uses uniform 0.5/0.5)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, mix, eta, conf = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Nominal transition structure (rows: A, U; cols: X, Y)
    T_nom = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)
    T_unif = np.array([[0.5, 0.5],
                       [0.5, 0.5]], dtype=float)

    # Anxiety reduces confidence in transitions (more uncertainty) and foresight (less max, more mean)
    conf_eff = np.clip(conf * (1.0 - 0.6 * stai_val), 0.0, 1.0)
    T = conf_eff * T_nom + (1.0 - conf_eff) * T_unif

    mix_eff = np.clip(mix * (1.0 - 0.5 * stai_val), 0.0, 1.0)  # higher anxiety -> rely more on mean than max
    eta_eff = np.clip(eta * (1.0 - 0.3 * stai_val), 0.0, 1.0)  # slightly dampen credit assignment under anxiety

    # Value tables
    q2 = np.zeros((2, 2))     # Q at stage 2: states X/Y x actions {0,1}
    q1_mf = np.zeros(2)       # Model-free Q at stage 1 for A/U

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        # Construct state values at stage 2 using mix of max and mean
        max_q2 = np.max(q2, axis=1)
        mean_q2 = np.mean(q2, axis=1)
        v2 = mix_eff * max_q2 + (1.0 - mix_eff) * mean_q2

        # Partially planned stage-1 values from transitions
        q1_mb = T @ v2

        # Combine with MF using a simple average (no extra parameter): avoids duplicating w_mb mechanism tried before
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy for realized state
        s = int(state[t])
        q2_s = q2[s]
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update via eligibility-like backprop from realized Q2 of chosen a2
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * eta_eff * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility RL with anxiety-modulated loss aversion and volatility-sensitive choice temperature.
    
    Idea:
    - Transform outcomes via a prospect-like utility function with curvature eta and loss aversion lambda_eff.
    - Track per-state outcome variance online; higher estimated variance reduces choice temperature (more exploration).
    - Anxiety increases loss aversion and sensitivity to variance (stronger temperature reduction).
    - Stage-1 is fully model-based using the known transition structure and current V2 (expected utility).
    
    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha_q, beta, eta, alpha_var, risk_temp]
        - alpha_q in [0,1]: learning rate for stage-2 Q in utility space
        - beta in [0,10]: base inverse temperature for both stages
        - eta in [0,1]: utility curvature (0 -> linear around zero via log1p approx; 1 -> more curved via sqrt)
        - alpha_var in [0,1]: learning rate for tracking reward variance in each state
        - risk_temp in [0,1]: strength of variance-dependent temperature scaling
        
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha_q, beta, eta, alpha_var, risk_temp = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 Q in utility space
    q2 = np.zeros((2, 2))
    # Track mean and variance of raw rewards per second-stage state (not per action for parsimony)
    rew_mean = np.zeros(2)
    rew_var = np.ones(2) * 0.25  # start with moderate uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Utility transform with curvature and loss aversion; modulated by anxiety
    def util(x):
        # Curvature: mix between linear and signed sqrt
        # u_pos = x if eta=0 else sign-preserving power 0.5; interpolate by eta
        # Keep function continuous and monotonic
        x = float(x)
        if x >= 0:
            base = x
            curved = np.sqrt(x + 1e-12)
            u_lin = base
            u_curved = curved
        else:
            base = x
            curved = -np.sqrt(-x + 1e-12)
            u_lin = base
            u_curved = curved
        u_shape = (1.0 - eta) * u_lin + eta * u_curved

        # Loss aversion: lambda_eff >= 1 increases weight on losses; depends on stai
        # Map lambda_base in [0,1] implicitly via stai to [1, 1+2*stai]
        lambda_eff = 1.0 + 2.0 * stai_val  # higher anxiety => stronger loss aversion
        if x < 0:
            return lambda_eff * u_shape
        else:
            return u_shape

    # Variance-to-temperature mapping; anxiety increases sensitivity to variance
    risk_sens = risk_temp * (0.5 + 0.5 * stai_val)

    for t in range(n_trials):
        # Compute V2(s) as expected utility over choices by softmax-expected value:
        # Here we use "max utility" as approximation of control; to keep parameters <=5, reuse beta with variance scaling.
        # Stage-2 policy for realized state uses beta2_eff that depends on variance in that state.
        # To compute stage-1 values, we form V2 as max over actions at each state.
        # Temperature for computing both stage-2 policy and V2 depends on current state's variance (for policy)
        # and average variance across states (for planning).
        avg_std = np.sqrt(np.mean(rew_var))
        beta_plan = beta / (1.0 + risk_sens * avg_std)

        # V2 for planning: softmax-expected utility with temperature beta_plan
        V2 = np.zeros(2)
        for s_pl in range(2):
            q2_s = q2[s_pl]
            q2_center = q2_s - np.max(q2_s)
            pi_s = np.exp(beta_plan * q2_center)
            denom = np.sum(pi_s) + eps
            pi_s = pi_s / denom
            V2[s_pl] = np.dot(pi_s, q2_s)

        # Stage-1 planning
        q1 = T @ V2
        q1_center = q1 - np.max(q1)
        probs_1 = np.exp(beta_plan * q1_center)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 choice in realized state with state-specific variance scaling
        s = int(state[t])
        std_s = np.sqrt(max(rew_var[s], 1e-12))
        beta_act = beta / (1.0 + risk_sens * std_s)
        q2_s = q2[s]
        q2_center = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta_act * q2_center)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])
        u_r = util(r)

        # TD update in utility space
        delta2 = u_r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # Update running mean and variance of rewards per state (Welford-like exponential smoothing)
        prev_mean = rew_mean[s]
        new_mean = (1.0 - alpha_var) * prev_mean + alpha_var * r
        # Update variance via exponential smoothing of squared deviation
        dev = r - prev_mean
        new_var = (1.0 - alpha_var) * rew_var[s] + alpha_var * (dev * dev)
        rew_mean[s] = new_mean
        rew_var[s] = max(new_var, 1e-8)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-outcome interaction with perseveration and anxiety-modulated parameters.
    
    Idea:
    - Stage-2 uses standard TD learning.
    - Stage-1 value includes a transition-outcome interaction signal that captures the classic
      two-step effect: reward after common transition encourages repeating the same first-stage choice,
      reward after rare transition encourages switching (and vice versa for non-reward).
    - Include first- and second-stage choice perseveration kernels.
    - Anxiety increases perseveration and reduces learning rate; also adds a baseline bias toward spaceship A for low anxiety.
    
    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Received coins.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, rho, xi, bias0]
        - alpha in [0,1]: learning rate for stage-2 Q
        - beta in [0,10]: inverse temperature for both stages
        - rho in [0,1]: base perseveration strength (choice kernels at both stages)
        - xi in [0,1]: weight of transition-outcome interaction signal on stage-1 decision values
        - bias0 in [0,1]: baseline bias toward spaceship A when anxiety is low (bias decreases with anxiety)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha, beta, rho, xi, bias0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known mapping: A commonly -> X (0), U commonly -> Y (1)
    common_state = np.array([0, 1], dtype=int)

    # Values and kernels
    q2 = np.zeros((2, 2))
    k1 = np.zeros(2)  # first-stage choice kernel
    k2 = np.zeros(2)  # second-stage choice kernel
    q1_mf = np.zeros(2)

    # Anxiety-modulated parameters
    alpha_eff = np.clip(alpha * (1.0 - 0.3 * stai_val), 0.0, 1.0)
    rho_eff = np.clip(rho * (0.5 + 0.5 * stai_val), 0.0, 1.0)
    xi_eff = np.clip(xi * (1.0 + 0.2 * stai_val), 0.0, 1.0)
    # Bias toward A (action 0) stronger at low anxiety
    bias_eff = (0.5 - stai_val) * bias0  # can be negative if anxiety > 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Variables to carry previous trial info for interaction term
    prev_a1 = None
    prev_common = None
    prev_reward = 0.0

    eps = 1e-10

    for t in range(n_trials):
        # Transition-outcome interaction signal based on previous trial
        interact = np.zeros(2)
        if prev_a1 is not None:
            # Common if reached state equals common_state[prev_a1]
            sign = 1.0 if prev_common else -1.0
            # Add to the previously chosen action a signed amount proportional to previous reward
            # and subtract from the alternative to keep relative code balanced
            val = xi_eff * sign * prev_reward
            interact[prev_a1] += val
            interact[1 - prev_a1] -= val

        # Stage-1 values: MF plus interaction, choice kernel, and baseline bias
        v1 = q1_mf + interact + k1 + np.array([bias_eff, -bias_eff])
        v1_center = v1 - np.max(v1)
        probs_1 = np.exp(beta * v1_center)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = int(state[t])
        v2 = q2[s] + k2
        v2_center = v2 - np.max(v2)
        probs_2 = np.exp(beta * v2_center)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = float(reward[t])

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * delta2

        # Simple MF bootstrapping for stage-1 Q using realized state-action value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha_eff * (target1 - q1_mf[a1])

        # Update choice kernels (perseveration)
        # Decay toward zero, then add to chosen action
        k1 *= (1.0 - rho_eff)
        k1[a1] += rho_eff
        k2 *= (1.0 - rho_eff)
        k2[a2] += rho_eff

        # Record info for next trial's interaction term
        prev_a1 = a1
        prev_common = (s == common_state[a1])
        prev_reward = r

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)