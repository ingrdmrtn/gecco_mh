def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning model-based control with anxiety-modulated uncertainty bonus and stickiness.
    
    Overview:
    - Learns transition probabilities T(a1 -> s) online and uses them for model-based valuation at stage 1.
    - Learns stage-2 Q-values via TD(0).
    - Adds an exploration bonus proportional to the transition uncertainty (entropy of next-state distribution).
    - Includes first-stage choice stickiness (perseveration).
    - Anxiety (stai) increases sensitivity to transition uncertainty (exploration bonus) and slightly speeds up transition learning,
      while decreasing perseveration.

    Parameters (model_parameters):
    - alpha_r: [0,1] learning rate for stage-2 reward values.
    - beta: [0,10] inverse temperature for softmax choice at both stages.
    - alpha_T: [0,1] base transition learning rate (modulated upward by stai internally).
    - eta: [0,1] exploration bonus weight for transition uncertainty (scaled up by stai internally).
    - pers: [0,1] baseline first-stage choice stickiness (scaled down by stai internally).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choice (0=A, 1=U).
    - state: array of ints {0,1}, second-stage state reached (0=X, 1=Y).
    - action_2: array of ints {0,1}, second-stage choice (0/1 within the reached planet).
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety score.
    - model_parameters: tuple/list of 5 params (alpha_r, beta, alpha_T, eta, pers).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, alpha_T, eta, pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Learned transition matrix T[a1, s] initialized near common/rare but neutral (0.5/0.5)
    T = np.ones((2, 2)) * 0.5

    # Stage-2 Q-values: Q[s, a2]
    Q2 = np.zeros((2, 2))

    # Storage for choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulations
    alpha_T_eff = np.clip(alpha_T * (1.0 + 0.5 * stai), 0.0, 1.0)
    eta_eff = eta * (0.5 + 0.5 * stai)  # more anxiety => stronger uncertainty bonus
    pers_eff = pers * (1.0 - 0.7 * stai)  # more anxiety => less stickiness

    # Stickiness memory (last chosen action at stage 1)
    last_a1 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Model-based first-stage Q via learned transitions and current second-stage Q
        max_Q2 = np.max(Q2, axis=1)  # best alien per planet
        Q1_MB = T @ max_Q2  # expected best outcome via transitions

        # Uncertainty (entropy) bonus per first-stage action
        # H(p) = -sum p log p, with 2 states so bounded in [0, log 2]
        ent = np.zeros(2)
        for a in range(2):
            p_x, p_y = T[a, 0], T[a, 1]
            ent[a] = -(p_x * np.log(p_x + eps) + p_y * np.log(p_y + eps))
        bonus = eta_eff * ent

        # Perseveration bonus for repeating last first-stage choice
        stick = np.zeros(2)
        if last_a1 in (0, 1):
            stick[last_a1] = pers_eff

        Q1 = Q1_MB + bonus + stick

        # First-stage policy
        Q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * Q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy in reached state
        s = state[t]
        Q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * Q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes
        r = reward[t]

        # Update transition model for the chosen first-stage action (supervised/Dirichlet-like update)
        # Move probability mass toward the observed state
        T[a1, :] = (1.0 - alpha_T_eff) * T[a1, :]
        T[a1, s] += alpha_T_eff
        # Ensure normalization (floating error guard)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        # Stage-2 TD(0) learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_r * delta2

        # Update stickiness memory
        last_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric learning with anxiety-modulated loss aversion and repetition bias.
    
    Overview:
    - Uses separate learning rates for positive and negative outcomes at stage 2.
    - Transforms outcomes via loss aversion: negative outcomes are weighted by rho(stai) > 1.
    - Propagates learned value to first-stage via model-free backup (bootstrapping from Q2).
    - Includes a repetition bias applied to both stages, reduced by higher anxiety.

    Parameters (model_parameters):
    - alpha_pos: [0,1] learning rate when the outcome utility is positive.
    - alpha_neg: [0,1] learning rate when the outcome utility is negative.
    - beta: [0,10] inverse temperature for both stages.
    - rho_base: [0,1] baseline loss aversion scaling factor; effective loss aversion is 1 + 4*rho_base*(0.5 + stai).
    - rep: [0,1] baseline repetition bias magnitude (applied to previous choices), scaled by (1 - stai).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choice (0=A,1=U).
    - state: array of ints {0,1}, second-stage state reached (0=X,1=Y).
    - action_2: array of ints {0,1}, second-stage choice.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha_pos, alpha_neg, beta, rho_base, rep).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, rho_base, rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Stage-2 and Stage-1 MF values
    Q2 = np.zeros((2, 2))   # Q(s, a2)
    Q1 = np.zeros(2)        # MF value for first-stage actions

    # Repetition memory
    last_a1 = -1
    last_a2 = -1
    last_s = -1

    # Anxiety-modulated parameters
    rho = 1.0 + 4.0 * rho_base * (0.5 + stai)  # maps rho_base in [0,1] to approx [1,3] as stai increases
    rep_eff = rep * (1.0 - stai)               # less repetition under higher anxiety

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Repetition bonuses
        stick1 = np.zeros(2)
        if last_a1 in (0, 1):
            stick1[last_a1] = rep_eff

        # First-stage policy (pure MF + stickiness)
        Q1_eff = Q1 + stick1
        q1c = Q1_eff - np.max(Q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with repetition within the reached state
        s = state[t]
        stick2 = np.zeros(2)
        if last_s == s and last_a2 in (0, 1):
            stick2[last_a2] = rep_eff

        Q2_eff = Q2[s] + stick2
        q2c = Q2_eff - np.max(Q2_eff)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome with loss aversion transform for learning
        r = reward[t]
        util = r if r >= 0.0 else -rho * abs(r)

        # Stage-2 learning with asymmetric alphas
        pe2 = util - Q2[s, a2]
        alpha2 = alpha_pos if pe2 >= 0 else alpha_neg
        Q2[s, a2] += alpha2 * pe2

        # Stage-1 MF backup using the post-update value as bootstrap
        boot = Q2[s, a2]
        pe1 = boot - Q1[a1]
        alpha1 = alpha_pos if pe1 >= 0 else alpha_neg
        Q1[a1] += alpha1 * pe1

        # Update repetition memory
        last_a1 = a1
        last_a2 = a2
        last_s = s

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated decisional noise and value decay (forgetting).
    
    Overview:
    - Mixture of model-based (fixed known transitions) and model-free values at stage 1.
    - Stage-2 values learned via TD(0); stage-1 MF values learn via bootstrapping from stage-2.
    - Global value decay simulates forgetting/drift, countering nonstationarity.
    - Anxiety reduces effective inverse temperature (more noise) while leaving planning weight fixed.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta0: [0,10] base inverse temperature (when stai=0).
    - w: [0,1] weight on model-based control at stage 1 (fixed across trials).
    - k_beta: [0,1] strength of anxiety-induced temperature reduction; beta_eff = beta0*(1 - k_beta*stai).
    - decay: [0,1] per-trial value decay factor applied to all Q-values (higher means more decay).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choice.
    - state: array of ints {0,1}, second-stage state.
    - action_2: array of ints {0,1}, second-stage choice.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta0, w, k_beta, decay).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta0, w, k_beta, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7)
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Values
    Q2 = np.zeros((2, 2))  # stage-2 Q(s, a2)
    Q1_mf = np.zeros(2)    # stage-1 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated beta (avoid <=0)
    beta_eff = max(1e-6, beta0 * (1.0 - k_beta * stai))

    eps = 1e-12

    for t in range(n_trials):
        # Decay (forgetting) applied each trial before decisions
        Q2 *= (1.0 - decay)
        Q1_mf *= (1.0 - decay)

        # Model-based first-stage values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid action values
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # First-stage policy
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # MF stage-1 update via bootstrap from stage-2 chosen action value (post-update)
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + 1e-12)) + np.sum(np.log(p_choice_2 + 1e-12)))
    return nll