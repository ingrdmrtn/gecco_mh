def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated arbitration between model-based and model-free control via state-uncertainty.
    
    Summary
    -------
    - Learns second-stage Q-values with a single learning rate.
    - Maintains a model-free first-stage value via TD backup from second-stage values.
    - Computes a model-based first-stage value using the fixed transition structure (0.7 common).
    - Arbitration weight w between MB and MF depends on:
        (i) base propensity omega0,
        (ii) current state-uncertainty (low discriminability at stage 2 increases MB reliance),
        (iii) participant anxiety (higher anxiety shifts arbitration via phi_anx).
    - Choice at each stage uses softmax with inverse temperature beta.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, omega0, kappa_unc, phi_anx]
        - alpha: learning rate for value updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - omega0: baseline model-based weight (pre-sigmoid prior) [0,1]
        - kappa_unc: sensitivity of arbitration to state-uncertainty [0,1]
        - phi_anx: sensitivity of arbitration to anxiety [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega0, kappa_unc, phi_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (states x actions), initialize neutral
    q2 = np.zeros((2, 2)) + 0.5
    # Stage-1 model-free Q-values over actions
    q1_mf = np.zeros(2)

    # Helper: transform omega0 in [0,1] to a logit prior
    eps = 1e-8
    logit_omega0 = np.log((omega0 + eps) / (1.0 - omega0 + eps))

    for t in range(n_trials):
        # Compute current state-uncertainty from discriminability in each state
        diff0 = abs(q2[0, 0] - q2[0, 1])
        diff1 = abs(q2[1, 0] - q2[1, 1])
        # Higher diff => more certainty; define uncertainty in [0,1]
        uncert = 1.0 - 0.5 * (diff0 + diff1)
        uncert = float(np.clip(uncert, 0.0, 1.0))

        # Model-based first-stage values from current q2
        max_q2 = np.max(q2, axis=1)  # value per state
        q1_mb = T @ max_q2

        # Arbitration weight via logistic transform; anxiety and uncertainty push w
        z = logit_omega0 + phi_anx * (st - 0.5) + kappa_unc * uncert
        w = 1.0 / (1.0 + np.exp(-z))

        # First-stage softmax over combined value
        q1 = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax within reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 model-free update: back up the obtained second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation planner with anxiety-shaped planning horizon and WSLS bias.
    
    Summary
    -------
    - Uses a successor representation (SR) over planets to evaluate first-stage actions.
      SR depends on a discount gamma that is increased by anxiety (longer planning horizon).
    - Learns second-stage action values with utility scaling rho (reduced by anxiety).
    - Adds a win-stay/lose-shift bias at stage 2, attenuated by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta, beta, gamma0, rho0, kappa_wsls]
        - eta: learning rate for stage-2 Q updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - gamma0: baseline SR discount parameter [0,1]
        - rho0: baseline reward sensitivity (utility scaling) [0,1]
        - kappa_wsls: strength of win-stay/lose-shift bias at stage 2 [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    eta, beta, gamma0, rho0, kappa_wsls = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values
    q2 = np.zeros((2, 2)) + 0.5

    # Track previous second-stage choice and outcome per state for WSLS
    prev_a2 = np.array([-1, -1], dtype=int)
    prev_r = np.array([0.0, 0.0], dtype=float)

    # Anxiety-modulated parameters
    # Planning horizon increases with anxiety (cap at 0.99 to keep (I - gamma*T) invertible)
    gamma = float(np.clip(gamma0 * (0.6 + 0.8 * st), 0.0, 0.99))
    # Reward sensitivity decreases with anxiety
    rho = float(np.clip(rho0 * (0.8 + 0.4 * (1.0 - st)), 0.0, 1.0))
    # WSLS bias attenuated by anxiety
    k_wsls_eff = float(kappa_wsls * (1.0 - 0.5 * st))

    # Precompute SR kernel inverse term per trial since gamma is fixed here
    I = np.eye(2)
    inv_term = np.linalg.inv(I - gamma * T)  # 2x2

    for t in range(n_trials):
        # SR-based first-stage values: v_a = T[a] @ (I - gamma T)^-1 @ max_q2
        max_q2 = np.max(q2, axis=1)  # value per state
        sr_values = np.zeros(2)
        for a in range(2):
            occ = T[a] @ inv_term  # discounted occupancy over states after choosing action a
            sr_values[a] = occ @ max_q2

        logits1 = beta * (sr_values - np.max(sr_values))
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with WSLS bias for the reached state
        s = state[t]
        base_logits2 = q2[s] - np.max(q2[s])

        wsls_bias = np.zeros(2)
        if prev_a2[s] != -1:
            if prev_r[s] > 0.0:
                wsls_bias[prev_a2[s]] += k_wsls_eff
            else:
                wsls_bias[prev_a2[s]] -= k_wsls_eff

        logits2 = beta * (base_logits2 + wsls_bias)
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        u = rho * r  # utility-scaled outcome

        # Update second-stage Q-values
        pe2 = u - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Update WSLS memory for this state
        prev_a2[s] = a2
        prev_r[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated credit assignment with anxiety-shaped learning rate and perseveration.
    
    Summary
    -------
    - Uses fixed transitions (0.7 common) to compute model-based Q1 and model-free Q1 (TD from stage 2).
    - Surprise (rare transition) gates how much MF credit is assigned back to the chosen first-stage action.
    - Learning rate at stage 2 increases with both surprise and anxiety.
    - Arbitration between MB and MF at stage 1 depends on anxiety (higher anxiety -> more MF).
    - First-stage perseveration bias increases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta0, beta, chi_surprise, pi_rep, zeta_anx]
        - eta0: base learning rate [0,1]
        - beta: inverse temperature for both stages [0,10]
        - chi_surprise: strength of surprise gating on MF credit and learning rate [0,1]
        - pi_rep: perseveration bias to repeat previous first-stage action [0,1]
        - zeta_anx: anxiety sensitivity affecting both arbitration and learning rate [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    eta0, beta, chi_surprise, pi_rep, zeta_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    # Anxiety-modulated arbitration weight: higher anxiety => lower MB weight
    w_mb = 1.0 / (1.0 + np.exp(zeta_anx * (st - 0.5) * 4.0))  # smooth, in (0,1)

    prev_a1 = None

    for t in range(n_trials):
        # Model-based Q1 from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # First-stage softmax with perseveration bias
        bias = np.zeros(2)
        if prev_a1 is not None:
            # Perseveration increases with anxiety
            bias[prev_a1] += pi_rep * (0.4 + 0.8 * st)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias
        logits1 = beta * (q1 - np.max(q1))
        probs_1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs_2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Determine whether the observed transition was common or rare (surprise)
        # Common mapping: A->X (0->0), U->Y (1->1)
        common = (a1 == s)
        p_common = 0.7 if common else 0.3
        surprise = 1.0 - p_common  # 0.3 for common, 0.7 for rare

        # Anxiety and surprise shape the effective learning rate at stage 2
        eta_t = eta0 * (0.5 + 0.5 * zeta_anx * st + chi_surprise * surprise)
        eta_t = float(np.clip(eta_t, 0.0, 1.0))

        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_t * pe2

        # Surprise-gated model-free credit assignment back to stage 1
        # Rare transitions reduce MF credit (classic two-step signature),
        # and anxiety increases this attenuation via zeta_anx.
        gate = 1.0 - chi_surprise * surprise * (0.6 + 0.4 * st * zeta_anx)
        gate = float(np.clip(gate, 0.0, 1.0))
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta0 * gate * pe1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)