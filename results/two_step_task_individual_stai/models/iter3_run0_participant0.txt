def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated learning with anxiety-weighted arbitration (MB vs. MF).

    This model learns stage-2 action values and their uncertainty, uses an
    uncertainty- and anxiety-dependent effective learning rate at stage 2,
    and arbitrates at stage 1 between model-based (MB) and model-free (MF)
    values using an arbitration weight that decreases with both anxiety and
    global uncertainty.

    Parameters (bounds):
    - alpha0: [0,1] base learning rate for stage-2 value updates
    - kappa_unc: [0,1] update rate for tracking uncertainty (PE^2) at stage 2
    - w_anx: [0,1] strength by which anxiety down-weights model-based arbitration
    - beta: [0,10] inverse temperature for both stages

    Inputs:
    - action_1: array of length T with first-stage choices (0 or 1)
    - state: array of length T with observed second-stage state (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1)
    - reward: array of length T with rewards (e.g., 0/1)
    - stai: array-like with one element in [0,1] indicating anxiety score
    - model_parameters: [alpha0, kappa_unc, w_anx, beta]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha0, kappa_unc, w_anx, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value and uncertainty for stage 2
    q2 = np.zeros((2, 2))
    var2 = np.ones((2, 2)) * 0.25  # initial uncertainty

    # Model-free values at stage 1
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Global normalized uncertainty (0..1-ish) based on current var2
        u = np.mean(var2)
        u_norm = u / (u + 0.1)

        # Arbitration weight: higher anxiety and higher uncertainty -> less MB
        w_mb = np.clip((1.0 - w_anx * stai_score) * (1.0 - u_norm), 0.0, 1.0)

        # Combine MB and MF for stage 1 decision values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (state-conditioned)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Update uncertainty tracker (variance-like)
        var2[s, a2] = (1.0 - kappa_unc) * var2[s, a2] + kappa_unc * (pe2 * pe2)

        # Uncertainty- and anxiety-gated learning rate
        alpha_eff = alpha0 / (1.0 + var2[s, a2])
        alpha_eff *= (1.0 + stai_score)  # more anxious -> faster updating
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Update stage-2 value
        q2[s, a2] += alpha_eff * pe2

        # Model-free backup to stage-1 action value
        q1_mf[a1] += alpha_eff * (q2[s, a2] - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based control with learned transition probabilities, anxiety-boosted
    transition learning.

    This model learns both second-stage action values and the first-stage
    transition structure. Stage-1 choices are purely model-based using the
    learned transitions. Transition learning rate increases with anxiety, capturing
    heightened sensitivity to contingency changes.

    Parameters (bounds):
    - alpha_r: [0,1] learning rate for stage-2 reward values
    - alpha_tr_base: [0,1] base learning rate for transition probabilities
    - anx_tr_gain: [0,1] multiplicative gain determining how much anxiety increases transition LR
    - beta: [0,10] inverse temperature for both stages

    Inputs:
    - action_1: array of length T with first-stage choices (0 or 1)
    - state: array of length T with observed second-stage state (0=X, 1=Y)
    - action_2: array of length T with second-stage choices (0 or 1)
    - reward: array of length T with rewards (e.g., 0/1)
    - stai: array-like with one element in [0,1] indicating anxiety score
    - model_parameters: [alpha_r, alpha_tr_base, anx_tr_gain, beta]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, alpha_tr_base, anx_tr_gain, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Learned transition matrix T[action, state], initialize uniform
    T = np.ones((2, 2)) * 0.5

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate
    alpha_tr = alpha_tr_base * (1.0 + anx_tr_gain * stai_score)
    alpha_tr = np.clip(alpha_tr, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based value using learned transitions
        max_q2 = np.max(q2, axis=1)  # over actions at each second-stage state
        q1_mb = T @ max_q2

        # Optional mild anxiety-dependent exploration: reduce precision with anxiety
        beta_eff = beta * (1.0 - 0.3 * stai_score)
        beta_eff = max(beta_eff, 0.0)

        # Stage 1 policy (pure MB)
        logits1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update transitions for the chosen first-stage action toward the observed state
        # Move probability mass toward the observed state s
        T[a1, :] = (1.0 - alpha_tr) * T[a1, :]
        T[a1, s] += alpha_tr
        # Ensure numeric normalization (should be normalized by construction)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        # Stage-2 reward learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive omission aversion with anxiety-driven lapse and stage-1 perseveration.

    This model learns utility-weighted values at stage 2, where missed rewards
    (omissions) are penalized via a loss-aversion-like factor that grows with anxiety.
    It includes an anxiety-driven lapse at both stages and a stage-1 choice
    perseveration that strengthens with anxiety. Stage-1 values combine MB and MF
    without adding extra parameters.

    Parameters (bounds):
    - alpha: [0,1] learning rate for value updates
    - lambda0: [0,1] baseline omission aversion (higher -> stronger penalty for 0 reward)
    - eps0: [0,1] baseline lapse scaling; effective lapse grows with anxiety
    - pers1: [0,1] baseline perseveration strength at stage 1
    - beta: [0,10] inverse temperature for both stages

    Anxiety usage:
    - Omission aversion: lambda_eff = clip(lambda0 * (1 + stai), 0, 1)
    - Lapse: epsilon = clip(eps0 * stai, 0, 0.5)
    - Perseveration: pers1_eff = pers1 * stai
    - Arbitration weight (no extra parameter): w_mb = 0.25 + 0.5 * (1 - stai)

    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, lambda0, eps0, pers1, beta]

    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, lambda0, eps0, pers1, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Stage-2 and stage-1 MF values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    # Effective parameters modulated by anxiety
    lambda_eff = np.clip(lambda0 * (1.0 + stai_score), 0.0, 1.0)
    epsilon = np.clip(eps0 * stai_score, 0.0, 0.5)
    pers1_eff = pers1 * stai_score
    w_mb = 0.25 + 0.5 * (1.0 - stai_score)  # in [0.25, 0.75]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = -1

    for t in range(n_trials):
        # Model-based at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Combine MB/MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add stage-1 perseveration bias for repeating last choice
        bias1 = np.zeros(2)
        if last_a1 != -1:
            bias1[last_a1] += pers1_eff

        # Stage 1 policy with lapse
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 /= np.sum(probs1)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with lapse
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Utility-transformed outcome: omission penalized
        r = reward[t]
        util = r - lambda_eff * (1.0 - r)

        # Update stage-2 values
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-1 MF via bootstrapped backup
        q1_mf[a1] += alpha * (q2[s, a2] - q1_mf[a1])

        # Update perseveration memory
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll