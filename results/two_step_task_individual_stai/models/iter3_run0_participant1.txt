def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MBâ€“MF with anxiety-gated arbitration and first-stage perseveration.
    
    This model combines a model-based (MB) evaluation that uses the known common/rare transition
    structure with a model-free (MF) first-stage value learned from second-stage outcomes.
    Anxiety increases reliance on the MF system (lower MB arbitration weight), implementing
    a shift toward habitual control in high anxiety. A first-stage perseveration term biases
    repeating the previous spaceship. A small lapse applies to both stages.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha: learning rate for second-stage MF values and backpropagation to first-stage MF (0..1).
    - beta: inverse temperature for both stages (0..10).
    - omega0: baseline arbitration weight for MB contribution at stage 1 (0..1).
    - phi_stai: strength by which anxiety reduces MB weight: omega = clip(omega0 - phi_stai*stai, 0, 1).
    - kappa1: first-stage perseveration strength (adds to the logit of the previously chosen spaceship; 0..1).
    - epsilon: lapse rate applied to both stages (0..1).

    Args:
        action_1: 1D int array of first-stage actions (0/1) chosen on each trial.
        state: 1D int array of encountered second-stage states (0 for X, 1 for Y).
        action_2: 1D int array of second-stage actions within the encountered state (0/1).
        reward: 1D float array of rewards (typically 0/1).
        stai: 1D array with a single float anxiety score in [0,1].
        model_parameters: iterable of parameters in the order above.

    Returns:
        Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, omega0, phi_stai, kappa1, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free values
    Q2 = np.zeros((2, 2))      # second-stage action values
    Q1_mf = np.zeros(2)        # first-stage model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    prev_a1 = -1  # for perseveration

    # Anxiety-gated arbitration weight
    omega = omega0 - phi_stai * stai
    omega = max(0.0, min(1.0, omega))
    beta_eff = max(beta, 1e-6)

    for t in range(n_trials):
        # Model-based forecast for each spaceship: value = T @ max_a Q2(s, a)
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Hybrid first-stage value
        Q1_hyb = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # First-stage perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # Stage 1 policy
        logits1 = beta_eff * Q1_hyb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within encountered state)
        s = state[t]
        logits2 = beta_eff * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning at stage 2 (MF)
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Backpropagate to first stage MF using the realized second-stage value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-free with asymmetric learning, anxiety-driven pessimistic utility, and Q decay.
    
    This model assumes choices are governed by model-free values only. Second-stage action values
    are learned with asymmetric learning rates for positive vs. negative prediction errors.
    Anxiety adds pessimism by penalizing zero-reward outcomes: r_eff = r - psi*stai*(1 - r),
    making omissions feel worse under higher anxiety. Values slowly decay toward a neutral prior
    (0.5) to capture forgetfulness/drift. Perseveration at the second stage biases repeating
    the previous alien choice within a planet.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha_pos: learning rate when prediction error is positive (0..1).
    - alpha_neg: learning rate when prediction error is negative (0..1).
    - beta: inverse temperature for both stages (0..10).
    - psi_anx: anxiety-driven pessimism strength (0..1).
    - decay: per-trial decay toward 0.5 for Q2 values (0..1).
    - kappa2: second-stage perseveration strength (0..1).

    Args:
        action_1: 1D int array of first-stage actions (0/1).
        state: 1D int array of second-stage states (0/1).
        action_2: 1D int array of second-stage actions within state (0/1).
        reward: 1D float array of rewards (0/1).
        stai: 1D array with a single float anxiety score in [0,1].
        model_parameters: iterable of parameters in the order above.

    Returns:
        Negative log-likelihood of observed choices across both stages.
    """
    alpha_pos, alpha_neg, beta, psi_anx, decay, kappa2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])
    beta_eff = max(beta, 1e-6)
    eps = 1e-12

    # Initialize values
    Q2 = np.full((2, 2), 0.5)   # start with neutral prior to align with decay anchor
    Q1 = np.zeros(2)            # first-stage MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a2 = np.array([-1, -1], dtype=int)  # previous a2 per state

    for t in range(n_trials):
        # Stage 1 policy from MF value only
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = 0.99 * probs1 + 0.01 * 0.5  # small built-in lapse to avoid degeneracy

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy within encountered state, with perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta_eff * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = 0.99 * probs2 + 0.01 * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome with anxiety-driven pessimism for omission
        r = reward[t]
        r_eff = r - psi_anx * stai * (1.0 - r)  # decreases value of 0 outcomes; leaves 1 unchanged

        # Learning at stage 2 with asymmetry and decay
        pe2 = r_eff - Q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0 else alpha_neg
        # Decay for all Q2 in this state toward 0.5
        Q2[s] = (1.0 - decay) * Q2[s] + decay * 0.5
        # Update chosen action after decay
        Q2[s, a2] += lr2 * pe2

        # Backpropagate to stage 1 MF using observed second-stage action value (SARSA(1))
        pe1 = Q2[s, a2] - Q1[a1]
        lr1 = alpha_pos if pe1 >= 0 else alpha_neg
        Q1[a1] += lr1 * pe1

        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB with anxiety-driven structure reset and dual perseveration.

    The agent learns transition probabilities from experience and second-stage rewards.
    Anxiety increases a hazard of 'structural reset': with probability zeta_reset*stai each trial,
    the current row of the transition matrix for the chosen spaceship is partially reset toward
    an uninformative prior (0.5, 0.5), capturing lapses in maintaining the task structure.
    Both stages include perseveration biases. A lapse parameter applies to both stages.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage rewards (0..1).
    - alphaT: learning rate for transition probabilities (0..1).
    - beta: inverse temperature for both stages (0..10).
    - zeta_reset: strength of anxiety-driven transition reset (0..1).
    - kappa1: first-stage perseveration strength (0..1).
    - kappa2: second-stage perseveration strength within state (0..1).
    - epsilon: lapse rate applied to both stages (0..1).

    Args:
        action_1: 1D int array of first-stage actions (0/1).
        state: 1D int array of observed second-stage states (0/1).
        action_2: 1D int array of second-stage actions within state (0/1).
        reward: 1D float array of rewards (0/1).
        stai: 1D array with a single float anxiety score in [0,1].
        model_parameters: iterable of parameters in the order above.

    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """
    alphaR, alphaT, beta, zeta_reset, kappa1, kappa2, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])
    beta_eff = max(beta, 1e-6)
    eps = 1e-12

    # Initialize transition beliefs to the canonical common/rare structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)
    # Second-stage values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # MB first-stage values from current transition belief
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # First-stage perseveration
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # Stage 1 policy
        logits1 = beta_eff * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perseveration within state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta_eff * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome learning
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Transition learning for the chosen spaceship
        oh = np.array([1.0 if j == s else 0.0 for j in range(2)], dtype=float)
        # Standard exponential averaging toward observed state
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        # Anxiety-driven partial reset toward uninformative prior (0.5, 0.5)
        reset_strength = zeta_reset * stai
        if reset_strength > 0.0:
            T[a1] = (1.0 - reset_strength) * T[a1] + reset_strength * 0.5

        # Renormalize the chosen row to ensure it remains a proper distribution
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / (row_sum + eps)

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll