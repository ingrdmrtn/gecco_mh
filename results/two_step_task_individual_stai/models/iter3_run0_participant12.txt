def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free controller with anxiety-modulated arbitration
    and nonstationarity-aware decay at the second stage.

    This model blends a fixed-transition model-based (MB) controller with a model-free (MF)
    controller, where the arbitration weight is shifted by anxiety (STAI). Second-stage values
    are allowed to drift toward a neutral prior each trial to capture gradual nonstationarity.
    Anxiety also scales the effective learning rate used to back up second-stage values to stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien in that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration and learning.
    model_parameters : list or array
        [alpha, beta, w_base, xi_stai, tau]
        Bounds:
          alpha in [0,1]     : base learning rate for value updates
          beta in [0,10]     : inverse temperature for softmax
          w_base in [0,1]    : baseline MB weight in stage-1 arbitration
          xi_stai in [0,1]   : sensitivity of arbitration and MF backup to STAI
          tau in [0,1]       : nonstationarity decay toward 0.5 for second-stage Q-values

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_base, xi_stai, tau = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)            # model-free first-stage values
    q2 = np.full((2, 2), 0.5)      # second-stage action values, start at neutral prior 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight and MF backup rate
    w = np.clip(w_base + xi_stai * (stai - 0.5), 0.0, 1.0)
    # Effective MF backup rate increases/decreases with STAI
    alpha_back = np.clip(alpha * (1.0 + xi_stai * (stai - 0.5)), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Nonstationarity-aware decay of second-stage values toward 0.5
        if tau > 0.0:
            q2 = (1.0 - tau) * q2 + tau * 0.5

        # Stage-1 MB values: expectation over next-state max-Q
        max_q2 = np.max(q2, axis=1)                    # per state: best second-stage value
        q1_mb = transition_matrix @ max_q2             # expected value for each first-stage action

        # Hybrid arbitration
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2_s = q2[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # MF backup to stage 1 (eligibility-like)
        q1_mf[a1] += alpha_back * (q2[s, a2] - q1_mf[a1])

        # Optional small direct TD correction toward realized second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * 0.0 * td1  # kept zero to avoid extra implicit parameterization

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-modulated transition learning and uncertainty bonuses.

    The agent learns the first-stage transition matrix from experience and combines a learned
    model-based controller with a model-free controller. Anxiety increases the transition learning
    rate and boosts directed exploration via an uncertainty bonus at the second stage.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien in that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate transition learning and exploration bonus.
    model_parameters : list or array
        [alpha, beta, omega, kappa_stai, novelty]
        Bounds:
          alpha in [0,1]        : learning rate for value updates and MF backup
          beta in [0,10]        : inverse temperature
          omega in [0,1]        : weight on model-based controller at stage 1
          kappa_stai in [0,1]   : scaling of transition learning rate by STAI
          novelty in [0,1]      : base weight of uncertainty bonus at stage 2

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, omega, kappa_stai, novelty = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition probabilities; start agnostic
    T = np.full((2, 2), 0.5)  # T[a, s] = P(state=s | action=a)
    # Second-stage Q-values
    q2 = np.full((2, 2), 0.5)
    # Model-free stage-1 values
    q1_mf = np.zeros(2)

    # Visit counters for uncertainty bonus
    visits = np.zeros((2, 2))  # visits[state, action2]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate and exploration weight
    lr_T = np.clip((0.1 + 0.9 * kappa_stai * stai), 0.0, 1.0)
    bonus_scale = novelty * (0.5 + 0.5 * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Uncertainty bonus for current state's second-stage actions
        # Smaller with more visits: 1/sqrt(N+1)
        bonus_s = bonus_scale / np.sqrt(visits[s] + 1.0)
        q2_aug = q2.copy()
        q2_aug[s] = q2[s] + bonus_s

        # Compute MB values using learned transitions and augmented q2
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Stage-1 arbitration
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy uses augmented q2 in the observed state
        q2_s_aug = q2_aug[s]
        q2_centered = q2_s_aug - np.max(q2_s_aug)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Transition learning: update T[a1, :]
        # Move probability mass toward the observed state with lr_T
        T[a1] = (1.0 - lr_T) * T[a1]
        T[a1, s] += lr_T
        # Ensure numerical stability (re-normalize)
        T[a1] = T[a1] / np.sum(T[a1])

        # Second-stage learning
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update visit counts after choice
        visits[s, a2] += 1.0

        # Model-free backup to stage 1, slightly amplified by anxiety
        lambd_eff = 0.5 + 0.5 * stai
        q1_mf[a1] += alpha * lambd_eff * (q2[s, a2] - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with anxiety-modulated lapse and within-state perseveration.

    The agent is model-based at stage 1 using the known transition structure but evaluates
    outcomes through a concave utility function, producing risk sensitivity that depends on a
    risk parameter. Anxiety increases a choice lapse rate (randomness independent of value)
    and induces within-state perseveration at stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien in that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate lapse rate and perseveration.
    model_parameters : list or array
        [alpha, beta, lambda_risk, lapse_base, phi_stai]
        Bounds:
          alpha in [0,1]        : learning rate for utility-based value updates
          beta in [0,10]        : inverse temperature for softmax
          lambda_risk in [0,1]  : base risk sensitivity for utility curvature
          lapse_base in [0,1]   : baseline lapse probability
          phi_stai in [0,1]     : sensitivity of lapse and perseveration to STAI

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lambda_risk, lapse_base, phi_stai = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Second-stage values (in utility space)
    q2 = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse rate and perseveration strength
    lapse = np.clip(lapse_base + phi_stai * (stai - 0.5), 0.0, 1.0)
    stick2_strength = phi_stai * stai  # bias toward repeating previous a2 within state

    prev_a2 = [-1, -1]  # previous second-stage action for each state

    # Risk sensitivity: concave utility u(r) = r^(1 - lambda_eff)
    lambda_eff = np.clip(lambda_risk * (0.5 + stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage-1 MB values via expected max utility-based second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2
        q1 = q1_mb.copy()

        # Stage-1 policy with lapse mixture
        q1_centered = q1 - np.max(q1)
        soft_1 = np.exp(beta * q1_centered)
        soft_1 = soft_1 / np.sum(soft_1)
        probs_1 = (1.0 - lapse) * soft_1 + lapse * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with within-state perseveration and lapse
        q2_s = q2[s].copy()
        if prev_a2[s] in (0, 1):
            bias = np.zeros(2)
            bias[prev_a2[s]] = stick2_strength
            q2_s = q2_s + bias

        q2_centered = q2_s - np.max(q2_s)
        soft_2 = np.exp(beta * q2_centered)
        soft_2 = soft_2 / np.sum(soft_2)
        probs_2 = (1.0 - lapse) * soft_2 + lapse * 0.5
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed reward
        r = reward[t]
        # Map 0->0, 1->1 with curvature; for r in {0,1}, preserves bounds
        u = r ** (1.0 - lambda_eff)

        # Update second-stage values in utility space
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)