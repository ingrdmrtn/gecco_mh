def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated planning weight and eligibility tracing.
    
    This model blends model-free (MF) and model-based (MB) action values at the
    first stage. MB values are computed from a fixed transition model and the
    current second-stage Q-values. MF values at stage 1 are updated via an
    eligibility trace that backs up second-stage values. Anxiety increases or
    decreases the reliance on MB planning via a linear gain on the MB weight.
    
    Parameters (model_parameters):
    - alpha_q: learning rate for Q-value updates (both stages), in [0,1]
    - beta: inverse temperature for both stages' softmax, in [0,10]
    - w0: baseline weight on MB contribution at stage 1, in [0,1]
    - g_w: anxiety gain for MB weight (positive -> more planning with higher anxiety), in [0,1]
    - elig: eligibility strength for backing up stage-2 value into stage-1 MF, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float anxiety score in [0,1]
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_q, beta, w0, g_w, elig = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure: rows ships [A,U], cols planets [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)        # stage-1 MF values for [A,U]
    q2 = np.zeros((2, 2))      # stage-2 values per planet x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight
    w_mb = w0 + g_w * (stai - 0.5)  # center at 0.5 for symmetry
    w_mb = np.clip(w_mb, 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB action values from current Q2 and known transitions
        max_q2 = np.max(q2, axis=1)     # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid first-stage values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        logits1 = beta * (q1 - np.max(q1))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy for observed planet
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * delta2

        # Back up stage-2 value to stage-1 MF via eligibility
        # Move q1_mf toward current second-stage action value for the chosen path.
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_q * elig * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pessimistic-planning model-based RL with anxiety-modulated risk attitude.
    
    This model uses a purely model-based (MB) first-stage evaluation with a
    pessimistic aggregation at the second stage: instead of using the max over
    second-stage options, it blends the best and worst second-stage Q-values.
    Anxiety modulates the pessimism parameter, shifting weight toward the worst
    outcome under higher anxiety. Stage 2 uses standard MF Q-learning.
    
    Parameters (model_parameters):
    - alpha: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - z0: baseline optimism (z=1 is max-only, z=0 is min-only), in [0,1]
    - g_z: anxiety gain on pessimism (negative values push toward min with higher stai), in [0,1]
      Effective z = clip(z0 + g_z*(0.5 - stai), 0, 1) so higher stai reduces z if g_z>0.
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float anxiety score in [0,1]
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, z0, g_z = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # planet x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated pessimism: higher stai -> lower z (more weight on worst)
    z = z0 + g_z * (0.5 - stai)
    z = np.clip(z, 0.0, 1.0)

    for t in range(n_trials):
        # Pessimistic aggregator per planet
        planet_best = np.max(q2, axis=1)
        planet_worst = np.min(q2, axis=1)
        planet_val = z * planet_best + (1.0 - z) * planet_worst

        # MB first-stage values via expected planet_val under transitions
        q1_mb = T @ planet_val

        # Stage-1 policy
        logits1 = beta * (q1_mb - np.max(q1_mb))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (standard softmax on q2 at reached planet)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update second-stage values
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Directed-exploration bonus with anxiety-modulated curiosity and count decay.
    
    This model uses MB action values at stage 1 computed from second-stage
    values augmented by an uncertainty-driven exploration bonus. Uncertainty is
    estimated from simple visit counts per planet-alien, with optional decay to
    capture non-stationarity. Anxiety modulates the curiosity strength: higher
    anxiety reduces the bonus (less directed exploration).
    
    Stage 2 choices also receive the same bonus, encouraging exploration when
    uncertainty is high. Learning at stage 2 is MF Q-learning.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - xi0: baseline exploration bonus strength, in [0,1]
    - g_xi: anxiety gain on exploration bonus (negative with higher stai if positive), in [0,1]
      Effective xi = clip(xi0 * (1.0 - g_xi * stai), 0, 1)
    - decay_n: count decay per trial toward zero (0=no decay, 1=full forgetting), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float anxiety score in [0,1]
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, beta, xi0, g_xi, decay_n = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # planet x alien Q-values
    n = np.zeros((2, 2))    # visit counts per planet x alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration strength
    xi = xi0 * (1.0 - g_xi * stai)
    xi = np.clip(xi, 0.0, 1.0)

    for t in range(n_trials):
        # Compute uncertainty bonus from counts (higher when fewer visits)
        # Use 1/sqrt(n+1) scaling for directed exploration
        bonus = xi * (1.0 / np.sqrt(n + 1.0))

        # Stage-1 MB values computed from (Q2 + bonus) on each planet
        q2_plus = q2 + bonus
        planet_best_aug = np.max(q2_plus, axis=1)
        q1_mb = T @ planet_best_aug

        # Stage-1 policy
        logits1 = beta * (q1_mb - np.max(q1_mb))
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy uses augmented values to capture exploration at choice
        s = state[t]
        logits2 = beta * (q2_plus[s] - np.max(q2_plus[s]))
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update counts with decay
        n *= (1.0 - decay_n)
        n[s, a2] += 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll