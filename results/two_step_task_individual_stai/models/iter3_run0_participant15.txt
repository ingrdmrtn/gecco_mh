def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """MB planning with anxiety-damped learning, UCB exploration at Stage-2, and surprise-driven bias at Stage-1.
    
    Mechanisms:
    - Stage-1: Model-based values from transition model; adds a surprise-driven bias that uses the previous trial's rare transition
      to favor the action that commonly leads to the last visited state. Surprise influence scales with anxiety.
    - Stage-2: Upper Confidence Bound (UCB) exploration bonus based on visit counts; exploration is reduced by higher anxiety.
    - Learning: Stage-2 MF learning with an anxiety-damped learning rate.
    
    Parameters (all used; total=5):
    - alpha_b: [0,1] Base learning rate for Stage-2 MF updates; reduced by anxiety.
    - beta: [0,10] Inverse temperature for both stages.
    - ucb: [0,1] Magnitude of exploration bonus at Stage-2; down-weighted by anxiety.
    - surpr: [0,1] Strength of surprise-driven Stage-1 bias after rare transitions; scaled by anxiety.
    - q0: [0,1] Initial Q-value for Stage-2 action values.
    
    Inputs:
    - action_1: array-like (n_trials,) First-stage choices (0=A, 1=U).
    - state: array-like (n_trials,) Observed planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,) Second-stage choices (0 or 1; alien on that planet).
    - reward: array-like (n_trials,) Observed reward (e.g., 0 or 1).
    - stai: array-like (1,) Anxiety score in [0,1].
    - model_parameters: iterable of the five parameters above in order.
    
    Returns:
    - Negative log-likelihood of the observed Stage-1 and Stage-2 choices.
    """
    alpha_b, beta, ucb, surpr, q0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Prob tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value and count estimates
    q2 = q0 * np.ones((2, 2))  # Q-values at Stage-2
    n_sa = np.zeros((2, 2))    # visit counts for UCB

    # Anxiety effects
    alpha_eff_scale = 1.0 - 0.5 * stai
    alpha_eff_scale = max(0.0, min(1.0, alpha_eff_scale))
    alpha = alpha_b * alpha_eff_scale

    ucb_eff = ucb * (1.0 - stai)  # less directed exploration with higher anxiety
    # Surprise bias magnitude grows with anxiety (more sensitivity to surprise)
    surpr_eff = surpr * stai

    # Surprise memory from previous trial
    prev_rare = 0.0
    prev_state = None

    for t in range(n_trials):
        s = state[t]

        # Stage-1 model-based values from current q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Surprise-driven bias: after a rare transition on previous trial,
        # favor the action that commonly leads to the previously visited state.
        bias = np.zeros(2)
        if prev_state is not None and prev_rare > 0.0:
            a_common_for_prev_state = prev_state  # A(0)->X(0) common, U(1)->Y(1) common
            bias[a_common_for_prev_state] += surpr_eff * prev_rare

        # Stage-1 policy
        z1 = beta * (mb_q1 + bias - np.max(mb_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 UCB-augmented policy in the observed state
        bonus = ucb_eff / np.sqrt(n_sa[s] + 1.0)
        q2_bonus = q2[s] + bonus
        z2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update visit counts for UCB
        n_sa[s, a2] += 1.0

        # Compute whether current transition is rare for next-trial bias
        # Rare if (A->Y) or (U->X)
        is_common = (a1 == s)  # since A(0)->X(0) common, U(1)->Y(1) common
        prev_rare = 1.0 - float(is_common)
        prev_state = s

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated volatility learning (Pearce–Hall) with perseveration and MB planning.
    
    Mechanisms:
    - Volatility estimate v_t tracks absolute PEs and modulates both learning rate and choice stochasticity.
    - Stage-2 learning rate: alpha_t = alpha_min + (gain * stai) * |PE_t|, clipped to [0,1].
    - Softmax temperature: beta_eff = beta * (1 - stai * v_t), reducing determinism under high volatility and anxiety.
    - Stage-1 values: Model-based from transition model, plus anxiety-scaled perseveration bias toward previous a1.
    
    Parameters (all used; total=5):
    - alpha_min: [0,1] Baseline learning rate floor.
    - beta: [0,10] Base inverse temperature for both stages (modulated to beta_eff).
    - vol0: [0,1] Initial volatility estimate v_0.
    - gain: [0,1] Volatility/gain parameter controlling sensitivity to |PE|; scaled by stai.
    - persever: [0,1] Strength of perseveration bias at Stage-1; scaled by stai.
    
    Inputs:
    - action_1: array-like (n_trials,) First-stage choices (0=A, 1=U).
    - state: array-like (n_trials,) Observed planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,) Second-stage choices (0 or 1).
    - reward: array-like (n_trials,) Observed reward.
    - stai: array-like (1,) Anxiety score in [0,1].
    - model_parameters: iterable of the five parameters above in order.
    
    Returns:
    - Negative log-likelihood of the observed Stage-1 and Stage-2 choices.
    """
    alpha_min, beta, vol0, gain, persever = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))

    # Volatility state and anxiety couplings
    v = float(vol0)
    gain_eff = gain * stai
    persever_eff = persever * stai  # stronger perseveration with higher anxiety

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Effective inverse temperature decreases with volatility and anxiety
        beta_eff = beta * (1.0 - stai * v)
        beta_eff = max(0.0, beta_eff)

        # Stage-1 MB values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Perseveration bias on previous a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = persever_eff

        # Stage-1 policy
        z1 = beta_eff * (mb_q1 + bias - np.max(mb_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and PE
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Pearce–Hall style learning rate
        alpha_t = alpha_min + gain_eff * abs(pe2)
        if alpha_t < 0.0:
            alpha_t = 0.0
        if alpha_t > 1.0:
            alpha_t = 1.0

        # Update Stage-2 values
        q2[s, a2] += alpha_t * pe2

        # Update volatility estimate with leaky integration of |PE|
        v = (1.0 - gain_eff) * v + gain_eff * abs(pe2)
        v = min(1.0, max(0.0, v))

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-driven meta-control fatigue: time-decaying MB weight, dual-stage temperatures, and MF eligibility.
    
    Mechanisms:
    - Stage-1 arbitration: mixture of model-based (MB) and model-free (MF) Q-values with a weight w_t that decays over trials
      as a function of anxiety (meta-control fatigue). Higher anxiety produces faster decay toward MF control.
    - Stage-2: Standard MF learning.
    - Stage-1 MF: Updated via bootstrapped TD using realized Stage-2 value at the visited state.
    - Choice stochasticity: separate inverse temperatures for Stage-1 and Stage-2. Stage-2 is additionally noise-amplified by anxiety.
    
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for both Stage-2 and Stage-1 MF updates.
    - beta1: [0,10] Inverse temperature for Stage-1.
    - beta2: [0,10] Base inverse temperature for Stage-2 (modulated by anxiety).
    - w_start: [0,1] Initial MB weight at trial 0.
    - drift: [0,1] Fatigue rate controlling how quickly MB weight decays; amplified by anxiety.
    
    Anxiety use:
    - Meta-control fatigue: w_t = sigmoid(logit(w_start) - (drift * stai) * t).
    - Additional noise at Stage-2: beta2_eff = beta2 * (1 - 0.5 * stai).
    
    Inputs:
    - action_1: array-like (n_trials,) First-stage choices (0=A, 1=U).
    - state: array-like (n_trials,) Observed planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,) Second-stage choices (0 or 1).
    - reward: array-like (n_trials,) Observed reward.
    - stai: array-like (1,) Anxiety score in [0,1].
    - model_parameters: iterable of the five parameters above in order.
    
    Returns:
    - Negative log-likelihood of the observed Stage-1 and Stage-2 choices.
    """
    alpha, beta1, beta2, w_start, drift = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 values and Stage-1 MF values
    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    # Helper for stable sigmoid/logit
    def clip01(x):
        return min(1.0, max(0.0, x))

    w0 = clip01(w_start)
    # avoid logit(0) or logit(1)
    eps_w = 1e-6
    w0 = min(1.0 - eps_w, max(eps_w, w0))
    logit_w0 = np.log(w0 / (1.0 - w0))
    fatigue = drift * stai

    for t in range(n_trials):
        s = state[t]

        # Time-varying MB weight driven by anxiety-induced fatigue
        w_t = 1.0 / (1.0 + np.exp(-(logit_w0 - fatigue * t)))
        w_t = clip01(w_t)

        # Model-based Q for Stage-1 from current q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Mixed Stage-1 Q
        q1_mix = w_t * mb_q1 + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        z1 = beta1 * (q1_mix - np.max(q1_mix))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-reduced inverse temperature
        beta2_eff = beta2 * (1.0 - 0.5 * stai)
        beta2_eff = max(0.0, beta2_eff)
        z2 = beta2_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes and updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF TD update using realized Stage-2 action value as bootstrap
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll