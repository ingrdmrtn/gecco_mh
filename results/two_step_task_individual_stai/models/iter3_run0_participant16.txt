def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free learner with anxiety-modulated loss sensitivity and arbitration.
    
    Idea:
    - Stage-2 values are learned via asymmetric learning rates (losses loom larger when anxiety is high).
    - Stage-1 action values combine model-based planning with model-free SARSA backup.
    - Anxiety increases the weight on model-based control and amplifies negative prediction-error learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 within reached state).
    reward : array-like of float in [0,1]
        Rewards received at the end of each trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase loss sensitivity and model-based arbitration weight.
    model_parameters : iterable of 5 floats
        - alpha_base in [0,1]: baseline learning rate for value updates.
        - beta1 in [0,10]: softmax inverse temperature for stage-1.
        - beta2 in [0,10]: softmax inverse temperature for stage-2.
        - w_base in [0,1]: baseline weight on model-based control at stage-1.
        - phi_base in [0,1]: baseline loss-sensitivity factor (scales negative PE learning).
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_base, beta1, beta2, w_base, phi_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition matrix: A->X common, U->Y common
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated arbitration and asymmetric learning
    # MB weight moves toward 1 with higher anxiety
    w = np.clip(w_base + 0.6 * stai * (1.0 - w_base), 0.0, 1.0)
    # Asymmetric learning rates: negative PE increased with anxiety
    alpha_pos = np.clip(alpha_base * (1.0 - 0.4 * stai * phi_base), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 + 0.6 * stai * phi_base), 0.0, 1.0)
    # Use a small amount of eligibility at stage-1 that also scales with anxiety via phi
    lam = np.clip(0.2 + 0.6 * stai * phi_base, 0.0, 1.0)

    # Value functions
    q1_mf = np.zeros(2)        # model-free Q at stage-1
    q2 = np.zeros((2, 2))      # stage-2 Q per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based lookahead: value of first-stage action = expected best second-stage value
        max_q2 = np.max(q2, axis=1)    # best within each state
        q1_mb = T @ max_q2

        # Hybrid policy at stage-1
        q1 = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta1 * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        logits2 = beta2 * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update with asymmetric learning
        pe2 = r - q2[s, a2]
        if pe2 >= 0.0:
            q2[s, a2] += alpha_pos * pe2
        else:
            q2[s, a2] += alpha_neg * pe2

        # Stage-1 model-free SARSA(Î») backup using the chosen second-stage value as target
        # TD target for stage-1 is the pre-update q2 value of the chosen second-stage action
        # Use the current q2 (post-update) but reduce with lambda to avoid over-crediting
        target1 = (1.0 - lam) * q1_mf[a1] + lam * q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use a balanced learning rate: average of pos/neg to keep scale consistent
        alpha1 = 0.5 * (alpha_pos + alpha_neg)
        q1_mf[a1] += alpha1 * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Exploration-bonus planner with anxiety-driven directed exploration and uncertainty tracking.
    
    Idea:
    - Stage-2 maintains both mean rewards and uncertainty (variance proxy) per option.
    - Anxiety increases directed exploration via an uncertainty bonus and speeds uncertainty adaptation.
    - Stage-1 plans over uncertainty-bonus-adjusted values (optimism under uncertainty), while stage-2
      choices use the same bonus in their softmax.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 within reached state).
    reward : array-like of float in [0,1]
        Rewards received at the end of each trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase the exploration bonus and uncertainty adaptation.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for mean reward estimates at stage-2.
        - beta1 in [0,10]: softmax inverse temperature for stage-1.
        - beta2 in [0,10]: softmax inverse temperature for stage-2.
        - xi_base in [0,1]: baseline coefficient for directed exploration bonus.
        - tau_base in [0,1]: baseline uncertainty-adaptation rate (variance update gain).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, xi_base, tau_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated exploration bonus and uncertainty adaptation
    xi = np.clip(xi_base + 0.8 * stai * (1.0 - xi_base), 0.0, 1.0)      # exploration bonus weight
    tau = np.clip(tau_base + 0.5 * stai * (1.0 - tau_base), 0.0, 1.0)   # uncertainty update gain

    # Stage-2 statistics
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # initial uncertainty near Bernoulli(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Risk/uncertainty-seeking planning via bonus on uncertain options
        bonus = xi * np.sqrt(np.maximum(q2_var, 1e-8))
        augmented = q2_mean + bonus

        # Stage-1 model-based values are expectations over best augmented value in each state
        max_aug = np.max(augmented, axis=1)
        q1_mb = T @ max_aug

        logits1 = beta1 * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        logits2 = beta2 * (q2_mean[s] + bonus[s])
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update means
        delta = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * delta

        # Update uncertainty proxy (variance-like), adapting faster with higher anxiety (via tau)
        # EWMA of squared prediction error
        q2_var[s, a2] = (1.0 - tau) * q2_var[s, a2] + tau * (delta ** 2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman filter learner with anxiety-modulated volatility and stage-1 perseveration.
    
    Idea:
    - Stage-2 values are learned with a simple Kalman filter per state-action, maintaining
      both means and uncertainties. Anxiety increases assumed process noise (volatility),
      which increases the Kalman gain (faster adaptation). Anxiety also slightly reduces
      stage-2 determinism, and we include a stage-1 perseveration term.
    - Stage-1 plans model-based over current mean estimates.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1 within reached state).
    reward : array-like of float in [0,1]
        Rewards received at the end of each trial.
    stai : array-like with single float in [0,1]
        Anxiety score; higher values increase volatility and reduce stage-2 inverse temperature.
    model_parameters : iterable of 5 floats
        - beta1 in [0,10]: softmax inverse temperature for stage-1.
        - beta2_base in [0,10]: baseline softmax inverse temperature for stage-2.
        - nu_base in [0,1]: baseline process noise (volatility) for Kalman prediction step.
        - zeta_base in [0,1]: baseline observation noise for Kalman update step.
        - kappa in [0,1]: stage-1 perseveration weight added to the previously chosen action.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta1, beta2_base, nu_base, zeta_base, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    T = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Anxiety-modulated parameters
    # Higher anxiety -> higher process noise (more volatility), slightly lower beta2 (less exploitation)
    nu = np.clip(nu_base + 0.7 * stai * (1.0 - nu_base), 0.0, 1.0)
    zeta = np.clip(zeta_base, 1e-6, 1.0)
    beta2 = max(0.0, beta2_base * (1.0 - 0.4 * stai))

    # Kalman statistics for stage-2 rewards per state-action
    m = np.zeros((2, 2))        # mean
    v = np.ones((2, 2)) * 0.2   # initial uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Stage-1 planning over means
        max_m = np.max(m, axis=1)
        q1_mb = T @ max_m

        # Perseveration at stage-1
        logits1 = beta1 * q1_mb
        if prev_a1 is not None:
            for a in range(2):
                if a == prev_a1:
                    logits1[a] += kappa
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice
        s = state[t]
        logits2 = beta2 * m[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Kalman prediction step: increase uncertainty due to process noise
        v[s, a2] = np.clip(v[s, a2] + nu, 1e-6, 10.0)

        # Kalman update step
        S = v[s, a2] + zeta  # innovation variance
        K = v[s, a2] / S     # Kalman gain in [0,1]
        pe = r - m[s, a2]
        m[s, a2] += K * pe
        v[s, a2] = (1.0 - K) * v[s, a2]

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)