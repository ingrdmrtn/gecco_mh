def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-biased transition-sensitivity hybrid with forgetting.

    Idea:
    - Stage 1 values are a hybrid of model-based (MB) and model-free (MF).
    - The arbitration weight on each trial depends on the previous trial's transition
      commonness (common vs. rare) and is attenuated by anxiety (STAI).
      Low anxiety => stronger sensitivity to previous transition type.
    - Stage 2 uses MF learning and softmax.
    - Unchosen values decay (forgetting), controlling stability vs. flexibility.

    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w0: baseline MB weight in [0,1]
    - phi: transition-sensitivity of MB weight in [0,1] (scaled by 1 - stai)
    - decay: forgetting rate for unchosen Q-values in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien (0=W/P, 1=S/H depending on planet)
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w0, phi, decay)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w0, phi, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q1_mf = np.zeros(2)       # MF values at stage 1 (actions A/U)
    Q2 = np.zeros((2, 2))     # MF values at stage 2 (state x action)

    # Track previous transition commonness to modulate next trial arbitration
    # prev_common = +1 for common, -1 for rare, 0 for undefined at t=0
    prev_common_flag = 0

    for t in range(n_trials):
        # Compute model-based action values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Trial-wise arbitration weight depends on the previous transition's commonness
        # Low anxiety (small st) => larger modulation; High anxiety => smaller modulation
        w_t = w0 + (1.0 - st) * phi * prev_common_flag * 0.5  # scale so modulation stays modest
        w_t = min(1.0, max(0.0, w_t))

        # Hybrid values and stage-1 policy
        Q1_hyb = w_t * Q1_mb + (1.0 - w_t) * Q1_mf
        pref1 = Q1_hyb
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy given observed state
        s = int(state[t])
        pref2 = Q2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD learning
        # Stage-1 MF bootstraps from Q2 at observed state-action
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Stage-2 MF update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Forgetting (decay) for unchosen options toward a neutral baseline (0.5)
        other_a1 = 1 - a1
        Q1_mf[other_a1] = (1 - decay) * Q1_mf[other_a1] + decay * 0.5
        other_a2 = 1 - a2
        Q2[s, other_a2] = (1 - decay) * Q2[s, other_a2] + decay * 0.5

        # Determine current trial's transition commonness to affect next trial's w_t
        # Common if action matches its more likely state: A->X or U->Y
        is_common = (a1 == s)
        prev_common_flag = 1 if is_common else -1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Stage-specific temperatures and anxiety-driven arbitration gain.

    Idea:
    - Stage 1 action values are a hybrid of MB and MF.
    - The MB weight increases (or decreases) with anxiety via a linear gain.
      w_mb = clip(w_base + k_anx * stai, 0, 1)
    - Different inverse temperatures for stage 1 and stage 2 choices (beta1, beta2).
    - TD(λ) with λ determined by anxiety: higher STAI strengthens credit assignment
      from outcome back to stage-1 (eligibility trace λ = stai).

    Parameters (model_parameters):
    - alpha: learning rate for Q-values in [0,1]
    - beta1: inverse temperature at stage 1 in [0,10]
    - beta2: inverse temperature at stage 2 in [0,10]
    - w_base: base MB weight (at stai=0) in [0,1]
    - k_anx: anxiety gain on MB weight in [0,1] (effective addition scaled by stai)

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien (0=W/P, 1=S/H)
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta1, beta2, w_base, k_anx)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta1, beta2, w_base, k_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated arbitration and trace
    w_mb = w_base + k_anx * st
    w_mb = min(1.0, max(0.0, w_mb))
    lam = st  # eligibility trace strength increases with anxiety

    for t in range(n_trials):
        # Model-based evaluation from Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Stage-1 policy (hybrid)
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        pref1 = Q1_hyb
        exp1 = np.exp(beta1 * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = int(state[t])
        pref2 = Q2[s]
        exp2 = np.exp(beta2 * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD updates with eligibility trace from stage-2 PE to stage-1 choice
        # Stage-2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapping
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Trace-based backpropagation of outcome to stage-1
        Q1_mf[a1] += alpha * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration at stage 2, anxiety-scaled, with action bias at stage 1.

    Idea:
    - Stage 1 uses MB/MF hybrid values with a fixed MB weight.
    - Stage 1 preference includes a bias toward spaceship A vs. U that scales with anxiety
      (e.g., anxious participants may prefer a "safer" option). The bias magnitude is kappa_bias.
    - Stage 2 uses MF values plus a directed exploration bonus based on the running
      magnitude of recent prediction errors (as a proxy for uncertainty/volatility).
      The exploration bonus is down-weighted by anxiety: bonus_eff = bonus_u * (1 - stai).

    Parameters (model_parameters):
    - alpha: learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - w_mb: MB weight for stage 1 in [0,1]
    - bonus_u: magnitude of directed exploration bonus in [0,1]
    - kappa_bias: stage-1 action bias magnitude in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien (0=W/P, 1=S/H)
    - reward: array-like of floats in [0,1]
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, w_mb, bonus_u, kappa_bias)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w_mb, bonus_u, kappa_bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Running estimate of uncertainty via moving average of absolute PE per state-action
    U = np.zeros((2, 2))  # uncertainty proxy in [0,1] approximately

    # Anxiety-scaled bonus (higher anxiety => less directed exploration)
    bonus_eff = bonus_u * (1.0 - st)

    for t in range(n_trials):
        # Model-based evaluation from Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Stage-1 bias: favor A (index 0) vs U (index 1) proportional to anxiety
        # bias vector: [+1, -1] scaled by kappa_bias * st
        bias_vec = np.array([1.0, -1.0]) * (kappa_bias * st)

        # Hybrid Q and policy
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        pref1 = Q1_hyb + bias_vec
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus
        s = int(state[t])
        pref2 = Q2[s] + bonus_eff * U[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD learning at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update uncertainty proxy (moving average of |PE|)
        U[s, a2] = (1 - alpha) * U[s, a2] + alpha * abs(pe2)

        # Stage-1 MF bootstrapping and modest outcome backpropagation
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1
        # Small direct outcome influence to stage-1 scaled by (1 - w_mb):
        Q1_mf[a1] += alpha * (1.0 - w_mb) * pe2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll