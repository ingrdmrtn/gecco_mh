def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free control with anxiety-modulated planning weight and bi-stage perseveration.

    Overview:
    - Stage-2 action values are learned via incremental reward prediction errors.
    - Stage-1 uses a hybrid of model-based (via known transitions) and model-free values.
    - The model-based weight is modulated by anxiety (stai).
    - Perseveration (choice stickiness) acts at both stages and is weaker with higher anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values and stage-1 bootstrapping
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = omega0 (0 to 1): baseline weight on model-based value at stage 1
    - model_parameters[3] = anx_mod (0 to 1): strength of anxiety modulation on model-based weight
       Effective MB weight: omega_eff = clip(omega0 + anx_mod * (0.5 - stai), 0, 1).
       Higher anxiety (stai>0.5) reduces model-based weight.
    - model_parameters[4] = pi (0 to 1): baseline perseveration magnitude; applied as pi*(1 - stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, omega0, anx_mod, pi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)        # model-free stage-1 values
    q2 = np.zeros((2, 2))      # stage-2 values: states x actions

    # Choice probabilities to accumulate
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration bookkeeping
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated weights
    omega_eff = np.clip(omega0 + anx_mod * (0.5 - stai_val), 0.0, 1.0)
    stick_strength = pi * (1.0 - stai_val)  # less stickiness with higher anxiety

    for t in range(n_trials):
        # Model-based Q at stage 1 via transition and current stage-2 max values
        max_q2 = np.max(q2, axis=1)                  # shape (2,)
        q1_mb = transition_matrix @ max_q2           # shape (2,)

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick_strength

        # Hybrid Q at stage 1
        q1_net = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf + bias1

        # Softmax for stage 1
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy: add perseveration in the reached state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick_strength

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 bootstrapping toward stage-2 value (model-free component)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free control with anxiety-modulated rare-transition credit assignment and perseveration.

    Overview:
    - Pure model-free values at both stages.
    - After rare transitions, the credit assigned from stage-2 to stage-1 is discounted.
    - The discount for rare transitions increases with anxiety (stai), simulating reduced trust in rare outcomes.
    - Perseveration bias is stronger with higher anxiety in this model (distinct assumption).

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate
    - model_parameters[1] = beta (0 to 10): inverse temperature at both stages
    - model_parameters[2] = zeta0 (0 to 1): baseline rare-transition credit discount
    - model_parameters[3] = anx_zeta (0 to 1): strength of anxiety modulation on rare credit
        zeta_eff = clip(zeta0 + anx_zeta * stai, 0, 1)
        Credit weight w_credit = 1 - zeta_eff if common, else zeta_eff
    - model_parameters[4] = phi (0 to 1): perseveration magnitude scaled by stai (phi * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, zeta0, anx_zeta, phi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Model-free values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    zeta_eff = np.clip(zeta0 + anx_zeta * stai_val, 0.0, 1.0)
    stick_strength = phi * stai_val  # here, higher anxiety => stronger perseveration

    for t in range(n_trials):
        # Perseveration at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick_strength

        q1_net = q1 + bias1
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]
        # Perseveration at stage 2 within the reached state
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick_strength

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Determine if transition is common or rare given action_1 -> state
        # Common if A->X (0->0) or U->Y (1->1)
        common = (action_1[t] == state[t])
        w_credit = (1.0 - zeta_eff) if common else zeta_eff

        # Stage-1 update with rare-transition credit modulation
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * w_credit * delta1

        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with anxiety-modulated forgetting and uncertainty bonus.

    Overview:
    - Stage-2 values are updated with learning rate alpha and also decay (forget) each trial.
    - A state-action uncertainty measure (based on visit counts) provides an exploration bonus.
    - Both the forgetting rate and the exploration bonus scale with anxiety (stai).
    - Stage-1 choices are fully model-based: compute expected values via transitions from
      the uncertainty-bonused stage-2 values.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = f_base (0 to 1): baseline forgetting rate per trial
    - model_parameters[3] = anx_forget (0 to 1): anxiety modulation of forgetting
        f_eff = clip(f_base + anx_forget * stai, 0, 1)
    - model_parameters[4] = bonus0 (0 to 1): baseline uncertainty bonus strength
        bonus_eff = bonus0 * (1 + stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, f_base, anx_forget, bonus0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Stage-2 values and visit counts for uncertainty
    q2 = np.zeros((2, 2))
    visits = np.zeros((2, 2))  # counts per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    f_eff = np.clip(f_base + anx_forget * stai_val, 0.0, 1.0)
    bonus_eff = bonus0 * (1.0 + stai_val)

    for t in range(n_trials):
        # Decay (forgetting) before computing policy
        q2 *= (1.0 - f_eff)

        # Uncertainty bonus derived from visit counts; higher for less visited actions
        unc = 1.0 / np.sqrt(visits + 1.0)  # shape (2,2); max 1.0 initially

        # Policy at stage 1 via model-based planning using bonused Q2
        q2_bonused = q2 + bonus_eff * unc
        max_q2_bonused = np.max(q2_bonused, axis=1)
        q1_mb = transition_matrix @ max_q2_bonused

        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state using bonused Q2
        s = state[t]
        q2c = q2_bonused[s] - np.max(q2_bonused[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning update at stage 2 (after observing reward)
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visit counts (after choice)
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll