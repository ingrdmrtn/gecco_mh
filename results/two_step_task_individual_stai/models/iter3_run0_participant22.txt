def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB-MF with learned transitions and anxiety-modulated arbitration.
    
    This two-step model learns stage-2 values (MF), learns the action->state transition
    probabilities, and arbitrates between model-based (MB) and model-free (MF) values
    at stage 1. Anxiety down-weights the MB contribution.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 values and TD update of stage-1 MF values.
    - beta: [0,10] inverse temperature for both stages (before any modulation).
    - tau_trans: [0,1] learning rate for actionâ†’state transition probabilities.
    - w0: [0,1] baseline weight on MB values at stage 1.
    - anx_down: [0,1] strength by which anxiety reduces MB weight: w_eff = clip(w0*(1 - anx_down*stai)).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship (A=0, U=1).
    - state: array of ints in {0,1}, reached planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at reached state.
    - reward: array of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, tau_trans, w0, anx_down].
    
    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha, beta, tau_trans, w0, anx_down = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Learned transition model: rows = actions (A=0,U=1), cols = next states (X=0,Y=1)
    # Initialize neutral (0.5/0.5) and learn from experience.
    T = np.ones((2, 2)) * 0.5

    # MF values
    q2 = np.zeros((2, 2))   # stage-2 values by state
    q1_mf = np.zeros(2)     # stage-1 MF values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB arbitration weight (trial-constant here)
    w_eff = w0 * (1.0 - anx_down * stai)
    w_eff = min(1.0, max(0.0, w_eff))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # MB Q at stage 1: use current transition model and greedy stage-2 values
        max_q2 = np.max(q2, axis=1)  # value of each state if acted optimally
        q1_mb = T @ max_q2           # action value = expected state value under learned transitions

        # Hybrid Q at stage 1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Policy stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Policy stage 2 (MF)
        logits2 = beta * (q2[s, :] - np.max(q2[s, :]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Transition learning: move probability mass of chosen action toward observed state
        # Simple delta rule toward one-hot next state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1 - tau_trans) * T[a1, :] + tau_trans * target
        # Renormalize to avoid drift
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

        # MF learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF update via bootstrapping on reached state value (greedy continuation)
        boot = np.max(q2[s, :])
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with asymmetric learning rates and anxiety-amplified loss learning plus perseveration.
    
    This two-stage MF RL uses separate learning rates for positive and negative prediction errors.
    Anxiety selectively amplifies the negative-error learning rate (heightened sensitivity to losses).
    A perseveration term biases repeating the previous choice at each stage.
    
    Parameters (model_parameters):
    - alpha_pos: [0,1] learning rate for positive PEs (both stages).
    - alpha_neg0: [0,1] baseline learning rate for negative PEs (both stages) before anxiety.
    - beta: [0,10] inverse temperature for both stages.
    - stick: [0,1] perseveration strength added to the previously chosen action's logit at each stage.
    - anx_loss: [0,1] scales how much anxiety increases negative learning: alpha_neg = clip(alpha_neg0*(1 + anx_loss*stai)).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats, coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha_pos, alpha_neg0, beta, stick, anx_loss].
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg0, beta, stick, anx_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Anxiety-modulated negative learning rate
    alpha_neg = alpha_neg0 * (1.0 + anx_loss * stai)
    alpha_neg = min(1.0, max(0.0, alpha_neg))

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage 1 policy with perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with perseveration (state-specific)
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick
        logits2 = beta * (q2[s, :] - np.max(q2[s, :])) + bias2
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Stage-2 MF update (asymmetric learning rates)
        pe2 = r - q2[s, a2]
        a2_lr = alpha_pos if pe2 >= 0 else alpha_neg
        q2[s, a2] += a2_lr * pe2

        # Stage-1 MF update with full-outcome credit assignment (lambda=1 style)
        pe1 = r - q1[a1]
        a1_lr = alpha_pos if pe1 >= 0 else alpha_neg
        q1[a1] += a1_lr * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Utility-transformed outcomes with model-based planning and anxiety-modulated lapses.
    
    Stage-2 learning uses a concave/convex utility transform of rewards to capture
    risk sensitivity. Stage-1 choices are purely model-based using known transitions
    (A->X common, U->Y common), and both stages include an epsilon-lapse that increases
    with anxiety (more random responding).
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for utility-based updates at stage 2.
    - beta: [0,10] inverse temperature for softmax policies (pre-lapse).
    - omega: [0,1] risk-sensitivity on rewards: utility u = r**omega (omega<1 -> risk seeking).
    - eps0: [0,1] baseline lapse probability.
    - anx_lapse: [0,1] how strongly anxiety increases lapse: eps = clip(eps0 + anx_lapse*stai).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship.
    - state: array of ints in {0,1}, reached planet.
    - action_2: array of ints in {0,1}, chosen alien.
    - reward: array of floats in [0,1], coins obtained.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, omega, eps0, anx_lapse].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, omega, eps0, anx_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],  # action A
                  [0.3, 0.7]]) # action U

    # Utility-transformed MF stage-2 values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse
    eps_lapse = eps0 + anx_lapse * stai
    eps_lapse = min(1.0, max(0.0, eps_lapse))

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage 1 model-based Q via planning over T and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Softmax with lapse at stage 1
        logits1 = beta * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        probs1 = (1.0 - eps_lapse) * probs1 + eps_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with lapse
        logits2 = beta * (q2[s, :] - np.max(q2[s, :]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        probs2 = (1.0 - eps_lapse) * probs2 + eps_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward update at stage 2
        u = r ** max(1e-8, omega)  # guard tiny omega -> well-defined
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll