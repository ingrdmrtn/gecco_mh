Below are three standalone cognitive models as Python functions. Each returns the negative log-likelihood of the observed first- and second-stage choices, uses the stai score meaningfully, respects parameter bounds, and uses all parameters. No imports are included (assume numpy as np is already available).


def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility backup.

    This model learns model-free (MF) Q-values for both stages and uses a fixed
    transition structure to compute model-based (MB) action values at stage 1.
    It arbitrates between MB and MF at stage 1 with a dynamically changing weight
    that depends on (i) current uncertainty in second-stage preferences (entropy)
    and (ii) anxiety (stai). An eligibility-like backup passes a fraction of the
    immediate reward directly to stage-1 MF values.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Reward received each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [eta, beta, tau, psi0, psi_anx]
        - eta in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax policies.
        - tau in [0,1]: eligibility backup strength from immediate reward to stage-1 MF.
        - psi0 in [0,1]: baseline arbitration bias toward MB (via logistic).
        - psi_anx in [0,1]: how much anxiety tilts arbitration toward MB when stai is low
                            and toward MF when stai is high.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    eta, beta, tau, psi0, psi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # shape (2 actions, 2 states)

    # MF Q-values
    q1_mf = np.zeros(2)        # stage-1 MF
    q2_mf = np.zeros((2, 2))   # stage-2 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    ln2 = np.log(2.0)

    for t in range(n_trials):
        # Compute MB action values at stage 1 from current MF stage-2 Q
        max_q2 = np.max(q2_mf, axis=1)          # best within each state
        q1_mb = T @ max_q2                      # expected best via transitions

        # Compute uncertainty (entropy) of stage-2 policies across states
        # Use current Q2 to form softmax policies, then average normalized entropy
        Hs = []
        for st in (0, 1):
            prefs2 = q2_mf[st]
            e2 = np.exp(beta * (prefs2 - np.max(prefs2)))
            p2 = e2 / np.sum(e2)
            H = -np.sum(p2 * (np.log(p2 + 1e-12)))
            Hs.append(H / ln2)  # normalize to [0,1]
        Hbar = 0.5 * (Hs[0] + Hs[1])

        # Arbitration weight toward MB; anxiety reduces MB weighting when high (if psi_anx > 0)
        # w in (0,1) via logistic; center stai at 0.5 so (0.5 - s) increases w for lower anxiety.
        z = (psi0 - 0.5) + psi_anx * (0.5 - s) - Hbar
        w = 1.0 / (1.0 + np.exp(-5.0 * z))  # steep logistic; bounded (uses psi0, psi_anx, stai, entropy)

        # Hybrid action values at stage 1
        q1_hyb = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy and likelihood
        e1 = np.exp(beta * (q1_hyb - np.max(q1_hyb)))
        p1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy and likelihood
        st = state[t]
        prefs2 = q2_mf[st]
        e2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        p2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Stage-2 MF update (TD(0))
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += eta * pe2

        # Stage-1 MF update using bootstrapped value from visited state-action (SARSA-style)
        pe1 = q2_mf[st, a2] - q1_mf[a1]
        q1_mf[a1] += eta * pe1

        # Eligibility backup: a fraction of the immediate reward directly to stage 1
        q1_mf[a1] += eta * tau * (r - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with learned transitions and anxiety-modulated directed exploration.

    This model learns:
      - Second-stage MF Q-values from reward.
      - First-stage transition probabilities P(state | action_1).
    The stage-1 MB values are computed from the learned transition model and current second-stage values.
    Directed exploration bonuses are added at the second stage based on action uncertainty (count-based),
    and are propagated to stage-1 via the MB backup. Anxiety modulates the exploration bonus magnitude.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the visited state.
    reward : array-like of float
        Reward received each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha_r, alpha_p, beta, zeta0, zeta_stai]
        - alpha_r in [0,1]: learning rate for reward values at the second stage.
        - alpha_p in [0,1]: learning rate for transition probabilities P(s|a1).
        - beta in [0,10]: inverse temperature for both stages.
        - zeta0 in [0,1]: baseline directed exploration bonus scale.
        - zeta_stai in [0,1]: how much anxiety reduces exploration bonus (higher stai -> smaller bonus).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, alpha_p, beta, zeta0, zeta_stai = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Learned transition model initialized neutral (0.5/0.5)
    T = np.ones((2, 2)) * 0.5  # rows: a1 in {0,1}; cols: state in {0,1}

    # Second-stage MF Q-values
    q2 = np.zeros((2, 2))

    # Count-based uncertainty for directed exploration
    N2 = np.zeros((2, 2))  # visit counts per state-action

    # Anxiety-modulated exploration scale: higher anxiety -> smaller directed exploration
    zeta = zeta0 + zeta_stai * (1.0 - s)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Directed exploration bonus at stage 2 (UCB-style): larger when less visited
        bonus2 = zeta / np.sqrt(N2 + 1.0)

        # MB action values for stage 1: expect max(Q2 + bonus) under learned transitions
        q2_plus = q2 + bonus2
        max_q2_plus = np.max(q2_plus, axis=1)  # best action in each state
        q1_mb = T @ max_q2_plus

        # Stage-1 policy
        e1 = np.exp(beta * (q1_mb - np.max(q1_mb)))
        p1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with directed exploration bonus in the visited state
        st = state[t]
        prefs2 = q2[st] + bonus2[st]
        e2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        p2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Update second-stage Q with reward learning
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_r * pe2

        # Update transition model using a simple delta rule toward the observed state
        onehot = np.array([1.0 if st == k else 0.0 for k in (0, 1)])
        T[a1] = T[a1] + alpha_p * (onehot - T[a1])
        # Ensure normalization (numerically stable)
        T[a1] = T[a1] / np.sum(T[a1])

        # Update visitation count for exploration
        N2[st, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk- and valence-sensitive model-free learner with anxiety-shaped PE asymmetry and forgetting.

    Pure model-free learning at both stages. A single learning rate updates values, but
    positive vs. negative prediction errors (PEs) are scaled asymmetrically by an anxiety-dependent
    factor. Additionally, values undergo per-trial forgetting toward an uninformative baseline (0.5),
    capturing drift in latent contingencies.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the visited state.
    reward : array-like of float
        Reward received each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, sigma0, sigma_stai, rho]
        - alpha in [0,1]: base learning rate for value updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - sigma0 in [0,1]: base asymmetry factor for scaling PEs (risk/valence sensitivity).
        - sigma_stai in [0,1]: how much anxiety increases negative-PE sensitivity and reduces positive-PE sensitivity.
        - rho in [0,1]: forgetting rate toward 0.5 applied each trial to all Q-values.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, sigma0, sigma_stai, rho = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF Q-values
    q1 = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-shaped asymmetry factor: higher anxiety -> stronger weighting of negative PEs
    sigma = min(1.0, max(0.0, sigma0 + sigma_stai * s))

    for t in range(n_trials):
        # Policies
        e1 = np.exp(beta * (q1 - np.max(q1)))
        p1 = e1 / np.sum(e1)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        st = state[t]
        e2 = np.exp(beta * (q2[st] - np.max(q2[st])))
        p2 = e2 / np.sum(e2)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        r = reward[t]

        # Forgetting toward 0.5 baseline (applied each trial before updates)
        q1 = (1.0 - rho) * q1 + rho * 0.5
        q2 = (1.0 - rho) * q2 + rho * 0.5

        # Stage-2 update with anxiety-shaped PE asymmetry
        pe2 = r - q2[st, a2]
        scale2 = (1.0 - sigma) if pe2 >= 0.0 else (1.0 + sigma)
        q2[st, a2] += alpha * scale2 * pe2

        # Stage-1 update bootstrapping from updated second-stage value (SARSA(0) style)
        pe1 = q2[st, a2] - q1[a1]
        scale1 = (1.0 - sigma) if pe1 >= 0.0 else (1.0 + sigma)
        q1[a1] += alpha * scale1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll