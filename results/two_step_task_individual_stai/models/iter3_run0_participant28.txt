def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and eligibility trace.
    
    A hybrid controller combines model-free (MF) and model-based (MB) values at stage 1.
    The MB component uses the known transition structure. The arbitration weight is
    modulated by anxiety: higher anxiety can tilt the balance via a gain parameter.
    An eligibility trace propagates stage-2 value prediction errors to stage-1 MF values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; modulates arbitration (MB weight).
    model_parameters : list or array-like of 5 floats
        [alpha, beta, mix0, gamma, trace]
        - alpha: learning rate for MF value updates at both stages [0,1]
        - beta: inverse temperature (shared across stages) [0,10]
        - mix0: baseline MB weight in [0,1]
        - gamma: anxiety gain on MB weight in [0,1]; w_eff = clip(mix0 * (1 + gamma*(stai-0.5)), 0, 1)
        - trace: eligibility trace from stage-2 to stage-1 MF values [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, mix0, gamma, trace = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],  # P(state | action=A)
                  [0.3, 0.7]]) # P(state | action=U)

    # Probabilities for choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)       # stage-1 actions
    q2 = np.zeros((2, 2))     # stage-2 (state, action)

    eps = 1e-10

    # Anxiety-modulated MB weight (fixed per subject, applied each trial)
    w_eff = mix0 * (1.0 + gamma * (stai_val - 0.5))
    if w_eff < 0.0:
        w_eff = 0.0
    if w_eff > 1.0:
        w_eff = 1.0

    for t in range(n_trials):
        # Compute MB action values at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)           # value of best action in each state
        q1_mb = T @ max_q2                    # expected value over transitions

        # Hybrid values for decision
        q1_hybrid = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Stage-1 policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (model-free within reached state)
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update with eligibility trace using realized stage-2 chosen value
        target1 = q2[s, a2]  # bootstrap from updated stage-2 value
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * trace * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based with learned transitions and anxiety-modulated lapses.
    
    The agent learns the transition function online and uses it to compute model-based
    action values at stage 1. A small stimulus-independent lapse mixes the softmax with
    uniform choice; lapse probability increases with anxiety. Stage-2 values are learned
    model-free. One inverse temperature governs both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; increases lapse rate.
    model_parameters : list or array-like of 5 floats
        [alpha_Q, alpha_M, beta, xi0, delta]
        - alpha_Q: learning rate for stage-2 Q updates [0,1]
        - alpha_M: learning rate for transition probabilities [0,1]
        - beta: inverse temperature for both stages [0,10]
        - xi0: baseline lapse probability (choice noise) [0,1]
        - delta: anxiety gain on lapse; xi = clip(xi0 * (1 + delta*(stai-0.5)), 0, 1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_Q, alpha_M, beta, xi0, delta = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition model T[a, s]
    T = np.full((2, 2), 0.5)

    # Stage-2 Q-values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Anxiety-modulated lapse
    xi = xi0 * (1.0 + delta * (stai_val - 0.5))
    if xi < 0.0:
        xi = 0.0
    if xi > 1.0:
        xi = 1.0

    for t in range(n_trials):
        # Compute MB values using learned transitions
        max_q2 = np.max(Q2, axis=1)     # value of best action in each state
        q1_mb = T @ max_q2              # expected value over learned transitions

        # Stage-1 policy with lapse
        q1c = q1_mb - np.max(q1_mb)
        ps1 = np.exp(beta * q1c)
        ps1 = ps1 / (np.sum(ps1) + eps)
        ps1 = (1.0 - xi) * ps1 + xi * 0.5  # mix with uniform
        a1 = int(action_1[t])
        p_choice_1[t] = ps1[a1]

        # Stage-2 policy with lapse within the reached state
        s = int(state[t])
        q2c = Q2[s] - np.max(Q2[s])
        ps2 = np.exp(beta * q2c)
        ps2 = ps2 / (np.sum(ps2) + eps)
        ps2 = (1.0 - xi) * ps2 + xi * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = ps2[a2]

        r = reward[t]

        # Update transitions for the chosen action a1 toward observed state s
        # One-hot target for state
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_M * (target - T[a1, sp])
        # Keep rows normalized within numeric limits
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, :] = T[a1, :] / row_sum

        # Stage-2 MF learning
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_Q * pe2

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-curvature utility, anxiety-shaped asymmetry, and forgetting.
    
    A model-free learner updates stage-2 values using a nonlinear utility function
    u(r) = r^curv (concave if curv<1). Learning is asymmetric for positive vs negative
    prediction errors, with asymmetry scaled by anxiety. Values undergo forgetting
    toward a neutral prior (0.5). A single inverse temperature governs both stages,
    and its effective strength is reduced for higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0=A, 1=U.
    state : array-like of int (0 or 1)
        Second-stage states: 0=X, 1=Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float in [0,1]
        Reward outcome.
    stai : array-like with one float in [0,1]
        Anxiety score; increases learning asymmetry and softens choice.
    model_parameters : list or array-like of 5 floats
        [alpha, beta, c_asym, forget, curv]
        - alpha: base learning rate [0,1]
        - beta: inverse temperature baseline [0,10]
        - c_asym: scales asymmetry via anxiety: 
                  alpha_plus = alpha * (1 + c_asym*stai), 
                  alpha_minus = alpha * (1 - c_asym*stai) [0,1]
        - forget: forgetting rate toward 0.5 for all Qs each trial [0,1]
        - curv: utility curvature for rewards in [0,1], u = r**curv [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, c_asym, forget, curv = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective inverse temperature decreases with anxiety (no extra parameter)
    beta_eff = beta * (1.0 - 0.5 * (stai_val - 0.5))
    if beta_eff < 1e-6:
        beta_eff = 1e-6

    # Q-values
    Q1 = np.zeros(2)        # MF stage-1 values
    Q2 = np.zeros((2, 2))   # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Compute asymmetric learning rates as functions of stai
    alpha_plus = alpha * (1.0 + c_asym * stai_val)
    alpha_minus = alpha * (1.0 - c_asym * stai_val)
    if alpha_plus > 1.0:
        alpha_plus = 1.0
    if alpha_minus < 0.0:
        alpha_minus = 0.0

    for t in range(n_trials):
        # Apply forgetting toward 0.5 prior
        Q1 = (1.0 - forget) * Q1 + forget * 0.5
        Q2 = (1.0 - forget) * Q2 + forget * 0.5

        # Stage-1 policy (MF)
        q1c = Q1 - np.max(Q1)
        ps1 = np.exp(beta_eff * q1c)
        ps1 = ps1 / (np.sum(ps1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = ps1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2c = Q2[s] - np.max(Q2[s])
        ps2 = np.exp(beta_eff * q2c)
        ps2 = ps2 / (np.sum(ps2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = ps2[a2]

        # Utility-transformed reward
        r = reward[t]
        u = r ** curv

        # Stage-2 update with asymmetry
        pe2 = u - Q2[s, a2]
        a2_lr = alpha_plus if pe2 >= 0.0 else alpha_minus
        Q2[s, a2] += a2_lr * pe2

        # Stage-1 bootstrapped MF update from realized stage-2 value
        pe1 = Q2[s, a2] - Q1[a1]
        a1_lr = alpha_plus if pe1 >= 0.0 else alpha_minus
        Q1[a1] += a1_lr * pe1

    neg_log_lik = -(np.sum(np.log(p_choice_1 + 1e-10)) + np.sum(np.log(p_choice_2 + 1e-10)))
    return neg_log_lik