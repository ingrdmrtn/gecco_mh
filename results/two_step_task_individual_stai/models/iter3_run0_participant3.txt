def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-damped model-based control with learned transitions and MF backup.

    Core ideas:
    - Stage-2 values (Q2) are learned via TD(0).
    - The transition model T(a1 -> s2) is learned with a dedicated learning rate.
    - Stage-1 combines model-based (using learned T) and model-free Q-values.
    - Anxiety reduces reliance on model-based control (linear damping of MB weight).
    - Perseveration bias at both stages.

    Parameters and bounds:
    - action_1: int array of shape (n_trials,), values in {0,1}; first-stage choices
    - state:    int array of shape (n_trials,), values in {0,1}; reached second-stage state
    - action_2: int array of shape (n_trials,), values in {0,1}; second-stage choices
    - reward:   float array of shape (n_trials,), rewards in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        eta   in [0,1]: learning rate for stage-2 values and stage-1 MF backup
        beta  in [0,10]: inverse temperature for softmax choice at both stages
        tau_T in [0,1]: learning rate for the transition model T
        omega in [0,1]: baseline weight on model-based control at stage-1
        kappa in [0,1]: perseveration strength added to the previously chosen action

    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    eta, beta, tau_T, omega, kappa = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition matrix (rows: first-stage action, cols: second-stage state)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Action probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q1_mf = np.zeros(2)       # model-free Q at stage-1
    q2 = np.zeros((2, 2))     # stage-2 Q(s2, a2)

    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage-1 via learned transitions and current Q2
        max_q2 = np.max(q2, axis=1)          # value of each second-stage state
        q1_mb = T @ max_q2                   # MB projection to stage-1 actions

        # Anxiety reduces MB reliance: omega_eff is damped by anxiety
        # Higher anxiety -> lower omega_eff
        omega_eff = omega * (1.0 - 0.7 * s_anx)
        omega_eff = np.clip(omega_eff, 0.0, 1.0)

        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Perseveration features
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Stage-1 policy
        logits1 = beta * q1 + kappa * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s_idx = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0

        logits2 = beta * q2[s_idx] + kappa * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model for the chosen first-stage action
        # Move that action's row toward the observed next state (one-hot)
        onehot_s = np.array([1.0 if i == s_idx else 0.0 for i in range(2)])
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * onehot_s
        # Keep normalization numerically stable
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Stage-2 TD update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += eta * delta2

        # Stage-1 MF backup:
        # (i) bootstrap toward the reached second-stage action value
        delta1_mf = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += eta * delta1_mf
        # (ii) propagate stage-2 RPE as an eligibility-like term (fixed strength)
        q1_mf[a1] += eta * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with decay and anxiety-driven lapses (pure model-based planning).

    Core ideas:
    - Rewards are transformed by a concave/convex utility with curvature shaped by anxiety.
    - Stage-2 values are learned on utility, not raw reward.
    - Forgetting/decay pulls Q2 toward zero each trial to capture volatility.
    - Choices are softmax with an epsilon-lapse component that increases with anxiety.
    - Stage-1 is purely model-based using fixed transition structure.

    Parameters and bounds:
    - action_1: int array (n_trials,), {0,1}
    - state:    int array (n_trials,), {0,1}
    - action_2: int array (n_trials,), {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha in [0,1]: learning rate for stage-2 on utility outcomes
        beta  in [0,10]: inverse temperature for softmax at both stages
        decay in [0,1]: forgetting factor applied to Q2 each trial
        theta in [0,1]: base utility curvature; combined with anxiety to set utility exponent
        lapse in [0,1]: base lapse rate; effective lapse increases with anxiety

    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    alpha, beta, decay, theta, lapse = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2))

    # Utility exponent shaped by anxiety:
    # As anxiety rises, curvature shifts toward (1 - theta), ensuring sensitivity changes with stai.
    gamma = theta * (1.0 - s_anx) + (1.0 - theta) * s_anx
    # Keep gamma in a safe numeric range
    gamma = float(np.clip(gamma, 0.05, 1.5))

    # Effective lapse increases with anxiety
    eps_lapse = lapse * (0.5 + 0.5 * s_anx)
    eps_lapse = float(np.clip(eps_lapse, 0.0, 0.5))  # keep lapse modest

    eps = 1e-12

    for t in range(n_trials):
        # Stage-1 MB values from current Q2 (no MF component)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy with lapse
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        softmax1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        probs1 = (1.0 - eps_lapse) * softmax1 + eps_lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapse
        s_idx = state[t]
        logits2 = beta * q2[s_idx]
        logits2 -= np.max(logits2)
        softmax2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        probs2 = (1.0 - eps_lapse) * softmax2 + eps_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Apply forgetting to all Q2 values (toward 0)
        q2 *= (1.0 - decay)

        # Utility-transformed reward and TD update
        r = reward[t]
        u = (r ** gamma)
        delta2 = u - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation hybrid with anxiety-driven novelty bonus at stage-2.

    Core ideas:
    - Learn a 2x2 successor-like mapping M(a1 -> s2) via simple prediction learning
      (approximates the transition structure but allows gradual adaptation).
    - Stage-1 values blend SR-derived values and MF values with weight w_sr.
    - Stage-2 values are learned via TD(0).
    - An anxiety-scaled novelty bonus encourages exploration of infrequently chosen
      second-stage actions: bonus ~ xi * stai / sqrt(visit_count+1).
    - Perseveration at stage-1.

    Parameters and bounds:
    - action_1: int array (n_trials,), {0,1}
    - state:    int array (n_trials,), {0,1}
    - action_2: int array (n_trials,), {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha in [0,1]: learning rate for both SR (M) and stage-2 Q-values
        beta  in [0,10]: inverse temperature for softmax at both stages
        w_sr in [0,1]: weight on SR-derived values at stage-1 (vs MF)
        xi   in [0,1]: scale for the novelty/exploration bonus at stage-2
        pi   in [0,1]: perseveration strength at stage-1

    Returns:
    - Negative log-likelihood of observed first- and second-stage actions.
    """
    alpha, beta, w_sr, xi, pi = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize SR-like mapping with the nominal transitions
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: a1, cols: s2

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    visit = np.zeros((2, 2))  # counts for novelty at stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    eps = 1e-12

    for t in range(n_trials):
        # SR-based stage-1 value by projecting max Q2 through M
        max_q2 = np.max(q2, axis=1)   # value of states
        q1_sr = M @ max_q2

        # Blend SR and MF values
        q1 = w_sr * q1_sr + (1.0 - w_sr) * q1_mf

        # Stage-1 perseveration
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * q1 + pi * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-driven novelty bonus
        s_idx = state[t]
        bonus = np.zeros(2)
        # Bonus is larger for less-visited actions and scales with anxiety
        for a in range(2):
            bonus[a] = xi * s_anx / np.sqrt(visit[s_idx, a] + 1.0)

        logits2 = beta * q2[s_idx] + bonus
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update SR mapping M for chosen a1 toward observed state
        onehot_s = np.array([1.0 if i == s_idx else 0.0 for i in range(2)])
        M[a1] = (1.0 - alpha) * M[a1] + alpha * onehot_s
        # Renormalize row
        M[a1] = M[a1] / (np.sum(M[a1]) + eps)

        # Stage-2 TD update
        delta2 = r - q2[s_idx, a2]
        q2[s_idx, a2] += alpha * delta2

        # Update MF stage-1 with both bootstrap and an eligibility-like term
        delta1 = q2[s_idx, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1
        q1_mf[a1] += alpha * delta2

        # Update visit counts after observing the choice
        visit[s_idx, a2] += 1.0

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll