def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB-MF with learned transitions and anxiety-weighted uncertainty aversion at stage 1.

    This model learns a model-free (MF) Q at both stages and a model-based (MB) planner that
    uses a learned transition matrix. First-stage action values are a convex combination
    of MF and MB values. Anxiety simultaneously (a) lowers reliance on MB control and
    (b) adds an uncertainty-avoidance penalty proportional to the transition entropy of
    each first-stage action.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; W/S on X; P/H on Y).
    reward : array-like of float
        Obtained rewards (typically 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values reduce MB weighting and increase
        uncertainty aversion at stage 1.
    model_parameters : array-like of floats, length 5
        [alpha, beta, omega0, chi_anx, nu]
        - alpha in [0,1]: learning rate for MF Q-values (both stages).
        - beta in [0,10]: inverse temperature for both stages' softmax.
        - omega0 in [0,1]: baseline MB weight at stage 1 before anxiety effects.
        - chi_anx in [0,1]: strength with which anxiety reduces MB weight and increases
                            uncertainty aversion.
        - nu in [0,1]: learning rate for the transition matrix (row for chosen action).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega0, chi_anx, nu = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective inverse temperature bounded
    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Anxiety-modulated MB weight: higher anxiety -> less MB
    omega_eff = omega0 * (1.0 - chi_anx * stai_val)
    if omega_eff < 0.0:
        omega_eff = 0.0
    if omega_eff > 1.0:
        omega_eff = 1.0

    # Uncertainty aversion weight applied to transition entropy at stage 1
    u_weight = chi_anx * stai_val  # in [0,1]

    # Initialize learned transition matrix T[a, s'] (rows sum to 1); start with a mild prior toward common mapping
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float) * 0.5 + 0.5 * np.array([[0.5, 0.5],
                                                                     [0.5, 0.5]], dtype=float)

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based first-stage value via current learned transitions
        max_q2 = np.max(q2_mf, axis=1)  # shape (2,)
        q1_mb = T @ max_q2  # shape (2,)

        # Transition uncertainty (Shannon entropy per action)
        # H(p) = -sum p log p; clamp to avoid log(0)
        pA = T[0]
        pU = T[1]
        eps = 1e-12
        H_A = -(pA[0] * np.log(pA[0] + eps) + pA[1] * np.log(pA[1] + eps))
        H_U = -(pU[0] * np.log(pU[0] + eps) + pU[1] * np.log(pU[1] + eps))
        H_vec = np.array([H_A, H_U])

        # Combine MB and MF with uncertainty aversion penalty
        q1_comb = (1.0 - omega_eff) * q1_mf + omega_eff * q1_mb - u_weight * H_vec

        # Stage 1 policy
        logits1 = beta_eff * q1_comb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF)
        s = int(state[t])
        logits2 = beta_eff * q2_mf[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # MF learning
        # TD for stage 1 toward second-stage value (SARSA style bootstrapping on chosen a2 within visited state)
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # TD for stage 2 toward reward
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * delta2

        # Learn transitions for chosen action toward observed state
        # One-hot target for observed next state
        target = np.array([1.0 if k == s else 0.0 for k in range(2)])
        T[a1] = (1.0 - nu) * T[a1] + nu * target
        # Normalize to maintain stochasticity
        T[a1] = T[a1] / (np.sum(T[a1]) + 1e-12)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free SARSA(λ) with anxiety-amplified loss aversion in the utility of outcomes.

    This purely model-free account uses an eligibility-trace to propagate second-stage
    reward prediction errors to the first-stage values. Rewards are transformed through
    a loss-averse utility; anxiety increases the loss-aversion parameter, effectively
    making unrewarded outcomes more punishing.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; W/S on X; P/H on Y).
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values increase loss aversion.
    model_parameters : array-like of floats, length 5
        [alpha, beta, lam, zeta_anx, rho]
        - alpha in [0,1]: learning rate for Q-values.
        - beta in [0,10]: inverse temperature for both stages.
        - lam in [0,1]: eligibility trace mixing of second-stage PE into first-stage value update.
        - zeta_anx in [0,1]: strength by which anxiety increases loss aversion.
        - rho in [0,1]: baseline loss-aversion for zero outcomes in utility transform.

        Utility transform:
          u(r) = r - rho_eff * (1 - r),  where rho_eff = rho + zeta_anx * stai.
        Thus u(1) = 1; u(0) = -rho_eff.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lam, zeta_anx, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    rho_eff = rho + zeta_anx * stai_val
    if rho_eff < 0.0:
        rho_eff = 0.0
    if rho_eff > 1.0:
        rho_eff = 1.0

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Policies
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        s = int(state[t])
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Utility-transformed reward
        r_raw = float(reward[t])
        u = r_raw - rho_eff * (1.0 - r_raw)

        # Second-stage TD
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # First-stage TD with eligibility trace lam mixing
        # Combine immediate bootstrapping term (q2 value) and the experienced utility via delta2
        # Equivalent to standard SARSA(λ) with two steps:
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * ((1.0 - lam) * delta1 + lam * delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Distorted-model planner with anxiety-modulated second-stage decisional noise.

    First-stage choices are computed from a model-based planner using a transition model
    that blends the objective common-rare structure with the learned MF values downstream.
    Anxiety increases reliance on a 'canonical' common-rare prior (i.e., more distortion toward
    0.7/0.3 transitions), and separately increases second-stage decisional noise.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float
        Obtained rewards.
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher values increase second-stage noise and
        increase distortion toward canonical transitions at stage 1.
    model_parameters : array-like of floats, length 5
        [alpha, beta1, beta2, k_anx_temp, mu]
        - alpha in [0,1]: learning rate for second-stage MF Q-values.
        - beta1 in [0,10]: inverse temperature for first-stage softmax.
        - beta2 in [0,10]: baseline inverse temperature for second-stage softmax.
        - k_anx_temp in [0,1]: anxiety scaling that reduces effective beta2 (more noise with anxiety).
        - mu in [0,1]: strength of anxiety-driven distortion of the transition model toward
                       the canonical common-rare structure at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta1, beta2, k_anx_temp, mu = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Clamp temperatures
    beta1_eff = beta1
    if beta1_eff < 0.0:
        beta1_eff = 0.0
    if beta1_eff > 10.0:
        beta1_eff = 10.0

    beta2_eff = beta2 * (1.0 - k_anx_temp * stai_val)
    if beta2_eff < 0.0:
        beta2_eff = 0.0
    if beta2_eff > 10.0:
        beta2_eff = 10.0

    # Canonical transition matrix for two-step task
    T_canonical = np.array([[0.7, 0.3],
                            [0.3, 0.7]], dtype=float)

    # Start with a neutral learned transition (equal) but we will not learn it here; instead distort toward canonical
    T_neutral = np.array([[0.5, 0.5],
                          [0.5, 0.5]], dtype=float)

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Anxiety-driven distortion coefficient toward canonical structure
        c = mu * stai_val  # in [0,1]
        # Distorted transition used for planning
        T_tilde = (1.0 - c) * T_neutral + c * T_canonical

        # First-stage MB values based on expected max Q at each planet
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T_tilde @ max_q2

        # Stage 1 policy
        logits1 = beta1_eff * q1_mb
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Stage 2 policy (anxiety increases noise)
        s = int(state[t])
        logits2 = beta2_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Second-stage learning from reward
        r = float(reward[t])
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll