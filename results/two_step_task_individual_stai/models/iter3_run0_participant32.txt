def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Transition-volatility learner with anxiety-modulated forgetting, directed exploration, and lapse.
    
    Core idea:
    - Learns first-stage transition probabilities online with a Dirichlet-like (count) update and volatility-based forgetting.
    - Higher anxiety increases perceived volatility (stronger forgetting of transition knowledge) and increases lapse.
    - Directed exploration bonus at stage 1 based on transition entropy, reduced by anxiety.
    - Second stage values learned with a simple delta rule.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states per trial (0=Planet X, 1=Planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial (alien index in reached planet).
    reward : array-like of float
        Obtained reward on each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+], here 0.3375 indicates medium anxiety.
    model_parameters : list or array-like of float
        [alpha_r, beta, zeta_vol, tau_explore, xi_lapse]
        Bounds:
        - alpha_r: [0,1] learning rate for second-stage values.
        - beta:    [0,10] inverse temperature for softmax.
        - zeta_vol: [0,1] volatility/forgetting gain on transition counts (scaled by anxiety).
        - tau_explore: [0,1] directed exploration bonus weight (scaled down by anxiety).
        - xi_lapse: [0,1] baseline lapse probability (scaled up by anxiety).
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, zeta_vol, tau_explore, xi_lapse = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition counts for each first-stage action -> states (Dirichlet-like)
    # Start with weak symmetric prior favoring the task's common transitions slightly.
    counts = np.array([[1.4, 0.6],   # for action 0 (A): X common
                       [0.6, 1.4]])  # for action 1 (U): Y common

    # Second-stage Q-values: Q[state, action]
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper to compute entropy of a Bernoulli (two-option) row
    def row_entropy(row_probs):
        p = np.clip(row_probs[0], 1e-8, 1 - 1e-8)
        return -(p * np.log(p) + (1 - p) * np.log(1 - p))

    for t in range(n_trials):
        # Form current transition probabilities from counts (row-normalized)
        row_sums = np.sum(counts, axis=1, keepdims=True)
        T = counts / row_sums  # shape (2,2)

        # Model-based Q1 via learned transitions
        max_q2 = np.max(q2, axis=1)  # value of best alien per planet
        q1_mb = T @ max_q2  # expected value for A/U

        # Directed exploration bonus: higher for actions with higher transition entropy
        H = np.array([row_entropy(T[0]), row_entropy(T[1])])  # entropy per first-stage action
        tau_eff = tau_explore * (1.0 - stai)  # anxiety reduces directed exploration
        q1_policy = q1_mb + tau_eff * H

        # Stage-1 choice probabilities with anxiety-modulated lapse
        logits1 = beta * (q1_policy - np.max(q1_policy))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        xi_eff = np.clip(xi_lapse * (0.5 + 0.5 * stai), 0.0, 1.0)
        probs1 = (1.0 - xi_eff) * probs1 + xi_eff * 0.5  # mix with uniform
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the reached state
        s = state[t]
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update second-stage values
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update transition knowledge with volatility-driven forgetting
        # Forget more when anxiety and zeta_vol are high
        forget = np.clip(zeta_vol * (0.5 + 0.5 * stai), 0.0, 1.0)
        counts = (1.0 - forget) * counts
        # Add 1 pseudo-count to the observed transition (a1 -> s)
        counts[a1, s] += 1.0

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Asymmetric (win/loss) learning with anxiety-modulated forgetting and choice persistence.
    
    Core idea:
    - Model-free learner with separate learning rates for positive vs. negative second-stage prediction errors.
    - Anxiety increases global forgetting of Q-values (toward a neutral prior), reflecting reduced confidence/maintenance.
    - Persistence (choice stickiness) at both stages, scaled up by anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial.
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [mu_win, mu_loss, beta, z_forget, psi_persist]
        Bounds:
        - mu_win: [0,1] learning rate used when the second-stage prediction error is positive.
        - mu_loss: [0,1] learning rate used when the second-stage prediction error is negative.
        - beta: [0,10] inverse temperature for both stages.
        - z_forget: [0,1] forgetting strength per trial, scaled by anxiety.
        - psi_persist: [0,1] baseline stickiness (persistence) strength, scaled by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    mu_win, mu_loss, beta, z_forget, psi_persist = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Model-free values
    q1_mf = np.zeros(2)        # values for first-stage actions A/U
    q2 = np.zeros((2, 2))      # values for second-stage state x action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_state = {0: None, 1: None}

    # Neutral prior to forget toward
    prior_q1 = 0.5
    prior_q2 = 0.5

    for t in range(n_trials):
        # Anxiety-scaled persistence
        stick = psi_persist * (0.5 + 0.5 * stai)

        # Stage 1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        logits1 = beta * (q1_mf - np.max(q1_mf)) + stick * bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with within-state stickiness
        s = state[t]
        q2_s = q2[s].copy()
        bias2 = np.zeros(2)
        if prev_a2_state[s] is not None:
            bias2[prev_a2_state[s]] = 1.0
        logits2 = beta * (q2_s - np.max(q2_s)) + stick * bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 with asymmetric rates
        r = reward[t]
        pe2 = r - q2[s, a2]
        lr2 = mu_win if pe2 >= 0.0 else mu_loss
        q2[s, a2] += lr2 * pe2

        # Update stage 1 model-free value via bootstrapping from second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        lr1 = mu_win if pe1 >= 0.0 else mu_loss
        q1_mf[a1] += lr1 * pe1

        # Anxiety-modulated forgetting toward neutral priors
        f = np.clip(z_forget * (0.5 + 0.5 * stai), 0.0, 1.0)
        q1_mf = (1.0 - f) * q1_mf + f * prior_q1
        q2 = (1.0 - f) * q2 + f * prior_q2

        prev_a1 = a1
        prev_a2_state[s] = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with anxiety-modulated subjective transition bias and surprise-controlled temperature.
    
    Core idea:
    - First-stage values combine model-based and model-free components.
    - Subjective transition matrix has a tunable 'common-transition' bias reduced by anxiety.
    - Softmax temperature dynamically increases on surprising (rare) transitions, more so with higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward on each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta_base, kappa_temp, nu_mb, bias_common]
        Bounds:
        - alpha: [0,1] learning rate for both stages (shared).
        - beta_base: [0,10] baseline inverse temperature.
        - kappa_temp: [0,1] gain for increasing temperature on surprising transitions.
        - nu_mb: [0,1] weight on model-based values (1.0 => purely MB).
        - bias_common: [0,1] strength of subjective bias toward common transitions at stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_base, kappa_temp, nu_mb, bias_common = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Subjective transition matrix (fixed form but tunable "commonness")
    # Anxiety reduces the bias toward common transitions (moves toward 0.5/0.5)
    c = 0.7 + 0.2 * bias_common * (1.0 - stai)  # in [0.7, 0.9] scaled down by anxiety
    c = np.clip(c, 0.5, 0.99)
    T_subj = np.array([[c, 1.0 - c],
                       [1.0 - c, c]])

    # Values
    q1_mf = np.zeros(2)      # model-free values for A/U
    q2 = np.zeros((2, 2))    # second-stage values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_subj @ max_q2

        # Combine MB and MF for policy
        q1 = nu_mb * q1_mb + (1.0 - nu_mb) * q1_mf

        # Determine surprise of current transition to modulate temperature
        # Use probability of the actually reached state under subjective transitions.
        a1 = action_1[t]
        s = state[t]
        p_obs = T_subj[a1, s]
        surprise = 1.0 - p_obs  # large when rare transition occurs
        beta_t = beta_base * np.exp(kappa_temp * surprise * (1.0 + stai))

        # Stage 1 choice probability
        logits1 = beta_t * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (use same beta_t for coherence with surprise/arousal)
        q2_s = q2[s]
        logits2 = beta_t * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage 2 delta
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 model-free update bootstrapping from current second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik