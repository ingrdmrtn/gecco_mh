def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based choice with anxiety-amplified uncertainty aversion at planning time.
    
    The agent plans using the known transition structure but subtracts a penalty
    for uncertainty in each second-stage state. Uncertainty is tracked online as
    an exponential moving estimate of reward variance for each alien. Anxiety
    increases aversion to this uncertainty, shifting choices away from first-stage
    actions that commonly lead to uncertain states. Second-stage behavior is
    standard softmax. A model-free first-stage value is still learned (alpha)
    though planning here relies on the uncertainty-penalized model-based values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, xi0, k_anx_xi, alpha2]
        - alpha in [0,1]: learning rate for first-stage model-free value (bootstraps from stage 2).
        - beta in [0,10]: inverse temperature (shared across stages).
        - xi0 in [0,1]: baseline uncertainty aversion weight.
        - k_anx_xi in [0,1]: how strongly anxiety increases uncertainty aversion.
        - alpha2 in [0,1]: learning rate for second-stage values and uncertainty.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, xi0, k_anx_xi, alpha2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective uncertainty aversion weight, grows with anxiety
    xi_eff = xi0 + k_anx_xi * stai_val
    if xi_eff < 0.0:
        xi_eff = 0.0

    # Fixed transition structure: rows are actions (A=0, U=1); columns are states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Q-values
    q1_mf = np.zeros(2, dtype=float)         # First-stage model-free values
    q2 = np.zeros((2, 2), dtype=float)       # Second-stage action values: state x action

    # Exponential moving estimate of reward variance per alien
    var2 = np.zeros((2, 2), dtype=float)

    for t in range(n_trials):
        # Compute uncertainty per state as the max alien uncertainty in that state
        # Use sqrt(var) (i.e., std) as uncertainty measure to keep units close to reward scale
        std_state = np.sqrt(np.maximum(0.0, np.max(var2, axis=1)))  # shape (2,)

        # Model-based first-stage values: expected max Q2 minus uncertainty penalty
        v_state_penalized = np.max(q2, axis=1) - xi_eff * std_state  # shape (2,)
        q1_mb = T @ v_state_penalized  # shape (2,)

        # First-stage policy (purely model-based with uncertainty penalty)
        logits1 = q1_mb.copy()
        logits1 -= np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (standard softmax)
        s = int(state[t])
        logits2 = q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = float(reward[t])

        # Second stage TD learning and uncertainty tracking
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Update variance estimate with squared TD error as innovation
        var2[s, a2] = (1.0 - alpha2) * var2[s, a2] + alpha2 * (delta2 ** 2)

        # First-stage model-free bootstrap from chosen second-stage action
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-sensitive surprise gating.
    
    The agent learns transition probabilities from experience and blends model-based
    and model-free action values at the first stage. After a surprising transition
    on the previous trial, high-anxiety agents rely less on model-based control on
    the next trial (surprise gating). Second-stage choices are softmax over Q-values.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, kappa, w_base, eta_anx]
        - alpha in [0,1]: TD learning rate (used for both stages).
        - beta in [0,10]: inverse temperature (shared across stages).
        - kappa in [0,1]: learning rate for transition probabilities P(state | action).
        - w_base in [0,1]: baseline weight on model-based control at stage 1.
        - eta_anx in [0,1]: strength by which anxiety reduces MB weight after a rare transition.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa, w_base, eta_anx = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Learned transition model initialized to uniform uncertainty
    T_hat = np.full((2, 2), 0.5, dtype=float)

    # Ground-truth common transitions to determine surprise (rare vs common)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    q1_mf = np.zeros(2, dtype=float)
    q2 = np.zeros((2, 2), dtype=float)

    # Surprise from previous trial (0 for first trial)
    prev_rare = 0.0

    for t in range(n_trials):
        # Compute model-based first-stage values using learned transitions
        max_q2 = np.max(q2, axis=1)        # value of each state
        q1_mb = T_hat @ max_q2             # shape (2,)

        # Surprise-gated MB weight for this trial
        w_eff = w_base - eta_anx * stai_val * prev_rare
        if w_eff < 0.0: 
            w_eff = 0.0
        if w_eff > 1.0:
            w_eff = 1.0

        # Hybrid first-stage policy
        q1_combined = w_eff * q1_mb + (1.0 - w_eff) * q1_mf
        logits1 = q1_combined - np.max(q1_combined)
        probs1 = np.exp(beta * logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = int(state[t])
        logits2 = q2[s].copy()
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning: second stage
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning: first stage model-free bootstrap
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Learn transition model for chosen action using simple delta rule
        # Move probability of observed state toward 1, other state toward 0
        T_hat[a1, s] += kappa * (1.0 - T_hat[a1, s])
        other_s = 1 - s
        T_hat[a1, other_s] += kappa * (0.0 - T_hat[a1, other_s])

        # Compute whether current transition was rare under true structure
        # Rare if went to the uncommon state for the chosen action
        is_rare = 1.0 if ((a1 == 0 and s == 1) or (a1 == 1 and s == 0)) else 0.0
        prev_rare = is_rare  # used to gate MB weight on the next trial

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MB/MF mixture with anxiety-specific exploration at stage 2 and second-stage stickiness.
    
    The agent blends model-based and model-free values at the first stage using a
    fixed mixture weight. At the second stage, anxiety selectively increases choice
    randomness (lower inverse temperature) while choices also exhibit within-state
    stickiness (tendency to repeat the same alien the next time that planet is reached).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (aliens) for each trial.
    reward : array-like of float
        Reward obtained on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, z_stai, psi2, chi]
        - alpha in [0,1]: TD learning rate for both stages.
        - beta in [0,10]: base inverse temperature.
        - z_stai in [0,1]: scales how much anxiety reduces second-stage beta:
                          beta2 = beta * (1 - z_stai * stai).
        - psi2 in [0,1]: second-stage stickiness weight added to the previously
                         chosen action in that state (perseveration bias).
        - chi in [0,1]: weight on model-based control in first-stage decision (MB/MF mix).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, z_stai, psi2, chi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition model for planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Effective second-stage inverse temperature decreases with anxiety
    beta2 = beta * (1.0 - z_stai * stai_val)
    if beta2 < 1e-6:
        beta2 = 1e-6  # avoid zero or negative

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    q1_mf = np.zeros(2, dtype=float)
    q2 = np.zeros((2, 2), dtype=float)

    # Track last chosen action for each state to implement stickiness
    prev_a2 = np.full(2, -1, dtype=int)

    for t in range(n_trials):
        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)   # value of each state
        q1_mb = T @ max_q2

        # Mixed first-stage values
        q1_mix = chi * q1_mb + (1.0 - chi) * q1_mf

        # First-stage policy with base beta
        logits1 = q1_mix - np.max(q1_mix)
        probs1 = np.exp(beta * logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with anxiety-modulated beta and stickiness
        s = int(state[t])
        logits2 = q2[s].copy()
        # Add within-state perseveration bias
        if prev_a2[s] >= 0:
            logits2[prev_a2[s]] += psi2
        logits2 -= np.max(logits2)
        probs2 = np.exp(beta2 * logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning at second stage
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Learning at first stage (model-free bootstrap from second stage)
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update stickiness memory
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)