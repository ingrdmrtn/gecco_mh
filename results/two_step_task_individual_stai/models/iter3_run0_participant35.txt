def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus MF-MB hybrid with anxiety-modulated exploration and forgetting.

    Core ideas
    - Second-stage values are learned model-free with an uncertainty-tracking bonus (UCB-like).
    - First-stage mixes model-based (via known transition matrix) and model-free values.
    - Anxiety increases directed exploration (larger uncertainty bonus) and increases forgetting.
      Anxiety also reduces reliance on model-based planning.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage action on each trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage action on each trial (aliens per state)
    - reward: array-like of floats in [0,1], obtained reward each trial
    - stai: array-like length-1, trait anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters
        alpha0: base learning rate for Q2 in [0,1]
        beta: inverse temperature for both stages in [0,10]
        kappa_ucb: base weight of uncertainty bonus at stage 2 in [0,1]
        trust_base: base reliance on model-based evaluation at stage 1 in [0,1]
        decay: base forgetting rate per trial in [0,1]

    How anxiety is used
    - Directed exploration bonus scales as: kappa_eff = kappa_ucb * (1 + stai)
      (more anxious -> more exploration)
    - Model-based reliance reduced by anxiety:
      w_mb = clip(trust_base * (1 - 0.5*stai), 0, 1)
    - Forgetting increases with anxiety:
      decay_eff = clip(decay * (0.5 + 0.5*stai), 0, 1)

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha0, beta, kappa_ucb, trust_base, decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)           # model-free first-stage values
    q2 = np.zeros((2, 2))         # second-stage action values for each state

    # Uncertainty tracker for Q2 (per state-action), initialized moderately uncertain
    u2 = np.full((2, 2), 0.5, dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated settings
    w_mb = min(1.0, max(0.0, trust_base * (1.0 - 0.5 * stai)))
    kappa_eff = kappa_ucb * (1.0 + stai)
    decay_eff = min(1.0, max(0.0, decay * (0.5 + 0.5 * stai)))

    for t in range(n_trials):
        # Compute model-based first-stage values from current Q2 (no bonus in planning)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Mix MB and MF for stage-1 decision values
        q1_decision = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Softmax for stage 1
        centered_q1 = q1_decision - np.max(q1_decision)
        logits1 = beta * centered_q1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2: add UCB-like uncertainty bonus to chosen state's action values
        s = int(state[t])
        bonus2 = kappa_eff * u2[s]  # per-action exploration bonus
        q2_with_bonus = q2[s] + bonus2

        centered_q2 = q2_with_bonus - np.max(q2_with_bonus)
        logits2 = beta * centered_q2
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]

        # Second stage update with learning rate boosted by uncertainty (bounded in [0,1])
        alpha2_eff = max(0.0, min(1.0, alpha0 + kappa_ucb * u2[s, a2]))
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2_eff * pe2

        # Update uncertainty: reduction where we updated, gentle diffusion elsewhere
        # Reduction proportional to learning (more learning -> less uncertainty)
        u2[s, a2] = (1.0 - alpha2_eff) * u2[s, a2]
        # Diffusion toward higher uncertainty for all entries (bounded)
        u2 = (1.0 - decay_eff) * u2 + decay_eff * 0.01

        # First stage MF bootstrapping toward realized second-stage action value (no bonus)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use same alpha base for MF update to keep parameter economy
        q1_mf[a1] += alpha0 * pe1

        # Forgetting on Q-values
        q2 *= (1.0 - decay_eff)
        q1_mf *= (1.0 - decay_eff)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB-MF arbitration with anxiety-modulated volatility and stickiness.

    Core ideas
    - Learns both reward values (Q2) and the transition matrix (T) online.
    - Model-based strength increases with transition confidence, but anxiety dampens MB reliance.
    - Anxious participants are assumed to perceive higher transition volatility, thus learn T faster.
    - Perseveration (action stickiness) increases with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage action (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage action
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters
        alpha_r: reward learning rate for Q2 in [0,1]
        beta: inverse temperature for both stages in [0,10]
        alpha_T: base transition learning rate in [0,1]
        w_conf_base: base factor scaling MB arbitration by transition confidence in [0,1]
        stick_base: base perseveration strength in [0,1]

    How anxiety is used
    - Transition learning rate: alpha_T_eff = clip(alpha_T * (1 + 0.5*stai), 0, 1)
    - MB weight: w_mb = clip(w_conf_base * conf * (1 - 0.4*stai), 0, 1)
      where conf is the mean deviation of learned T from 0.5 (higher = more confident).
    - Stickiness increases with anxiety: stick = stick_base * (1 + 0.5*stai)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_r, beta, alpha_T, w_conf_base, stick_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition model close to known structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For stickiness
    last_a1 = None
    stick = stick_base * (1.0 + 0.5 * stai)
    alpha_T_eff = min(1.0, max(0.0, alpha_T * (1.0 + 0.5 * stai)))

    for t in range(n_trials):
        # Compute confidence from transition model: average |p - 0.5|
        conf = 0.5 * (abs(T[0, 0] - 0.5) + abs(T[1, 1] - 0.5))
        w_mb = min(1.0, max(0.0, w_conf_base * conf * (1.0 - 0.4 * stai)))

        # Model-based values via current learned T
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add perseveration bias to stage-1 logits
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick

        q1_decision = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        centered_q1 = q1_decision - np.max(q1_decision)
        logits1 = beta * centered_q1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (no stickiness)
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        logits2 = beta * centered_q2
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward update
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # First-stage MF update toward realized Q2
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Learn transition probabilities via EMA toward observed next state
        # If we chose a1 and observed s, move T[a1] toward one-hot of s
        if s == 0:
            T[a1, 0] = (1.0 - alpha_T_eff) * T[a1, 0] + alpha_T_eff * 1.0
            T[a1, 1] = 1.0 - T[a1, 0]
        else:
            T[a1, 1] = (1.0 - alpha_T_eff) * T[a1, 1] + alpha_T_eff * 1.0
            T[a1, 0] = 1.0 - T[a1, 1]

        # Keep rows normalized and within bounds
        T[a1] = np.clip(T[a1], 0.01, 0.99)
        T[a1] /= np.sum(T[a1])

        # Update last action
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive SARSA(位) with anxiety-driven rare-transition switch bias.

    Core ideas
    - Second-stage rewards pass through a concave utility transform (risk aversion),
      scaled by anxiety (more anxious -> more risk-averse).
    - Credit assignment from stage 2 to stage 1 uses an eligibility trace 位.
      Anxiety increases 位, prolonging credit assignment from recent outcomes.
    - Anxious participants tend to switch after rare transitions: a first-stage bias
      penalizes repeating the previous first-stage action if the previous transition was rare.

    Task structure
    - Common transitions: A->X, U->Y (prob=0.7). Rare otherwise.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage action
    - state: array-like of ints in {0,1}, observed state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage action
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, anxiety score in [0,1]
    - model_parameters: tuple/list of 5 parameters
        alpha: learning rate for Q-values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        eta_risk: base risk-sensitivity in [0,1] (concavity of utility)
        trace: base eligibility trace in [0,1]
        kappa_rare: base magnitude of rare-transition switch bias in [0,1]

    How anxiety is used
    - Utility transform: u(r) = r^(gamma), where gamma = 1 - eta_risk*stai (gamma in [0,1])
      Higher stai -> smaller gamma -> more concave utility (risk aversion).
    - Eligibility trace increases with anxiety: lambda_eff = clip(trace * (0.5 + 0.5*stai), 0, 1)
    - Rare-transition switch bias magnitude: bias_eff = kappa_rare * stai
      If previous trial's transition was rare, subtract bias_eff from the logit of repeating a1.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, eta_risk, trace, kappa_rare = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    def was_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated settings
    gamma = max(0.0, min(1.0, 1.0 - eta_risk * stai))
    lambda_eff = max(0.0, min(1.0, trace * (0.5 + 0.5 * stai)))
    bias_eff = kappa_rare * stai

    # Memory for rare-transition-induced switch bias
    prev_a1 = None
    prev_rare = False

    for t in range(n_trials):
        # Stage-1 bias: discourage repeating previous action if previous transition was rare
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_rare:
            bias1[prev_a1] -= bias_eff

        # Softmax at stage 1
        centered_q1 = (q1 + bias1) - np.max(q1 + bias1)
        logits1 = beta * centered_q1
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        s = int(state[t])
        centered_q2 = q2[s] - np.max(q2[s])
        logits2 = beta * centered_q2
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward
        r_raw = reward[t]
        # Ensure numerical stability of power transform
        r_util = r_raw**gamma if r_raw >= 0.0 else -((-r_raw)**gamma)

        # TD error at stage 2 and update
        pe2 = r_util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # SARSA(位)-like bootstrapping to stage 1 using the realized second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Eligibility trace pushes some credit to the non-chosen first-stage action as a decay toward zero
        # (keeps values bounded and implements trace-based generalization)
        other = 1 - a1
        q1[other] += alpha * lambda_eff * (target1 - q1[other]) * 0.5

        # Update rare/common memory
        prev_rare = not was_common(a1, s)
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll