def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: Volatility-adaptive hybrid with anxiety-modulated exploration and perseveration.
    
    Overview
    --------
    The agent learns second-stage action values (Q2) with a standard TD rule and backs up a model-free
    first-stage value (Q1_mf). A fixed transition model (0.7 common, 0.3 rare) supports model-based
    evaluation (Q1_mb). First-stage choice uses a hybrid of MB and MF values.
    
    Novel mechanisms:
      - Volatility-adaptive inverse temperature: beta is down-regulated as estimated reward volatility
        increases, and further reduced by anxiety (stai).
      - Perseveration bias at both stages to capture choice stickiness.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within the observed state.
    reward : array-like of float
        Rewards (typically 0 or 1).
    stai : array-like of float
        Anxiety score; stai[0] in [0,1]. Higher anxiety reduces effective exploration temperature.
    model_parameters : list or array
        [alpha, beta, k_vol, w_MB, stickiness]
        - alpha in [0,1]: learning rate for Q updates (both stages).
        - beta in [0,10]: base inverse temperature for both stages.
        - k_vol in [0,1]: volatility learning rate controlling sensitivity to reward PE variance.
        - w_MB in [0,1]: weight on model-based values at stage 1 (1=fully MB).
        - stickiness in [0,1]: strength of choice perseveration bias at both stages.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta_base, k_vol, w_MB, stickiness = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Volatility estimator (exponential moving average of squared PEs)
    v = 0.0

    # Previous choices for perseveration
    prev_a1 = 0
    prev_a2 = 0

    # Anxiety-modulated exploration: higher stai reduces baseline beta; scale in [0,1]
    beta_base_eff = beta_base * (1.0 - 0.5 * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid combination
        Q1 = w_MB * Q1_mb + (1.0 - w_MB) * Q1_mf

        # Volatility-adaptive beta (down-regulate when v high, and with anxiety)
        beta_t = beta_base_eff / (1.0 + v)

        # Add perseveration biases (additive to logits)
        bias1 = np.array([0.0, 0.0])
        bias1[prev_a1] += stickiness

        # Stage-1 policy
        logits1 = beta_t * (Q1 - np.max(Q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration bias
        bias2 = np.array([0.0, 0.0])
        bias2[prev_a2] += stickiness
        logits2 = beta_t * (Q2[s] - np.max(Q2[s])) + bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Second stage TD update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Update volatility estimate with squared PE
        v = (1.0 - k_vol) * v + k_vol * (pe2 * pe2)

        # First stage MF backup from realized second-stage value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Transition-learning hybrid with anxiety-sensitive MB reliance and lapse noise.
    
    Overview
    --------
    The agent learns:
      - Q2(s, a2) via a TD rule from reward.
      - A model-free first-stage value Q1_mf via TD backup from Q2(s, a2).
      - A transition model T learned from observed transitions using alpha_tr.
    
    First-stage action values are purely model-based (from learned T) when transitions are certain,
    but reliance on the model is reduced when transition uncertainty is high, especially under higher
    anxiety (stai). Additionally, a lapse component mixes the softmax policy with uniform random choice,
    and the lapse rate increases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float
        Anxiety score; stai[0] in [0,1]. Higher anxiety reduces MB reliance when transitions are uncertain,
        and increases lapse rate.
    model_parameters : list or array
        [alpha, beta, alpha_tr, k_anxMB, lapse]
        - alpha in [0,1]: learning rate for Q2 and Q1_mf.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - alpha_tr in [0,1]: learning rate for transition probabilities.
        - k_anxMB in [0,1]: gain by which anxiety and transition uncertainty reduce MB reliance.
        - lapse in [0,1]: base lapse probability; effective lapse grows with anxiety.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, alpha_tr, k_anxMB, lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model close to uniform (uncertain)
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Lapse increases with anxiety
    lapse_eff = np.clip(lapse * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Update transition model T based on observed transition (a1 -> s)
        # Simple delta rule toward one-hot outcome
        for sp in range(2):
            target = 1.0 if sp == s else 0.0
            T[a1, sp] += alpha_tr * (target - T[a1, sp])
        # Ensure normalization (small numerical drift safeguard)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        # Compute transition uncertainty (entropy) for each first-stage action
        # H ranges [0, ln2]; normalize to [0,1]
        def entropy_row(row):
            eps = 1e-12
            p = np.clip(row, eps, 1.0)
            p = p / np.sum(p)
            return -(p * np.log(p)).sum()

        H0 = entropy_row(T[0])
        H1 = entropy_row(T[1])
        H_norm = np.array([H0, H1]) / np.log(2.0)

        # Model-based Q via learned transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Anxiety-sensitive reliance on MB: reduce where uncertainty high
        # Effective weight per action: w = 1 - k_anxMB * stai * H_norm
        w_vec = 1.0 - k_anxMB * stai * H_norm
        w_vec = np.clip(w_vec, 0.0, 1.0)

        # Combine per-action MF and MB using action-specific weights
        Q1 = w_vec * Q1_mb + (1.0 - w_vec) * Q1_mf

        # Stage-1 policy (softmax), then apply lapse mixing with uniform
        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax), with the same lapse mechanism
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning updates
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Risk-sensitive utility with anxiety-shaped reference point and SARSA-vs-Q mixture.
    
    Overview
    --------
    Rewards are transformed through a reference-dependent, risk-sensitive utility before learning:
      - A running reference point R_t is tracked; utility depends on r - R_t with exponent gamma.
      - Higher anxiety increases effective risk aversion (more concavity).
    The second-stage Q2 learns from utility rather than raw reward. First-stage MF value uses a
    mixture of SARSA and Q-learning backups; anxiety shifts the mixture toward SARSA (more myopic).
    A fixed transition model supports MB evaluation at stage 1; hybridization is through the SR
    mixture shaping the MF target.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0: spaceship A, 1: spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0: planet X, 1: planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices.
    reward : array-like of float
        Rewards per trial (0/1).
    stai : array-like of float
        Anxiety score; stai[0] in [0,1]. Higher anxiety increases risk aversion and SARSA weighting.
    model_parameters : list or array
        [alpha, beta, tau_risk, k_ref, w_SRL]
        - alpha in [0,1]: learning rate for Q updates and reference tracking.
        - beta in [0,10]: inverse temperature for both stages.
        - tau_risk in [0,1]: baseline risk sensitivity controlling utility curvature.
        - k_ref in [0,1]: reference point learning rate (EMA of recent rewards).
        - w_SRL in [0,1]: baseline weight for SARSA vs. Q-learning in first-stage MF backup.
    
    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, tau_risk, k_ref, w_SRL = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Reference point for utility (start neutral at 0.5)
    R = 0.5

    # Anxiety increases effective risk aversion: gamma in (0,1]
    # Lower gamma => more concave utility
    gamma_base = 1.0 - 0.5 * tau_risk
    gamma_eff = np.clip(gamma_base * (1.0 - 0.5 * stai), 0.01, 1.0)

    # Anxiety increases SARSA weighting (more myopic/choice-contingent)
    w_sarsa_eff = np.clip(w_SRL * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation for stage 1 using fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid policy at stage 1: combine MB with MF Q1
        # Here we let MF be Q1_mf, MB be Q1_mb; mix implicitly via softmax scaling by both terms
        # by simply summing them (equivalent to w=0.5 in units). To respect parameter bounds and
        # keep parameters under 5, we rely on the SR mixture (below) to shape Q1_mf dynamics.
        Q1 = 0.5 * Q1_mb + 0.5 * Q1_mf

        logits1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Reference update (EMA of raw reward)
        R = (1.0 - k_ref) * R + k_ref * r

        # Utility transformation around reference with concavity gamma_eff
        x = r - R
        # Symmetric power utility around reference
        util = np.sign(x) * (np.abs(x) ** gamma_eff)

        # Second-stage learning with utility as outcome
        pe2 = util - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # First-stage MF update: mixture of SARSA and Q-learning
        # SARSA target uses the value of the chosen a2 in the visited state
        target_sarsa = Q2[s, a2]
        # Q-learning target uses the max second-stage value for the visited state
        target_qlearn = np.max(Q2[s])
        target1 = w_sarsa_eff * target_sarsa + (1.0 - w_sarsa_eff) * target_qlearn

        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    eps = 1e-12
    neg_log_likelihood = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_log_likelihood)