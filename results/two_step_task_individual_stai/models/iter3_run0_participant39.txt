def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with learned transitions and anxiety-weighted surprise credit assignment.
    
    The agent learns:
      - Second-stage Q-values model-free.
      - First-stage transition probabilities from experience.
      - First-stage action values from both model-based planning using the learned transitions
        and model-free bootstrapping. The latter is boosted by transition surprise (how
        unexpected the reached planet was), and this surprise effect increases with anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage states (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; e.g., alien choices)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha_val:   [0,1] learning rate for second-stage values and stage-1 MF values
        alpha_tr:    [0,1] learning rate for transition model
        beta:        [0,10] inverse temperature for both stages
        w_plan0:     [0,1] baseline weight on model-based control
        surpr0:      [0,1] baseline weight on surprise-boosted MF credit at stage 1

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_val, alpha_tr, beta, w_plan0, surpr0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition model T[a, s'].
    # Start neutral at 0.5/0.5 for each action.
    T = np.full((2, 2), 0.5)

    # Model-free Q-values
    q1_mf = np.zeros(2)         # first-stage MF values for A/U
    q2_mf = np.zeros((2, 2))    # second-stage MF values within each planet

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects:
    # - Reduce planning weight (higher anxiety -> less planning).
    w_plan = np.clip(w_plan0 * (1.0 - stai0), 0.0, 1.0)
    # - Increase surprise impact on MF credit.
    surpr_w = np.clip(surpr0 * (0.5 + 0.5 * stai0), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        a1 = int(action_1[t])

        # Model-based first-stage values via current transition model and max second-stage values
        max_q2 = np.max(q2_mf, axis=1)           # shape (2,)
        q1_mb = T @ max_q2                       # shape (2,)

        # Hybrid first-stage action values
        q1 = w_plan * q1_mb + (1.0 - w_plan) * q1_mf

        # Softmax policy for first-stage
        q1c = q1 - np.max(q1)  # stability
        p1 = np.exp(beta * q1c)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Second-stage policy at observed state
        s2 = int(state[t])
        a2 = int(action_2[t])

        q2 = q2_mf[s2]
        q2c = q2 - np.max(q2)
        p2 = np.exp(beta * q2c)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Outcome
        r = float(reward[t])

        # Transition model update for the chosen first-stage action using a simple delta rule
        # Move probability mass toward observed state, away from the other.
        for sp in (0, 1):
            target = 1.0 if sp == s2 else 0.0
            T[a1, sp] += alpha_tr * (target - T[a1, sp])
        # Renormalize against numerical drift
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

        # Second-stage MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha_val * pe2

        # First-stage MF update with surprise-weighted bootstrapping target
        # Surprise is 1 - P(reached state | chosen action) using pre-update T estimate.
        # We approximated with post-update T; to keep the signal bounded and meaningful,
        # compute surprise from the previous probabilities by inverting the update step:
        # As a practical proxy, we use surpr = 1 - T[a1, s2] after update; it remains in [0,1].
        surpr = 1.0 - T[a1, s2]
        td_target1 = q2_mf[s2, a2] * (1.0 + surpr_w * surpr)
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_val * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation hybrid with anxiety-modulated trace and second-stage stickiness.
    
    The agent:
      - Learns a simple successor map M[a1, s2] (probability of landing on each planet
        given a spaceship) and uses it to compute model-based values q1_sr = M @ max(Q2).
      - Also learns model-free values at both stages, with an eligibility trace that
        propagates second-stage prediction errors to first-stage MF values.
      - First-stage choice uses a mix of SR-based and MF values.
      - Second-stage choices include a perseveration bias that increases with anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha:    [0,1] learning rate for values and SR map
        beta:     [0,10] inverse temperature for both stages
        lambda0:  [0,1] baseline eligibility trace from stage 2 PE to stage 1 MF
        w_sr0:    [0,1] baseline weight on SR-based planning at stage 1
        stick2:   [0,1] baseline perseveration strength for second-stage choices

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, lambda0, w_sr0, stick2 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Successor representation for one-step environment ~ transition model
    M = np.full((2, 2), 0.5)  # start neutral

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    # - SR/map learning slightly reduced with anxiety
    alpha_sr = np.clip(alpha * (1.0 - 0.5 * stai0), 0.0, 1.0)
    # - Planning weight reduced with anxiety
    w_sr = np.clip(w_sr0 * (1.0 - stai0), 0.0, 1.0)
    # - Eligibility trace increased with anxiety (faster MF credit assignment)
    lam = np.clip(lambda0 * (1.0 + stai0), 0.0, 1.0)
    # - Second-stage perseveration increased with anxiety
    k2 = stick2 * (1.0 + stai0)

    prev_a2 = None

    eps = 1e-12
    for t in range(n_trials):
        a1 = int(action_1[t])

        # SR-based planning value (uses current map M)
        max_q2 = np.max(q2_mf, axis=1)   # shape (2,)
        q1_sr = M @ max_q2               # shape (2,)

        # First-stage hybrid value
        q1 = w_sr * q1_sr + (1.0 - w_sr) * q1_mf

        # First-stage policy
        q1c = q1 - np.max(q1)
        p1 = np.exp(beta * q1c)
        p1 = p1 / (np.sum(p1) + eps)
        p_choice_1[t] = p1[a1]

        # Second-stage policy with perseveration bias
        s2 = int(state[t])
        a2 = int(action_2[t])

        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] = 1.0
        q2 = q2_mf[s2] + k2 * bias2

        q2c = q2 - np.max(q2)
        p2 = np.exp(beta * q2c)
        p2 = p2 / (np.sum(p2) + eps)
        p_choice_2[t] = p2[a2]

        # Outcome
        r = float(reward[t])

        # Update SR map row for chosen a1 toward observed state (akin to transitions)
        for sp in (0, 1):
            target = 1.0 if sp == s2 else 0.0
            M[a1, sp] += alpha_sr * (target - M[a1, sp])
        # Renormalize
        row_sum = M[a1].sum()
        if row_sum > 0:
            M[a1] /= row_sum

        # Second-stage MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # First-stage MF eligibility-trace update driven by second-stage PE
        q1_mf[a1] += alpha * lam * pe2

        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-bonus exploration with anxiety-modulated lapse and forgetting.
    
    The agent:
      - Learns second-stage values model-free with learning rate alpha.
      - Tracks per-alien volatility via running unsigned prediction errors and adds an
        exploration bonus proportional to volatility (bonus decreases with anxiety).
      - Uses fixed, known transitions (common .7) to plan model-based first-stage values
        from second-stage values plus exploration bonus.
      - Includes a lapse component in both stages that increases with anxiety.
      - Applies value forgetting/decay that grows with anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), trait anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha:       [0,1] learning rate for value and volatility traces
        beta:        [0,10] inverse temperature for both stages
        lapse0:      [0,1] baseline lapse probability (uniform-choice mixing)
        decay0:      [0,1] baseline forgetting toward priors
        vol_bonus0:  [0,1] baseline weight on volatility bonus

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, lapse0, decay0, vol_bonus0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Values and volatility traces
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))
    vol = np.zeros((2, 2))  # running unsigned PEs for each alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    lapse = np.clip(lapse0 * (0.2 + 0.8 * stai0), 0.0, 0.5)  # cap lapse to <=0.5 for identifiability
    decay = np.clip(decay0 * (1.0 + 0.5 * stai0), 0.0, 1.0)
    vol_w = np.clip(vol_bonus0 * (1.0 - stai0), 0.0, 1.0)

    eps = 1e-12
    for t in range(n_trials):
        # Apply forgetting before choice
        # Stage-2 values decay toward 0.5 prior; stage-1 MF toward 0 prior.
        q2_mf = (1.0 - decay) * q2_mf + decay * 0.5
        q1_mf = (1.0 - decay) * q1_mf

        # Model-based value from second-stage values plus exploration bonus
        max_q2_bonus = np.max(q2_mf + vol_w * vol, axis=1)  # shape (2,)
        q1_mb = T_known @ max_q2_bonus

        # Combine MB with MF (simple average weighted by confidence in MF via 1 - decay)
        w_mb = 0.5 + 0.5 * (1.0 - stai0)  # more planning when less anxious
        w_mb = np.clip(w_mb, 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage action probabilities with lapse
        q1c = q1 - np.max(q1)
        p1_soft = np.exp(beta * q1c)
        p1_soft = p1_soft / (np.sum(p1_soft) + eps)
        p1 = (1.0 - lapse) * p1_soft + lapse * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage action probabilities with volatility bonus and lapse
        s2 = int(state[t])
        q2b = q2_mf[s2] + vol_w * vol[s2]
        q2c = q2b - np.max(q2b)
        p2_soft = np.exp(beta * q2c)
        p2_soft = p2_soft / (np.sum(p2_soft) + eps)
        p2 = (1.0 - lapse) * p2_soft + lapse * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Outcome and learning
        r = float(reward[t])

        # Second-stage PE and updates
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Volatility trace update (running unsigned PE)
        vol[s2, a2] = (1.0 - alpha) * vol[s2, a2] + alpha * abs(pe2)

        # Simple MF update at stage 1 using the updated second-stage value as bootstrap
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll