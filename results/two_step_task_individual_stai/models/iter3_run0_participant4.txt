def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-modulated exploration with anxiety-sensitive temperature and eligibility trace.

    Overview
    - Stage-2 values learned via Rescorla–Wagner (RW).
    - Maintains an uncertainty estimate per state-action from running absolute PEs.
    - Anxiety scales down the effective inverse temperature in proportion to uncertainty (risk sensitivity).
    - An uncertainty-driven optimism/exploration bonus is added to Q-values, attenuated by anxiety.
    - Stage-1 uses a combination of model-based evaluation (via known transitions) and a model-free
      value that is updated by an eligibility trace from stage-2 prediction errors.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:      [0,1]   Learning rate for Q updates (RW) and uncertainty trace.
    - beta:       [0,10]  Base inverse temperature for softmax choice.
    - lambda_e:   [0,1]   Eligibility trace strength for propagating stage-2 PE to stage-1 MF values.
    - tau_ent:    [0,1]   Weight of uncertainty-based exploration bonus added to Q-values.
    - xi_anx:     [0,1]   Strength with which anxiety and uncertainty reduce effective beta.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, lambda_e, tau_ent, xi_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, lambda_e, tau_ent, xi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # actions (A,U) x states (X,Y)

    # Value and uncertainty initialization
    q2 = np.zeros((2, 2)) + 0.5       # stage-2 Q(s, a2)
    u2 = np.zeros((2, 2)) + 0.25      # stage-2 uncertainty proxy (running |PE|); start moderately uncertain
    q1_mf = np.zeros(2) + 0.0         # stage-1 model-free values

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Uncertainty-driven exploration bonus, attenuated by anxiety
        bonus2 = tau_ent * (1.0 - stai) * u2  # per state-action
        # Effective temperature reduced by anxiety-weighted uncertainty (state-average for stability)
        mean_u_state = float(np.mean(u2[s]))
        beta2_eff = beta * (1.0 - xi_anx * stai * mean_u_state)
        beta2_eff = max(1e-3, beta2_eff)

        # Stage-2 policy
        logits2 = beta2_eff * (q2[s] + bonus2[s])
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Stage-1 model-based evaluation uses expected max over second stage including bonus
        max_q2_with_bonus = np.max(q2 + bonus2, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2_with_bonus

        # Stage-1 effective temperature reduced by expected uncertainty along transitions
        exp_u = transition_matrix @ np.mean(u2, axis=1)
        beta1_eff = beta * (1.0 - xi_anx * stai * float(exp_u[a1]))
        beta1_eff = max(1e-3, beta1_eff)

        # Combine MB and MF (equal weight, no extra parameter) for choice
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf
        logits1 = beta1_eff * q1_comb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Learning updates
        pe2 = r - q2[s, a2]
        # Stage-2 RW update
        q2[s, a2] += alpha * pe2
        # Update uncertainty proxy with running absolute PE
        u2[s, a2] = (1.0 - alpha) * u2[s, a2] + alpha * abs(pe2)
        # Eligibility trace to stage-1 MF
        q1_mf[a1] += alpha * lambda_e * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-weighted arbitration between model-based and model-free control with forgetting and repetition bias.

    Overview
    - Learns stage-2 values via RW with value forgetting toward 0.5.
    - Stage-1 combines MB and MF values using a dynamic weight w_t computed from:
        • MB reliability (contrast between planets' best options).
        • Transition surprise (rare vs common).
      Anxiety increases the impact of surprise (shifting toward MF) and reduces the impact of MB reliability.
    - Repetition bias (choice kernel) for both stages.
    
    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:       [0,1]   Learning rate for Q updates.
    - beta:        [0,10]  Inverse temperature for softmax.
    - chi_forget:  [0,1]   Forgetting rate pulling Q2 toward 0.5 each trial.
    - phi_arbit:   [0,1]   Strength of arbitration modulation by reliability and surprise with anxiety scaling.
    - kappa_rep:   [0,1]   Repetition bias strength for both stages.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, chi_forget, phi_arbit, kappa_rep].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, chi_forget, phi_arbit, kappa_rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2) + 0.0

    # Repetition kernels (logit bias)
    rep1 = np.zeros(2)
    rep2 = np.zeros((2, 2))
    last_a1 = None
    last_a2 = np.array([-1, -1])  # per state

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply forgetting toward 0.5 to stage-2
        q2 = (1.0 - chi_forget) * q2 + chi_forget * 0.5

        # Repetition biases (decay old bias slightly)
        rep1 *= (1.0 - 0.5 * kappa_rep)
        rep2[s] *= (1.0 - 0.5 * kappa_rep)
        if last_a1 is not None:
            rep1[last_a1] += kappa_rep
        if last_a2[s] in (0, 1):
            rep2[s, last_a2[s]] += kappa_rep

        # Stage-2 policy
        logits2 = beta * q2[s] + rep2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Model-based evaluation for stage-1
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2

        # Arbitration weight w_t in [0,1]
        reliability = abs(max_q2[0] - max_q2[1])  # contrast between planets
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        surprise = 1 - is_common  # 1 if rare

        # Compute logit for w with opposing effects of reliability and surprise, anxiety-weighted
        base_logit = 0.0  # centered at 0 -> w ~ 0.5 when no signals
        logit_w = base_logit \
                  + phi_arbit * (1.0 - stai) * (reliability - 0.5) \
                  - phi_arbit * stai * surprise
        w_t = 1.0 / (1.0 + np.exp(-logit_w))
        w_t = min(1.0, max(0.0, w_t))

        # Combine MB and MF for stage-1 choice
        q1_comb = w_t * q1_mb + (1.0 - w_t) * q1_mf
        logits1 = beta * q1_comb + rep1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        q1_mf[a1] += alpha * pe2  # eligibility propagation

        # Update last choices
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-modulated negativity bias and adaptive transition-belief plus lapse.

    Overview
    - Separate learning rates for positive vs. negative PEs at stage 2.
    - Anxiety increases learning from negative outcomes and reduces learning from positive outcomes.
    - Maintains an adaptive belief about transition commonness for each first-stage action (p_common^A, p_common^U),
      updated from observed transitions; model-based values use these beliefs.
    - Includes an anxiety-scaled lapse at both stages.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha_pos:  [0,1]   Base learning rate for positive PEs.
    - alpha_neg:  [0,1]   Base learning rate for negative PEs.
    - beta:       [0,10]  Inverse temperature for softmax.
    - theta_trans:[0,1]   Learning rate for updating transition commonness beliefs.
    - rho_lapse:  [0,1]   Base lapse rate; effective lapse scales with anxiety.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha_pos, alpha_neg, beta, theta_trans, rho_lapse].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha_pos, alpha_neg, beta, theta_trans, rho_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Stage-2 values and stage-1 MF values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2) + 0.0

    # Transition belief: p_common for each first-stage action
    # For action A: P(X|A) = p_common_A, P(Y|A) = 1 - p_common_A
    # For action U: P(Y|U) = p_common_U, P(X|U) = 1 - p_common_U
    p_common_A = 0.7
    p_common_U = 0.7

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Anxiety-modulated valence asymmetry
        alpha_pos_eff = alpha_pos * (1.0 - 0.5 * stai)
        alpha_neg_eff = alpha_neg * (1.0 + 0.5 * stai)
        alpha_pos_eff = min(1.0, max(0.0, alpha_pos_eff))
        alpha_neg_eff = min(1.0, max(0.0, alpha_neg_eff))

        # Lapse proportional to anxiety
        lapse = min(0.2, rho_lapse * stai)

        # Build current transition matrix from beliefs
        # Row 0 corresponds to A, row 1 to U; columns X(0), Y(1)
        trans_A = np.array([p_common_A, 1.0 - p_common_A])
        trans_U = np.array([1.0 - p_common_U, p_common_U])
        transition_matrix = np.vstack([trans_A, trans_U])

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Stage-1 model-based values using current beliefs
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2
        # Combine MB and MF without extra parameter by simple averaging
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        logits1 = beta * q1_comb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Learning updates
        pe2 = r - q2[s, a2]
        if pe2 >= 0.0:
            q2[s, a2] += alpha_pos_eff * pe2
            q1_mf[a1] += alpha_pos_eff * pe2
        else:
            q2[s, a2] += alpha_neg_eff * pe2
            q1_mf[a1] += alpha_neg_eff * pe2

        # Update transition belief for chosen first-stage action based on whether transition was common
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        target = float(is_common)  # 1 if common observed, 0 if rare
        if a1 == 0:
            p_common_A = (1.0 - theta_trans) * p_common_A + theta_trans * target
        else:
            p_common_U = (1.0 - theta_trans) * p_common_U + theta_trans * target

        # Keep beliefs within [0.05, 0.95] to avoid degeneracy
        p_common_A = min(0.95, max(0.05, p_common_A))
        p_common_U = min(0.95, max(0.05, p_common_U))

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)