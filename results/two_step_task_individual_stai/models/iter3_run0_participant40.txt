def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated arbitration with anxiety-weighted exploration and perseveration.
    
    The agent learns second-stage values (Q2) and uses a model-based (MB) plan at stage 1
    using the known transition structure. It also learns a model-free (MF) first-stage value (Q1_MF).
    Uncertainty in second-stage outcomes is tracked per state-action as an exponential running variance.
    Anxiety (stai) increases exploration under uncertainty by reducing the effective inverse temperature
    proportionally to the predicted uncertainty for each action. Stage-1 action values are a convex
    combination of MB and MF values, plus a perseveration bias that is stronger at lower anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, eta, zeta, rho)
        Bounds:
        - alpha2 in [0,1]: learning rate for second-stage Q-values and stage-1 MF backup.
        - beta in [0,10]: inverse temperature baseline for softmax.
        - eta in [0,1]: arbitration weight; 1=model-based, 0=model-free at stage 1.
        - zeta in [0,1]: uncertainty sensitivity scaling the anxiety-gated exploration.
        - rho in [0,1]: perseveration strength at stage 1; is down-weighted by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, eta, zeta, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q2 = np.zeros((2, 2))       # second-stage values per state-action
    Q1_MF = np.zeros(2)         # model-free first-stage values

    # Running statistics for uncertainty (variance) per state-action
    m = np.zeros((2, 2))        # running mean of rewards
    v = np.zeros((2, 2))        # running variance proxy of rewards

    prev_a1 = None

    for t in range(n_trials):
        # Model-based stage-1 values via transition model and current Q2
        max_Q2 = np.max(Q2, axis=1)         # value of each state by best alien
        Q1_MB = T @ max_Q2                  # expected value for each spaceship

        # Arbitration between MB and MF
        Q1_mix = eta * Q1_MB + (1.0 - eta) * Q1_MF

        # Anxiety-weighted perseveration bias (stronger when anxiety is low)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = rho * (1.0 - stai_val)

        # Uncertainty-dependent temperature at stage 1:
        # uncertainty for each action = expected state-uncertainty (max action var per state)
        state_unc = np.max(v, axis=1)  # per-state uncertainty summary
        unc_a = T @ state_unc          # expected uncertainty per first-stage action
        beta1_eff_per_action = beta / (1.0 + zeta * stai_val * (unc_a + 1e-8))

        # Softmax with per-action temperatures -> implement as action-specific scaling of Q
        # Equivalently, compute logits_i = beta_i * Q_i + bias_i
        logits1 = beta1_eff_per_action * Q1_mix + bias1
        # stabilize
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: uncertainty-gated temperature at visited state
        s2 = int(state[t])
        a2 = int(action_2[t])

        beta2_eff = beta / (1.0 + 0.5 * zeta * stai_val * (np.max(v[s2]) + 1e-8))
        logits2 = beta2_eff * Q2[s2]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update running mean and variance proxy for uncertainty tracking
        # Mean update with alpha2
        m_old = m[s2, a2]
        m[s2, a2] = (1.0 - alpha2) * m_old + alpha2 * r
        # Variance proxy using exponential smoothing of squared deviation
        dev = r - m_old
        v[s2, a2] = (1.0 - alpha2) * v[s2, a2] + alpha2 * (dev * dev)

        # Model-free first-stage backup from realized second-stage chosen value
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free SARSA with anxiety-modulated eligibility on rare transitions and repetition bias.
    
    The agent learns both stage-1 (Q1) and stage-2 (Q2) values via temporal-difference (TD) learning.
    Credit assignment from the second to the first stage is controlled by an eligibility parameter
    that is reduced following rare transitions, with the reduction amplified by anxiety (stai).
    A repetition bias encourages sticking with the previous action at both stages, but this bias
    is weaker at higher anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, lambda0, rare_bias, rep)
        Bounds:
        - alpha in [0,1]: TD learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - lambda0 in [0,1]: base eligibility strength backing up from stage 2 to stage 1.
        - rare_bias in [0,1]: strength of anxiety-dependent reduction in eligibility on rare transitions.
        - rep in [0,1]: repetition (stickiness) strength; diminished by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lambda0, rare_bias, rep = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known common/rare structure for detecting transition type
    # Common: A->X (0->0) or U->Y (1->1)
    def is_common(a1, s2):
        return (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    prev_a1 = None
    prev_a2 = None
    prev_s2 = None

    for t in range(n_trials):
        a1 = int(action_1[t])

        # Repetition bias at stage 1 (weaker when anxiety is high)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = rep * (1.0 - stai_val)

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])
        a2 = int(action_2[t])

        # Repetition bias at stage 2 is state-specific (also anxiety-damped)
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s2 is not None and prev_s2 == s2:
            bias2[prev_a2] = rep * (1.0 - stai_val)

        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD updates
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Eligibility for backing up to Q1 depends on transition type and anxiety
        common = is_common(a1, s2)
        if common:
            # Slightly boost eligibility on common transitions; effect weakens with anxiety
            lam_eff = lambda0 * (1.0 + 0.5 * rare_bias * (1.0 - stai_val))
        else:
            # Reduce eligibility on rare transitions more strongly when anxiety is high
            lam_eff = lambda0 * (1.0 - rare_bias * stai_val)
        # Keep within [0,1]
        lam_eff = max(0.0, min(1.0, lam_eff))

        # Backup from realized second-stage chosen value (SARSA(Î») single-step)
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1[a1]
        Q1[a1] += alpha * lam_eff * delta1

        prev_a1 = a1
        prev_a2 = a2
        prev_s2 = s2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with anxiety-scaled loss aversion, value decay, and transition bonus.
    
    The agent evaluates rewards through an asymmetric utility function that is more loss-averse
    at higher anxiety. Second-stage values (Q2) are learned from utility, not raw reward.
    Values decay toward zero each trial, with decay strength increasing with anxiety.
    First-stage choices are model-based using the known transition structure, and a transition
    consistency bonus adjusts first-stage preferences based on whether the observed transition
    matched the common route and the sign of the recent utility.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, delta, nu, la)
        Bounds:
        - alpha2 in [0,1]: learning rate for second-stage Q-values from utility.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - delta in [0,1]: base decay rate; effective decay scales with anxiety.
        - nu in [0,1]: transition-consistency bonus magnitude applied to stage-1 values.
        - la in [0,1]: base loss-aversion parameter; effective loss aversion increases with anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, delta, nu, la = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))  # second-stage values
    # We maintain a small first-stage bonus vector that carries over transiently
    bonus1 = np.zeros(2)

    # Effective decay increases with anxiety
    decay = delta * stai_val

    def util(x):
        # Loss aversion coefficient increases with anxiety: xi in [1, 1+4*la]
        xi = 1.0 + 4.0 * la * stai_val
        return x if x >= 0.0 else -xi * (-x)

    for t in range(n_trials):
        # Decay values and bonus each trial
        Q2 *= (1.0 - decay)
        bonus1 *= (1.0 - decay)

        # Model-based stage-1 values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Stage-1 policy with bonus
        logits1 = beta * Q1_MB + bonus1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Observe reward and compute utility
        r = reward[t]
        u = util(r)

        # Update Q2 with utility-based TD
        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update stage-1 transition-consistency bonus based on observed transition and utility sign
        # Common transitions: A->X (0->0), U->Y (1->1)
        common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        sign_u = 0.0
        if u > 0:
            sign_u = 1.0
        elif u < 0:
            sign_u = -1.0

        # If common and utility positive: reinforce chosen action.
        # If rare and utility positive: dampen reinforcement proportional to anxiety.
        # If utility negative: invert the sign (discourage the chosen action), stronger when rare and anxious.
        if sign_u != 0.0:
            adj = nu * sign_u * (1.0 - (0.5 if common else 1.0) * stai_val)
            bonus_vec = np.zeros(2)
            bonus_vec[a1] = adj
            bonus1 += bonus_vec

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll