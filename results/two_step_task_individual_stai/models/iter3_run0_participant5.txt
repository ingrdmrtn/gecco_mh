def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-modulated hybrid with eligibility trace and perseveration.

    Idea:
    - Stage 1 mixes model-based (MB) and model-free (MF) values.
    - Anxiety diminishes MB arbitration weight and enhances perseveration.
    - Credit assignment to stage-1 MF values is eligibility-trace-like and
      further reduced after rare transitions, especially under anxiety.
    - Stage 2 uses standard Q-learning.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (e.g., 0 or 1).
    stai : array-like of float in [0,1]
        Participant anxiety score; higher values reduce MB weight and increase
        perseveration and rare-transition gating.
    model_parameters : array-like of float
        [alpha0, beta, mix0, elig, persever0]
        - alpha0 in [0,1]: baseline learning rate for Q updates (both stages).
        - beta in [0,10]: inverse temperature (both stages).
        - mix0 in [0,1]: baseline MB arbitration weight (scale reduced by anxiety).
        - elig in [0,1]: eligibility-trace intensity for credit assignment to stage-1 MF.
        - persever0 in [0,1]: baseline perseveration strength (scaled up by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha0, beta, mix0, elig, persever0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common.
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Values
    q1_mf = np.zeros(2)          # stage-1 MF values for A/U
    q2 = 0.5 * np.ones((2, 2))   # stage-2 values for aliens on planets X/Y

    # Perseveration biases
    bias1 = np.zeros(2)
    bias2 = np.zeros((2, 2))

    # Anxiety-adjusted arbitration and perseveration
    # Higher anxiety reduces MB weight and increases perseveration.
    w_mb = np.clip(mix0 * (1.0 - 0.6 * st), 0.0, 1.0)
    kappa = persever0 * (0.3 + 0.7 * st)
    # Eligibility trace intensity (scaled mildly up by anxiety)
    e_eff = (0.1 + 0.9 * elig) * (0.7 + 0.3 * st)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    last_a1 = None
    last_s = None
    last_a2 = None

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB action-values via one-step lookahead
        max_q2 = np.max(q2, axis=1)             # best alien on each planet
        q1_mb = transition_matrix @ max_q2      # expected value from each spaceship

        # Combine MB and MF plus stickiness
        q1_comb = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        # Stage-1 policy
        logits1 = beta * (q1_comb - np.max(q1_comb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2 policy (with within-state perseveration)
        q2_biased = q2[s] + bias2[s]
        logits2 = beta * (q2_biased - np.max(q2_biased))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Determine whether the observed transition was common or rare
        # For A=0 common->X=0, U=1 common->Y=1
        is_common = (a1 == s)

        # Update Stage-2 values (standard Q-learning)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha0 * pe2

        # Stage-1 MF credit assignment uses an eligibility-like gate
        # Rare transitions down-weight the back-propagation more when anxiety is high.
        rare_gate = (1.0 - 0.8 * st) if (not is_common) else 1.0
        alpha1_eff = alpha0 * e_eff * np.clip(rare_gate, 0.0, 1.0)

        target1 = q2[s, a2]  # bootstrap from attained state-action value
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1_eff * pe1

        # Update perseveration biases
        bias1[:] = 0.0
        bias1[a1] += kappa

        bias2[:] = 0.0
        bias2[s, a2] += kappa

        last_a1, last_s, last_a2 = a1, s, a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Learned-transitions with UCB exploration, anxiety-weighted.

    Idea:
    - The agent learns both stage-2 rewards and stage-1 transition probabilities.
    - Stage-1 is purely model-based using the learned transition matrix.
    - Stage-2 adds a directed exploration (UCB) bonus inversely related to anxiety.
    - Anxiety reduces prior trust that transitions are strongly common (A->X, U->Y),
      and reduces the exploration bonus magnitude.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher anxiety lowers prior trust in common transitions
        and dampens UCB exploration.
    model_parameters : array-like of float
        [alpha_r, beta, ucb0, trust0]
        - alpha_r in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature (both stages).
        - ucb0 in [0,1]: base strength of UCB exploration bonus at stage 2.
        - trust0 in [0,1]: scales prior trust that A->X and U->Y are common.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_r, beta, ucb0, trust0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Stage-2 values and visitation counts for UCB
    q2 = 0.5 * np.ones((2, 2))
    n_sa = np.zeros((2, 2))  # visit counts per state-action

    # Dirichlet-like counts for transition learning: counts[action, state]
    counts = np.zeros((2, 2))
    # Anxiety-weighted prior: trust reduces with anxiety (flattening towards 0.5/0.5)
    trust_eff = np.clip(trust0 * (1.0 - 0.8 * st), 0.0, 1.0)
    # Map trust to prior probability for "common" transitions: 0.5 -> 0.5, 1.0 -> 0.7
    p_common_prior = 0.5 + 0.2 * trust_eff
    base = 2.0  # symmetric pseudo-count mass
    # For A=0, common state is X=0; for U=1, common state is Y=1
    counts[0, 0] = base * p_common_prior
    counts[0, 1] = base * (1.0 - p_common_prior)
    counts[1, 1] = base * p_common_prior
    counts[1, 0] = base * (1.0 - p_common_prior)

    # Anxiety-weighted UCB scale (anxious explore less)
    ucb_scale = ucb0 * (0.3 + 0.7 * (1.0 - st))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Update transition counts from previous choice and observed state
        counts[a1, s] += 1.0
        # Current learned transition matrix
        T = counts / np.clip(np.sum(counts, axis=1, keepdims=True), 1.0, None)

        # Stage-1 purely MB using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2: Q + UCB exploration bonus
        # UCB bonus ~ sqrt(log(t+2) / (n_sa + 1))
        n_sa_term = n_sa[s] + 1.0
        bonus = ucb_scale * np.sqrt(np.log(t + 2.0) / n_sa_term)
        q2_bonus = q2[s] + bonus

        logits2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Update Q2 and visitation counts
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2
        n_sa[s, a2] += 1.0

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Meta-control via anxiety-gated temperature, MB weight, and reward forgetting.

    Idea:
    - Stage 1 uses a hybrid of MB and MF, with MB weight higher when anxiety is low.
    - Softmax temperature increases when anxiety is low (more exploitation).
    - Stage 2 values decay (forget) over trials; decay grows with anxiety,
      modeling difficulty maintaining stable reward beliefs under high anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce beta (more randomness), reduce MB weight,
        and increase forgetting of stage-2 values.
    model_parameters : array-like of float
        [alpha, beta_base, forget, mbw0]
        - alpha in [0,1]: learning rate for Q updates.
        - beta_base in [0,10]: baseline inverse temperature.
        - forget in [0,1]: base forgetting factor for stage-2 values.
        - mbw0 in [0,1]: baseline MB weight at stage 1 (boosted when anxiety is low).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta_base, forget, mbw0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Transition model (fixed known structure)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Anxiety-gated meta-control
    # Lower anxiety => higher MB weight and higher beta (more deterministic).
    w_mb = np.clip(mbw0 + (1.0 - st) * (1.0 - mbw0) * 0.5, 0.0, 1.0)
    beta_eff = np.clip(beta_base * (0.5 + 0.5 * (1.0 - st)), 0.0, 10.0)
    # Forgetting increases with anxiety
    forget_eff = np.clip(forget * (0.5 + 0.5 * st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB values from expected max Q2 per planet
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Stage-2 forgetting before update (decay towards 0.5 baseline)
        q2 = (1.0 - forget_eff) * q2 + forget_eff * 0.5

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF learning from attained stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Mild anxiety-driven reduction of stage-1 learning
        alpha1 = alpha * (0.8 + 0.2 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)