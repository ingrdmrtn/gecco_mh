def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like planning with MF integration, anxiety-modulated arbitration, and memory decay.

    Core idea:
    - Stage 1 values blend a successor-like (transition-based) planner and a model-free cache.
    - Low anxiety increases planning weight and a structure-based stay/shift bias tied to prior common/rare transitions.
    - Soft forgetting (decay) prevents overcommitment to stale values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, omega, zeta, bias_prev]
        - alpha in [0,1]: learning rate for value updates (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - omega in [0,1]: base weight on transition-based planning at stage 1.
        - zeta in [0,1]: forgetting rate toward zero for unrefreshed Q-values.
        - bias_prev in [0,1]: magnitude of structure-based stay/shift bias at stage 1:
            after a common rewarded trial, bias to repeat; after a rare punished trial, bias to switch.
            Anxiety reduces the impact of this bias.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega, zeta, bias_prev = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)        # model-free cache for stage 1 actions
    q2 = np.zeros((2, 2))      # stage 2 action values per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated weights
    omega_eff = np.clip(omega * (1.0 - 0.4 * stai), 0.0, 1.0)
    bias_prev_eff_base = np.clip(bias_prev * (1.0 - stai), 0.0, 1.0)  # lower with anxiety

    # Keep track of previous trial for bias
    a1_prev = None
    s_prev = None
    r_prev = None

    for t in range(n_trials):
        # Planning component from successor-like expectation (transition-based)
        max_q2_by_state = np.max(q2, axis=1)     # best attainable value on each planet
        q1_plan = T @ max_q2_by_state           # expected value of each spaceship

        # Structure-based stay/shift bias from prior trial outcome and transition type
        pref = np.zeros(2)
        if a1_prev is not None:
            was_common = 1 if ((a1_prev == 0 and s_prev == 0) or (a1_prev == 1 and s_prev == 1)) else 0
            sign = 1.0 if r_prev > 0 else (-1.0 if r_prev < 0 else 0.0)
            # Rewarded-common or punished-rare encourages repeating; reverse otherwise
            bias = bias_prev_eff_base * sign * (1.0 if was_common else -1.0)
            # Apply bias as a preference to repeat or switch previous action
            pref[a1_prev] += bias
            pref[1 - a1_prev] -= bias

        # Hybrid stage-1 value
        q1 = omega_eff * q1_plan + (1.0 - omega_eff) * q1_mf + pref

        # Policy stage 1
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Policy stage 2 (based only on the current state's Q-values)
        s = int(state[t])
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        # Observe reward
        r = reward[t]

        # Soft forgetting (global) before updates
        q2 *= (1.0 - zeta)
        q1_mf *= (1.0 - zeta)

        # Stage 2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 MF update bootstrapping from the obtained second-stage action value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Bookkeeping for next-trial bias
        a1_prev, s_prev, r_prev = a1, s, r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Prospect-utility hybrid RL: risk/loss sensitivity and anxiety-adjusted planning.

    Core idea:
    - Outcomes are transformed via a prospect-like utility: concavity for gains (gamma) and loss aversion (lambda).
    - Stage 1 blends model-based planning and model-free caching.
    - Anxiety increases loss aversion and risk aversion (more concavity), and reduces model-based control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta, gamma, lambda_loss, w_mb]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - gamma in [0,1]: curvature for gains (utility ~ r^gamma).
        - lambda_loss in [0,1]: base loss aversion; mapped to [1, 3] range internally.
        - w_mb in [0,1]: base weight on model-based control at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, gamma, lambda_loss, w_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-adjusted utility parameters
    # More anxiety -> more concavity (lower gamma), higher loss aversion
    gamma_eff = np.clip(gamma * (1.0 - 0.5 * stai), 1e-4, 1.0)
    lambda_eff = 1.0 + 2.0 * np.clip(lambda_loss * (0.5 + 0.5 * stai), 0.0, 1.0)  # in [1,3]
    w_eff = np.clip(w_mb * (1.0 - 0.4 * stai), 0.0, 1.0)

    def utility(x):
        if x >= 0:
            return (x + 1e-12) ** gamma_eff
        else:
            return -lambda_eff * ((-x + 1e-12) ** gamma_eff)

    for t in range(n_trials):
        # Model-based evaluation for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid stage-1 action values
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage 1 policy
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2_s = q2[s]
        q2_shift = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_shift)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]
        u = utility(r)

        # Stage 2 update with prospect utility
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage 1 MF update bootstrapping from realized stage-2 value (in utility space)
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-driven exploration with learned transitions and anxiety-modulated temperature.

    Core idea:
    - Stage 2 uses an Upper Confidence Bound (UCB) bonus derived from a running variance estimate per state-action.
    - Stage 1 arbitrates between model-based (learned transitions) and model-free values via an entropy-based gate.
    - Anxiety increases uncertainty-driven exploration (higher UCB weight) and reduces decisiveness (lower beta).
    - A mild perseveration bias at stage 1 captures motor/choice inertia.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states reached (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float
        Anxiety score in [0,1]; use stai[0].
    model_parameters : sequence of floats
        [alpha, beta0, ucb, tau_tr, pers]
        - alpha in [0,1]: learning rate for Q-values and variance tracker.
        - beta0 in [0,10]: base inverse temperature.
        - ucb in [0,1]: base weight of the uncertainty bonus at stage 2.
        - tau_tr in [0,1]: learning rate for learning transition probabilities.
        - pers in [0,1]: base perseveration strength to repeat last stage-1 choice.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, ucb, tau_tr, pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition model near task structure, but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values and uncertainty (running variance proxy) for stage 2
    q2 = np.zeros((2, 2))
    var2 = np.ones((2, 2))  # start with high uncertainty

    # Stage 1 MF value
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety adjustments
    beta = max(1e-6, beta0 * (1.0 - 0.5 * stai))                    # higher anxiety -> more randomness
    ucb_eff = np.clip(ucb * (0.5 + 0.5 * stai), 0.0, 1.0)           # higher anxiety -> more uncertainty-seeking
    pers_eff = np.clip(pers * (1.0 - 0.3 * stai), 0.0, 1.0)         # anxiety mildly reduces perseveration

    # Previous action for perseveration
    a1_prev = None

    for t in range(n_trials):
        # Stage 2: compute exploration bonus for each state (added to Q)
        bonus = ucb_eff * np.sqrt(np.maximum(var2, 1e-12))
        # For stage 1 planning, we need the max over actions including bonus
        max_q2_with_bonus = np.max(q2 + bonus, axis=1)

        # Model-based evaluation with learned transitions
        q1_mb = T @ max_q2_with_bonus

        # Arbitration based on stage-2 uncertainty-induced entropy (no extra parameter)
        # Estimate probs under current q2+bonus for each state and compute entropy
        ent = np.zeros(2)
        for s in range(2):
            logits = (q2[s] + bonus[s]) - np.max(q2[s] + bonus[s])
            p = np.exp(beta * logits)
            p = p / np.sum(p)
            ent[s] = -np.sum(p * (np.log(p + 1e-12)))
        # Lower entropy -> trust MB more
        w = 1.0 / (1.0 + np.exp(4.0 * (np.mean(ent) - 0.5)))  # sigmoid gate in [0,1]

        # Perseveration preference
        pref = np.zeros(2)
        if a1_prev is not None:
            pref[a1_prev] += pers_eff
            pref[1 - a1_prev] -= pers_eff

        # Hybrid stage-1 values
        q1 = w * q1_mb + (1.0 - w) * q1_mf + pref

        # Stage 1 policy
        q1_shift = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_shift)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state with UCB bonus
        s = int(state[t])
        logits2 = (q2[s] + bonus[s]) - np.max(q2[s] + bonus[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage 2 updates: value and variance (running average of squared TD errors)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2
        var2[s, a2] = (1.0 - alpha) * var2[s, a2] + alpha * (delta2 ** 2)

        # Stage 1 MF update
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Learn transition model from observed transition
        T[a1, :] = (1.0 - tau_tr) * T[a1, :]
        T[a1, s] += tau_tr
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

        # Update perseveration memory
        a1_prev = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)