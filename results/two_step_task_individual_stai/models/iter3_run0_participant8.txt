def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-shaped arbitration and information-seeking bonus.

    Summary
    -------
    - Stage 2 values (aliens) learned via delta rule.
    - Stage 1 uses a hybrid of model-based (MB) and model-free (MF) values.
    - An information-seeking bonus (uncertainty bonus) is added to the MB value to favor
      planets whose alien rewards are uncertain; this bonus is maximal for medium anxiety.
    - Arbitration weight between MB and MF depends on anxiety (higher anxiety pushes weight
      toward a neutral 0.5, reducing reliance on either extreme).
    - First-stage perseveration bias is reduced by anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet (0/1).
    reward : array-like of float
        Trial outcomes (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, omega0, tau_info, kappa_pers]
        Bounds:
        - alpha: stage-2 reward learning rate [0,1]
        - beta: inverse temperature for both stages [0,10]
        - omega0: base MB weight for stage-1 arbitration [0,1]
        - tau_info: base strength of information-seeking bonus [0,1]
        - kappa_pers: first-stage perseveration strength [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, omega0, tau_info, kappa_pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 action values (planet x alien)
    q2 = np.zeros((2, 2)) + 0.5

    # Stage-1 MF values (for spaceships A/U)
    q1_mf = np.zeros(2)

    prev_a1 = None

    # Anxiety-modulated arbitration:
    # As anxiety increases, pull omega toward 0.5 (less extreme reliance on MB/MF).
    omega = omega0 * (1.0 - st) + 0.5 * st
    omega = np.clip(omega, 0.0, 1.0)

    # Information-seeking bonus: emphasize uncertainty around medium anxiety.
    # Gaussian bump centered at ~0.41 (mid of medium range .31-.51) with width ~0.08.
    bump = np.exp(-((st - 0.41) ** 2) / (2.0 * (0.08 ** 2)))
    info_scale = tau_info * (0.5 + 0.5 * bump)  # in [0,1]

    # Perseveration reduced by anxiety
    persev = kappa_pers * (1.0 - st)

    for t in range(n_trials):
        # Compute per-planet uncertainty bonus: Bernoulli variance q*(1-q) per alien,
        # then take the max within planet to capture targetable information.
        var_per_planet = q2 * (1.0 - q2)
        unc_bonus = np.max(var_per_planet, axis=1)  # shape (2,)

        # Model-based action values via transitions and info bonus
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ (max_q2 + info_scale * unc_bonus)

        # Combine MB and MF
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += persev

        # Stage 1 policy
        logits1 = q1 + bias1
        logits1 = logits1 - np.max(logits1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy in reached state
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-1 MF from stage-2 values (one-step bootstrapping)
        # TD target: value of observed second-stage choice
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated transition learning with anxiety-weighted stochasticity and stage-2 stickiness.

    Summary
    -------
    - Learns transition probabilities T(a1 -> planet) from experience.
    - Stage 1 is purely model-based using learned T and current stage-2 values.
    - Surprise on observed transitions accelerates transition learning and lowers effective
      choice precision; both effects are amplified by anxiety.
    - Stage 2 has a perseveration bias (stickiness) that increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [eta, beta, alpha_T0, zeta, psi_st2]
        Bounds:
        - eta: stage-2 reward learning rate [0,1]
        - beta: base inverse temperature [0,10]
        - alpha_T0: base transition learning rate [0,1]
        - zeta: surprise gain applied to both transition learning and stochasticity [0,1]
        - psi_st2: base second-stage perseveration strength [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta, beta, alpha_T0, zeta, psi_st2 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize transitions to uniform uncertainty
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2)) + 0.5

    # Stage-2 perseveration (increases with anxiety)
    stick2 = psi_st2 * (1.0 + 0.5 * st)
    prev_a2_by_state = {0: None, 1: None}
    prev_r_by_state = {0: None, 1: None}  # not used in policy but could be extended

    for t in range(n_trials):
        # Expected impurity over transitions (higher = more uncertainty)
        # Use mean Gini impurity across actions to derive a global beta1_eff
        gini_rows = 1.0 - np.sum(T ** 2, axis=1)  # shape (2,)
        avg_impurity = 0.5 * (gini_rows[0] + gini_rows[1])
        beta1_eff = beta * (1.0 - zeta * st * avg_impurity)
        beta1_eff = max(beta1_eff, 1e-3)

        # Model-based Q at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 policy
        logits1 = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta1_eff * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = q2[s].copy()
        # Add stage-2 perseveration bias to the previously chosen alien in this state
        if prev_a2_by_state[s] is not None:
            logits2[prev_a2_by_state[s]] += stick2
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 values
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Compute surprise on the observed transition and update transitions
        surpr = 1.0 - T[a1, s]  # high when transition was unlikely
        alpha_T_eff = alpha_T0 + (1.0 - alpha_T0) * (zeta * st * surpr)
        # Move T[a1] toward one-hot of observed state with rate alpha_T_eff
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alpha_T_eff) * T[a1] + alpha_T_eff * target
        # Normalize to be safe
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        prev_a2_by_state[s] = a2
        prev_r_by_state[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Robust (pessimistic) planner with anxiety-weighted robustness, value forgetting, and WSLS at stage 2.

    Summary
    -------
    - Stage 2 values learned via delta rule with decay toward 0.5 (forgetting).
    - Stage 1 uses robust planning that mixes best- and worst-alien values in each planet:
      V_planet = (1 - xi)*max(Q2) + xi*min(Q2), where xi grows with anxiety (more pessimistic).
    - Fixed transition structure (common=0.7, rare=0.3) used for model-based planning.
    - Stage 2 includes a win-stay/lose-shift bias: with higher anxiety, lose-shift strengthens,
      while win-stay weakens (approach-avoidance asymmetry).

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [alpha, beta, xi0, rho, kappa_wsls]
        Bounds:
        - alpha: stage-2 reward learning rate [0,1]
        - beta: inverse temperature [0,10]
        - xi0: base robustness/pessimism weight [0,1]
        - rho: forgetting rate toward 0.5 at stage 2 [0,1]
        - kappa_wsls: base WSLS strength at stage 2 [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, xi0, rho, kappa_wsls = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2)) + 0.5

    # Anxiety-weighted pessimism (robustness)
    xi = np.clip(xi0 * st, 0.0, 1.0)

    # WSLS parameters: attenuate win-stay with anxiety, amplify lose-shift with anxiety
    ws_gain = kappa_wsls * (1.0 - st)   # win-stay bias strength
    ls_gain = kappa_wsls * st           # lose-shift bias strength

    prev_a2_by_state = {0: None, 1: None}
    prev_r_by_state = {0: None, 1: None}

    for t in range(n_trials):
        # Stage-1 robust planet values
        max_q = np.max(q2, axis=1)
        min_q = np.min(q2, axis=1)
        v_planet = (1.0 - xi) * max_q + xi * min_q

        q1 = T @ v_planet

        # Stage-1 policy
        logits1 = q1 - np.max(q1)
        probs_1 = np.exp(beta * logits1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with WSLS bias
        s = state[t]
        logits2 = q2[s].copy()
        if prev_a2_by_state[s] is not None and prev_r_by_state[s] is not None:
            last_a2 = prev_a2_by_state[s]
            last_r = prev_r_by_state[s]
            if last_r >= 0.5:
                # Win-stay: increase logit for previously chosen action
                logits2[last_a2] += ws_gain
            else:
                # Lose-shift: decrease logit for previously chosen action
                logits2[last_a2] -= ls_gain
        logits2 = logits2 - np.max(logits2)
        probs_2 = np.exp(beta * logits2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 forgetting toward 0.5
        q2 = (1.0 - rho) * q2 + rho * 0.5

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        prev_a2_by_state[s] = a2
        prev_r_by_state[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll