def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Controllability-weighted arbitration with learned transitions and stage-1 perseveration.
    
    Idea:
    - The agent learns second-stage Q-values from reward (MF).
    - It also learns the transition structure T_hat(a -> s) via a simple delta rule.
    - Arbitration at stage 1 blends model-based (MB, via T_hat and max Q on each planet)
      and model-free (MF, TD-propagated from stage 2) with a weight that depends on
      perceived controllability and anxiety:
        w = clip((1 - stai) * mean_action_determinism, 0, 1)
      where determinism per action is |T_hat(a, X) - 0.5| * 2.
      Thus, higher anxiety and less predictable transitions reduce MB control.
    - There is stage-1 perseveration: repeating the previous spaceship receives a bonus.
    
    Parameters (bounds):
    - alpha2: [0,1] learning rate for stage-2 Q-learning and stage-1 MF backup
    - gamma_trans: [0,1] learning rate for learning the transition matrix T_hat
    - pers1: [0,1] strength of stage-1 perseveration bonus for repeating last action
    - beta: [0,10] inverse temperature for both stages
    
    Inputs:
    - action_1: int array of length T with values in {0,1} (0=A, 1=U)
    - state: int array of length T with values in {0,1} (0=X, 1=Y)
    - action_2: int array of length T with values in {0,1} (aliens per planet)
    - reward: float array of length T with values in [0,1]
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha2, gamma_trans, pers1, beta]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha2, gamma_trans, pers1, beta = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Initialize learned transition model T_hat: rows=actions(A,U), cols=states(X,Y)
    T_hat = np.full((2, 2), 0.5)

    # MF values
    q1_mf = np.zeros(2)           # stage-1 MF values for A,U
    q2 = np.zeros((2, 2))         # stage-2 values for each state and action

    # For stage-1 perseveration
    last_a1 = -1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based evaluation: value of each action is expectation over learned transitions
        max_q2_per_state = np.max(q2, axis=1)  # [X_best, Y_best]
        q1_mb = T_hat @ max_q2_per_state       # size 2 (A,U)

        # Arbitration weight from controllability and anxiety
        determinism = np.abs(T_hat[:, 0] - 0.5) * 2.0  # per action in [0,1]
        w = np.clip((1.0 - stai_score) * np.mean(determinism), 0.0, 1.0)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 perseveration bonus
        logits1 = q1.copy()
        if last_a1 != -1:
            logits1[last_a1] += pers1

        # Softmax for stage 1
        l1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within the observed state)
        s = state[t]
        logits2 = q2[s].copy()
        l2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning: stage 2 Q-learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # MF backup to stage 1 (TD(1) using obtained second-stage value)
        target1 = q2[s, a2]
        q1_mf[a1] += alpha2 * (target1 - q1_mf[a1])

        # Learn transitions T_hat via delta rule toward the realized state
        # For chosen action a1, update probability of observed state s toward 1, other toward 0
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            T_hat[a1, sp] += gamma_trans * (target - T_hat[a1, sp])

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-split temperatures and approach-to-reward bias at stage 2 with MB/MF arbitration from anxiety.
    
    Idea:
    - Stage-2 values learned by MF TD.
    - Stage-1 combines MB (using known transition structure) and MF with a weight w = 1 - stai
      so higher anxiety yields more MF reliance.
    - Anxiety also splits choice stochasticity across stages:
        beta1 = beta * (1 - chi * stai)   (more exploratory at stage 1 with anxiety)
        beta2 = beta * (1 + chi * stai)   (more exploitative at stage 2 with anxiety)
      with clipping to remain positive.
    - Stage-2 approach bias: tend to repeat the last stage-2 action that produced reward within
      each state. The bias magnitude increases with anxiety.
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for stage-2 Q and MF propagation to stage 1
    - beta: [0,10] base inverse temperature
    - chi: [0,1] temperature split gain modulated by anxiety
    - app2: [0,1] baseline stage-2 approach bias toward last rewarded action in a state
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, beta, chi, app2]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, beta, chi, app2 = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Arbitration weight
    w = np.clip(1.0 - stai_score, 0.0, 1.0)

    # Temperature split
    beta1 = max(1e-6, beta * (1.0 - chi * stai_score))
    beta2 = max(1e-6, beta * (1.0 + chi * stai_score))

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track last rewarded action per state (-1 if none yet)
    last_rew_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB at stage 1 from known transitions and current stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage 1 softmax
        l1 = beta1 * (q1 - np.max(q1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 with approach-to-reward bias
        s = state[t]
        logits2 = q2[s].copy()
        # Anxiety-scaled bias bonus for the last rewarded action in this state
        if last_rew_a2[s] != -1:
            bias_mag = app2 * (0.5 + 0.5 * stai_score)
            logits2[last_rew_a2[s]] += bias_mag

        l2 = beta2 * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update last rewarded action memory
        if r > 0.0:
            last_rew_a2[s] = a2

        # MF propagation to stage 1 (TD(1))
        q1_mf[a1] += alpha * (q2[s, a2] - q1_mf[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-dependent MF credit with anxiety-driven spillover and ship-A bias.
    
    Idea:
    - Standard stage-2 MF learning from rewards.
    - Stage-1 MF receives TD backup from obtained second-stage value.
    - Additionally, after rare transitions, a fraction of the TD signal is misassigned
      to the unchosen first-stage action (transition-dependent MF). This fraction grows
      with anxiety, capturing stress-driven miscrediting.
    - Stage-2 "spillover" generalization: the reward also partially updates the same-index
      alien in the unvisited planet; spillover increases with anxiety.
    - Stage-1 policy includes a baseline bias toward spaceship A.
    - Stage-1 choice value is a mixture of MB (known transitions) and MF with weight
      w = 1 - stai (more MF under higher anxiety).
    
    Parameters (bounds):
    - alpha: [0,1] learning rate for all TD updates
    - spill: [0,1] baseline fraction of reward that generalizes to the other state's same action
    - rare_bias: [0,1] baseline fraction of TD signal credited to the unchosen first-stage action after rare transitions
    - beta: [0,10] inverse temperature for both stages
    - biasA: [0,1] additive bias toward choosing spaceship A at stage 1
    
    Inputs:
    - action_1, state, action_2, reward: arrays of length T
    - stai: array-like with one element in [0,1]
    - model_parameters: [alpha, spill, rare_bias, beta, biasA]
    
    Returns:
    - Negative log-likelihood of observed choices (stage 1 + stage 2).
    """
    alpha, spill, rare_bias, beta, biasA = model_parameters
    n_trials = len(action_1)
    stai_score = float(stai[0])

    # Known transitions (A->X common, U->Y common)
    T_known = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Arbitration weight
    w = np.clip(1.0 - stai_score, 0.0, 1.0)

    # Anxiety-scaled effects
    spill_eff = spill * (0.5 + 0.5 * stai_score)
    rare_eff = rare_bias * (0.5 + 0.5 * stai_score)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB values for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Add explicit bias toward spaceship A (action index 0)
        logits1 = q1.copy()
        logits1[0] += biasA

        # Stage-1 softmax
        l1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(l1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = q2[s].copy()
        l2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(l2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Determine if the transition was common or rare
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Spillover: update same-index action in the other state
        other_s = 1 - s
        q2[other_s, a2] += alpha * spill_eff * (r - q2[other_s, a2])

        # Stage-1 MF backup from obtained second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Transition-dependent miscrediting to unchosen first-stage action after rare transitions
        if not is_common:
            a1_other = 1 - a1
            # Use the same TD target magnitude but attenuated
            q1_mf[a1_other] += alpha * rare_eff * (q2[s, a2] - q1_mf[a1_other])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll