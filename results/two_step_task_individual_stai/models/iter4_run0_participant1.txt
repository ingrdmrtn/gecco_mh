def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF RL with anxiety-gated model-based control and eligibility trace to stage 1.
    
    This model learns second-stage action values and forms a model-based (MB) plan at stage 1
    using a fixed transition structure (common = 0.7). It also learns a model-free (MF) value 
    for stage 1 via an eligibility trace. Anxiety (stai) modulates the weight placed on MB versus MF
    control: higher anxiety reduces the effective MB weight (more MF-driven).
    
    Parameters (all in [0,1] except beta in [0,10]):
    - alpha2: learning rate for second-stage values Q2 (0..1).
    - beta: inverse temperature for both stages (0..10).
    - theta_mb: baseline MB control level, transformed via sigmoid to a weight in [0,1] (0..1).
    - xi_anx: strength by which anxiety shifts MB weight toward MF; MB weight w = sigmoid(theta_mb + xi_anx*(0.5 - stai)) (0..1).
    - lambda_: eligibility trace strength for updating stage-1 MF values from outcome (0..1).
    - kappa1: first-stage perseveration (stickiness to repeat last spaceship) (0..1).
    - epsilon: lapse rate applied to both stages' choice probabilities (0..1).
    
    Args:
        action_1: 1D array of first-stage choices (0 or 1).
        state: 1D array of second-stage states encountered (0 or 1).
        action_2: 1D array of second-stage choices (0 or 1), conditional on the state.
        reward: 1D array of rewards (typically 0 or 1).
        stai: 1D array with one element in [0,1]: anxiety score for this participant.
        model_parameters: tuple/list in the order (alpha2, beta, theta_mb, xi_anx, lambda_, kappa1, epsilon).
    
    Returns:
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, theta_mb, xi_anx, lambda_, kappa1, epsilon = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    Q2 = np.zeros((2, 2))   # state x action
    Q1_mf = np.zeros(2)     # model-free at stage 1

    p1 = np.zeros(n)
    p2 = np.zeros(n)

    prev_a1 = -1
    eps = 1e-12

    # MB weight as a function of anxiety
    # Map theta_mb in [0,1] to centered space via logit-like shift using a symmetric sigmoid around 0.5
    # w = sigmoid( logit(theta_mb) + xi_anx*(0.5 - stai) ), approximated by simple sigmoid around 0.5.
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    # Use centered parameterization to ensure full [0,1] coverage with parameters in [0,1].
    # Center theta_mb around 0 via (theta_mb - 0.5)*4 to allow a wide range, then add anxiety term.
    base = (theta_mb - 0.5) * 4.0
    anx_shift = xi_anx * (0.5 - stai) * 4.0
    w_mb = sigmoid(base + anx_shift)  # fixed across trials but anxiety-dependent

    for t in range(n):
        # Stage-1 MB action values from Q2 via transition model
        max_q2 = np.max(Q2, axis=1)  # per state
        Q1_mb = T @ max_q2

        # Combine MB and MF with anxiety-gated weight; add first-stage stickiness
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # Choice probabilities stage 1
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Stage-2 choice in encountered state
        s = int(state[t])
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = reward[t]

        # Learning stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha2 * pe2

        # Eligibility-trace update to stage-1 MF value from the outcome
        # Drives MF value toward realized return with strength lambda_.
        Q1_mf[a1] += alpha2 * lambda_ * (r - Q1_mf[a1])

        prev_a1 = a1

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MB planning at stage 1 with anxiety-amplified UCB exploration at stage 2 and stickiness.
    
    The model computes stage-1 action values via a fixed transition model and the max second-stage values.
    At stage 2, choices are driven by a softmax over Q2 plus an uncertainty bonus (UCB-like) that
    decays with visit count. Anxiety increases the exploration bonus, encouraging sampling under uncertainty.
    A second-stage perseveration bias is included. Lapse affects both stages.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage Q2 (0..1).
    - beta: inverse temperature for both stages (0..10).
    - b0: baseline UCB exploration bonus scale (0..1).
    - zeta_anx: anxiety amplification of exploration, b_eff = b0 * (1 + zeta_anx * stai) (0..1).
    - kappa2: second-stage perseveration (repeat same alien within a state) (0..1).
    - epsilon: lapse rate (0..1).
    
    Args:
        action_1: 1D array of first-stage choices (0/1).
        state: 1D array of encountered second-stage states (0/1).
        action_2: 1D array of second-stage actions (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: anxiety score.
        model_parameters: tuple/list in order (alphaR, beta, b0, zeta_anx, kappa2, epsilon).
    
    Returns:
        Negative log-likelihood of observed choices at both stages.
    """
    alphaR, beta, b0, zeta_anx, kappa2, epsilon = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    # Visit counts for UCB at stage 2
    N = np.zeros((2, 2))  # counts per state-action
    prev_a2 = np.array([-1, -1])  # last action per state

    # Effective exploration bonus scale with anxiety
    b_eff = b0 * (1.0 + zeta_anx * stai)

    eps = 1e-12

    for t in range(n):
        # Stage-1 MB values from current Q2
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        logits1 = beta * Q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = int(action_1[t])
        p1[t] = probs1[a1]

        # Stage-2 with UCB and perseveration
        s = int(state[t])
        bonus = b_eff / np.sqrt(N[s] + 1.0)  # diminishing uncertainty bonus
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta * Q2[s] + bonus + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = reward[t]

        # Learning stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Update counters and history
        N[s, a2] += 1.0
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-gated rare-transition sensitivity and counterfactual update.
    
    The agent learns both transition probabilities and second-stage rewards.
    At stage 1, it blends model-based (MB) values (from learned transitions) with model-free (MF) values.
    The MB weight increases after rarer-than-expected transitions, and this rare-transition sensitivity
    is amplified by anxiety. It also performs counterfactual updating at stage 2 on the unchosen alien,
    scaled by anxiety. First-stage perseveration and a lapse are included.
    
    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage Q2 (0..1).
    - alphaT: learning rate for transition probabilities (row-wise exponential averaging) (0..1).
    - beta: inverse temperature for both stages (0..10).
    - psi_rare: base sensitivity of MB weight to rarity (0..1).
    - chi_anx: scales both (a) amplification of rarity sensitivity by anxiety and 
               (b) counterfactual learning rate at stage 2 (0..1).
    - kappa1: first-stage perseveration (0..1).
    - epsilon: lapse rate applied to both stages (0..1).
    
    Args:
        action_1: 1D array of first-stage choices (0/1).
        state: 1D array of second-stage states encountered (0/1).
        action_2: 1D array of second-stage choices (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: anxiety score.
        model_parameters: tuple/list in order (alphaR, alphaT, beta, psi_rare, chi_anx, kappa1, epsilon).
    
    Returns:
        Negative log-likelihood of the observed choices across both stages.
    """
    alphaR, alphaT, beta, psi_rare, chi_anx, kappa1, epsilon = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Initialize transitions near the canonical structure, but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n)
    p2 = np.zeros(n)

    prev_a1 = -1
    eps = 1e-12

    for t in range(n):
        # Stage-1 MB values from learned transitions
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Compute rarity of the realized transition under current T BEFORE updating it
        a1 = int(action_1[t])
        s = int(state[t])
        p_trans = T[a1, s]  # probability assigned to the realized state
        # Rarity signal: positive when rarer-than-0.5, negative when more common-than-0.5
        rarity_signal = 0.5 - p_trans
        # Anxiety-amplified MB weight adjustment; map to [0,1] with clipping
        w_mb = 0.5 + psi_rare * rarity_signal * (1.0 + chi_anx * stai)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Combine MB and MF with first-stage perseveration bias
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = int(action_2[t])
        p2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 learning (chosen action)
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Counterfactual update on unchosen action, scaled by anxiety via chi_anx
        a2_cf = 1 - a2
        cf_rate = alphaR * (chi_anx * stai)
        if cf_rate > 0.0:
            pe2_cf = r - Q2[s, a2_cf]
            Q2[s, a2_cf] += cf_rate * pe2_cf

        # Stage-1 MF update using outcome as a teaching signal (simple delta toward reward)
        Q1_mf[a1] += alphaR * (r - Q1_mf[a1])

        # Transition learning: move chosen row toward the observed state one-hot, then renormalize
        oh = np.array([1.0 if j == s else 0.0 for j in range(2)])
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * oh
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        prev_a1 = a1

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll