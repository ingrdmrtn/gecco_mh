def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated model-based weight, perceived transition, and novelty bonus.
    
    This model blends model-based (MB) and model-free (MF) control at stage 1. Anxiety (stai) shifts:
      1) the arbitration weight toward/away from MB control,
      2) the agent's perceived transition structure (more anxious => perceives more rare transitions),
      3) the novelty/exploration bonus at stage 2 (more anxious => reduced novelty seeking).
    
    Parameters
    ----------
    action_1 : array-like of int in {0, 1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0, 1}
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int in {0, 1}
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float in {0, 1}
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1, used to modulate MB weight, perceived transitions, and novelty.
        Interpretation in this task:
          low < 0.31, medium in [0.31, 0.51], high > 0.51.
    model_parameters : list or array
        [alpha, beta, omega0, xi_stai, b0]
        Bounds:
          alpha   in [0, 1]   : learning rate for MF updates
          beta    in [0, 10]  : inverse-temperature for both stages
          omega0  in [0, 1]   : baseline MB arbitration weight
          xi_stai in [0, 1]   : sensitivity of parameters to stai (modulates MB weight, perceived transitions, novelty)
          b0      in [0, 1]   : baseline novelty bonus magnitude
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, omega0, xi_stai, b0 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Model-free values
    q1_mf = np.zeros(2)          # stage-1 MF action values for spaceships A/U
    q2_mf = np.zeros((2, 2))     # stage-2 MF action values per planet/alien

    # Novelty tracking (visit counts for each alien per planet)
    visits = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated perceived transition matrix:
    # Base common prob = 0.7. Anxious agents perceive more rare transitions (lower common probability).
    delta_common = 0.2 * xi_stai * (stai - 0.5)  # stai>0.5 => reduce common prob; stai<0.5 => increase
    p_common_eff = np.clip(0.7 - delta_common, 0.5, 0.9)
    T = np.array([[p_common_eff, 1.0 - p_common_eff],
                  [1.0 - p_common_eff, p_common_eff]])

    # Anxiety-modulated MB weight and novelty
    w_mb = np.clip(omega0 + xi_stai * (stai - 0.5), 0.0, 1.0)
    bonus_eff = max(0.0, b0 * (1.0 - xi_stai * stai))  # more anxiety => smaller novelty bonus

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Novelty-augmented second-stage values for planning
        bonus_state = np.zeros(2)
        for a in range(2):
            bonus_state[a] = bonus_eff / np.sqrt(1.0 + visits[s, a])
        q2_aug = q2_mf + bonus_eff / np.sqrt(1.0 + visits)  # used for MB backup across states

        # Model-based computation for stage 1 using max over augmented q2 per next state
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy uses novelty-augmented values in the visited state
        q2s_aug = q2_mf[s].copy() + bonus_state
        q2c = q2s_aug - np.max(q2s_aug)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Update counts and MF values
        visits[s, a2] += 1.0

        # Stage-2 MF update
        pe2 = reward[t] - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage-1 MF backup from realized second-stage action value (no extra lambda parameter)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-based control with anxiety-modulated risk aversion.
    
    The agent learns both the mean and uncertainty (variance) of each alien's payout.
    Choices are based on a risk-sensitive utility: U = mean - gamma * stdev.
    Anxiety increases risk aversion (gamma) and thus promotes avoiding uncertain aliens.
    Stage 1 uses pure model-based planning over risk-sensitive second-stage values.
    
    Parameters
    ----------
    action_1 : array-like of int in {0, 1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0, 1}
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int in {0, 1}
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float in {0, 1}
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1, used to modulate risk aversion.
    model_parameters : list or array
        [alpha, beta, gamma0, phi_stai]
        Bounds:
          alpha    in [0, 1]   : learning rate for mean/variance updates
          beta     in [0, 10]  : inverse temperature for both stages
          gamma0   in [0, 1]   : baseline risk-aversion coefficient
          phi_stai in [0, 1]   : scaling of risk aversion with stai
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, gamma0, phi_stai = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition matrix (true/task-level)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Track mean and variance per alien per planet
    m = np.zeros((2, 2)) + 0.5    # initialize around chance
    v = np.zeros((2, 2)) + 0.25   # initial uncertainty (stdev ~ 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated risk aversion
    gamma = np.clip(gamma0 * (1.0 + phi_stai * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Risk-sensitive second-stage utilities
        sd = np.sqrt(np.maximum(v, 1e-12))
        u2 = m - gamma * sd

        # Model-based stage-1 values from risk-sensitive utilities
        max_u2 = np.max(u2, axis=1)
        q1_mb = T @ max_u2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        u2s = u2[s].copy()
        u2c = u2s - np.max(u2s)
        probs_2 = np.exp(beta * u2c) / np.sum(np.exp(beta * u2c))
        p_choice_2[t] = probs_2[a2]

        # Updates: mean and variance (incremental, unbiased for bounded alpha)
        err = reward[t] - m[s, a2]
        m[s, a2] += alpha * err
        # Update variance toward squared error (online variance tracking)
        v[s, a2] += alpha * ((err ** 2) - v[s, a2])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Confidence-weighted arbitration between MB and MF with anxiety-modulated bias and stage-2 perseveration.
    
    This model learns model-free values at both stages and computes model-based values via the transition model.
    An arbitration weight w_t favors the controller (MB vs MF) with higher confidence (lower recent surprise).
    Anxiety shifts arbitration toward model-free control and increases stage-2 perseveration.
    
    Confidence proxies:
      - MF surprise: moving average of absolute stage-2 prediction error |PE2|.
      - MB surprise: moving average of transition surprise (rare=1, common=0).
    Larger surprise => lower confidence.
    
    Parameters
    ----------
    action_1 : array-like of int in {0, 1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0, 1}
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int in {0, 1}
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float in {0, 1}
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1, used to bias arbitration toward MF and to scale perseveration.
    model_parameters : list or array
        [alpha, beta, arb0, z_stai, kappa_ps]
        Bounds:
          alpha    in [0, 1]   : learning rate (both stages)
          beta     in [0, 10]  : inverse temperature for both stages
          arb0     in [0, 1]   : baseline arbitration bias for MB (higher => more MB)
          z_stai   in [0, 1]   : strength of stai impact on arbitration (higher stai => more MF)
          kappa_ps in [0, 1]   : baseline stage-2 perseveration (stickiness) magnitude
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, arb0, z_stai, kappa_ps = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition matrix (task)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Confidence trackers (exponentially weighted moving averages)
    mf_surprise = 0.5  # initialize mid-range
    mb_surprise = 0.3
    conf_alpha = 0.2   # smoothing for confidence signals (fixed, not a free parameter)

    # Likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration at stage 2
    prev_a2 = None
    # Anxiety increases perseveration
    kappa_eff = kappa_ps * (1.0 + z_stai * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based Q for stage 1 from MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight based on confidence
        # Higher surprise => lower confidence; map to confidence in [0,1]
        conf_mf = 1.0 - np.clip(mf_surprise, 0.0, 1.0)
        conf_mb = 1.0 - np.clip(mb_surprise, 0.0, 1.0)
        # Baseline MB bias (arb0), anxiety tilts away from MB toward MF as stai increases
        arb_bias = np.clip(arb0 - z_stai * stai, 0.0, 1.0)
        # Combine: normalized MB weight
        if (conf_mb + conf_mf) > 1e-8:
            w_mb = np.clip(arb_bias * (conf_mb / (conf_mb + conf_mf)), 0.0, 1.0)
        else:
            w_mb = arb_bias  # fallback

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with perseveration
        q2s = q2_mf[s].copy()
        if prev_a2 is not None:
            stick = np.zeros(2)
            stick[prev_a2] = 1.0
            q2s = q2s + kappa_eff * stick
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        # Stage-2 MF update
        pe2 = reward[t] - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Stage-1 MF update with modest eligibility (fixed lambda=0.5)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1 * 0.5

        # Update confidence trackers
        mf_surprise = (1.0 - conf_alpha) * mf_surprise + conf_alpha * np.abs(pe2)

        # Transition surprise: 0 if common, 1 if rare
        # Identify whether transition was common for the chosen a1
        common_for_a1 = (s == 0 and a1 == 0) or (s == 1 and a1 == 1)  # A->X, U->Y are common
        trans_surprise = 0.0 if common_for_a1 else 1.0
        mb_surprise = (1.0 - conf_alpha) * mb_surprise + conf_alpha * trans_surprise

        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll