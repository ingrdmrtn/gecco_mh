def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-gated rare-transition learning boost.
    
    This model combines model-based (MB) and model-free (MF) values at stage 1,
    uses MF Q-learning at stage 2, and modulates the learning rate specifically
    after rare transitions. Higher anxiety amplifies learning from rare events,
    capturing heightened sensitivity to surprising outcomes.
    
    Parameters (model_parameters):
    - alpha: base learning rate for Q-updates (stage 2 and TD backup to stage 1), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - w_h: hybrid weight for MB at stage 1 (0: pure MF, 1: pure MB), in [0,1]
    - k_rare: multiplicative boost applied to alpha on rare transitions (as factor-1), in [0,1]
              Effective alpha on rare trials â‰ˆ alpha * (1 + k_rare * anx_gain)
    - g_anx: anxiety gain scaling the rare-transition boost by stai, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (typically 0/1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, w_h, k_rare, g_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure for two-step task
    T = np.array([[0.7, 0.3],  # A -> X (common), Y (rare)
                  [0.3, 0.7]]) # U -> X (rare),   Y (common)

    # Value structures
    q2 = np.zeros((2, 2))   # Stage-2 values per planet and alien
    q1_mf = np.zeros(2)     # Stage-1 model-free cached values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated multiplier applied only on rare transitions
    rare_boost_base = np.clip(1.0 + (k_rare * g_anx * stai), 0.0, 2.0)

    for t in range(n_trials):
        # Model-based estimate for stage 1 from current q2
        max_q2 = np.max(q2, axis=1)          # best alien per planet
        q1_mb = T @ max_q2                   # expected value per spaceship

        # Hybrid stage-1 action values
        q1 = w_h * q1_mb + (1.0 - w_h) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (based on reached planet)
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Determine whether the observed transition was common or rare
        is_rare = 1 if T[a1, s] < 0.5 else 0
        # Effective learning rate: boosted on rare transitions, scaled by anxiety
        alpha_eff = alpha * (rare_boost_base if is_rare == 1 else 1.0)
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Stage-1 MF TD backup toward immediate post-transition value (without extra params)
        # Use the chosen alien's updated Q as target for the chosen spaceship
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MB planning with probability weighting and anxiety-amplified stage-2 choice kernel.
    
    Stage 1: Pure model-based evaluation using the fixed transition, but applies a
    Prelec-like probability weighting to the second-stage values, controlled by rho.
    This captures distortions in belief about how valuable planets are (risk/weighting).
    
    Stage 2: MF Q-learning plus a state-dependent choice kernel (tendency to repeat the
    last alien chosen on that planet). Anxiety increases the kernel's influence, modeling
    heightened habitization at the second stage under stress.
    
    Parameters (model_parameters):
    - alpha: learning rate for MF Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - rho: probability-weighting curvature (lower -> more overweighting of extremes), in [0,1]
    - k2_base: base strength of stage-2 choice kernel bias added to logits, in [0,1]
    - g_k2: anxiety gain scaling the kernel strength by stai, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, rho, k2_base, g_k2 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))             # stage-2 MF values
    last_a2 = np.array([-1, -1])      # last chosen alien per planet (init none)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kernel strength modulated by anxiety
    kernel_strength = k2_base * np.clip(1.0 + g_k2 * (stai - 0.31), 0.0, 2.0)

    # Probability weighting function operating on value-as-probability in [0,1]
    def weight_prob(p, rho_param):
        # Smoothly handles edges; rho in (0,1] -> more curvature for smaller rho
        p = np.clip(p, 1e-6, 1.0 - 1e-6)
        # Prelec-like simple form via power distortion of log-odds
        # w(p) = p^rho / [p^rho + (1-p)^rho]^(1/rho)  (bounded in [0,1])
        pr = p**rho_param
        qr = (1.0 - p)**rho_param
        denom = (pr + qr)**(1.0 / max(rho_param, 1e-6))
        return np.clip(pr / denom, 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 MB values derived from current q2 via max per planet
        max_q2 = np.max(q2, axis=1)  # interpret as estimated reward probabilities
        # Apply probability weighting to distort perceived planet values
        max_q2_w = weight_prob(max_q2, rho)
        q1_mb = T @ max_q2_w

        # Stage-1 policy (pure MB with probability weighting)
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: MF values plus choice kernel bias at the reached planet
        s = state[t]
        bias2 = np.zeros(2)
        if last_a2[s] != -1:
            bias2[last_a2[s]] = kernel_strength  # add to logit of previously chosen alien on this planet

        logits2 = beta * q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update choice kernel memory
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-weighted MB/MF arbitration with learned transitions and anxiety effect.
    
    This model learns the transition matrix online via decayed Dirichlet counts and
    arbitrates between MB and MF control at stage 1 based on current transition
    uncertainty and anxiety. Higher uncertainty reduces MB reliance; higher anxiety
    further tilts arbitration away from MB (or modulates sensitivity to uncertainty).
    Stage 2 is MF Q-learning. Both stages contribute to likelihood.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - alpha_t: transition learning/forgetting rate for Dirichlet counts and uncertainty trace, in [0,1]
    - theta0: baseline arbitration bias toward MB (mapped to logit space), in [0,1]
    - g_s: anxiety gain that increases sensitivity of arbitration to anxiety/uncertainty, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (0/1)
    - stai: array-like with single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, beta, alpha_t, theta0, g_s = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Dirichlet counts for transitions; start uninformative and symmetric
    C = np.ones((2, 2))  # rows: spaceships, cols: planets

    # Values
    q2 = np.zeros((2, 2))  # stage-2 MF values
    q1_mf = np.zeros(2)    # stage-1 MF cached values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Running uncertainty estimate (higher when transitions are surprising)
    u = 0.5

    # Helper: convert counts to transition probabilities
    def counts_to_T(counts):
        T = counts / (np.sum(counts, axis=1, keepdims=True) + 1e-12)
        return np.clip(T, 1e-6, 1.0)

    T = counts_to_T(C)

    for t in range(n_trials):
        # Compute MB values from learned T and current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight w_t = sigmoid(z); higher uncertainty -> lower w_t (less MB)
        # Map theta0 in [0,1] to centered logit bias ~ [-2, +2]
        z0 = (theta0 - 0.5) * 4.0
        # Anxiety term: higher anxiety reduces MB reliance (negative contribution)
        z_anx = -4.0 * g_s * (stai - 0.31)
        # Uncertainty term: higher u decreases MB reliance
        z_unc = -4.0 * (u - 0.5)
        z = z0 + z_anx + z_unc
        w_t = 1.0 / (1.0 + np.exp(-z))
        w_t = np.clip(w_t, 0.0, 1.0)

        # Stage-1 combined values
        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Stage-1 MF TD backup (toward current post-transition value)
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * td1

        # Transition learning with decayed Dirichlet counts
        # Forgetting: shrink counts toward uniform before adding new evidence
        C[a1] = (1.0 - alpha_t) * C[a1] + alpha_t * (np.ones(2) * 0.5 * np.sum(C[a1]))
        # Add one-hot evidence for observed planet
        ev = np.array([0.0, 0.0])
        ev[s] = 1.0
        C[a1] += alpha_t * ev
        # Recompute T
        T_prev = T.copy()
        T = counts_to_T(C)

        # Update uncertainty trace based on transition surprise relative to prior T
        surprise = 1.0 - np.clip(T_prev[a1, s], 0.0, 1.0)
        u = (1.0 - alpha_t) * u + alpha_t * surprise

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll