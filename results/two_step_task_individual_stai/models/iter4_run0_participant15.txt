def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated MB/MF hybrid with anxiety-modulated arbitration and MF eligibility.
    
    Mechanism:
    - Stage-2 MF values (Q2) are learned with a standard delta-rule.
    - Stage-1 uses a hybrid of MB and MF valuations: Q1 = omega_t * Q1_MB + (1 - omega_t) * Q1_MF.
    - Arbitration weight omega_t is a dynamic sigmoid of:
        (i) a base mixing weight (omega0 in [0,1]),
        (ii) running state-action uncertainty (mean squared PE at stage-2),
        (iii) participant anxiety (stai).
      Higher uncertainty and higher anxiety shift the system toward model-based control (or away, depending on chi sign).
    - MF eligibility at Stage-1: Q1_MF is updated toward the value experienced at Stage-2 with strength alpha*e.
    
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for Q2 and (reused) for uncertainty tracking; also scales eligibility.
    - beta: [0,10] Inverse temperature for both stages.
    - omega0: [0,1] Base model-based mixing weight.
    - e: [0,1] Eligibility strength for propagating Stage-2 value to Stage-1 MF.
    - chi: [0,1] Sensitivity of arbitration to uncertainty and anxiety (scaled within function).
    
    Anxiety use:
    - Arbitration logit is shifted by (2*stai - 1) * chi_scaled, biasing toward/away from MB control.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1.
    - state: array of ints in {0,1}, observed planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at Stage-2 within observed state.
    - reward: array of floats (e.g., 0 or 1), coins received.
    - stai: array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, omega0, e, chi]
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, omega0, e, chi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    q2 = 0.5 * np.ones((2, 2))     # Stage-2 MF Q-values
    q1_mf = np.zeros(2)            # Stage-1 MF Q-values

    # Uncertainty tracker: running mean squared PE for each state-action
    u2 = np.zeros((2, 2))

    # Helper: stable sigmoid and logit
    eps = 1e-8
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    base_logit = np.log(omega0 + eps) - np.log(1.0 - omega0 + eps)
    # Scale chi for arbitration magnitude
    chi_scale = 4.0 * chi  # maps [0,1] to [0,4] effect size

    for t in range(n_trials):
        s = state[t]

        # Model-based evaluation at Stage-1 from current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Dynamic arbitration weight omega_t
        mean_unc = np.mean(u2)  # scalar uncertainty summary
        # Center uncertainty around ~0.25 (typical for Bernoulli rewards in [0,1])
        centered_unc = mean_unc - 0.25
        anx_term = (2.0 * stai - 1.0)  # in [-1,1]
        omega_logit = base_logit + chi_scale * (centered_unc + 0.5 * anx_term)
        omega_t = sigmoid(omega_logit)

        # Hybrid Stage-1 values
        q1 = omega_t * mb_q1 + (1.0 - omega_t) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in observed state
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty with same alpha (reuses alpha parameter)
        u2[s, a2] = (1.0 - alpha) * u2[s, a2] + alpha * (pe2 * pe2)

        # Stage-1 MF eligibility update toward experienced Stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * e * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration at Stage-2 with anxiety-dampened novelty bonus, MB planning at Stage-1.
    
    Mechanism:
    - Stage-2 MF values (Q2) learned with delta-rule.
    - A directed exploration (novelty) bonus is added to Stage-2 values: bonus = nu_eff / sqrt(N[s,a] + 1),
      where N is a decaying visit count. This increases choice probability for less-tried aliens.
    - Stage-1 uses MB planning from bonus-augmented Stage-2 values.
    
    Parameters (all used; total=5):
    - alpha: [0,1] Learning rate for Q2 updates.
    - beta: [0,10] Inverse temperature for both stages.
    - nu: [0,1] Base strength of directed exploration bonus.
    - sigma: [0,1] Decay rate for visit counts per trial (higher = faster forgetting of visits).
    - psi: [0,1] Anxiety coupling: effective bonus nu_eff = nu * (1 - psi * stai).
    
    Anxiety use:
    - Higher STAI reduces directed exploration via nu_eff = nu * (1 - psi * stai).
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1.
    - state: array of ints in {0,1}, observed planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at Stage-2 within observed state.
    - reward: array of floats (e.g., 0 or 1), coins received.
    - stai: array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [alpha, beta, nu, sigma, psi]
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha, beta, nu, sigma, psi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))
    # Decaying visit counts for directed exploration
    N = np.zeros((2, 2))

    nu_eff = nu * (1.0 - psi * stai)
    # Ensure non-negativity
    nu_eff = max(0.0, nu_eff)

    for t in range(n_trials):
        s = state[t]

        # Compute exploration bonuses for current state
        bonus_state = nu_eff / np.sqrt(N[s] + 1.0)

        # Stage-1 model-based planning uses bonus-augmented Q2
        mb_q1 = transition_matrix @ np.max(q2 + np.vstack([bonus_state, bonus_state]), axis=1)

        # Stage-1 policy
        z1 = beta * (mb_q1 - np.max(mb_q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with bonus in the observed state
        q2b = q2[s] + bonus_state
        z2 = beta * (q2b - np.max(q2b))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update Q2
        pe = r - q2[s, a2]
        q2[s, a2] += alpha * pe

        # Decay visit counts globally for the visited state, then increment chosen
        N[s, :] = (1.0 - sigma) * N[s, :]
        N[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning-rate via learned volatility with anxiety-scaling and temperature dampening.
    
    Mechanism:
    - Tracks a local volatility estimate per state-action from absolute prediction errors.
    - Trial-wise learning rate alpha_t increases with estimated volatility (faster adaptation).
    - Anxiety increases effective volatility and reduces choice temperature (more noise).
    - Stage-1 values are model-based from current Stage-2 values.
    
    Parameters (all used; total=5):
    - a0: [0,1] Base learning-rate floor; when volatility is low, alpha_t ~ a0.
    - beta: [0,10] Baseline inverse temperature.
    - vol: [0,1] Initial volatility per state-action.
    - eta: [0,1] Update rate for volatility from absolute PE.
    - tmult: [0,1] Anxiety scaling of temperature: beta_eff = beta * (1 - tmult * stai).
    
    Anxiety use:
    - Volatility inflation: vol_eff = vol_est * (0.5 + 0.5 * stai) increases learning rate under higher anxiety.
    - Temperature dampening: beta_eff decreases linearly with stai via tmult.
    
    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship at Stage-1.
    - state: array of ints in {0,1}, observed planet (X=0, Y=1).
    - action_2: array of ints in {0,1}, chosen alien at Stage-2 within observed state.
    - reward: array of floats (e.g., 0 or 1), coins received.
    - stai: array-like with one float in [0,1], participant anxiety score.
    - model_parameters: [a0, beta, vol, eta, tmult]
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    a0, beta, vol, eta, tmult = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))
    vol_est = vol * np.ones((2, 2))  # local volatility estimate per state-action

    # Anxiety-dampened temperature
    beta_eff = beta * (1.0 - tmult * stai)
    beta_eff = max(0.0, beta_eff)  # ensure non-negative

    for t in range(n_trials):
        s = state[t]

        # Stage-1 model-based using current Q2
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Stage-1 policy
        z1 = beta_eff * (mb_q1 - np.max(mb_q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # PE and volatility update
        pe = r - q2[s, a2]
        vol_est[s, a2] = (1.0 - eta) * vol_est[s, a2] + eta * abs(pe)

        # Anxiety-inflated volatility and adaptive learning rate
        vol_eff = vol_est[s, a2] * (0.5 + 0.5 * stai)
        vol_eff = min(1.0, max(0.0, vol_eff))
        alpha_t = a0 * (1.0 - vol_eff) + vol_eff  # interpolates between a0 and 1 with volatility
        alpha_t = min(1.0, max(0.0, alpha_t))

        # Q2 update with adaptive alpha
        q2[s, a2] += alpha_t * pe

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll