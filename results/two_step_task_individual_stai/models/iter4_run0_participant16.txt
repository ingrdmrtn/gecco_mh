def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-weighted MB/MF arbitration with anxiety-dampened planning.
    
    Idea
    ----
    A hybrid controller at stage 1 blends model-based (MB) and model-free (MF) action values.
    The MB weight increases with recent outcome uncertainty (|PE|), but anxiety (stai) attenuates
    the shift toward MB planning. Stage 2 uses MF learning. Both stages use softmax choice.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached planet (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices (aliens).
    reward : array-like of float in [0,1]
        Received coins (can be fractional).
    stai : array-like with a single float in [0,1]
        Anxiety score; higher dampens the arbitration shift toward MB control.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for MF values (both stages).
        - beta1 in [0,10]: inverse temperature for stage-1 choices.
        - beta2 in [0,10]: inverse temperature for stage-2 choices.
        - w0_base in [0,1]: baseline MB weight at stage 1.
        - chi in [0,1]: strength of uncertainty-driven arbitration; anxiety scales this downward.
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha, beta1, beta2, w0_base, chi = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))     # stage-2 MF Q(s, a)
    q1_mf = np.zeros(2)       # stage-1 MF Q(a)
    p_choice_1 = np.zeros(n)
    p_choice_2 = np.zeros(n)

    # Running estimate of outcome uncertainty via unsigned PE EMA
    # Start from moderate uncertainty
    unc = 0.25
    lam_unc = 0.9  # decay for uncertainty trace (fixed within [0,1])

    # Anxiety-dampened arbitration gain
    # Higher stai -> smaller effective chi
    chi_eff = chi * (1.0 - 0.8 * stai)

    for t in range(n):
        # Model-based stage-1 value = expected max Q at second stage under transitions
        max_q2 = np.max(q2, axis=1)     # value of states X,Y by their best alien
        q1_mb = T @ max_q2

        # Arbitration weight increases with uncertainty (unc in [0,1] approx),
        # but anxiety damps it toward baseline w0_base.
        # Map to [0,1] via convex combination in logit space for flexibility:
        def logit(p):
            p = np.clip(p, 1e-6, 1 - 1e-6)
            return np.log(p) - np.log(1 - p)
        def inv_logit(z):
            return 1 / (1 + np.exp(-z))

        w_logit = logit(w0_base) + chi_eff * (unc - 0.25) * 4.0  # centered at 0.25, scaled
        w = float(inv_logit(w_logit))
        w = np.clip(w, 0.0, 1.0)

        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage 1 softmax
        logits1 = beta1 * q1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax conditioned on reached state
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning
        # Stage-2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty trace from absolute PE (bounded proxy for local volatility)
        unc = lam_unc * unc + (1 - lam_unc) * np.clip(abs(pe2), 0.0, 1.0)

        # Stage-1 MF bootstraps from obtained stage-2 value (SARSA-style)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-tuned transition-learning model with stage-2 perseveration.
    
    Idea
    ----
    The agent learns the transition matrix online (which spaceship leads to which planet).
    Anxiety (stai) increases the transition learning rate (hypervigilance) and slightly
    reduces choice determinism. Stage-2 choices also exhibit perseveration (stickiness).
    Stage-1 is purely model-based using the learned transitions; Stage-2 is MF.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached planet (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received coins.
    stai : array-like with a single float in [0,1]
        Anxiety score. Higher -> faster transition learning, slightly lower beta.
    model_parameters : iterable of 5 floats
        - alpha_r in [0,1]: stage-2 reward learning rate.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - alpha_T in [0,1]: baseline transition learning rate.
        - kappa2 in [0,1]: stage-2 perseveration weight.
        - zeta in [0,1]: anxiety sensitivity controlling how much stai speeds transition learning and reduces beta.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, alpha_T, kappa2, zeta = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Initialize learned transitions as uniform
    # T[a, s] = P(state=s | first-action=a), rows each sum to 1
    T = np.ones((2, 2)) * 0.5

    # MF values at stage 2
    q2 = np.zeros((2, 2))

    # Logs
    p_choice_1 = np.zeros(n)
    p_choice_2 = np.zeros(n)

    # Effective parameters shaped by anxiety
    # Higher anxiety -> higher transition learning rate, slightly softer policies
    alpha_T_eff = np.clip(alpha_T + zeta * stai * (1.0 - alpha_T), 0.0, 1.0)
    beta_eff = max(0.0, beta * (1.0 - 0.3 * zeta * stai))

    # Stage-2 perseveration trace per state
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n):
        # Stage-1 model-based Q via learned transitions and max second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage-1 softmax with beta_eff
        logits1 = beta_eff * q1_mb
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with state-dependent perseveration
        s = int(state[t])
        logits2 = beta_eff * q2[s].copy()
        if prev_a2[s] is not None:
            # Add perseveration bias toward repeating previous a2 in this state
            logits2[prev_a2[s]] += kappa2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Update transitions from observed (a1 -> s)
        # Simple delta rule toward one-hot observation
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - alpha_T_eff) * T[a1] + alpha_T_eff * oh
        # Ensure numeric stability (renormalize)
        T[a1] = T[a1] / np.sum(T[a1])

        # Update MF stage-2 values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update perseveration trace
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Entropy-regularized exploration with anxiety-driven entropy and lapse rates.
    
    Idea
    ----
    Choices maximize a combination of value and entropy (soft actor). Anxiety (stai)
    increases an entropy bonus (tau) and a value-free lapse rate (epsilon), promoting
    exploration, particularly at stage 2 where immediate uncertainty is higher.
    Stage 1 uses MB planning from fixed transitions blended with a small MF trace.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached planet (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received coins.
    stai : array-like with a single float in [0,1]
        Anxiety score; higher -> more entropy bonus and higher lapse probability.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: MF learning rate for both stages.
        - beta1 in [0,10]: baseline inverse temperature for stage 1.
        - beta2 in [0,10]: baseline inverse temperature for stage 2.
        - tau_base in [0,1]: baseline entropy weight; anxiety increases it.
        - eps_base in [0,1]: baseline lapse probability; anxiety increases it.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, tau_base, eps_base = model_parameters
    n = len(action_1)
    stai = float(stai[0])

    # Fixed transitions (known structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n)
    p_choice_2 = np.zeros(n)

    # Anxiety-shaped parameters
    # Entropy bonus increases with stai; lapse also increases with stai
    tau1 = np.clip(tau_base + 0.6 * stai * (1.0 - tau_base), 0.0, 1.0)
    tau2 = np.clip(tau_base + 0.9 * stai * (1.0 - tau_base), 0.0, 1.0)
    eps1 = np.clip(eps_base + 0.3 * stai * (1.0 - eps_base), 0.0, 1.0)
    eps2 = np.clip(eps_base + 0.6 * stai * (1.0 - eps_base), 0.0, 1.0)

    for t in range(n):
        # Model-based value for stage 1 from max stage-2 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        # Small MF trace blended in to capture habitual tendencies
        w_mf = 0.2 + 0.2 * stai  # more anxiety -> slightly more MF carryover
        q1 = (1 - w_mf) * q1_mb + w_mf * q1_mf

        # Stage 1: entropy-regularized softmax is equivalent to softmax with adjusted logits.
        # We implement entropy-regularized policy by mixing softmax with a uniform prior via tau.
        logits1 = beta1 * q1
        logits1 = logits1 - np.max(logits1)
        soft1 = np.exp(logits1)
        soft1 = soft1 / np.sum(soft1)
        uni = np.array([0.5, 0.5])
        pi1 = (1 - tau1) * soft1 + tau1 * uni

        # Apply value-free lapse: final policy is mixture with uniform
        pi1 = (1 - eps1) * pi1 + eps1 * uni
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage 2: same entropy-regularized approach
        s = int(state[t])
        logits2 = beta2 * q2[s]
        logits2 = logits2 - np.max(logits2)
        soft2 = np.exp(logits2)
        soft2 = soft2 / np.sum(soft2)
        pi2 = (1 - tau2) * soft2 + tau2 * uni
        pi2 = (1 - eps2) * pi2 + eps2 * uni
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        r = float(reward[t])

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF trace bootstraps from current state/action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)