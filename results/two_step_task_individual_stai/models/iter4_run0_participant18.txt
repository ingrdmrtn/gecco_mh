def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus exploration with valence-asymmetric learning and anxiety-modulated stickiness.

    Description:
    - Stage 2 action values (Q2) are learned with separate learning rates for positive/negative prediction errors.
    - An information bonus (U2) encourages exploration toward uncertain options; its strength decreases with anxiety.
    - Stage 1 uses a hybrid of model-based (MB; using fixed transitions) and model-free (MF) values with MB weight w_mb = 1 - stai.
    - Stickiness (perseveration) biases repeating previous actions; its effective strength increases with anxiety.
    - Eligibility propagation from stage 2 to stage 1 is included via a fixed lambda = 0.5.

    Parameters (model_parameters):
    - alpha_pos: learning rate for positive RPEs at stage 2 in [0,1]
    - alpha_neg: learning rate for negative RPEs at stage 2 in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - xi_info: information-bonus scale in [0,1] (effective bonus = xi_info * (1 - stai))
    - tau_stick: stickiness scale in [0,1] (effective stickiness = tau_stick * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha_pos, alpha_neg, beta, xi_info, tau_stick)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_pos, alpha_neg, beta, xi_info, tau_stick = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q1_mf = np.zeros(2)      # stage-1 MF values
    Q2 = np.zeros((2, 2))    # stage-2 values: state x action

    # Uncertainty estimates per state-action for stage 2
    U2 = np.ones((2, 2))     # start uncertain
    decay_u = 0.9            # fixed uncertainty decay

    # Anxiety-modulated components
    w_mb = max(0.0, min(1.0, 1.0 - st))             # MB weight for stage 1
    bonus_scale = xi_info * (1.0 - st)              # exploration bonus diminished by anxiety
    stick_eff = tau_stick * st                       # stickiness stronger with anxiety

    prev_a1 = None
    prev_a2 = [None, None]

    for t in range(n_trials):
        s = int(state[t])

        # Stage-1 model-based values via planning over Q2 (no info bonus in planning)
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid S1
        pref1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        if prev_a1 is not None:
            pref1[prev_a1] += stick_eff

        # Softmax S1
        z1 = pref1 - np.max(pref1)
        probs1 = np.exp(beta * z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 preferences with information bonus and stickiness
        pref2 = Q2[s] + bonus_scale * U2[s]
        if prev_a2[s] is not None:
            pref2[prev_a2[s]] += stick_eff

        z2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD errors
        delta1 = Q2[s, a2] - Q1_mf[a1]
        delta2 = r - Q2[s, a2]

        # MF updates
        Q1_mf[a1] += 0.5 * delta1  # bootstrap with fixed lambda=0.5
        # Valence-asymmetric update at stage 2
        lr = alpha_pos if delta2 >= 0.0 else alpha_neg
        Q2[s, a2] += lr * delta2
        # Eligibility propagation from reward to S1
        Q1_mf[a1] += 0.5 * lr * delta2

        # Uncertainty update: decay and add surprise magnitude
        U2[s, a2] = decay_u * U2[s, a2] + (1.0 - decay_u) * abs(delta2)

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transition model with anxiety-dependent risk sensitivity and arbitration.

    Description:
    - The agent learns the transition matrix T via delta-rule updates (alpha_T).
    - Stage 2 values learn expected utility, where utility has curvature gamma = 1 - k_risk * stai
      (higher anxiety -> more concave utility -> risk aversion for variable payouts).
    - Stage 1 is a hybrid of model-based (using learned T) and model-free values, with MB weight
      w_mb(stai) = (1 - stai)*w_mb0 + stai*(1 - w_mb0) (anxiety shifts arbitration away from baseline).
    - A single inverse temperature beta governs both stages.

    Parameters (model_parameters):
    - alpha_q: learning rate for Q-updates at stage 2 in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - alpha_T: learning rate for transition learning in [0,1]
    - w_mb0: baseline model-based weight in [0,1]
    - k_risk: utility curvature gain in [0,1] (gamma = 1 - k_risk * stai)

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha_q, beta, alpha_T, w_mb0, k_risk)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_q, beta, alpha_T, w_mb0, k_risk = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T (rows sum to 1)
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    w_mb = (1.0 - st) * w_mb0 + st * (1.0 - w_mb0)
    w_mb = max(0.0, min(1.0, w_mb))
    gamma = 1.0 - k_risk * st
    gamma = max(0.0, min(1.0, gamma))

    for t in range(n_trials):
        s = int(state[t])

        # Stage-1 model-based values via learned T
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid preference
        pref1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Softmax S1
        z1 = pref1 - np.max(pref1)
        probs1 = np.exp(beta * z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Softmax S2
        pref2 = Q2[s]
        z2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])
        # Utility transform of reward with anxiety-dependent curvature
        u = r ** gamma

        # TD errors and updates
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * delta1

        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha_q * delta2

        # Transition learning for the chosen action a1 toward observed state s
        # Move the a1-th row toward the one-hot vector of observed state
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - alpha_T) * T[a1] + alpha_T * target
        # Ensure row normalization (should already hold but keep numerically stable)
        row_sum = T[a1].sum()
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-aware arbitration: anxiety reduces MB control after rare transitions.

    Description:
    - Stage 1 choice is a hybrid of MB and MF values. The baseline MB weight is omega_base,
      but after rare transitions it is multiplicatively down-weighted by (1 - k_rare * stai).
    - Rare vs. common is defined by the fixed task structure: A->X and U->Y are common.
    - Stage 2 is MF.
    - Stage 1 MF receives eligibility-trace credit from stage 2 reward with elig weight
      that is larger after common transitions and further reduced after rare transitions
      in proportion to anxiety and k_rare.

    Parameters (model_parameters):
    - alpha: learning rate for value updates in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - omega_base: baseline model-based arbitration weight in [0,1]
    - k_rare: penalty strength for MB after rare transitions in [0,1]
    - lambda_elig: eligibility trace base weight for propagating reward to stage 1 in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, omega_base, k_rare, lambda_elig)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, omega_base, k_rare, lambda_elig = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition matrix for MB planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    for t in range(n_trials):
        s = int(state[t])

        # Compute MB values for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Determine common vs rare transition given chosen a1 and observed s
        a1 = int(action_1[t])
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        rare = 0 if is_common else 1

        # Anxiety-dependent arbitration: reduce MB after rare transitions
        omega_t = omega_base
        if rare == 1:
            omega_t = omega_t * (1.0 - k_rare * st)
        omega_t = max(0.0, min(1.0, omega_t))

        # Hybrid preference for S1
        pref1 = omega_t * Q1_mb + (1.0 - omega_t) * Q1_mf

        # Softmax S1
        z1 = pref1 - np.max(pref1)
        probs1 = np.exp(beta * z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax on MF values
        a2 = int(action_2[t])
        pref2 = Q2[s]
        z2 = pref2 - np.max(pref2)
        probs2 = np.exp(beta * z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # TD errors
        delta1 = Q2[s, a2] - Q1_mf[a1]
        delta2 = r - Q2[s, a2]

        # MF updates
        Q2[s, a2] += alpha * delta2
        Q1_mf[a1] += alpha * delta1

        # Eligibility trace to stage 1 depends on transition type and anxiety
        # After rare transitions, reduce the eligibility by (1 - k_rare * st)
        elig_t = lambda_elig * (1.0 if is_common else (1.0 - k_rare * st))
        elig_t = max(0.0, min(1.0, elig_t))
        Q1_mf[a1] += alpha * elig_t * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll