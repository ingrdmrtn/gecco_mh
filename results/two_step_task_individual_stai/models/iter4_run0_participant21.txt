def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """TD(lambda) model-free control with anxiety-modulated eligibility and lapse.

    Overview:
    - Stage-2 values are learned with standard TD updates.
    - Stage-1 values are updated via SARSA(λ)-style eligibility that propagates the stage-2 TD error.
    - Anxiety increases random lapses (epsilon-greedy mixture) and reduces credit assignment span (λ).

    Parameters (bounds):
    - model_parameters[0] = alpha2 (0 to 1): learning rate for stage-2 action values
    - model_parameters[1] = alpha1 (0 to 1): learning rate for stage-1 values
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = trace0 (0 to 1): baseline eligibility trace strength
    - model_parameters[4] = anx_lapse (0 to 1): how strongly anxiety increases lapse (random choice) rate

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha2, alpha1, beta, trace0, anx_lapse = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: rows are spaceships A,U; columns are planets X,Y
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Action values
    q1 = np.zeros(2)        # stage-1 model-free values for A,U
    q2 = np.zeros((2, 2))   # stage-2 values for planets X,Y and their two aliens

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse (random choice mixing)
    # Higher anxiety => higher lapse probability at both stages
    eps_lapse = np.clip(anx_lapse * stai_val, 0.0, 0.5)

    for t in range(n_trials):
        # Stage-1 policy (softmax over MF q1, with lapse)
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        # Mix with lapse uniform policy
        probs_1 = (1.0 - eps_lapse) * probs_1 + eps_lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy (softmax over q2 at reached planet, with lapse)
        q2_net = q2[s]
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        probs_2 = (1.0 - eps_lapse) * probs_2 + eps_lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # TD updates
        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Stage-1 update with eligibility trace λ
        # Anxiety reduces the depth of credit assignment (smaller λ)
        lambda_eff = np.clip(trace0 * (1.0 - 0.6 * stai_val), 0.0, 1.0)

        # Bootstrapped update includes both immediate stage-2 value and the stage-2 TD error
        # This implements a SARSA(λ)-like update using the propagated delta2
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha1 * (delta1 + lambda_eff * delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with anxiety-skewed transition beliefs and reward compression.

    Overview:
    - Stage-2 values learned via TD.
    - Stage-1 choices derive purely from model-based planning that uses a biased transition model.
    - Anxiety reduces confidence in the canonical transition structure (skews toward the opposite structure).
    - Anxiety also compresses rewards toward neutrality, dampening learning from outcomes.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = psi0 (0 to 1): baseline trust in canonical transitions (higher => more canonical)
    - model_parameters[3] = anx_skew (0 to 1): how strongly anxiety shifts beliefs away from canonical transitions
      Effective trust: tau_eff = clip(psi0 + anx_skew * (0.5 - stai), 0, 1).
    - model_parameters[4] = zeta_r (0 to 1): reward compression toward 0.5, amplified by anxiety

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, psi0, anx_skew, zeta_r = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Canonical transition structure
    T_canon = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Opposite (flipped) structure swaps common/rare per spaceship
    T_flip = np.array([[0.3, 0.7],
                       [0.7, 0.3]])

    # Anxiety-modulated trust in canonical transitions
    tau_eff = np.clip(psi0 + anx_skew * (0.5 - stai_val), 0.0, 1.0)
    T_eff = tau_eff * T_canon + (1.0 - tau_eff) * T_flip

    # Stage-2 Q-values
    q2 = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Reward compression: anxiety pushes rewards toward 0.5 baseline
    comp = np.clip(zeta_r * (0.5 + 0.5 * stai_val), 0.0, 1.0)  # effective compression weight in [0,1]

    for t in range(n_trials):
        # Model-based stage-1 values from current q2 and biased transitions
        max_q2 = np.max(q2, axis=1)            # best action value per planet
        q1_mb = T_eff @ max_q2                 # expected value per spaceship

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at reached planet
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Compressed reward signal
        r = reward[t]
        r_tilde = (1.0 - comp) * r + comp * 0.5

        # Stage-2 learning
        delta2 = r_tilde - q2[s, a2]
        q2[s, a2] += alpha * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive-temperature hybrid control driven by volatility, modulated by anxiety.

    Overview:
    - Stage-2 values learned via TD.
    - Stage-1 combines model-based and model-free values with a weight that decreases with estimated volatility.
    - Decision noise (1/beta) increases when recent outcome volatility is high, especially under anxiety.
    - Includes small stage-1 perseveration that weakens with anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 and stage-1 MF values
    - model_parameters[1] = beta0 (0 to 10): baseline inverse temperature
    - model_parameters[2] = nu_vol (0 to 1): strength of volatility impact on temperature
    - model_parameters[3] = anx_beta (0 to 1): how strongly anxiety amplifies volatility-induced noise
    - model_parameters[4] = stick1 (0 to 1): baseline stage-1 perseveration magnitude

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta0, nu_vol, anx_beta, stick1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q1_mf = np.zeros(2)       # model-free stage-1
    q2 = np.zeros((2, 2))     # stage-2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    # Volatility tracker: exponentially-weighted mean absolute TD error at stage 2
    vol = 0.0

    for t in range(n_trials):
        # Model-based component from current q2 and known transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Volatility-dependent mixture weight (higher vol => more MF reliance)
        w_mb = 1.0 - np.clip(vol, 0.0, 1.0)              # in [0,1]
        w_mb *= (1.0 - 0.5 * stai_val)                   # anxiety reduces MB weighting further (without extra param)
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Perseveration bias (reduced by anxiety)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick1 * (1.0 - stai_val)

        # Combine MB and MF with bias
        q1_net = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1

        # Adaptive temperature based on volatility and anxiety
        beta_eff = beta0 / (1.0 + nu_vol * vol * (1.0 + anx_beta * stai_val))
        beta_eff = np.clip(beta_eff, 1e-6, 10.0)

        # Stage-1 policy
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at reached planet with same beta_eff
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update volatility estimate (faster adaptation under anxiety)
        vol_lr = 0.3 + 0.5 * stai_val  # in [0.3,0.8]
        vol = (1.0 - vol_lr) * vol + vol_lr * abs(delta2)
        vol = np.clip(vol, 0.0, 1.0)

        # Stage-1 MF bootstrapping from the obtained stage-2 value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll