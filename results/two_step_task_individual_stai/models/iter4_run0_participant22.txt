def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-modulated arbitration.
    
    This model learns second-stage values model-free and first-stage transition
    structure model-based. First-stage choices are guided by a weighted blend
    of model-based (MB) and model-free (MF) action values. The arbitration
    weight depends on the entropy (uncertainty) of the currently chosen action's
    transition row and on the participant's anxiety level.
    
    Parameters (model_parameters):
    - alpha_mf: [0,1] model-free learning rate for Q at both stages.
    - beta: [0,10] inverse temperature for both stages.
    - trans_lr: [0,1] learning rate for updating the transition matrix rows.
    - arbit_slope: [0,1] slope controlling sensitivity of arbitration to uncertainty and anxiety.
    - mb_bias: [0,1] baseline bias toward MB control (mapped to [-2,2] internally).
    
    Inputs:
    - action_1: int array of length T; 0 or 1 (spaceship A vs U).
    - state: int array of length T; 0 or 1 (planet X vs Y) actually reached.
    - action_2: int array of length T; 0 or 1 (alien choice on that planet).
    - reward: float array of length T; received coins (assumed in [0,1]).
    - stai: array-like length 1; anxiety score in [0,1].
    
    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_mf, beta, trans_lr, arbit_slope, mb_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix near the task's structure but allow learning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free values
    q1_mf = np.zeros(2)        # first-stage MF values
    q2 = np.zeros((2, 2))      # second-stage MF values per state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Eligibility trace at stage-1 is modulated by anxiety (not an extra parameter)
    # Higher anxiety reduces bootstrapping credit assignment.
    lam_eff = max(0.0, min(1.0, 0.6 - 0.4 * stai))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based first-stage Q via planning through learned transitions
        max_q2 = np.max(q2, axis=1)       # best attainable value at each planet
        q1_mb = T @ max_q2                # plan over transitions

        # Uncertainty (entropy) of each action's transition row
        # H(p) = -sum p log p; compute per action and take the chosen action's value.
        # Use it to compute an arbitration weight: higher uncertainty and higher anxiety -> more MF
        row_entropies = np.zeros(2)
        for a in range(2):
            p_row = T[a]
            # numerical safety
            p_row = np.clip(p_row, 1e-12, 1.0)
            row_entropies[a] = -np.sum(p_row * np.log(p_row))
        H_chosen = row_entropies[a1]

        # Map mb_bias in [0,1] to centered bias in [-2,2] to allow a wide range of weights
        bias_centered = 4.0 * (mb_bias - 0.5)

        # Arbitration signal: positive favors MB; uncertainty and anxiety push toward MF
        # w in [0,1]
        arb_signal = bias_centered - 3.0 * arbit_slope * (H_chosen + stai)
        w = 1.0 / (1.0 + np.exp(-arb_signal))
        w = min(1.0, max(0.0, w))

        # Hybrid Q for first-stage policy
        q1_hybrid = w * q1_mb + (1.0 - w) * q1_mf

        # First-stage choice probability (softmax)
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability (softmax on q2 at reached state)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Learning: update model-free values
        # Stage-2 TD
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * delta2

        # Stage-1 MF bootstrapping with eligibility from stage-2 PE
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alpha_mf * delta1
        q1_mf[a1] += alpha_mf * lam_eff * delta2

        # Learn the transition matrix row for the chosen first-stage action from observed state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - trans_lr) * T[a1, :] + trans_lr * one_hot_s
        # Renormalize for safety
        T[a1, :] = np.clip(T[a1, :], 1e-8, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with Pearce-Hall associability, anxiety-weighted negative PEs, and risk-sensitive utility.
    
    This model is purely model-free at both stages. The effective learning rate
    is dynamically adapted via an associability term (Pearce-Hall): recent absolute
    prediction errors increase associability and hence learning rate. Anxiety
    selectively amplifies negative prediction errors. Rewards are transformed by
    a risk-sensitive utility that becomes more concave with higher anxiety.
    
    Parameters (model_parameters):
    - alpha0: [0,1] baseline learning-rate scalar multiplied by associability.
    - beta: [0,10] inverse temperature for both stages.
    - phi: [0,1] associability update rate (how quickly associability tracks |PE|).
    - anx_neg: [0,1] scales amplification of negative PEs as (1 + anx_neg*stai).
    - risk_sens: [0,1] risk-sensitivity strength; higher makes utility more concave with anxiety.
    
    Inputs:
    - action_1: int array of length T; 0 or 1 (spaceship).
    - state: int array of length T; 0 or 1 (planet).
    - action_2: int array of length T; 0 or 1 (alien).
    - reward: float array of length T; coins in [0,1].
    - stai: array-like length 1; anxiety score in [0,1].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha0, beta, phi, anx_neg, risk_sens = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Associability terms per state-action at stage-2, and a single term for stage-1 actions
    A1 = np.ones(2) * 0.5
    A2 = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Utility transform exponent: more concave with higher anxiety and risk_sens
    # exponent in (0,1]: 1 means linear; lower means more risk-averse
    util_exp = max(0.05, 1.0 - 0.7 * risk_sens * stai)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r_raw = reward[t]

        # Risk-sensitive utility
        r = (r_raw + 1e-12) ** util_exp

        # Policies
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Stage-2 TD update with associability and anxiety-weighted negative PE
        delta2 = r - q2[s, a2]
        delta2_eff = delta2 if delta2 >= 0 else (1.0 + anx_neg * stai) * delta2
        A2[s, a2] = (1.0 - phi) * A2[s, a2] + phi * abs(delta2_eff)
        alpha2_eff = alpha0 * np.clip(A2[s, a2], 0.0, 1.0)
        q2[s, a2] += alpha2_eff * delta2_eff

        # Stage-1 bootstrapped TD using current state-action value and associability at stage-1
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        # Update stage-1 associability with absolute TD
        A1[a1] = (1.0 - phi) * A1[a1] + phi * abs(delta1)
        alpha1_eff = alpha0 * np.clip(A1[a1], 0.0, 1.0)
        q1[a1] += alpha1_eff * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planning with confirmation-biased transition learning modulated by anxiety.
    
    This model plans using a learned transition matrix and second-stage model-free values.
    The transition learning rate exhibits confirmation bias: if the observed transition
    matches the most likely transition under current beliefs, the update rate differs from
    when it contradicts expectations. Anxiety increases this asymmetry.
    
    Parameters (model_parameters):
    - alpha: [0,1] learning rate for second-stage Q-values.
    - beta: [0,10] inverse temperature for both stages.
    - trans_lr: [0,1] baseline transition learning rate.
    - conf0: [0,1] baseline confirmation bias strength (0=no asymmetry, 1=strong).
    - anx_conf: [0,1] scales how much anxiety amplifies confirmation bias.
    
    Inputs:
    - action_1: int array of length T; 0 or 1 (spaceship).
    - state: int array of length T; 0 or 1 (planet).
    - action_2: int array of length T; 0 or 1 (alien).
    - reward: float array of length T; coins in [0,1].
    - stai: array-like length 1; anxiety score in [0,1].
    
    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, trans_lr, conf0, anx_conf = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transitions with moderate prior; learn from experience
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    q2 = np.zeros((2, 2))  # model-free second-stage values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Model-based first-stage Q by planning through T and q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # First-stage policy
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p_choice_2[t] = probs2[a2]

        # Update second-stage values (MF)
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Confirmation-biased transition learning for the chosen action
        # Determine whether the observed state matches current expectation
        expected_state = int(np.argmax(T[a1, :]))
        is_confirming = 1 if s == expected_state else 0

        # Effective bias magnitude (0 to ~2*conf) with anxiety amplification
        bias_mag = conf0 * (1.0 + anx_conf * stai)
        # Adjust learning rate: increase for confirming updates, decrease for disconfirming
        # Map to multiplier in [1 - bias_mag, 1 + bias_mag]
        lr_mult = 1.0 + (2 * is_confirming - 1) * bias_mag
        lr_mult = max(0.0, lr_mult)  # avoid negative
        lr_eff = max(0.0, min(1.0, trans_lr * lr_mult))

        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1, :] = (1.0 - lr_eff) * T[a1, :] + lr_eff * one_hot_s
        # Renormalize and clip
        T[a1, :] = np.clip(T[a1, :], 1e-8, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll