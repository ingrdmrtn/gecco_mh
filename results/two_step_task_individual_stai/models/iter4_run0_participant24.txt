def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-warped transition model and entropy-based arbitration.
    
    This model blends model-based (MB) and model-free (MF) control at the first stage.
    The MB component uses a fixed transition structure (A->X, U->Y) but anxiety warps
    the transition belief toward indecision (uniform), weakening planning. Arbitration
    shifts toward MB when second-stage policies are confident (low entropy); anxiety
    also pushes arbitration toward MF.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Reward received on each trial (e.g., 0.0 or 1.0).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha, beta, w_base, kappa_ent, xi_anx]
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w_base in [0,1]: baseline MB arbitration weight at stage 1.
        - kappa_ent in [0,1]: increases MB weight when stage-2 policy entropy is low.
        - xi_anx in [0,1]: how much anxiety (stai) warps transition beliefs toward uniform
          AND directly shifts arbitration toward MF.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, w_base, kappa_ent, xi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Known transition structure, anxiety-warped toward uniform (0.5)
    T_true = np.array([[0.7, 0.3],  # A -> (X,Y)
                       [0.3, 0.7]])  # U -> (X,Y)

    # Stage-1 MF Q-values and Stage-2 MF Q-values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    def softmax_probs(vals, beta_):
        v = vals - np.max(vals)
        e = np.exp(beta_ * v)
        return e / np.sum(e)

    # Helper to compute normalized entropy of a 2-action policy (0..1)
    def ent01(probs):
        eps = 1e-12
        H = -np.sum(probs * np.log(probs + eps))
        return H / np.log(2.0)

    # Convert baseline MB weight to logit space for smooth adjustments
    eps_w = 1e-8
    w_base_clip = np.clip(w_base, eps_w, 1 - eps_w)
    logit_w0 = np.log(w_base_clip) - np.log(1 - w_base_clip)

    for t in range(n_trials):
        st = state[t]

        # Anxiety-warped transition matrix toward uniform
        xi = xi_anx * s  # 0 (no warp) .. xi_anx (max warp)
        T_eff = (1 - xi) * T_true + xi * 0.5

        # MB action values at stage 1 from current MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)  # [X_best, Y_best]
        q1_mb = T_eff @ max_q2  # expected value via transition model

        # Stage-2 choice policy at visited state
        probs2 = softmax_probs(q2_mf[st], beta)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Use stage-2 policy entropy to arbitrate toward MB when confident (low entropy)
        H2 = ent01(probs2)  # 0 (deterministic) .. 1 (uniform)
        # MB weight increases as entropy decreases; anxiety reduces MB via -xi_anx*s
        logit_w = logit_w0 + kappa_ent * (1 - H2) - xi_anx * s
        w = 1.0 / (1.0 + np.exp(-logit_w))
        w = np.clip(w, 0.0, 1.0)

        # Stage-1 blended values and policy
        q1_blend = w * q1_mb + (1 - w) * q1_mf
        probs1 = softmax_probs(q1_blend, beta)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning
        r = reward[t]

        # MF update at stage 2 (SARSA(0) on terminal reward)
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += alpha * pe2

        # MF update at stage 1 toward obtained stage-2 value
        pe1 = q2_mf[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MF with adaptive volatility-driven learning rate, eligibility, and anxiety scaling.
    
    This is a pure model-free account that adapts the second-stage learning rate to surprise
    magnitude (volatility proxy). Anxiety increases sensitivity to surprise, boosting learning
    rates when outcomes are unexpected. First-stage learning uses an eligibility trace that
    inherits the adaptive rate.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float
        Reward received each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha0, beta, lam, v0, v_anx]
        - alpha0 in [0,1]: baseline learning rate floor.
        - beta in [0,10]: inverse temperature at both stages.
        - lam in [0,1]: eligibility factor that scales first-stage learning.
        - v0 in [0,1]: baseline sensitivity to surprise for adapting the learning rate.
        - v_anx in [0,1]: additional surprise sensitivity per unit anxiety (stai).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """
    alpha0, beta, lam, v0, v_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    def softmax_probs(vals, beta_):
        v = vals - np.max(vals)
        e = np.exp(beta_ * v)
        return e / np.sum(e)

    for t in range(n_trials):
        st = state[t]

        # Policies
        probs1 = softmax_probs(q1, beta)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        probs2 = softmax_probs(q2[st], beta)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Adaptive learning rate driven by surprise magnitude at stage 2
        pe2 = r - q2[st, a2]
        surprise = abs(pe2)
        alpha2_t = alpha0 + (v0 + v_anx * s) * surprise
        alpha2_t = np.clip(alpha2_t, 0.0, 1.0)

        # Update stage 2
        q2[st, a2] += alpha2_t * pe2

        # Eligibility-based update at stage 1 toward the realized stage-2 value
        pe1 = q2[st, a2] - q1[a1]
        alpha1_t = np.clip(lam * alpha2_t, 0.0, 1.0)
        q1[a1] += alpha1_t * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-modulated confirmation bias and lapses.
    
    The agent learns transition probabilities and MF values. The MB contribution depends
    on the certainty of learned transitions and is down-weighted by anxiety. Transition
    learning exhibits confirmation bias: updates are stronger when observations confirm
    current beliefs, and anxiety amplifies this bias. Policies include an epsilon-lapse
    that increases with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage states: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state.
    reward : array-like of float
        Reward received each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha, beta, tau_tr, zeta_conf, eps0]
        - alpha in [0,1]: MF learning rate (both stages).
        - beta in [0,10]: inverse temperature (both stages).
        - tau_tr in [0,1]: base transition learning rate.
        - zeta_conf in [0,1]: confirmation-bias strength; anxiety scales this bias.
        - eps0 in [0,1]: baseline lapse; effective lapse = min(0.5, eps0 * (1 + stai)).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, tau_tr, zeta_conf, eps0 = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Learned transition matrix initialized to indifference (0.5)
    T_hat = np.full((2, 2), 0.5)  # rows: actions (A,U), cols: states (X,Y), row sums to 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    def softmax_probs(vals, beta_):
        v = vals - np.max(vals)
        e = np.exp(beta_ * v)
        return e / np.sum(e)

    def mix_with_lapse(probs, eps):
        return (1 - eps) * probs + eps * 0.5

    for t in range(n_trials):
        st = state[t]

        # MB action values using current transition estimates
        max_q2 = np.max(q2_mf, axis=1)  # best values at X and Y
        q1_mb = T_hat @ max_q2

        # Compute per-action MB weights from certainty and anxiety
        # Certainty for an action: distance from 0.5; w_a in [0,1]
        cert = 2.0 * np.abs(T_hat[:, 0] - 0.5)  # equals abs(pX - pY) * 1
        w_mb_per_action = cert * (1.0 - 0.5 * s)
        w_mb_per_action = np.clip(w_mb_per_action, 0.0, 1.0)

        # Blended first-stage values per action
        q1_blend = w_mb_per_action * q1_mb + (1 - w_mb_per_action) * q1_mf

        # Lapse increases with anxiety (capped at 0.5)
        eps_lapse = min(0.5, eps0 * (1.0 + s))

        # Policies with lapses
        probs1 = mix_with_lapse(softmax_probs(q1_blend, beta), eps_lapse)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        probs2 = mix_with_lapse(softmax_probs(q2_mf[st], beta), eps_lapse)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # MF updates
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += alpha * pe2

        pe1 = q2_mf[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Transition learning with confirmation bias
        # Current belief for chosen action
        p_row = T_hat[a1].copy()
        # Which state is currently believed more likely?
        believed_state = int(p_row[0] >= p_row[1])  # 0 if pX>=pY else 1
        confirmed = 1.0 if st == believed_state else -1.0

        # Anxiety-amplified confirmation bias scales the transition learning rate
        tau_eff = tau_tr * (1.0 + zeta_conf * confirmed * (2.0 * s - 1.0))
        tau_eff = np.clip(tau_eff, 0.0, 1.0)

        # Target one-hot for observed state
        target = np.array([1.0, 0.0]) if st == 0 else np.array([0.0, 1.0])

        # Update the transition row for the chosen action toward the observed state
        p_row_new = p_row + tau_eff * (target - p_row)
        # Normalize to avoid drift (should already sum to 1 with the above update)
        p_row_new = p_row_new / np.sum(p_row_new)
        T_hat[a1] = p_row_new

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll