def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration by learned transition uncertainty.
    The agent learns second-stage values model-free and learns first-stage transitions from experience.
    Arbitration between model-based (MB) and model-free (MF) first-stage values depends on transition uncertainty
    (measured via entropy of the learned transition probabilities). Anxiety amplifies the influence of uncertainty
    on arbitration toward the model-based system.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha_q: [0,1] — learning rate for Q-values (both stages)
        beta: [0,10] — inverse temperature for softmax at both stages
        omega0: [0,1] — baseline MB weight in arbitration
        k_unc: [0,1] — strength of uncertainty-driven arbitration shift
        anx_mix: [0,1] — anxiety gain on uncertainty-driven arbitration shift

    Returns
    - Negative log-likelihood of observed action_1 and action_2 sequences.
    """
    alpha_q, beta, omega0, k_unc, anx_mix = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Second-stage MF values
    q2 = np.zeros((2, 2))
    # First-stage MF values
    q1_mf = np.zeros(2)

    # Learn transitions via counts; initialize pseudo-counts to avoid zero-entropy extremes
    trans_counts = np.ones((2, 2))  # counts[a, s]
    # Helper for entropy of Bernoulli pair
    def entropy_row(p01):
        p0, p1 = p01
        eps = 1e-12
        p0 = np.clip(p0, eps, 1.0 - eps)
        p1 = np.clip(p1, eps, 1.0 - eps)
        return -(p0 * np.log(p0) + p1 * np.log(p1)) / np.log(2.0)  # normalized to [0,1] for 2 outcomes

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Current transition estimates
        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)  # shape (2,2)

        # Model-based first-stage values: expectation over max second-stage values
        max_q2 = np.max(q2, axis=1)  # value of each planet
        q1_mb = trans_probs @ max_q2  # shape (2,)

        # Uncertainty-driven arbitration weight omega in [0,1]
        ent_A = entropy_row(trans_probs[0])
        ent_U = entropy_row(trans_probs[1])
        mean_ent = 0.5 * (ent_A + ent_U)
        omega = omega0 + k_unc * mean_ent * (1.0 + anx_mix * stai_val)
        omega = np.clip(omega, 0.0, 1.0)

        # Hybrid first-stage value
        q1_hyb = (1.0 - omega) * q1_mf + omega * q1_mb

        # First-stage choice probability
        logits1 = beta * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Updates
        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # First-stage MF update bootstrapping on obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Transition learning via counts
        trans_counts[a1, s] += 1.0

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model with anxiety-modulated loss aversion and directed exploration.
    The agent encodes utility with asymmetric weighting of misses vs. gains; anxiety increases loss sensitivity.
    Additionally, it uses a directed exploration bonus that decreases with visit count; anxiety dampens exploration.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for second-stage utilities and first-stage MF backup
        beta: [0,10] — inverse temperature for both stages
        kappa_loss: [0,1] — baseline weight on losses (1 - reward)
        phi_bonus: [0,1] — baseline directed exploration bonus scale
        anx_risk: [0,1] — anxiety gain scaling for loss aversion; also reduces exploration

    Returns
    - Negative log-likelihood of observed action_1 and action_2 sequences.
    """
    alpha, beta, kappa_loss, phi_bonus, anx_risk = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective parameters
    kappa_eff = np.clip(kappa_loss * (1.0 + anx_risk * stai_val), 0.0, 1.0)
    phi_eff = phi_bonus * (1.0 - 0.7 * stai_val * anx_risk)  # anxiety reduces directed exploration

    # Utility-based Q for second stage
    q2 = np.zeros((2, 2))
    # First-stage MF values (utility domain)
    q1_mf = np.zeros(2)

    # Fixed transition model (commonly 0.7)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Visit counts for directed exploration bonus
    visit_counts = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Directed exploration bonus per state-action
        bonus = phi_eff / np.sqrt(1.0 + visit_counts)

        # Model-based evaluation of first stage: expected max of (q2 + bonus) in each state
        mb_state_values = np.max(q2 + bonus, axis=1)  # for X and Y
        q1_mb = transition_matrix @ mb_state_values

        # Use purely MB at first stage but also allow MF backup to shape learning (not decision)
        q1_eval = q1_mb

        # First-stage choice probability
        logits1 = beta * (q1_eval - np.max(q1_eval))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second stage choice probability with bonus
        s = state[t]
        logits2 = beta * ((q2[s] + bonus[s]) - np.max(q2[s] + bonus[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update counts and values
        visit_counts[s, a2] += 1.0

        # Risk-sensitive utility transform: reward vs miss (1 - r)
        r = reward[t]
        util = r - kappa_eff * (1.0 - r)  # in [-kappa_eff, 1]; encourages avoiding misses under high anxiety

        # Second-stage update in utility space
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage MF backup (not used for decision this model but learned for completeness)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-Representation-inspired first-stage evaluation with anxiety-modulated discounting and forgetting.
    The agent learns second-stage MF values and a compact state-occupancy predictor for first-stage actions.
    Anxiety reduces the effective planning horizon (discount) and increases forgetting of learned values.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for Q-values (both stages)
        beta_base: [0,10] — base inverse temperature for both stages
        gamma0: [0,1] — baseline discount factor for SR-style evaluation
        sr_forget: [0,1] — base forgetting rate applied to Q-values
        anx_gamma: [0,1] — how strongly anxiety reduces the discount and increases forgetting

    Returns
    - Negative log-likelihood of observed action_1 and action_2 sequences.
    """
    alpha, beta_base, gamma0, sr_forget, anx_gamma = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated discount and forgetting
    gamma_eff = np.clip(gamma0 * (1.0 - 0.6 * anx_gamma * stai_val), 0.0, 1.0)
    forget_eff = np.clip(sr_forget * (1.0 + 0.8 * anx_gamma * stai_val), 0.0, 1.0)

    beta = beta_base

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    # Successor-like predictor: occupancy of states from first-stage actions (rows: actions; cols: states)
    # Initialize with the known common transitions
    M = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Also track transitions via counts to refine M
    trans_counts = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Compute transition probabilities
        trans_probs = trans_counts / np.sum(trans_counts, axis=1, keepdims=True)

        # Update SR-like matrix mildly toward empirical transitions (one-step SR with discount)
        # Here SR reduces to discounted immediate transitions
        M = (1.0 - alpha) * M + alpha * trans_probs

        # First-stage values via SR: discounted expected planet value
        v_states = np.max(q2, axis=1)
        q1_sr = gamma_eff * (M @ v_states)

        # First-stage choice
        logits1 = beta * (q1_sr - np.max(q1_sr))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Apply anxiety-modulated forgetting to Q-values before update
        q2 *= (1.0 - forget_eff)

        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Transition counts update to refine M
        trans_counts[a1, s] += 1.0

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)