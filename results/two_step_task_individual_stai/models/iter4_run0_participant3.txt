def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-uncertainty aversion with learned transitions and nonlinear utility.

    Core mechanisms:
    - Learn a transition model T(a1 -> s2) online.
    - Learn stage-2 Q-values via TD(0) using a nonlinear utility of reward.
    - First-stage is purely model-based, but penalizes actions whose transition row is uncertain
      via an entropy penalty on T (uncertainty aversion).
    - Anxiety increases the weight of the uncertainty penalty and also increases concavity
      of the reward utility (risk sensitivity).

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}; first-stage choices taken by the participant.
    - state:    int array (n_trials,), values in {0,1}; second-stage state reached (X=0, Y=1).
    - action_2: int array (n_trials,), values in {0,1}; second-stage action (alien) chosen.
    - reward:   float array (n_trials,), in [0,1]; obtained coins.
    - stai:     float array with a single float in [0,1]; anxiety score.
    - model_parameters: iterable of five parameters:
        alpha_v in [0,1]: learning rate for stage-2 Q-values
        beta    in [0,10]: inverse temperature for softmax at both stages
        tau_M   in [0,1]: learning rate for the transition model T
        chi_u   in [0,1]: baseline weight on transition uncertainty (entropy) penalty at stage-1
        rho_r   in [0,1]: baseline exponent for utility u(r)=r^rho (concavity)

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_v, beta, tau_M, chi_u, rho_r = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition model; start from common/rare prior
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q2(s2, a2) values
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    # Anxiety effects:
    # - stronger aversion to transition uncertainty
    chi_eff = chi_u * (0.5 + 0.5 * s_anx)  # scales up to chi_u for s_anx=1
    # - more concave utility with anxiety: interpolate rho toward 0.5 with anxiety
    rho_eff = 0.5 * s_anx + (1.0 - s_anx) * max(1e-6, rho_r)

    for t in range(n_trials):

        # Compute model-based Q1 from learned transitions and current Q2
        vmax = np.max(Q2, axis=1)  # value of each second-stage state

        # Entropy of transition rows (uncertainty)
        # H(p) = -sum p log p
        row_entropy = np.zeros(2)
        for a1_idx in range(2):
            p_row = np.clip(T[a1_idx], eps, 1.0)
            row_entropy[a1_idx] = -(p_row[0] * np.log(p_row[0]) + p_row[1] * np.log(p_row[1]))

        q1_mb = T @ vmax
        # Penalize actions with uncertain transitions
        q1 = q1_mb - chi_eff * row_entropy

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and apply nonlinear utility
        r_raw = reward[t]
        # Ensure r in [0,1], then apply u(r) = r^rho_eff
        r_u = max(0.0, min(1.0, r_raw)) ** rho_eff

        # Update transition model T with simple delta rule toward one-hot observed state
        oh = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        T[a1] = (1.0 - tau_M) * T[a1] + tau_M * oh
        # Renormalize row
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # TD(0) update for Q2
        delta2 = r_u - Q2[s2, a2]
        Q2[s2, a2] += alpha_v * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-gated asymmetric learning with eligibility and stickiness; hybrid MB/MF arbitration.

    Core mechanisms:
    - Stage-2 Q-values learned via TD(0) with asymmetric learning rates for negative vs positive
      prediction errors, where anxiety amplifies negative-learning rate.
    - Stage-1 MF values receive eligibility-based credit assignment from stage-2 TD error.
    - Stage-1 decision is a hybrid: weighted combination of model-based (fixed transitions) and
      model-free values. Arbitration weight decreases with anxiety (more anxious -> less MB).
    - Action stickiness (perseveration) at both stages.

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}
    - state:    int array (n_trials,), values in {0,1}
    - action_2: int array (n_trials,), values in {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with a single float in [0,1]
    - model_parameters: iterable of five parameters:
        alpha0   in [0,1]: baseline learning rate
        beta     in [0,10]: inverse temperature (both stages)
        neg_boost in [0,1]: scales extra learning from negative PEs, amplified by anxiety
        trace_w in [0,1]: eligibility weight from stage-2 PE to stage-1 MF
        stick   in [0,1]: strength of action stickiness

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha0, beta, neg_boost, trace_w, stick = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure (known common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-12

    for t in range(n_trials):

        # Model-based projection from Q2 via fixed transitions
        vmax = np.max(Q2, axis=1)
        Q1_mb = T @ vmax

        # Anxiety reduces MB reliance
        w_mb = max(0.0, min(1.0, 1.0 - s_anx))
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stickiness/logit bias
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        logits1 = beta * Q1 + stick * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with stickiness
        s2 = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0

        logits2 = beta * Q2[s2] + stick * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD error at stage 2
        delta2 = r - Q2[s2, a2]

        # Asymmetric, anxiety-gated learning rate
        if delta2 < 0.0:
            alpha_eff = alpha0 * (1.0 + neg_boost * s_anx)
        else:
            alpha_eff = alpha0 * (1.0 - 0.5 * neg_boost * s_anx)
        alpha_eff = max(0.0, min(1.0, alpha_eff))

        # Update Q2
        Q2[s2, a2] += alpha_eff * delta2

        # Eligibility assignment to stage-1 MF value of chosen action
        Q1_mf[a1] += trace_w * alpha_eff * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-sensitive exploration and transition-type avoidance shaped by anxiety.

    Core mechanisms:
    - Learn Q2 via TD(0) and track per-state volatility via running estimate of absolute PE.
    - Stage-1 hybrid MB/MF choice: MB uses expected Q2 but subtracts a volatility penalty
      (discouraging transitions to volatile states), where anxiety strengthens the penalty.
    - Anxiety also promotes avoidance of repeating an action after a rare transition (post-rare
      avoidance bias at stage-1).
    - Fixed transition structure (common=0.7), with optional MF mixture at stage-1.

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}
    - state:    int array (n_trials,), values in {0,1}
    - action_2: int array (n_trials,), values in {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with a single float in [0,1]
    - model_parameters: iterable of five parameters:
        alpha    in [0,1]: learning rate for Q2 (and volatility tracker)
        beta     in [0,10]: inverse temperature for softmax
        gamma    in [0,1]: baseline penalty strength for volatility in MB values
        w_mix    in [0,1]: weight of model-based control at stage-1 (1=MB only)
        bias_pr  in [0,1]: strength of post-rare avoidance bias at stage-1

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, gamma, w_mix, bias_pr = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    # Volatility estimate per second-stage state (higher means more surprising outcomes)
    vol = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # For post-rare avoidance bias, track previous trial's transition rarity and action
    prev_a1 = -1
    prev_was_rare = False

    eps = 1e-12

    for t in range(n_trials):

        # Compute MB action values as expected vmax minus volatility penalty
        vmax = np.max(Q2, axis=1)
        # Anxiety-strengthened volatility penalty
        gamma_eff = gamma * (0.5 + 0.5 * s_anx)
        # Expected volatility cost per action
        vol_cost = T @ vol
        Q1_mb = (T @ vmax) - gamma_eff * vol_cost

        # Hybrid with MF
        Q1 = w_mix * Q1_mb + (1.0 - w_mix) * Q1_mf

        # Post-rare avoidance bias: if last transition was rare, bias against repeating prev_a1
        bias_vec = np.zeros(2)
        if prev_a1 in (0, 1) and prev_was_rare:
            # Anxiety scales the bias strength
            bias_vec[prev_a1] = -bias_pr * (0.5 + 0.5 * s_anx)

        # Stage-1 policy
        logits1 = beta * Q1 + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = state[t]
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 TD update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Update volatility estimate with running absolute PE
        vol[s2] += alpha * (abs(delta2) - vol[s2])

        # MF backup to Q1 (simple SARSA-style from observed Q2)
        Q1_mf[a1] += alpha * (Q2[s2, a2] - Q1_mf[a1])

        # Determine if current transition was rare for next-trial bias
        # Common if (A->X) or (U->Y)
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        prev_was_rare = (not is_common)
        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll