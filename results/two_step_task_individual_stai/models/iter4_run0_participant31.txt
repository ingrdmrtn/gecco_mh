def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with risk-sensitive utility modulated by anxiety.

    Core ideas:
    - Stage-2 learning is model-free but uses a concave utility transform u(r) = r^(1 - rho_eff).
      Higher anxiety increases effective risk aversion (larger rho_eff), making utilities more
      concave and dampening learning from high rewards.
    - Stage-1 action values are a mixture of model-based (via transitions) and model-free values,
      weighted by omega.
    - Policies at both stages use a softmax with inverse temperature beta.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choice within state (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Obtained rewards (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases risk aversion in utility.
    model_parameters : array-like of floats, length 5
        [alpha, beta, omega, rho0, k_anx_risk]
        - alpha in [0,1]: learning rate for MF values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - omega in [0,1]: weight on model-based value at stage 1.
        - rho0 in [0,1]: baseline risk aversion shaping the utility exponent.
        - k_anx_risk in [0,1]: how much anxiety increases risk aversion.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, omega, rho0, k_anx_risk = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition model: rows are A,U; columns are planets X,Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q_stage1_mf = np.zeros(2)       # MF for spaceships A,U
    q_stage2_mf = np.zeros((2, 2))  # MF for aliens on planet X (row 0) and Y (row 1)

    # Anxiety-modulated risk parameter
    rho_eff = rho0 + k_anx_risk * stai
    if rho_eff < 0.0:
        rho_eff = 0.0
    if rho_eff > 1.0:
        rho_eff = 1.0

    # bounded beta
    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for trial in range(n_trials):

        # Model-based evaluation at stage 1 from current MF stage-2 values
        max_q_stage2 = np.max(q_stage2_mf, axis=1)              # best alien expected value per planet
        q_stage1_mb = transition_matrix @ max_q_stage2          # expected value per spaceship via transitions

        # Hybrid stage-1 value
        q1_hybrid = omega * q_stage1_mb + (1.0 - omega) * q_stage1_mf

        # policy for the first choice
        logits1 = beta_eff * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[trial]
        p_choice_1[trial] = p1[a1]

        # policy for the second choice
        s = state[trial]
        logits2 = beta_eff * q_stage2_mf[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[trial]
        p_choice_2[trial] = p2[a2]

        # Utility-transform the reward (risk-sensitive, anxiety-modulated)
        r = reward[trial]
        # Ensure input is within [0,1]; then apply concave utility
        if r < 0.0:
            r = 0.0
        if r > 1.0:
            r = 1.0
        util = r ** (1.0 - rho_eff)

        # Learning updates
        # Stage-1 MF bootstraps from the chosen stage-2 action value (SARSA-style)
        delta1 = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1

        # Stage-2 MF learns from utility-transformed outcome
        delta2 = util - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

    eps = 1e-10
    log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return log_loss


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-adaptive exploration with anxiety scaling and choice perseveration (purely MF).

    Core ideas:
    - A purely model-free SARSA learner at both stages.
    - The inverse temperature used to generate choices is adapted on each trial based on the
      previous trial's surprise |r - Q|. Greater surprise leads to more exploration (lower beta).
    - Anxiety amplifies the effect of surprise on exploration.
    - A choice perseveration kernel adds a bias toward repeating the last chosen action at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S or P/H).
    reward : array-like of float
        Obtained rewards (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases surprise-driven exploration.
    model_parameters : array-like of floats, length 5
        [alpha, beta, chi_surp, pi, xi_anx]
        - alpha in [0,1]: learning rate for Q.
        - beta in [0,10]: base inverse temperature.
        - chi_surp in [0,1]: strength of surprise impact on exploration.
        - pi in [0,1]: perseveration bias magnitude added to last chosen action at each stage.
        - xi_anx in [0,1]: scales how strongly anxiety amplifies surprise-driven exploration.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, chi_surp, pi, xi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None
    prev_a2 = [None, None]  # separate memory per state for stage-2

    # Surprise from previous trial initializes to moderate level
    prev_surprise = 0.5

    for t in range(n_trials):

        # Trial-wise inverse temperature: larger surprise => more exploration (smaller beta)
        # beta_t = beta * exp(-chi_surp * (1 + xi_anx*stai) * prev_surprise)
        scale = chi_surp * (1.0 + xi_anx * stai) * prev_surprise
        if scale < 0.0:
            scale = 0.0
        beta_t = beta * np.exp(-scale)
        if beta_t < 0.0:
            beta_t = 0.0
        if beta_t > 10.0:
            beta_t = 10.0

        # Stage-1 softmax with perseveration
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += pi
        logits1 = beta_t * q1 + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 softmax with state-specific perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += pi
        logits2 = beta_t * q2[s] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Learning updates (SARSA-style bootstrapping)
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update memories for perseveration
        prev_a1 = a1
        prev_a2[s] = a2

        # Compute surprise to inform next trial's exploration
        # Use absolute RPE magnitude at stage-2 as surprise proxy
        prev_surprise = abs(delta2)
        if prev_surprise > 1.0:
            prev_surprise = 1.0

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Leaky-belief model-based planner with anxiety-modulated volatility and stickiness.

    Core ideas:
    - Maintain belief-like values over second-stage options that drift toward an uninformative prior
      via a leak term (captures assumed volatility). Anxiety increases effective leak (more volatility).
    - Choices are purely model-based at stage 1: value of a spaceship is the transition-weighted
      max over the planet's current alien beliefs.
    - A stickiness bias encourages repeating the previous choices at both stages.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state (0/1; e.g., W/S or P/H).
    reward : array-like of float
        Obtained rewards (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score; uses stai[0]. Higher anxiety increases effective leak (perceived volatility).
    model_parameters : array-like of floats, length 5
        [alpha, beta, zeta_leak, k_anx_vol, stick]
        - alpha in [0,1]: learning rate toward observed reward at stage 2.
        - beta in [0,10]: inverse temperature for softmax policies (both stages).
        - zeta_leak in [0,1]: baseline leak toward 0.5 prior on every trial before learning.
        - k_anx_vol in [0,1]: how much anxiety increases the leak (volatility).
        - stick in [0,1]: stickiness bias added to the last chosen action at each stage.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, zeta_leak, k_anx_vol, stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition model: rows are A,U; columns are planets X,Y
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Belief-like values for aliens on each planet (initialized to neutral 0.5)
    q2 = np.ones((2, 2)) * 0.5
    q1_mf_placeholder = np.zeros(2)  # not used for learning, but needed for delta form below

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = [None, None]

    # Effective leak increases with anxiety
    leak_eff = zeta_leak * (1.0 + k_anx_vol * stai)
    if leak_eff < 0.0:
        leak_eff = 0.0
    if leak_eff > 1.0:
        leak_eff = 1.0

    # Bound beta
    beta_eff = beta
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    for t in range(n_trials):

        # Model-based evaluation at stage 1 using current beliefs
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick
        logits1 = beta_eff * q1_mb + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Stage-2 policy with stickiness within the encountered state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += stick
        logits2 = beta_eff * q2[s] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Leak toward prior (0.5) to capture volatility assumptions (anxiety-modulated)
        q2 = (1.0 - leak_eff) * q2 + leak_eff * 0.5

        # Learning: move the chosen second-stage value toward the outcome
        r = reward[t]
        if r < 0.0:
            r = 0.0
        if r > 1.0:
            r = 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Optional placeholder to maintain delta structure (not updating q1 values, purely MB)
        _ = q1_mf_placeholder  # ensures the placeholder is used

        # Update stickiness memories
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll