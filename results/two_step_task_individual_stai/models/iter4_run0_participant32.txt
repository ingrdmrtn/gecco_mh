def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with anxiety-weighted arbitration and volatility tracking.

    Idea:
    - Learns second-stage action values (model-free) with a single learning rate.
    - Computes model-based first-stage values via known transition matrix and current second-stage values.
    - Arbitration weight w_t between MB and MF depends on baseline w0 and a running estimate of outcome volatility.
      Anxiety increases the impact of volatility, shifting weight toward MF when volatility is high.
    - Includes first-stage choice stickiness.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1) per trial (aliens within a planet).
    reward : array-like of float
        Obtained reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score array; uses stai[0] in [0,1] space.
    model_parameters : list or array-like of float
        [alpha, beta, w0, tau_vol, kappa_stick]
        Bounds:
        - alpha: [0,1] learning rate for value updates at both stages.
        - beta: [0,10] inverse temperature for both stages.
        - w0: [0,1] baseline weight on model-based values at the first stage.
        - tau_vol: [0,1] learning rate for a running volatility estimate of the outcome process.
        - kappa_stick: [0,1] strength of first-stage choice stickiness.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, w0, tau_vol, kappa_stick = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Initialize value functions
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2 = np.zeros((2, 2))       # second-stage values per state and action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Running volatility estimate of outcome-generating process
    vol = 0.0

    # Stickiness memory
    prev_a1 = None

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage values
        max_q2 = np.max(q2, axis=1)             # best value in each second-stage state
        q1_mb = transition_matrix @ max_q2      # expected value for A/U via transitions

        # Anxiety-weighted arbitration: higher anxiety + higher volatility reduces MB weight
        w = w0 * (1.0 - np.clip(stai * vol, 0.0, 1.0))
        w = np.clip(w, 0.0, 1.0)

        # First-stage policy with stickiness
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        stick = kappa_stick * (0.5 + 0.5 * stai)  # higher anxiety -> stronger stickiness
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1_mix - np.max(q1_mix)) + stick * bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]

        # Update second-stage value (model-free)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update first-stage MF value towards realized second-stage value (TD(1))
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update volatility estimate (unsigned PE of second stage)
        vol = (1.0 - tau_vol) * vol + tau_vol * np.abs(pe2)

        # Update stickiness memory
        prev_a1 = a1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive second-stage learning and transition-surprise bonus at first stage, anxiety-modulated.

    Idea:
    - Second-stage utility is mean-variance: u = r - z_risk_eff * Var_est, where Var_est ≈ q*(1-q) for Bernoulli rewards.
      Anxiety increases effective risk sensitivity, penalizing uncertain options.
    - First-stage model-free values receive a transient surprise bonus based on transition improbability (rare > common).
      Anxiety amplifies the influence of surprise on subsequent first-stage values/policy.
    - Includes first-stage choice persistence.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial (0/1).
    reward : array-like of float
        Reward outcome per trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta, z_risk, xi_surp, persev]
        Bounds:
        - alpha: [0,1] learning rate for both stages.
        - beta: [0,10] inverse temperature for both stages.
        - z_risk: [0,1] baseline risk aversion weight for variance penalty at second stage.
        - xi_surp: [0,1] baseline transition-surprise bonus strength at first stage.
        - persev: [0,1] first-stage choice persistence strength.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, z_risk, xi_surp, persev = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition probabilities
    trans = np.array([[0.7, 0.3],
                      [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None

    for t in range(n_trials):
        # Compute first-stage policy with persistence and surprise bias (from previous transition)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        stick = persev * (0.5 + 0.5 * stai)

        # Surprise bonus computed on the fly for the action actually taken this trial after observing transition
        # For the choice policy, we use the pre-transition expectation of surprise from last trial (zero at t=0).
        # Implement as an action-dependent additive bias derived from immediate transition likelihood.
        # We'll compute it after seeing the state, then apply to MF update (not to current softmax since state not yet known).
        surprise_bias = np.zeros(2)  # zero for the policy; surprise affects learning below

        logits1 = beta * (q1_mf - np.max(q1_mf)) + stick * bias1 + surprise_bias
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Second-stage policy (risk sensitivity only influences learning, not policy temperature)
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage update with risk-sensitive utility (penalize high-variance options)
        # Bernoulli variance estimate for chosen option: v ≈ q*(1-q)
        q_sa = q2[s, a2]
        var_est = q_sa * (1.0 - q_sa)
        z_eff = z_risk * (0.5 + 0.5 * stai)  # anxiety increases risk penalty
        u = r - z_eff * var_est
        pe2 = u - q_sa
        q2[s, a2] += alpha * pe2

        # First-stage MF update with transition surprise bonus
        # Surprise = -log P(s | a1). Higher surprise -> larger bonus magnitude
        p_s_given_a = trans[a1, s]
        surpr = -np.log(max(p_s_given_a, 1e-8))
        xi_eff = xi_surp * (0.5 + 0.5 * stai)
        bonus = xi_eff * surpr  # nonnegative

        target1 = q2[s, a2] + bonus
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_a1 = a1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    MB win-stay/lose-switch heuristic bias with anxiety-scaled lapses and eligibility trace.

    Idea:
    - Core model-free learner (both stages) with eligibility trace from second stage to first stage.
    - Adds a model-based heuristic bias on first-stage choices: after a rewarded common transition, tend to repeat;
      after a rewarded rare transition, tend to switch; patterns invert after losses. Anxiety amplifies this bias.
    - Includes an epsilon-lapse component that increases with anxiety, blending uniform choice with softmax.
    - Separate softmax is used at both stages; epsilon applies at both stages.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Observed second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage actions per trial (0/1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Trait anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha, beta, eps0, tau_comm, eta_trace]
        Bounds:
        - alpha: [0,1] learning rate for value updates.
        - beta: [0,10] inverse temperature for both stages.
        - eps0: [0,1] baseline lapse probability; effective epsilon increases with anxiety.
        - tau_comm: [0,1] strength of MB win-stay/lose-switch heuristic bias at first stage.
        - eta_trace: [0,1] eligibility trace strength from second-stage PE to first-stage value update.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, eps0, tau_comm, eta_trace = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure for determining common vs. rare
    # Common: A->X (0->0) and U->Y (1->1) with prob 0.7
    common_map = {(0, 0): True, (0, 1): False, (1, 0): False, (1, 1): True}

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous trial info for heuristic bias
    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # Compute heuristic bias for first-stage action based on previous trial outcome and transition type
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            was_common = common_map[(prev_a1, prev_s)]
            # Heuristic: after rewarded common -> repeat; rewarded rare -> switch
            # after non-rewarded common -> switch; non-rewarded rare -> repeat
            tendency_repeat = (prev_r > 0 and was_common) or (prev_r == 0 and not was_common)
            a_repeat = prev_a1
            a_switch = 1 - prev_a1
            tau_eff = tau_comm * (0.5 + 0.5 * stai)  # anxiety amplifies heuristic
            if tendency_repeat:
                bias1[a_repeat] += tau_eff
            else:
                bias1[a_switch] += tau_eff

        # First-stage softmax with lapse
        logits1 = beta * (q1_mf - np.max(q1_mf)) + bias1
        probs1_soft = np.exp(logits1 - np.max(logits1))
        probs1_soft = probs1_soft / np.sum(probs1_soft)
        eps = np.clip(eps0 * (0.5 + 0.5 * stai), 0.0, 1.0)  # anxiety increases lapse
        probs1 = (1.0 - eps) * probs1_soft + eps * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with lapse
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2_soft = np.exp(logits2 - np.max(logits2))
        probs2_soft = probs2_soft / np.sum(probs2_soft)
        probs2 = (1.0 - eps) * probs2_soft + eps * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # First-stage TD update with eligibility trace from second-stage PE
        # Update the chosen first-stage action toward the updated second-stage value,
        # and also add an eligibility component proportional to the immediate second-stage PE.
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * (pe1 + eta_trace * pe2)

        # Store for next trial heuristic
        prev_a1 = a1
        prev_s = s
        prev_r = r

    tiny = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + tiny)) + np.sum(np.log(p_choice_2 + tiny)))
    return neg_log_lik