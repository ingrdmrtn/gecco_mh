def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-suppressed model-based control and eligibility traces.
    
    The agent blends model-free (MF) and model-based (MB) values at the first stage.
    Anxiety reduces the MB weight, shifting control toward MF. A TD(Î»)-style
    eligibility trace propagates second-stage value prediction errors back to the
    first stage. Single learning rate is used for both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U) for each trial.
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y) for each trial.
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1 for the aliens within the state) for each trial.
    reward : array-like of float
        Reward obtained on each trial (e.g., 0 or 1 coins).
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alphaQ, beta, w_mb0, k_anx_w, lambda_elig]
        - alphaQ in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - w_mb0 in [0,1]: baseline weight for MB control at stage 1.
        - k_anx_w in [0,1]: how strongly anxiety reduces MB weight
          (effective w_mb = clip(w_mb0 * (1 - k_anx_w * stai), 0, 1)).
        - lambda_elig in [0,1]: eligibility trace from stage 2 PE to stage 1 MF value.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alphaQ, beta, w_mb0, k_anx_w, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective MB weight reduced by anxiety
    w_mb = w_mb0 * (1.0 - k_anx_w * stai_val)
    if w_mb < 0.0:
        w_mb = 0.0
    if w_mb > 1.0:
        w_mb = 1.0

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF action values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    for t in range(n_trials):
        # Model-based first-stage action values via one-step lookahead
        max_q2 = np.max(q2_mf, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid Q for policy
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Softmax for stage 1
        logits1 = q1_hybrid - np.max(q1_hybrid)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = q2_mf[s] - np.max(q2_mf[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD updates
        # Stage 2
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alphaQ * delta2

        # Stage 1 MF with eligibility trace from stage 2 value
        boot = q2_mf[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alphaQ * lambda_elig * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MF with anxiety-amplified uncertainty bonus, value decay, and second-stage stickiness.
    
    The agent is model-free but augments first-stage action preferences with an
    exploration bonus proportional to expected uncertainty of the next state.
    Uncertainty is higher when second-stage action values in a state are similar.
    Anxiety scales this uncertainty bonus, encouraging exploration under higher anxiety.
    Values decay toward 0.5 to capture forgetting/drift. Second-stage choices exhibit
    within-state stickiness (perseveration).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien 0 or 1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher means greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alpha, beta, k_unc_anx, tau_forget, zeta_stick2]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for both stages.
        - k_unc_anx in [0,1]: scales how strongly anxiety increases the uncertainty bonus.
          Bonus scale = k_unc_anx * stai.
        - tau_forget in [0,1]: per-trial decay of Q-values toward 0.5.
        - zeta_stick2 in [0,1]: weight for second-stage within-state stickiness.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, k_unc_anx, tau_forget, zeta_stick2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure for computing expected uncertainty
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    bonus_scale = k_unc_anx * stai_val

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track previous second-stage choices per state for stickiness
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Value decay toward 0.5 (forgetting/drift)
        q1 = (1.0 - tau_forget) * q1 + tau_forget * 0.5
        q2 = (1.0 - tau_forget) * q2 + tau_forget * 0.5

        # Compute state uncertainty: high when the two aliens are similar
        # u_s in [0,1], where 1 means maximally uncertain (values equal), 0 means certain (values far apart by 1).
        diff = np.abs(q2[:, 0] - q2[:, 1])
        u_state = 1.0 - diff  # already bounded since q in [0,1] typically

        # Expected uncertainty bonus for each first-stage action
        # b[a] = E[u_state | choose a] using transition matrix
        bonus = T @ u_state  # shape (2,)

        # First-stage policy: MF Q plus anxiety-weighted uncertainty bonus
        logits1 = q1 + bonus_scale * bonus
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with within-state stickiness
        s = state[t]
        stick_vec = np.zeros(2)
        if prev_a2[s] != -1:
            stick_vec[prev_a2[s]] = 1.0

        logits2 = q2[s] + zeta_stick2 * stick_vec
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Bootstrap to stage 1
        boot = q2[s, a2]
        delta1 = boot - q1[a1]
        q1[a1] += alpha * delta1

        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with anxiety-biased transition learning and first-stage perseveration.
    
    The agent learns the first-stage transition probabilities online and uses them
    for model-based evaluation. Anxiety biases transition learning: rare transitions
    (relative to current belief) are learned faster when anxiety is high, while
    common transitions are learned more conservatively. Additionally, first-stage
    choices exhibit perseveration that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Observed second-stage state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (alien 0 or 1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score in [0,1]; higher indicates greater anxiety. Only stai[0] is used.
    model_parameters : list/tuple of 5 floats
        [alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate]
        - alphaQ in [0,1]: learning rate for MF values (both stages).
        - beta in [0,10]: inverse temperature for both stages.
        - tau_T in [0,1]: base learning rate for transition probabilities.
        - k_anx_trans_bias in [0,1]: scales anxiety-dependent modulation of transition learning.
          Effective tau: increased for rare transitions, decreased for common ones:
          tau_eff = tau_T * (1 + k_anx_trans_bias * stai) if rare else tau_T * (1 - k_anx_trans_bias * stai).
        - psi_perseverate in [0,1]: base perseveration weight at stage 1; actual bias is psi_perseverate * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize transition model; start uninformative (0.5/0.5) to allow learning
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    prev_a1 = None

    for t in range(n_trials):
        # Model-based first-stage values from current transition estimate
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Perseveration at stage 1 scales with anxiety
        stick_vec = np.zeros(2)
        if prev_a1 is not None:
            stick_vec[prev_a1] = 1.0
        perseveration_bias = psi_perseverate * stai_val * stick_vec

        # Combine MB and MF equally (no extra parameter to keep total <= 5)
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage 1 policy
        logits1 = q1_comb + perseveration_bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update transition model for the chosen first-stage action a1
        # Determine if observed transition was "rare" under current belief
        p_obs = T[a1, s]
        is_rare = p_obs < 0.5
        if is_rare:
            tau_eff = tau_T * (1.0 + k_anx_trans_bias * stai_val)
        else:
            tau_eff = tau_T * (1.0 - k_anx_trans_bias * stai_val)
        # Clamp tau_eff into [0,1] to be safe
        if tau_eff < 0.0:
            tau_eff = 0.0
        if tau_eff > 1.0:
            tau_eff = 1.0

        # Row-wise update toward the observed state s
        # Increase T[a1, s], decrease the other entry to keep row stochastic
        T[a1, s] += tau_eff * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] = 1.0 - T[a1, s]

        # MF learning at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * delta2

        # MF bootstrap to stage 1
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alphaQ * delta1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)