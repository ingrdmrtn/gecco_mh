Below are three standalone cognitive models that compute the negative log-likelihood of the participant’s observed choices. Each function uses all inputs, keeps parameters within the requested bounds, and incorporates the participant’s STAI anxiety score in a meaningful way. No imports are included; assume numpy as np is already available.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-weighted arbitration with anxiety-amplified stickiness.

    Overview
    - Learns second-stage values model-free and propagates to the first stage (MF).
    - Computes a model-based estimate from the fixed transition model.
    - Arbitrates (blends) MB and MF by an uncertainty signal derived from running second-stage PE^2.
    - Higher anxiety increases reliance on MF under uncertainty and increases choice stickiness.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U).
    - state: array-like of ints in {0,1}, second-stage states reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1).
    - reward: array-like of floats in [0,1], outcomes.
    - stai: array-like length-1 with scalar in [0,1], anxiety score.
    - model_parameters: tuple/list of 5 params
        alpha2: stage-2 learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        phi_arbit: base arbitration weight in [0,1], scales MB influence
        kappa_stick: base choice stickiness in [0,1]
        zeta_unc: scales how much uncertainty reduces MB control with anxiety in [0,1]

    Mechanisms and anxiety usage
    - Uncertainty u_t ~ running average of PE^2 at stage-2: higher u_t → lower MB weight.
    - Effective MB weight w_mb_t = clip( phi_arbit*(1 - 0.4*stai) * (1 - (zeta_unc*stai)*u_t) ).
      Thus anxiety amplifies the uncertainty penalty on MB control.
    - Choice stickiness magnitude = kappa_stick * (0.5 + 0.5*stai).
      Anxiety increases perseveration.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha2, beta, phi_arbit, kappa_stick, zeta_unc = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A→X common, U→Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)        # MF action values at stage 1 for A/U
    q2 = np.zeros((2, 2))      # Second-stage state-action values: states X/Y x aliens 0/1

    # Uncertainty tracker (running mean PE^2 at stage-2)
    unc = 0.0                  # scalar uncertainty summary
    alpha_unc = max(1e-6, min(1.0, 0.25 + 0.5 * alpha2))  # link to learning rate

    # Choice stickiness (keeps previous choice bias)
    prev_a1 = None
    stick_mag = kappa_stick * (0.5 + 0.5 * stai)
    stick_vec = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):

        # MB shortcut: expected best second-stage value per planet
        max_q2 = np.max(q2, axis=1)            # [X, Y]
        q1_mb = T @ max_q2                     # project planets to ships

        # Arbitration weight with uncertainty and anxiety
        w_base = max(0.0, min(1.0, phi_arbit * (1.0 - 0.4 * stai)))
        penalty = max(0.0, min(1.0, (zeta_unc * stai) * unc))
        w_mb_t = max(0.0, min(1.0, w_base * (1.0 - penalty)))

        # Stage-1 decision values + stickiness
        if prev_a1 is not None:
            stick_vec[:] = 0.0
            stick_vec[prev_a1] = stick_mag
        else:
            stick_vec[:] = 0.0

        q1_dec = w_mb_t * q1_mb + (1.0 - w_mb_t) * q1_mf + stick_vec

        # Softmax stage 1
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = max(eps, probs1[a1])

        # Stage-2 policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = max(eps, probs2[a2])

        # Outcomes and learning
        r = reward[t]

        # Stage-2 PE and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update uncertainty (running PE^2)
        unc = (1.0 - alpha_unc) * unc + alpha_unc * (pe2 * pe2)
        # Clip to [0,1] (rewards in [0,1] -> PE in [-1,1] -> PE^2 <= 1)
        unc = max(0.0, min(1.0, unc))

        # Stage-1 MF bootstrapping from realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1  # uses same alpha2 to limit params

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning model with anxiety-sensitive rare-transition aversion and forgetting.

    Overview
    - Learns its own transition matrix T_est via delta rule.
    - Computes model-based Q1 from learned transitions and blends with MF Q1 using a confidence term.
    - Confidence = 1 - entropy(row), per chosen ship; higher entropy -> less MB control.
    - Anxiety increases aversion to rare transitions and increases value forgetting (decay).
    - Second stage learned model-free and bootstrapped to stage 1.

    Parameters
    - action_1: array-like of ints {0,1}, first-stage actions.
    - state: array-like of ints {0,1}, reached state (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage actions.
    - reward: array-like of floats in [0,1].
    - stai: array-like length-1, scalar in [0,1].
    - model_parameters: 5 parameters
        alpha2: stage-2 learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        alpha_T: transition learning rate in [0,1]
        omega_rare: base penalty for rare transitions in [0,1]
        psi_decay: base value decay rate in [0,1]

    Mechanisms and anxiety usage
    - Transition learning: T_est[a, :] updated toward observed state with alpha_T.
    - Confidence-weighted arbitration: w_mb_t = conf(a1)*(1 - 0.6*stai), conf = 1 - H(T_est[a1,:])/H_max.
    - Rare-transition aversion: add bias against choosing the ship that just produced a rare transition,
      magnitude = omega_rare * (0.5 + 0.5*stai).
    - Forgetting: q2 and q1_mf decay toward 0 each trial by psi_eff = psi_decay*(0.3 + 0.7*stai).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha2, beta, alpha_T, omega_rare, psi_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transitions near canonical structure
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Action values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Rare transition tracker for biasing next choice
    last_rare = np.zeros(2)  # indicator per action (A/U) if last time it led to rare transition
    rare_penalty_base = omega_rare * (0.5 + 0.5 * stai)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    H_max = np.log(2.0)

    for t in range(n_trials):

        # Apply forgetting
        psi_eff = psi_decay * (0.3 + 0.7 * stai)
        q2 *= (1.0 - psi_eff)
        q1_mf *= (1.0 - psi_eff)

        # Compute MB plan from learned transitions
        max_q2 = np.max(q2, axis=1)      # [X, Y]
        q1_mb = T_est @ max_q2

        # Confidence of current action's transition (we approximate using each ship's entropy)
        ent_A = -np.sum(T_est[0] * np.log(T_est[0] + eps))
        ent_U = -np.sum(T_est[1] * np.log(T_est[1] + eps))
        conf = np.array([1.0 - ent_A / H_max, 1.0 - ent_U / H_max])  # in [0,1]

        # Anxiety reduces MB weight
        w_mb_vec = conf * (1.0 - 0.6 * stai)  # per action weights used as linear blending proxies

        # Rare-transition aversion bias against ships recently rare
        bias_rare = -rare_penalty_base * last_rare

        # Combine MB and MF with action-specific weights (approximate by averaging)
        w_mb = np.mean(w_mb_vec)
        w_mb = max(0.0, min(1.0, w_mb))
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias_rare

        # Stage-1 softmax
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = max(eps, probs1[a1])

        # Stage-2 policy softmax
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = max(eps, probs2[a2])

        r = reward[t]

        # Determine if transition was rare relative to T_est
        # Rare if prob of observed state under chosen action < 0.5 of the larger prob in that row
        row = T_est[a1]
        common_prob = np.max(row)
        rare_now = 1 if row[s] < (0.5 * common_prob) else 0
        last_rare = np.zeros(2)
        last_rare[a1] = rare_now

        # Update transitions by delta rule toward one-hot of observed state
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T_est[a1] = (1.0 - alpha_T) * T_est[a1] + alpha_T * target_row
        # Ensure normalization (should already sum to 1)
        T_est[a1] /= np.sum(T_est[a1])

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk- and safety-sensitive learning with anxiety-modulated eligibility and planet safety bias.

    Overview
    - Uses an eligibility-trace-like update: stage-2 PE also adjusts stage-1 MF via lambda.
    - Tracks per (state, action) reward variance and applies risk-averse shaping to decision values.
    - Anxiety increases risk aversion and strengthens a safety-seeking planet bias (toward lower-variance planet).
    - Second-stage softmax uses risk-shaped Q-values; first-stage decision includes MB projection plus safety bias.

    Parameters
    - action_1: array-like of ints {0,1}, first-stage actions.
    - state: array-like of ints {0,1}, reached state (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage actions (0/1).
    - reward: array-like of floats in [0,1].
    - stai: array-like length-1, scalar in [0,1].
    - model_parameters: 5 parameters
        alpha: base learning rate in [0,1] (used for both stages)
        beta: inverse temperature in [0,10]
        lambda_elig: base eligibility strength in [0,1]
        nu_risk: base risk-aversion weight in [0,1] (penalizes reward variance)
        chi_alarm: base safety-bias weight in [0,1] added at stage 1

    Mechanisms and anxiety usage
    - Eligibility: lambda_eff = clip(lambda_elig * (0.5 + 0.5*stai)).
      Higher anxiety increases credit assignment from stage-2 PE to stage-1.
    - Risk shaping at stage 2: Q2_tilde = Q2 - nu_eff * Var2, with
      nu_eff = nu_risk * (0.5 + 0.5*stai). Var2 is an EW variance estimate per (state,action).
    - Safety bias at stage 1: compute planet variances VarX, VarY (mean over actions).
      Bias toward lower-variance planet, magnitude = chi_eff = chi_alarm * (0.5 + 0.5*stai).
      Action bias = T @ [bias_X, bias_Y] where bias for safer planet is +chi_eff, riskier is -chi_eff.
    - MB planning uses fixed transitions.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, lambda_elig, nu_risk, chi_alarm = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and variance trackers
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    # Track EW mean and EW variance per (state,action) for rewards
    m2 = np.zeros((2, 2))   # running mean
    v2 = np.zeros((2, 2))   # running variance (EW)
    alpha_var = max(1e-6, min(1.0, 0.25 + 0.5 * alpha))  # smoother variance learning

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    lambda_eff = max(0.0, min(1.0, lambda_elig * (0.5 + 0.5 * stai)))
    nu_eff = max(0.0, min(1.0, nu_risk * (0.5 + 0.5 * stai)))
    chi_eff = max(0.0, min(1.0, chi_alarm * (0.5 + 0.5 * stai)))

    eps = 1e-12

    for t in range(n_trials):

        # Compute safety bias from planet variances
        # Use average across actions as planet variance summary
        VarX = float(np.mean(v2[0]))
        VarY = float(np.mean(v2[1]))

        if VarX < VarY:
            desirability = np.array([+chi_eff, -chi_eff])  # prefer X
        elif VarY < VarX:
            desirability = np.array([-chi_eff, +chi_eff])  # prefer Y
        else:
            desirability = np.array([0.0, 0.0])

        action_bias = T @ desirability  # bias added to A/U

        # MB from current Q2 (max per planet)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB and MF (simple average; eligibility will couple learning)
        q1_dec = 0.5 * q1_mb + 0.5 * q1_mf + action_bias

        # Stage-1 policy
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = max(eps, probs1[a1])

        # Stage-2 risk-shaped policy
        # Penalize options with higher estimated reward variance
        s = int(state[t])
        q2_risk_adj = q2[s] - nu_eff * v2[s]
        q2c = q2_risk_adj - np.max(q2_risk_adj)
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = max(eps, probs2[a2])

        # Outcome
        r = reward[t]

        # Update EW mean/variance for (s,a2)
        # EW mean
        m_prev = m2[s, a2]
        m_new = (1.0 - alpha_var) * m_prev + alpha_var * r
        # EW variance update using squared deviation to new mean
        # v_new = (1 - alpha_var)*v_prev + alpha_var*(r - m_prev)^2 adjusted; use standard EW var update
        dev = r - m_prev
        v_new = (1.0 - alpha_var) * v2[s, a2] + alpha_var * (dev * dev)
        m2[s, a2] = m_new
        v2[s, a2] = max(0.0, min(1.0, v_new))

        # Stage-2 PE and update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update via eligibility trace using stage-2 PE
        q1_mf[a1] += alpha * lambda_eff * pe2

    nll = -(np.sum(np.log(p_choice_1)) + np.sum(np.log(p_choice_2)))
    return nll