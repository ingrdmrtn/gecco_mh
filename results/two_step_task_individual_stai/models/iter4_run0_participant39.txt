def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman hybrid RL with anxiety-modulated uncertainty and temperature scaling.

    The agent maintains Gaussian beliefs (mean and variance) over second-stage
    values for each alien and updates them using a Kalman filter. Model-based
    values at the first stage are computed from the current second-stage value
    means. Uncertainty reduces second-stage choice precision and anxiety inflates
    perceived process/measurement noise, further lowering effective precision.
    Planning weight is reduced by anxiety; a perseveration bias is present at
    stage 1.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), reached second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array with scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters
        beta: [0,10] base inverse temperature for both stages
        proc_base: [0,1] baseline process noise (diffusion) for second-stage values
        mix_mb_base: [0,1] baseline planning weight at stage 1
        stick1: [0,1] perseveration strength at stage 1
        unc_temp_base: [0,1] strength of uncertainty-based softmax temperature reduction at stage 2

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    beta, proc_base, mix_mb_base, stick1, unc_temp_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure (A->X, U->Y common)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Second-stage value means and variances (Kalman)
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.25  # initial uncertainty

    # First-stage model-free values (kept minimal; used only if stickiness requires baseline)
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Process noise increases with anxiety (more perceived volatility)
    tau = (0.001 + 0.499 * proc_base) * (1.0 + stai0)
    # Measurement noise inflated by anxiety
    sigma2 = 0.20 + 0.30 * stai0  # within reasonable range for Bernoulli rewards

    # Planning weight reduced by anxiety
    w_mb = np.clip(mix_mb_base * (1.0 - 0.5 * stai0), 0.0, 1.0)
    # Perseveration increased by anxiety
    kappa = stick1 * (1.0 + stai0)

    prev_a1 = None
    eps = 1e-12

    for t in range(n_trials):
        # Diffusion: increase uncertainty for all second-stage actions
        q2_var += tau

        # Model-based Q at stage 1 from current q2 means
        max_q2 = np.max(q2_mean, axis=1)  # size 2
        q1_mb = transition_matrix @ max_q2

        # Combine with stage-1 MF (kept at zero baseline) and stickiness
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        if prev_a1 is not None:
            stick = np.zeros(2); stick[prev_a1] = 1.0
            q1 = q1 + kappa * stick

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with uncertainty-modulated temperature
        s2 = state[t]
        # Use average uncertainty in current state to scale temperature
        u_state = np.mean(q2_var[s2])
        u_norm = u_state / (u_state + sigma2 + eps)  # in [0,1)
        beta2 = beta * max(1e-3, 1.0 - unc_temp_base * u_norm * (1.0 + stai0))

        q2 = q2_mean[s2]
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta2 * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and Kalman update for visited (s2, a2)
        r = reward[t]
        # Prior variance already diffused by tau above
        v_prior = q2_var[s2, a2]
        K = v_prior / (v_prior + sigma2 + eps)
        pe = r - q2_mean[s2, a2]
        q2_mean[s2, a2] = q2_mean[s2, a2] + K * pe
        q2_var[s2, a2] = (1.0 - K) * v_prior

        # Optional minimal MF bootstrapping for stage-1 (helps identifiability)
        q1_target = q2_mean[s2, a2]
        q1_mf[a1] += 0.05 * (q1_target - q1_mf[a1])  # tiny fixed-rate update

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Dual learning rates with eligibility trace and anxiety-modulated sensitivity to rare transitions.

    The agent learns second-stage values with asymmetric learning rates for
    positive vs negative prediction errors. Stage-1 values are updated via an
    eligibility trace that propagates second-stage prediction errors backward.
    First-stage choices are a hybrid of model-based and model-free values; the
    planning weight is modulated by whether the transition was rare vs common,
    and anxiety reduces the rare-transition upweighting.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), reached second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array with scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters
        alpha_pos: [0,1] learning rate for positive PEs at stage 2
        alpha_neg: [0,1] learning rate for negative PEs at stage 2
        beta:      [0,10] inverse temperature for both stages
        w_mb_base: [0,1] baseline planning weight (before transition- and anxiety-modulation)
        lambda_elig: [0,1] eligibility trace factor for propagating PE2 to stage 1

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, w_mb_base, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Determine if observed transition was rare
        a1 = action_1[t]
        s2 = state[t]
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        rare = 0 if is_common else 1

        # Planning weight with rare-transition gain that is dampened by anxiety
        # If rare==1, weight shifts by factor proportional to (1 - 2*stai): low anxiety upweights, high anxiety downweights
        rare_gain = 1.0 + 0.6 * (1.0 - 2.0 * stai0) * rare
        w_mb = np.clip(w_mb_base * rare_gain, 0.0, 1.0)

        # Hybrid stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2 = q2_mf[s2]
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2_mf[s2, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2_mf[s2, a2] += alpha2 * pe2

        # Eligibility-trace update at stage 1 (propagate stage-2 PE back)
        # Anxiety reduces effective trace (shallower backpropagation)
        lam_eff = lambda_elig * (1.0 - 0.5 * stai0)
        q1_mf[a1] += lam_eff * alpha2 * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Controllability-belief hybrid RL with anxiety-modulated lapse and strategic common-ship bias.

    The agent learns both reward values and an internal belief about transition
    controllability (how often chosen ships go to their common planets). The
    controllability belief scales the contribution of model-based values at the
    first stage. Anxiety reduces perceived controllability and increases a lapse
    probability (random responding). A strategic bias favors the ship that more
    commonly reaches the currently more valuable planet; anxiety attenuates this bias.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), reached second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward:   np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai:     np.array with scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters
        alpha: [0,1] learning rate for second-stage values and controllability belief
        beta_base: [0,10] base inverse temperature for both stages
        ctrl_base: [0,1] baseline scaling for model-based control via controllability belief
        lapse_base: [0,1] base lapse probability coefficient
        bias_common_base: [0,1] strength of strategic bias toward the ship leading to the better planet

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta_base, ctrl_base, lapse_base, bias_common_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # second-stage MF values
    q1_mf = np.zeros(2)    # optional MF at stage 1 (kept minimal via bootstrap)

    # Controllability belief phi: tracks probability that chosen ship goes to its common planet
    phi = 0.7  # prior consistent with task

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based component from current q2
        max_q2 = np.max(q2, axis=1)  # value of X and Y
        q1_mb = transition_matrix @ max_q2

        # Anxiety reduces perceived controllability; ctrl scales MB contribution
        ctrl_eff = np.clip(ctrl_base * phi * (1.0 - stai0), 0.0, 1.0)

        # Strategic common-ship bias: favor ship that tends to reach the better planet
        # If planet X currently looks better, bias toward ship A; else toward ship U.
        better_X = max_q2[0] >= max_q2[1]
        bias_vec = np.array([1.0, 0.0]) if better_X else np.array([0.0, 1.0])
        bias_strength = bias_common_base * (1.0 - stai0)  # anxiety attenuates strategy use

        # Combine values
        q1 = ctrl_eff * q1_mb + (1.0 - ctrl_eff) * q1_mf + bias_strength * bias_vec

        # Policies with lapse; anxiety increases lapse
        beta = beta_base
        lapse = np.clip(lapse_base * stai0, 0.0, 1.0)

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        soft1 = np.exp(beta * q1c)
        soft1 = soft1 / (np.sum(soft1) + eps)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (with same beta; no lapse at stage 2 to keep identifiability clean)
        s2 = state[t]
        q2c = q2[s2] - np.max(q2[s2])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2
        # Minimal bootstrap to stage-1 MF
        q1_mf[a1] += 0.1 * alpha * (q2[s2, a2] - q1_mf[a1])

        # Update controllability belief phi: did we see a common transition?
        is_common = (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)
        obs = 1.0 if is_common else 0.0
        phi += alpha * (obs - phi)
        # Keep phi within [0,1]
        phi = min(1.0, max(0.0, phi))

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll