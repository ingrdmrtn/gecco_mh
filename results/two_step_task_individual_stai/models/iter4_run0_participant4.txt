def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated hybrid MB/MF with anxiety-scaled arbitration and choice kernels.

    Overview
    - Stage-2 values learned via Rescorla–Wagner.
    - Stage-1 choice values are a hybrid of model-based (using fixed transitions) and model-free.
    - The MB/MF weight increases with second-stage uncertainty (entropy), scaled by anxiety (stai).
    - Choice kernels (perseveration) bias both stages toward previously chosen actions.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]       Learning rate for Q updates at both stages.
    - beta:  [0,10]      Inverse temperature for softmax.
    - w0: [0,1]          Baseline weight on model-based planning at stage 1.
    - xi_anx: [0,1]      Strength with which anxiety scales uncertainty-driven MB weight.
    - kappa: [0,1]       Choice-kernel learning rate/strength for perseveration on both stages.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, w0, xi_anx, kappa].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, w0, xi_anx, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],   # from A to [X, Y]
                  [0.3, 0.7]])  # from U to [X, Y]

    # Values
    q1_mf = np.zeros(2) + 0.5           # stage-1 model-free
    q2 = np.zeros((2, 2)) + 0.5         # stage-2 Q(s,a)

    # Choice kernels (perseveration)
    ck1 = np.zeros(2)
    ck2 = np.zeros((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Function: entropy in [0, log 2]; normalize to [0,1]
    def norm_entropy(probs):
        p = np.clip(probs, 1e-8, 1.0)
        H = -np.sum(p * np.log(p))
        return H / np.log(2.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute model-based Q for stage 1 using current stage-2 values
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Uncertainty proxy: average entropy over planets of the softmax over aliens
        logitsX = beta * q2[0]
        logitsY = beta * q2[1]
        softX = np.exp(logitsX - np.max(logitsX))
        softX = softX / np.sum(softX)
        softY = np.exp(logitsY - np.max(logitsY))
        softY = softY / np.sum(softY)
        U = 0.5 * (norm_entropy(softX) + norm_entropy(softY))  # in [0,1]

        # Anxiety-scaled arbitration weight for MB control (clip to [0,1])
        w_t = w0 + xi_anx * stai * U
        w_t = 0.0 if w_t < 0.0 else (1.0 if w_t > 1.0 else w_t)

        # Hybrid Q for stage 1 plus perseveration kernel
        q1_hyb = w_t * q1_mb + (1.0 - w_t) * q1_mf
        logits1 = beta * q1_hyb + ck1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy with perseveration kernel
        logits2 = beta * q2[s] + ck2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Learning: stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Propagate to stage-1 MF via TD(1) from obtained state value
        # Use the obtained state's chosen action value as target
        q1_mf[a1] += alpha * pe2

        # Update choice kernels (perseveration)
        ck1 = (1.0 - kappa) * ck1
        ck1[a1] += kappa
        ck2[s] = (1.0 - kappa) * ck2[s]
        ck2[s, a2] += kappa

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-gated eligibility traces with rare-transition sensitivity and lapse.

    Overview
    - Stage-2 learning via Rescorla–Wagner.
    - Stage-1 model-free credit assignment uses an eligibility trace λ_t.
    - λ_t increases on rare transitions, and this increase scales with anxiety (stai).
    - Includes a small lapse probability shared across stages.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]        Learning rate for Q updates.
    - beta:  [0,10]       Inverse temperature for softmax.
    - lambda0: [0,1]      Baseline eligibility for propagating PE2 to stage-1 values.
    - eta_anx: [0,1]      Strength with which anxiety boosts λ on rare transitions.
    - rho_lapse: [0,1]    Lapse probability added to both stages (uniform choice).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, lambda0, eta_anx, rho_lapse].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, lambda0, eta_anx, rho_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Determine rare transition given the task structure
    def is_rare(a1, s):
        # A(0)->X(0) common, A->Y rare; U(1)->Y(1) common, U->X rare
        if a1 == 0:
            return 1 if s == 1 else 0
        else:
            return 1 if s == 0 else 0

    # Values
    q1 = np.zeros(2) + 0.5
    q2 = np.zeros((2, 2)) + 0.5

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Policies with lapse
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - rho_lapse) * soft1 + rho_lapse * 0.5
        p1[t] = probs1[a1]

        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - rho_lapse) * soft2 + rho_lapse * 0.5
        p2[t] = probs2[a2]

        # Learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Anxiety-gated eligibility increases on rare transitions
        rare = is_rare(a1, s)
        lam_t = lambda0 + eta_anx * stai * rare
        lam_t = 0.0 if lam_t < 0.0 else (1.0 if lam_t > 1.0 else lam_t)

        # Propagate stage-2 PE to stage-1 chosen action
        q1[a1] += alpha * lam_t * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Online transition learning with anxiety-biased uncertainty and stickiness.

    Overview
    - Learns the transition matrix online from observed transitions.
    - Stage-1 uses model-based values computed from learned transitions.
    - Anxiety inflates perceived transition uncertainty by blending learned transitions
      toward uniform, reducing planning precision.
    - Stage-2 values learned via Rescorla–Wagner.
    - Includes stickiness at stage 1 to capture perseveration.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha: [0,1]      Learning rate for Q updates at stage 2 (and bootstrapping).
    - beta:  [0,10]     Inverse temperature for softmax.
    - tau_T: [0,1]      Learning rate for updating transition probabilities.
    - zeta_pess: [0,1]  Strength of anxiety-driven pessimistic/uncertain blending of transitions.
    - xi_stick: [0,1]   Stickiness added to the previously chosen stage-1 action.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, tau_T, zeta_pess, xi_stick].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, tau_T, zeta_pess, xi_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition probabilities close to common/rare
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Stage values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2) + 0.5  # MF backup for stage-1 (from stage-2 PE)

    # Stickiness (only stage 1)
    prev_a1 = None

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Anxiety-biased transition blending toward uniform (more uncertainty with higher stai)
        blend = zeta_pess * stai  # in [0,1]
        T_eff = (1.0 - blend) * T_hat + blend * 0.5  # 0.5 for both outcomes

        # Model-based Q for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        # Add MF component (simple hybrid 50-50 for robustness)
        q1_h = 0.5 * q1_mb + 0.5 * q1_mf

        # Add stickiness to previously chosen action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += xi_stick

        logits1 = beta * q1_h + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Learning: stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # MF backup at stage 1 from stage-2 PE
        q1_mf[a1] += alpha * pe2

        # Transition learning: update row a1 toward observed state s
        onehot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_hat[a1] = (1.0 - tau_T) * T_hat[a1] + tau_T * onehot
        # Ensure normalization (numerical stability)
        T_row_sum = np.sum(T_hat[a1])
        if T_row_sum > 0:
            T_hat[a1] /= T_row_sum

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)