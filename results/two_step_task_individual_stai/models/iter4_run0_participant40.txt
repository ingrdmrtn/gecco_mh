def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-weighted arbitration and stage-2 stickiness.
    
    The agent combines a model-based (MB) estimate using the known transition
    structure with a model-free (MF) estimate learned from experience. Anxiety
    down-weights the MB contribution. Stage-2 choices include a stickiness bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (0/1 alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, chi, kappa2, nu)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - chi in [0,1]: baseline weight on model-based control at stage 1.
        - kappa2 in [0,1]: stage-2 stickiness strength within a planet.
        - nu in [0,1]: anxiety sensitivity that reduces MB weight.
          Effective MB weight: omega = chi * (1 - nu*stai).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, chi, kappa2, nu = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed, known transition structure (rows: A/U; cols: X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of the observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))         # stage-2 state-action values
    Q1_MF = np.zeros(2)           # stage-1 model-free values

    # Stage-2 stickiness: separate previous action per planet
    prev_a2 = [None, None]

    # Effective MB weight shaped by anxiety
    omega = chi * (1.0 - nu * stai_val)
    if omega < 0.0:
        omega = 0.0
    if omega > 1.0:
        omega = 1.0

    for t in range(n_trials):

        # Model-based stage-1 values: expectation over next-state max Q2
        max_Q2 = np.max(Q2, axis=1)          # per state
        Q1_MB = T @ max_Q2                   # per action

        # Hybrid value for stage-1
        Q1_hyb = omega * Q1_MB + (1.0 - omega) * Q1_MF

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_hyb
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (with within-state stickiness)
        s2 = int(state[t])
        a2 = int(action_2[t])

        bias2 = np.zeros(2)
        if prev_a2[s2] is not None:
            bias2[prev_a2[s2]] = kappa2

        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Propagate MF credit to stage-1 chosen action (bootstrapped by observed stage-2 value)
        # This assigns credit to the chosen first-stage action in proportion to the realized value at stage 2.
        target_Q1 = Q2[s2, a2]
        Q1_MF[a1] += alpha2 * (target_Q1 - Q1_MF[a1])

        # Update stickiness memory
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-free learner with anxiety-enhanced loss aversion and stage-1 perseveration.
    
    The agent learns purely model-free values but transforms rewards through a
    risk-sensitive utility. Anxiety increases loss aversion. A stage-1
    perseveration bias encourages repeating the previous spaceship.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (0/1 alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, lambda_loss, gamma_u, kappa1)
        - alpha2 in [0,1]: learning rate for stage-2 and propagated stage-1 MF values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - lambda_loss in [0,1]: baseline loss aversion factor.
        - gamma_u in [0,1]: utility curvature; values <1 compress magnitudes.
        - kappa1 in [0,1]: first-stage perseveration strength.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, lambda_loss, gamma_u, kappa1 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Probabilities of the observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))
    Q1 = np.zeros(2)

    # Perseveration memory for stage 1
    prev_a1 = None

    # Utility transformation parameters (anxiety increases loss weighting)
    # Effective loss aversion multiplier: lambda_eff >= lambda_loss
    lambda_eff = lambda_loss * (1.0 + stai_val)

    def utility(x):
        if x >= 0.0:
            # concave for gains
            return (x ** gamma_u) if x >= 0 else 0.0
        else:
            # steeper for losses with anxiety amplification
            return -lambda_eff * ((-x) ** gamma_u)

    for t in range(n_trials):

        # Stage-1 policy with perseveration bias
        a1 = int(action_1[t])
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = kappa1

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (pure MF)
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning with risk-sensitive utility
        r = reward[t]
        u = utility(r)

        # Stage-2 update toward utility
        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 model-free update bootstrapped by realized stage-2 value
        target_Q1 = Q2[s2, a2]
        Q1[a1] += alpha2 * (target_Q1 - Q1[a1])

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MB values plus a reward-by-transition choice kernel at stage 1 modulated by anxiety.
    
    Stage-1 decisions combine model-based values (from the known transition
    structure) with an additive decision kernel that captures the classic
    reward x transition interaction: after common transitions, rewards promote
    repeating the same spaceship; after rare transitions, rewards discourage
    repeating. Anxiety amplifies the rare-transition effect and dampens the
    common-transition effect. Stage-2 values are learned model-free.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (0/1 alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, alphaK, thetaC, thetaR)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax.
        - alphaK in [0,1]: learning rate for the decision kernel K at stage 1.
        - thetaC in [0,1]: base kernel magnitude after common transitions.
        - thetaR in [0,1]: base kernel magnitude after rare transitions.
          Anxiety modulation: thetaC_eff = thetaC*(1 - stai), thetaR_eff = thetaR*(1 + stai).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha2, beta, alphaK, thetaC, thetaR = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure for MB component
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of the observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 values
    Q2 = np.zeros((2, 2))

    # Decision kernel over stage-1 actions (encodes reward x transition history)
    K = np.zeros(2)

    # Helper to determine whether the observed transition was common or rare
    def is_common(a1, s2):
        # A -> X common (0->0), U -> Y common (1->1)
        return int(a1) == int(s2)

    for t in range(n_trials):

        # Model-based Q1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Stage-1 policy: MB values plus kernel biases
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB + K
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= np.sum(p1)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy (MF)
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= np.sum(p2)
        p_choice_2[t] = p2[a2]

        # Learning stage-2 values
        r = reward[t]
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update the stage-1 decision kernel
        # Sign of the update driven by reward (positive encourages, negative discourages)
        rew_sign = 0.0
        if r > 0:
            rew_sign = 1.0
        elif r < 0:
            rew_sign = -1.0

        common = is_common(a1, s2)
        thetaC_eff = thetaC * (1.0 - stai_val)   # anxiety dampens common effect
        thetaR_eff = thetaR * (1.0 + stai_val)   # anxiety amplifies rare effect
        mag = thetaC_eff if common else thetaR_eff

        # Decay kernel towards zero to prevent unbounded growth
        K *= (1.0 - alphaK)

        # Apply update to the chosen action; opposite bias to the unchosen action
        # After common: rewarded -> repeat (positive bias to chosen)
        # After rare: rewarded -> switch (negative bias to chosen)
        signed_mag = mag * rew_sign * (1.0 if common else -1.0)
        K[a1] += alphaK * signed_mag
        K[1 - a1] -= alphaK * signed_mag  # symmetric counter-bias

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll