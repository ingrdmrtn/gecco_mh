def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-gated transition-surprise arbitration (common vs. rare transition dependent MB weight).

    Idea:
    - Stage 1 uses a hybrid of model-based (MB) and model-free (MF) values.
    - The arbitration weight depends on whether the previous transition was common or rare.
      After common transitions, participants tend to rely more on MB control; after rare
      transitions, they rely more on MF control. Anxiety accentuates this asymmetry by
      boosting MF reliance after rare transitions and dampening MB reliance after rare transitions.
    - Stage 2 is standard MF with learning rate alpha.

    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the planet).
    reward : array-like of float in [0,1]
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score. Higher values amplify the gap between rare- vs common-based arbitration
        by further reducing MB weight after rare transitions.
    model_parameters : array-like of float
        [alpha, beta, wC0, wR0]
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - wC0 in [0,1]: baseline MB weight used after common transitions.
        - wR0 in [0,1]: baseline MB weight used after rare transitions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, wC0, wR0 = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Fixed transition structure of the task
    T = np.array([[0.7, 0.3],  # action A -> X common, Y rare
                  [0.3, 0.7]]) # action U -> X rare,   Y common

    # Value functions
    q1_mf = np.zeros(2)         # MF values for first-stage actions
    q2 = 0.5 * np.ones((2, 2))  # Second-stage Q-values

    # Anxiety-modulated arbitration weights:
    # Encourage MB more after common transitions (slightly boosted when anxiety is low)
    wC = np.clip(wC0 + (1.0 - st) * 0.2 * (1.0 - wC0), 0.0, 1.0)
    # Discourage MB after rare transitions, especially when anxiety is high
    wR = np.clip(wR0 - st * 0.4 * wR0, 0.0, 1.0)

    eps = 1e-12
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    # Track previous transition type: start neutral (use average of wC/wR)
    prev_common = None

    for t in range(n):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation for current trial (before observing current transition)
        max_q2 = np.max(q2, axis=1)  # best option at each planet
        q1_mb = T @ max_q2

        # Arbitration weight for this decision based on previous transition type
        if prev_common is None:
            w_t = 0.5 * (wC + wR)
        else:
            w_t = wC if prev_common else wR

        q1 = w_t * q1_mb + (1.0 - w_t) * q1_mf

        # Stage 1 choice probability
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 choice probability
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Determine whether the observed transition was common or rare for the chosen action
        # Common if (A->X) or (U->Y)
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

        # Learning
        # Stage 2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward realized second-stage value (SARSA-like bootstrapping)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_common = is_common

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-attenuated intrinsic exploration with count-based bonus and eligibility.

    Idea:
    - Stage 2 choices include an intrinsic, count-based exploration bonus that diminishes
      with experience (1/sqrt(N)), encouraging trying less-sampled aliens.
    - Anxiety reduces the exploration bonus (more anxious -> less intrinsic exploration).
    - Stage 1 is MF but uses an eligibility trace to propagate stage-2 prediction error
      back to stage-1; the effective trace is dampened by anxiety (weaker credit assignment).
    - Softmax uses a single beta across stages.

    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the planet).
    reward : array-like of float in [0,1]
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce the exploration bonus and the eligibility trace.
    model_parameters : array-like of float
        [alpha, beta, eta0, trace1]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for softmax.
        - eta0 in [0,1]: base scale of the intrinsic exploration bonus.
        - trace1 in [0,1]: base eligibility trace to backpropagate PE2 to stage-1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, eta0, trace1 = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Fixed transitions for computing model-based target if needed (not used in policy here)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Counts for intrinsic bonus
    N = np.ones((2, 2))  # start at 1 to avoid division by zero and to moderate early bonus

    # Anxiety effects
    eta_eff = np.clip(eta0 * (1.0 - st), 0.0, 1.0)           # less exploration when anxious
    lam_eff = np.clip(trace1 * (1.0 - 0.5 * st), 0.0, 1.0)   # reduced eligibility when anxious

    eps = 1e-12
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    for t in range(n):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Intrinsic exploration bonus at stage 2 (state-dependent, action-dependent)
        bonus2 = eta_eff / np.sqrt(N[s])
        q2_aug = q2[s] + bonus2

        # Stage 1 policy: MF values (no direct MB in policy, but learning uses eligibility)
        logits1 = beta * (q1_mf - np.max(q1_mf))
        probs1 = np.exp(logits1); probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 policy with intrinsic bonus
        logits2 = beta * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2); probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Update counts for visited second-stage pair (after using it for policy)
        N[s, a2] += 1.0

        # Learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility-style backpropagation of PE2 to stage 1 action value
        q1_mf[a1] += alpha * lam_eff * pe2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-driven loss aversion and transition-reactive temperature with hybrid control.

    Idea:
    - Subjective utility is asymmetric: 0 outcomes are treated as aversive relative to 1.
      Anxiety increases this loss aversion (penalizes zero-reward outcomes more).
    - Softmax temperature drops after rare transitions, and this effect is stronger with anxiety
      (more randomness or caution after unexpected transitions).
    - Stage 1 policy is a hybrid of MB and MF, with MB weight determined by anxiety alone
      (lower anxiety -> more MB). Stage 2 is MF with the same alpha.

    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the planet).
    reward : array-like of float in [0,1]
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase loss aversion and increase temperature sensitivity
        to rare transitions.
    model_parameters : array-like of float
        [alpha, beta, phi0, kappa0]
        - alpha in [0,1]: learning rate for MF updates.
        - beta in [0,10]: baseline inverse temperature.
        - phi0 in [0,1]: base loss aversion (penalty weight on zero reward).
        - kappa0 in [0,1]: base sensitivity of beta to rare transitions (reduces beta after rare).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, phi0, kappa0 = model_parameters
    n = len(action_1)
    st = float(stai[0])

    # Transition matrix (known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Anxiety effects
    # Loss aversion applied to learning signal: u(r) = r - phi*(1-r)
    phi = np.clip(phi0 * (0.5 + 0.5 * st), 0.0, 1.0)
    # MB weight governed by anxiety: less anxious -> more MB
    w_mb = np.clip(0.5 * (1.0 - st) + 0.1, 0.0, 1.0)
    # Beta reduction after rare transitions, increasing with anxiety
    kappa = np.clip(kappa0 * (0.5 + 0.5 * st), 0.0, 1.0)

    eps = 1e-12
    p1 = np.zeros(n)
    p2 = np.zeros(n)

    prev_rare = False  # for temperature adjustment based on previous trial's transition

    for t in range(n):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Subjective utility
        # r in {0,1} -> u(r) = 1 if r=1; u(0) = -phi (aversive)
        u = r - phi * (1.0 - r)

        # Compute MB estimate for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Adjust temperature if the previous transition was rare
        beta_eff = beta * (1.0 - kappa * (1.0 if prev_rare else 0.0))
        beta_eff = np.clip(beta_eff, 0.0, 10.0)

        # Stage 1 choice probability
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1); probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 choice probability
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2); probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Determine whether current transition is rare for chosen action
        is_rare = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)

        # Learning with subjective utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward realized stage-2 value (using subjective utility target)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        prev_rare = is_rare

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)