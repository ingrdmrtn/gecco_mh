def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated risk sensitivity (utility curvature and loss aversion).

    Overview:
    - Stage-2 values are learned on a utility-transformed reward, capturing risk/loss sensitivity.
    - Stage-1 action values are a mixture of model-based (known transitions) and model-free values.
    - Anxiety (stai) increases loss aversion and increases concavity (more risk-averse), affecting
      both learning and decision values via the utility transform. Planning weight is fixed.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1 (0 = pure MF, 1 = pure MB).
    - risk0: [0,1] baseline utility curvature; higher -> more linear. Used to set exponent xi.
    - k_anx: [0,1] strength with which anxiety increases loss aversion and concavity.
      Effective xi decreases with stai*k_anx; effective loss aversion lambda increases with stai*k_anx.

    Inputs:
    - action_1: array of ints {0,1}, first-stage choices.
    - state: array of ints {0,1}, observed second-stage state.
    - action_2: array of ints {0,1}, second-stage choices.
    - reward: array of floats, obtained coins (can be negative).
    - stai: array-like with a single float in [0,1], trait anxiety score.
    - model_parameters: tuple/list of 5 params (alpha, beta, w, risk0, k_anx).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, risk0, k_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure: A->X common, U->Y common
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Q-values
    Q2 = np.zeros((2, 2))   # stage-2 Q(s, a2) in utility space
    Q1_mf = np.zeros(2)     # stage-1 MF values

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated utility parameters
    # Utility exponent xi in (0,1]; more concave with higher stai*k_anx
    xi_eff = 0.5 + 0.5 * max(0.0, min(1.0, (1.0 - stai * k_anx) * risk0))
    xi_eff = max(1e-3, min(1.0, xi_eff))
    # Loss aversion lambda >= 1; increases with stai*k_anx
    lam_loss = 1.0 + 4.0 * (stai * k_anx)

    def u_transform(r):
        # Power utility with loss aversion
        if r >= 0.0:
            return (abs(r) ** xi_eff)
        else:
            return -lam_loss * (abs(r) ** xi_eff)

    eps = 1e-12

    for t in range(n_trials):
        # Model-based stage-1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage-1 policy
        q1 = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2 = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Learning with utility-transformed reward
        r = reward[t]
        u = u_transform(r)

        # Stage-2 TD update in utility space
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstraps from current stage-2 value (in utility space)
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with directed exploration bonus modulated by anxiety and novelty.

    Overview:
    - Stage-2 values are learned via TD(0).
    - Both stages include a directed exploration bonus that decays with visit count (novelty seeking).
    - Anxiety reduces novelty seeking: higher stai dampens the exploration bonus.
    - Stage-1 decisions are a mixture of MB (from Q2) and MF values.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta: [0,10] inverse temperature for both stages.
    - w_mb: [0,1] weight on model-based control at stage 1.
    - b0: [0,1] baseline directed exploration strength.
    - k_anx: [0,1] degree to which anxiety reduces exploration; bonus *= (1 - k_anx*stai).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choices.
    - state: array of ints {0,1}, observed second-stage state.
    - action_2: array of ints {0,1}, second-stage choices.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta, w_mb, b0, k_anx).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_mb, b0, k_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Q-values and visit counters for bonuses
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)
    N1 = np.ones(2)       # initialize to 1 to avoid huge bonuses on first trial
    N2 = np.ones((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective exploration scaling given anxiety
    bonus_scale = b0 * max(0.0, 1.0 - k_anx * stai)

    eps = 1e-12

    for t in range(n_trials):
        # Compute MB values from Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Directed exploration bonuses (decay with visits)
        B1 = bonus_scale / np.sqrt(N1)
        B2 = bonus_scale / np.sqrt(N2)

        # Stage-1 combined values with bonus
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf + B1

        # Stage-1 policy
        q1 = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with bonus
        s = state[t]
        q2v = (Q2[s] + B2[s]) - np.max(Q2[s] + B2[s])
        probs_2 = np.exp(beta * q2v)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Update visit counts post choice (counts drive next-trial bonuses)
        N1[a1] += 1.0
        N2[s, a2] += 1.0

        # Learning
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated replay/generalization across unchosen actions.

    Overview:
    - Stage-2 values are learned via TD(0).
    - Additionally, after outcome, values of unchosen actions are updated via a replay/generalization
      process within the same state, controlled by g_eff.
    - Stage-1 MF values also receive a replay-like update towards the obtained stage-2 value for the
      unchosen first-stage action, scaled by g_eff (simulating counterfactual/replay updating).
    - Anxiety increases replay/generalization strength (more mental simulation when anxious).

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for direct (chosen) value updates.
    - beta: [0,10] inverse temperature for both stages.
    - w_mb: [0,1] weight on model-based control at stage 1.
    - g_base: [0,1] baseline replay/generalization strength.
    - k_g: [0,1] increase of replay with anxiety; g_eff = clip(g_base + k_g*stai, 0, 1).

    Inputs:
    - action_1: array of ints {0,1}, first-stage choices.
    - state: array of ints {0,1}, observed second-stage state.
    - action_2: array of ints {0,1}, second-stage choices.
    - reward: array of floats, obtained coins.
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta, w_mb, g_base, k_g).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_mb, g_base, k_g = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions (known)
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Q-values
    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    g_eff = max(0.0, min(1.0, g_base + k_g * stai))
    eps = 1e-12

    for t in range(n_trials):
        # Model-based at stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        q1 = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2v = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2v)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Standard TD update for chosen stage-2 action
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Replay/generalization: update unchosen stage-2 action within same state towards the same outcome
        a2_other = 1 - a2
        delta2_other = r - Q2[s, a2_other]
        Q2[s, a2_other] += alpha * g_eff * delta2_other

        # Stage-1 MF update for chosen action
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Replay-like update for unchosen first-stage action towards the obtained bootstrapped value
        a1_other = 1 - a1
        delta1_other = boot - Q1_mf[a1_other]
        Q1_mf[a1_other] += alpha * g_eff * delta1_other

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll