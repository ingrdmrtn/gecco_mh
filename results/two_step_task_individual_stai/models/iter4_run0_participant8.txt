def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free controller with anxiety-gated arbitration, 
    eligibility trace from stage 2 to stage 1, and anxiety-weighted confirmation bias.
    
    Summary
    -------
    - Learns second-stage action values (Q2) via a delta rule.
    - Maintains stage-1 model-free values (Q1_MF) updated via an eligibility trace from Q2.
    - Computes a model-based (MB) plan for stage 1 via the fixed transition matrix.
    - Arbitration weight between MB and MF is a logistic function of a baseline (arb0) 
      and the participant's anxiety (stai). Higher anxiety shifts arbitration toward MF.
    - Confirmation bias in learning at stage 2: positive and negative PEs are weighted
      differently as a function of anxiety and a bias strength parameter (conf).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet (0/1).
    reward : array-like of float
        Trial outcomes (0.0 or 1.0 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [lr, beta, arb0, lam, conf]
        - lr: base learning rate for value updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - arb0: baseline arbitration bias (positive favors MB); combined with stai [0,1]
        - lam: eligibility trace strength from stage 2 to stage 1 [0,1]
        - conf: confirmation bias strength (weights pos vs neg PE) [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, arb0, lam, conf = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (A -> X common, U -> Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Storage for likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values
    Q2 = np.zeros((2, 2)) + 0.5  # state x action
    Q1_MF = np.zeros(2) + 0.0

    # Anxiety-gated arbitration: logistic on (arb0 - k*stai)
    # Higher anxiety reduces MB weight by shifting the logit negative.
    k_anx = 2.0  # fixed sensitivity to anxiety
    w_mb = 1.0 / (1.0 + np.exp(-(arb0 - k_anx * st)))  # in (0,1)

    for t in range(n_trials):
        # Model-based stage-1 Q as expected max Q2 under transitions
        max_Q2 = np.max(Q2, axis=1)  # length 2 (planet X, Y)
        Q1_MB = T @ max_Q2  # shape (2,)

        # Hybrid Q for stage 1
        Q1 = w_mb * Q1_MB + (1.0 - w_mb) * Q1_MF

        # Stage-1 softmax
        logits1 = Q1 - np.max(Q1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax (within reached state)
        s = state[t]
        logits2 = Q2[s] - np.max(Q2[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Confirmation bias: weight positive vs negative PE as a function of stai and conf
        pe2 = r - Q2[s, a2]
        pos = 1.0 if pe2 >= 0.0 else 0.0
        neg = 1.0 - pos
        # Effective learning rate for this PE:
        # positive PEs amplified by (1 + conf*st), negative PEs attenuated by (1 - conf*st)
        lr_eff = lr * (pos * (1.0 + conf * st) + neg * (1.0 - conf * st))
        lr_eff = min(max(lr_eff, 0.0), 1.0)
        Q2[s, a2] += lr_eff * pe2

        # Eligibility trace from stage 2 to stage 1 (model-free backup)
        td1 = Q2[s, a2] - Q1_MF[a1]
        Q1_MF[a1] += lam * lr * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with anxiety-modulated information bonus via uncertainty tracking.
    
    Summary
    -------
    - Learns second-stage Q-values (Q2) via delta rule.
    - Tracks action uncertainty U2 per planet-action via a leaky integrator of absolute PEs.
    - Adds an information-seeking bonus proportional to uncertainty when choosing at stage 2.
      The bonus strength decreases with anxiety (higher anxiety -> less directed exploration).
    - Stage 1 is purely model-based, rolling the bonus-adjusted Q2 through the known transitions.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0.0 or 1.0 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [lr, beta, info0, decay_u]
        - lr: learning rate for Q2 updates [0,1]
        - beta: inverse temperature for both stages [0,10]
        - info0: baseline information-bonus strength [0,1]
        - decay_u: uncertainty tracker update rate [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, info0, decay_u = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize value and uncertainty
    Q2 = np.zeros((2, 2)) + 0.5
    U2 = np.zeros((2, 2)) + 1.0  # start uncertain

    # Anxiety-modulated info bonus: higher anxiety -> smaller bonus
    k_info = info0 * (0.2 + 0.8 * (1.0 - st))  # in [0,1], shrinks with stai

    for t in range(n_trials):
        # Bonus-adjusted Q2 for both states
        Q2_bonus = Q2 + k_info * U2

        # Stage-1 MB planning using bonus-adjusted Q2
        max_Q2b = np.max(Q2_bonus, axis=1)
        Q1_MB = T @ max_Q2b

        # Stage-1 softmax
        logits1 = Q1_MB - np.max(Q1_MB)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with info bonus in the reached state
        s = state[t]
        logits2 = Q2_bonus[s] - np.max(Q2_bonus[s])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += lr * pe2

        # Uncertainty tracker: leaky integration of absolute PE
        U2[s, a2] = (1.0 - decay_u) * U2[s, a2] + decay_u * abs(pe2)
        # Keep uncertainty bounded to [0,1]
        U2[s, a2] = min(max(U2[s, a2], 0.0), 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Two-step SARSA(λ) with anxiety-modulated eligibility and temperature split,
    plus state-specific win-stay/lose-shift bias at stage 2.
    
    Summary
    -------
    - Purely model-free temporal-difference control across both stages.
    - Uses an eligibility trace (λ) to propagate stage-2 value to stage-1; λ decreases with anxiety.
    - Separate effective temperature at stage 1 vs stage 2: stage-1 temperature increases with anxiety 
      (noisier choices), while stage-2 uses the base beta.
    - Adds a state-specific WSLS bias at stage 2: tendency to repeat the last action in the same planet
      after a win and to switch after a loss; the lose-shift component strengthens with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = spaceship A, 1 = spaceship U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (0.0 or 1.0 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1].
    model_parameters : iterable of floats
        [lr, beta, lam0, wsls0, temp_split]
        - lr: learning rate for Q updates [0,1]
        - beta: base inverse temperature [0,10]
        - lam0: baseline eligibility trace parameter [0,1]
        - wsls0: baseline WSLS strength at stage 2 [0,1]
        - temp_split: strength of anxiety effect on stage-1 temperature [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, lam0, wsls0, temp_split = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values
    Q1 = np.zeros(2) + 0.0
    Q2 = np.zeros((2, 2)) + 0.5

    # State-specific previous action and reward (for WSLS)
    prev_a2 = [-1, -1]  # for states 0 and 1
    prev_r = [0.0, 0.0]

    # Anxiety-modulated parameters
    lam = lam0 * (1.0 - 0.5 * st)  # higher anxiety -> shorter credit assignment
    lam = min(max(lam, 0.0), 1.0)

    # Stage-1 inverse temperature decreases with anxiety (more noise)
    beta1 = beta * (0.5 + (1.0 - temp_split * st))  # in [0.5*beta, 1.5*beta] if temp_split<=1
    beta1 = max(beta1, 0.0)

    # WSLS: stronger lose-shift with anxiety; keep win-stay near baseline
    wsls_win = wsls0 * (1.0 - 0.3 * st)
    wsls_lose = wsls0 * (1.0 + 0.7 * st)

    for t in range(n_trials):
        # Stage 1 policy
        logits1 = Q1 - np.max(Q1)
        probs1 = np.exp(beta1 * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with WSLS bias in reached state
        s = state[t]
        base2 = Q2[s].copy()

        # Construct bias towards repeating/swapping previous action in this state
        bias2 = np.zeros(2)
        if prev_a2[s] in (0, 1):
            if prev_r[s] >= 0.5:
                # Win-stay: add bias to previous action
                bias2[prev_a2[s]] += wsls_win
            else:
                # Lose-shift: add bias to the alternative action
                bias2[1 - prev_a2[s]] += wsls_lose

        logits2 = (base2 + bias2) - np.max(base2 + bias2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # TD learning at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += lr * pe2

        # Backup to stage 1 via eligibility trace using current Q2 of chosen action
        td1 = Q2[s, a2] - Q1[a1]
        Q1[a1] += lr * lam * td1

        # Update WSLS memory for this state
        prev_a2[s] = a2
        prev_r[s] = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll