def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated associability with model-based planning, second-stage uncertainty bonus, and first-stage perseveration.

    Idea
    - Second-stage learning uses a dynamic associability (Pearceâ€“Hall style): the learning rate adapts to unsigned prediction error.
    - Anxiety scales associability upward, producing faster, more volatile learning when stai is high.
    - First-stage choices are model-based using the fixed 0.7/0.3 transition structure.
    - An uncertainty bonus at stage 2 (based on current associability) promotes exploration; its impact is attenuated or amplified by stai.
    - First-stage perseveration bias favors repeating the most recent first-stage choice.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices on the planet (0 or 1).
    - reward: array-like (n_trials,), obtained reward (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 6 parameters, all used:
        alpha0   : initial associability/learning-rate seed for second stage.
        beta     : inverse temperature for both stages [0,10].
        kappa    : associability update inertia (higher -> slower changes).
        xi_ucb   : weight on the second-stage uncertainty bonus (directed exploration).
        pers1    : first-stage perseveration bias to repeat last first-stage action.
        phi_stai : multiplicative scaling of associability/uncertainty by anxiety.

    Returns
    - Negative log-likelihood over the observed first- and second-stage choices.
    """
    alpha0, beta, kappa, xi_ucb, pers1, phi_stai = model_parameters

    # Clip parameters to bounds
    alpha0 = min(1.0, max(0.0, alpha0))
    beta = min(10.0, max(1e-6, beta))
    kappa = min(1.0, max(0.0, kappa))
    xi_ucb = min(1.0, max(0.0, xi_ucb))
    pers1 = min(1.0, max(0.0, pers1))
    phi_stai = min(1.0, max(0.0, phi_stai))

    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (A->X common; U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions and tracking
    q2 = np.zeros((2, 2))         # second-stage Q
    assoc = np.ones((2, 2)) * alpha0  # associability per state-action
    last_a1 = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage MB values from second-stage Q
        max_q2 = np.max(q2, axis=1)  # best action per state
        q1_mb = T @ max_q2
        q1 = q1_mb.copy()

        # Add first-stage perseveration (bias towards last chosen first-stage action)
        if last_a1 is not None:
            q1[last_a1] += pers1

        # Softmax for first-stage policy
        exp1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with uncertainty bonus based on associability
        s = state[t]
        # Uncertainty (associability) scaled by anxiety
        ucb_bonus = xi_ucb * assoc[s] * (1.0 + phi_stai * st)
        q2_aug = q2[s] + ucb_bonus
        exp2 = np.exp(beta * (q2_aug - np.max(q2_aug)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update second-stage Q via anxiety-scaled associability
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Update associability: A <- (1 - kappa) * A + kappa * |PE|
        assoc[s, a2] = (1.0 - kappa) * assoc[s, a2] + kappa * abs(pe2)
        # Effective learning rate with anxiety amplification
        alpha_eff = assoc[s, a2] * (1.0 + phi_stai * st)
        alpha_eff = min(1.0, max(0.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with Dirichlet transition learning and anxiety-shaped structural priors plus second-stage perseveration.

    Idea
    - Transition probabilities are learned with a simple Dirichlet count model per first-stage action.
      The prior strength is increased with anxiety, making high-anxiety agents more conservative in updating transitions.
    - First-stage action values are a convex combination of model-based (using learned transitions) and model-free values.
    - Model-free first-stage value learns from second-stage reward via a simple TD rule.
    - Second-stage perseveration biases repeating the last second-stage choice at the same state.
    - Reward is also transformed by a concavity parameter influenced by anxiety (soft utility sensitivity).

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), obtained reward (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 6 parameters, all used:
        eta_r   : second-stage learning rate for Q2 (and MF backprop to Q1).
        beta    : inverse temperature for both stages [0,10].
        omega0  : baseline arbitration weight on model-based control (0=MF, 1=MB).
        nu0     : base transition prior strength (mapped to total pseudo-counts).
        chi_anx : anxiety gain on prior strength (higher stai -> stronger prior -> slower transition adaptation).
        kappa2  : second-stage perseveration bias to repeat last stage-2 action within the same state.

    Returns
    - Negative log-likelihood over the observed first- and second-stage choices.
    """
    eta_r, beta, omega0, nu0, chi_anx, kappa2 = model_parameters

    # Clip parameters
    eta_r = min(1.0, max(0.0, eta_r))
    beta = min(10.0, max(1e-6, beta))
    omega0 = min(1.0, max(0.0, omega0))
    nu0 = min(1.0, max(0.0, nu0))
    chi_anx = min(1.0, max(0.0, chi_anx))
    kappa2 = min(1.0, max(0.0, kappa2))

    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transitions via Dirichlet priors per first-stage action.
    # Prior strength depends on nu0 and anxiety.
    prior_strength = 2.0 + 20.0 * nu0 * (1.0 + chi_anx * st)
    # Prior means reflect common transitions (0.7/0.3)
    prior_A = np.array([0.7, 0.3]) * prior_strength
    prior_U = np.array([0.3, 0.7]) * prior_strength
    counts = np.vstack([prior_A, prior_U])  # shape (2 actions, 2 states)

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    # Track last second-stage choice per state for perseveration
    last_a2 = np.array([-1, -1], dtype=int)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Utility sensitivity: transform reward with concavity shaped by anxiety
    # rho_eff in [0,1]; higher anxiety -> stronger concavity (diminished sensitivity)
    rho_eff = max(0.0, min(1.0, 1.0 - 0.5 * st))  # no extra parameter; ties anxiety to reward curvature

    for t in range(n_trials):
        # Expected transitions from current counts
        T_hat = counts / np.sum(counts, axis=1, keepdims=True)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Combine MB and MF for stage 1
        q1 = omega0 * q1_mb + (1.0 - omega0) * q1_mf

        # First-stage policy
        exp1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with perseveration
        s = state[t]
        q2_aug = q2[s].copy()
        if last_a2[s] != -1:
            q2_aug[last_a2[s]] += kappa2
        exp2 = np.exp(beta * (q2_aug - np.max(q2_aug)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and apply utility curvature modulated by anxiety
        r_raw = reward[t]
        r_util = (r_raw ** rho_eff)  # with 0/1 rewards, this reduces sensitivity to partial rewards if present

        # Update second-stage Q
        pe2 = r_util - q2[s, a2]
        q2[s, a2] += eta_r * pe2

        # Update model-free first-stage Q via TD(1) from realized second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta_r * td1

        # Update transition counts (Dirichlet) for the chosen first-stage action
        counts[a1, s] += 1.0

        # Update perseveration memory
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration via pseudo-count bonuses, with anxiety-dependent exploration/exploitation and first-stage choice kernel.

    Idea
    - Stage-2: add a directed exploration bonus b2 ~ phi_b2 / sqrt(N_s,a + 1) encouraging trying less-visited actions.
    - Stage-1: model-based values plus a directed exploration bonus based on transition uncertainty (via action visit counts),
      b1 ~ phi_b1 / sqrt(N_a + 1); the bonus is attenuated by anxiety (higher stai -> less directed exploration).
    - Anxiety also reduces effective beta (more random exploration): beta_eff = beta / (1 + xi_beta * stai).
    - First-stage choice kernel (perseveration) encourages repeating the last first-stage action.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), obtained reward (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 6 parameters, all used:
        alpha_r : second-stage learning rate for rewards.
        beta    : base inverse temperature [0,10].
        phi_b1  : magnitude of first-stage directed exploration bonus.
        phi_b2  : magnitude of second-stage directed exploration bonus.
        xi_beta : anxiety gain controlling softmax temperature (higher stai -> lower beta_eff).
        stick1  : first-stage perseveration bias to repeat last chosen ship.

    Returns
    - Negative log-likelihood over the observed first- and second-stage choices.
    """
    alpha_r, beta, phi_b1, phi_b2, xi_beta, stick1 = model_parameters

    # Clip parameters
    alpha_r = min(1.0, max(0.0, alpha_r))
    beta = min(10.0, max(1e-6, beta))
    phi_b1 = min(1.0, max(0.0, phi_b1))
    phi_b2 = min(1.0, max(0.0, phi_b2))
    xi_beta = min(1.0, max(0.0, xi_beta))
    stick1 = min(1.0, max(0.0, stick1))

    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and counts
    q2 = np.zeros((2, 2))
    counts_a1 = np.zeros(2)       # how often each first-stage action was chosen
    counts_s2 = np.zeros((2, 2))  # how often each (state, action2) was chosen

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):
        # Effective beta reduced by anxiety
        beta_eff = beta / (1.0 + xi_beta * st)
        beta_eff = min(10.0, max(1e-6, beta_eff))

        # Model-based first-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Directed exploration bonus for stage 1 (less bonus with higher anxiety)
        bonus1 = np.zeros(2)
        for a in (0, 1):
            bonus1[a] = phi_b1 * (1.0 - st) / np.sqrt(counts_a1[a] + 1.0)

        q1 = q1_mb + bonus1

        # First-stage perseveration
        if last_a1 is not None:
            q1[last_a1] += stick1

        # First-stage policy
        exp1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage directed exploration bonus
        s = state[t]
        bonus2 = np.zeros(2)
        for a2_idx in (0, 1):
            bonus2[a2_idx] = phi_b2 * (1.0 - st) / np.sqrt(counts_s2[s, a2_idx] + 1.0)

        q2_aug = q2[s] + bonus2

        exp2 = np.exp(beta_eff * (q2_aug - np.max(q2_aug)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update Q2
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update counts
        counts_a1[a1] += 1.0
        counts_s2[s, a2] += 1.0

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll