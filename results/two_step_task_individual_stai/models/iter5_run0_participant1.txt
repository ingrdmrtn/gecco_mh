def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-tuned exploration, eligibility trace, and UCB-style bonus.

    This model blends a model-based (MB) first-stage planner with a model-free (MF) first-stage value learned
    from second-stage prediction errors via an eligibility-like trace. Anxiety (stai) softens first-stage choices
    and increases directed exploration at the second stage via an uncertainty bonus.

    Parameters (bounds):
    - alpha: [0,1] learning rate for second-stage Q-values and MF first-stage updates via trace.
    - beta: [0,10] inverse temperature for softmax choices (base).
    - w_hyb: [0,1] weight of MB plan at stage 1; (1 - w_hyb) is MF contribution.
    - nu_anx: [0,1] strength by which anxiety reduces stage-1 beta: beta1_eff = beta * (1 - nu_anx*stai).
    - tau_ucb: [0,1] scale of UCB directed exploration bonus at stage 2; bonus = (tau_ucb*stai)/sqrt(visit_count+1).
    - eta_tr: [0,1] eligibility trace strength to backpropagate second-stage PE to first-stage MF value.

    Args:
        action_1: array-like, shape (T,), first-stage actions (0/1).
        state: array-like, shape (T,), reached second-stage states (0/1).
        action_2: array-like, shape (T,), second-stage actions within reached state (0/1).
        reward: array-like, shape (T,), rewards (0/1 or real in [0,1]).
        stai: array-like with one element in [0,1], participant anxiety score.
        model_parameters: iterable with parameters in the order above.

    Returns:
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w_hyb, nu_anx, tau_ucb, eta_tr = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (known to the participant)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    Q2 = np.zeros((2, 2))      # second-stage Q(s,a)
    Q1_mf = np.zeros(2)        # first-stage model-free value

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Visit counts for UCB at second stage
    N2 = np.zeros((2, 2))  # counts per (state, action)

    eps = 1e-12

    for t in range(n_trials):
        # Effective beta at stage 1 is softened by anxiety
        beta1_eff = beta * (1.0 - nu_anx * stai)
        beta1_eff = max(beta1_eff, 1e-3)
        beta2_eff = max(beta, 1e-3)

        # Model-based plan at stage 1: expected max second-stage value under T
        max_q2 = np.max(Q2, axis=1)               # shape (2,)
        Q1_mb = T @ max_q2                        # shape (2,)

        # Hybrid first-stage value
        Q1 = w_hyb * Q1_mb + (1.0 - w_hyb) * Q1_mf

        # First-stage policy
        logits1 = beta1_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with UCB-style directed exploration modulated by anxiety
        s = state[t]
        bonus = np.zeros(2)
        bonus[:] = (tau_ucb * stai) / np.sqrt(N2[s] + 1.0)
        logits2 = beta2_eff * Q2[s] + bonus
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update values
        r = reward[t]

        # Second-stage update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        N2[s, a2] += 1.0

        # First-stage MF update via eligibility-like trace from second-stage PE
        # Also a direct TD update towards the chosen second-stage action value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (eta_tr * pe2 + (1.0 - eta_tr) * td1)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-gated MB/MF arbitration with anxiety bias and lapses.

    The model tracks recent outcome volatility and uses it, together with anxiety, to arbitrate between
    model-based planning and model-free habits at stage 1. Higher volatility increases MB control; anxiety
    biases arbitration toward either MB or MF depending on xi_anx. A small lapse rate adds stimulus-independent
    choice noise. Stage-2 values are learned with a simple delta rule.

    Parameters (bounds):
    - alphaR: [0,1] learning rate for second-stage rewards.
    - beta: [0,10] base inverse temperature for both stages.
    - k_switch: [0,1] volatility learning rate (how fast volatility estimate adapts).
    - xi_anx: [0,1] anxiety arbitration bias strength; positive values push toward MB as stai increases.
    - epsilon: [0,1] lapse rate applied to both stages (mixture with uniform).
    - p_common_bias: [0,1] prior MB weight (baseline arbitration set-point around which volatility/anxiety shift).

    Arbitration:
      w_mb = sigmoid( logit(p_common_bias) + c_vol * vol + c_anx * stai )
      where c_vol = 4*(2*k_switch - k_switch**2) and c_anx = 4*(2*xi_anx - xi_anx**2)
      (smooth, positive coefficients within [0,4] that scale respective influences)

    Args:
        action_1: array-like, shape (T,), first-stage actions (0/1).
        state: array-like, shape (T,), reached second-stage states (0/1).
        action_2: array-like, shape (T,), second-stage actions (0/1).
        reward: array-like, shape (T,), rewards in [0,1].
        stai: array-like with one element in [0,1], participant anxiety score.
        model_parameters: iterable in the order specified above.

    Returns:
        Negative log-likelihood of observed choices at both stages.
    """
    alphaR, beta, k_switch, xi_anx, epsilon, p_common_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    Tmat = np.array([[0.7, 0.3],
                     [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Volatility estimate (running mean absolute PE)
    vol = 0.0

    eps = 1e-12

    # Helper functions
    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    def sigmoid(z):
        return 1.0 / (1.0 + np.exp(-z))

    # Coefficients scaling volatility and anxiety effects smoothly within [0,4]
    c_vol = 4.0 * (2.0 * k_switch - k_switch**2)
    c_anx = 4.0 * (2.0 * xi_anx - xi_anx**2)

    beta_eff = max(beta, 1e-3)

    for t in range(n_trials):
        # Model-based planner for stage 1
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = Tmat @ max_q2

        # Arbitration weight for MB control
        z = logit(p_common_bias) + c_vol * (vol - 0.25) + c_anx * (stai - 0.5)
        w_mb = sigmoid(z)
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # First-stage policy with lapses
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with lapses
        s = state[t]
        logits2 = beta_eff * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Update MF first-stage value towards the experienced second-stage value
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alphaR * td1

        # Update volatility estimate from absolute PE
        vol = (1.0 - k_switch) * vol + k_switch * abs(pe2)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk- and valence-asymmetric learning with anxiety-weighted confirmation bias.

    The model is purely model-based at stage 1 (planning via known transitions) but incorporates:
    - Valence-asymmetric learning at stage 2 (alpha_pos for positive PEs, alpha_neg for negative PEs).
    - Risk-sensitivity controlling PE gain nonlinearly.
    - Anxiety-weighted confirmation bias at stage 1: tendency to repeat the previous first-stage action if it
      was rewarded, stronger with higher stai.

    Parameters (bounds):
    - alpha_pos: [0,1] learning rate for positive second-stage PEs.
    - alpha_neg: [0,1] learning rate for negative second-stage PEs.
    - beta: [0,10] inverse temperature for both stages.
    - theta_risk: [0,1] exponent shaping PE gain: effective gain scales with |PE|^theta_eff.
    - chi_conf: [0,1] strength of confirmation bias on first-stage logits when prior choice was rewarded.
    - zeta_anx: [0,1] anxiety weighting of valence asymmetry: increases sensitivity to negative PEs with stai.

    Args:
        action_1: array-like, shape (T,), first-stage actions (0/1).
        state: array-like, shape (T,), reached second-stage states (0/1).
        action_2: array-like, shape (T,), second-stage actions (0/1).
        reward: array-like, shape (T,), rewards in [0,1].
        stai: array-like with one element in [0,1], participant anxiety score.
        model_parameters: iterable with parameters in the order above.

    Returns:
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, theta_risk, chi_conf, zeta_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous first-stage action and its reward to implement confirmation bias
    prev_a1 = -1
    prev_r1 = 0.0

    eps = 1e-12
    beta_eff = max(beta, 1e-3)

    for t in range(n_trials):
        # Stage-1 model-based values: expected max over second stage
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Confirmation bias on logits if previous first-stage action was rewarded
        bias1 = np.zeros(2)
        if prev_a1 in (0, 1) and prev_r1 is not None:
            # Additive bias favoring repeating rewarded first-stage action; scaled by anxiety
            bias1[prev_a1] += chi_conf * stai * (2.0 * prev_r1 - 1.0)  # + if rewarded, - if not

        # First-stage policy
        logits1 = beta_eff * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy (pure softmax on Q2)
        s = state[t]
        logits2 = beta_eff * Q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        pe2 = r - Q2[s, a2]

        # Anxiety-weighted valence asymmetry: increase negative-learning with stai, decrease positive slightly
        alpha_pos_eff = np.clip(alpha_pos * (1.0 - 0.5 * zeta_anx * stai), 0.0, 1.0)
        alpha_neg_eff = np.clip(alpha_neg * (1.0 + 0.5 * zeta_anx * stai), 0.0, 1.0)

        # Risk-sensitive PE gain
        theta_eff = theta_risk * (0.5 + 0.5 * stai)  # stronger nonlinearity with higher anxiety
        gain = (np.abs(pe2) + 1e-12) ** theta_eff

        if pe2 >= 0:
            Q2[s, a2] += alpha_pos_eff * gain * pe2
        else:
            Q2[s, a2] += alpha_neg_eff * gain * pe2

        # Update confirmation memory for next trial
        prev_a1 = a1
        prev_r1 = float(r)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll