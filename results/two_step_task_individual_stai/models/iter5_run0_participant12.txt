def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety- and surprise-modulated arbitration.

    This model mixes model-free (MF) and model-based (MB) control at stage 1.
    The arbitration weight for MB starts at a baseline and is modulated by:
      - Anxiety (stai): higher or lower MB weighting depending on xi_stai.
      - Transition surprise (rare transitions): increases/decreases MB weighting via kappa_trans,
        with the direction and strength scaled by anxiety.

    Stage 2 values are learned model-free; stage 1 MF values are learned via TD from stage 2.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial on the visited planet (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration and surprise sensitivity.
    model_parameters : list or array
        [alpha, beta, mb_base, xi_stai, kappa_trans]
        Bounds:
          alpha in [0,1]       : learning rate for MF updates
          beta in [0,10]       : inverse temperature
          mb_base in [0,1]     : baseline MB weight at stage 1
          xi_stai in [0,1]     : anxiety influence on MB weight (0 reduces MB with higher STAI, 1 increases it)
          kappa_trans in [0,1] : strength of surprise-driven MB reweighting

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, mb_base, xi_stai, kappa_trans = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure: A(0)->X(0) common; U(1)->Y(1) common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Q-values
    q1_mf = np.zeros(2)            # stage-1 MF
    q2_mf = np.zeros((2, 2))       # stage-2 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map STAI and xi_stai to signed modulation in [-1,1]
    stai_signed = 2.0 * stai - 1.0           # [-1, 1]
    xi_signed = 2.0 * xi_stai - 1.0          # [-1, 1]

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute MB action values via one-step lookahead over the learned stage-2 MF values
        max_q2 = np.max(q2_mf, axis=1)           # best alien per planet
        q1_mb = transition_matrix @ max_q2       # expected value per spaceship

        # Anxiety-modulated baseline MB weight
        w_base = mb_base + 0.5 * stai_signed * xi_signed
        w_base = np.clip(w_base, 0.0, 1.0)

        # Surprise of transition (1 if rare, 0 if common)
        common_to = a1  # 0->state 0 is common; 1->state 1 is common
        surprise = 1.0 if s != common_to else 0.0

        # Surprise adjustment scaled by anxiety and kappa
        w = w_base + kappa_trans * surprise * stai_signed
        w = np.clip(w, 0.0, 1.0)

        # Hybrid stage-1 Q
        q1 = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (pure MF)
        q2s = q2_mf[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Learning updates
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha * pe2

        # Backup to stage-1 MF (TD toward obtained second-stage value)
        td1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive MB with anxiety-modulated utility curvature and forgetting.

    This model chooses using a model-based (MB) planner at stage 1 that evaluates spaceships
    via the transition matrix and second-stage utilities. Utilities are risk-transformed:
      u(r) = r^gamma, where gamma depends on a baseline risk parameter and anxiety (stai).
    Second-stage MF values are learned on utilities and subject to forgetting toward 0.5.

    Anxiety increases or decreases risk aversion (via gamma) and also softens choice (lower beta)
    proportionally to stai_gain.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate utility curvature and softmax temperature.
    model_parameters : list or array
        [alpha, beta, risk_base, forget, stai_gain]
        Bounds:
          alpha in [0,1]    : learning rate for utility-based MF updates
          beta in [0,10]    : baseline inverse temperature
          risk_base in [0,1]: base risk sensitivity (maps to gamma in [0.5, 1.5])
          forget in [0,1]   : forgetting rate toward 0.5 for all second-stage Q-values
          stai_gain in [0,1]: strength of STAI effects on risk curvature and temperature

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, risk_base, forget, stai_gain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Utility-transformed MF Q at stage 2
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map risk_base to curvature gamma in [0.5, 1.5]
    gamma_base = 0.5 + risk_base  # in [0.5, 1.5]
    # Anxiety shifts gamma: stai in [0,1], stai_gain in [0,1] -> delta in [-0.5, +0.5] scaled
    stai_signed = 2.0 * stai - 1.0  # [-1, 1]
    delta_gamma = 0.5 * stai_signed * (2.0 * stai_gain - 1.0)  # in [-0.5, 0.5]
    gamma = np.clip(gamma_base + delta_gamma, 0.25, 2.0)

    # Anxiety also reduces effective beta (more exploration with higher anxiety if stai_gain>0.5)
    beta_eff = beta / (1.0 + stai_gain * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Apply global forgetting toward 0.5 baseline (uninformative utility)
        q2 = (1.0 - forget) * q2 + forget * 0.5

        # Stage-2 policy on current planet (MF with utility Q)
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta_eff * q2c) / np.sum(np.exp(beta_eff * q2c))
        p_choice_2[t] = probs_2[a2]

        # Utility of observed reward and learning update
        util = r ** gamma
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MB evaluation using expected utilities
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta_eff * q1c) / np.sum(np.exp(beta_eff * q1c))
        p_choice_1[t] = probs_1[a1]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus exploration with anxiety-modulated bonus and MF/MB integration.

    This model learns second-stage MF values and their uncertainties (running variance).
    An exploration bonus proportional to uncertainty is added to second-stage values when choosing,
    scaled by a parameter and modulated by anxiety. The model computes stage-1 MB values from the
    exploration-augmented second-stage values. It also maintains a model-free stage-1 value via
    an eligibility trace and mixes MB and MF at stage 1 with a STAI-dependent arbitration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Modulates exploration bonus and arbitration.
    model_parameters : list or array
        [alpha, beta, k_unc, stai_explore, elig]
        Bounds:
          alpha in [0,1]        : learning rate for MF and variance tracking
          beta in [0,10]        : inverse temperature
          k_unc in [0,1]        : scale of uncertainty exploration bonus
          stai_explore in [0,1] : strength and direction of STAI modulation on exploration/arbitration
          elig in [0,1]         : eligibility trace backing up stage-2 PE to stage-1 MF

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, k_unc, stai_explore, elig = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Transition structure
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    # Stage-2 MF values and running variance (per state-action)
    q2 = np.zeros((2, 2))
    var2 = np.ones((2, 2)) * 0.25  # start with moderate uncertainty

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # STAI mappings
    stai_signed = 2.0 * stai - 1.0                # [-1, 1]
    explore_signed = 2.0 * stai_explore - 1.0     # [-1, 1]

    # STAI-modulated arbitration weight between MB and MF at stage 1
    w_mb = np.clip(0.5 + 0.5 * stai_signed * explore_signed, 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Uncertainty bonus for the current planet: UCB-like (std dev)
        std_s = np.sqrt(np.maximum(var2[s], 1e-12))
        bonus_scale = k_unc * (1.0 + stai_signed * explore_signed)  # increases with anxiety if explore_signed>0
        bonus_scale = np.clip(bonus_scale, 0.0, 2.0 * k_unc)
        q2_aug = q2[s] + bonus_scale * std_s

        # Stage-2 policy on augmented values
        q2c = q2_aug - np.max(q2_aug)
        probs_2 = np.exp(beta * q2c) / np.sum(np.exp(beta * q2c))
        p_choice_2[t] = probs_2[a2]

        # Learning: update mean and variance for chosen state-action using exponential estimates
        pe = r - q2[s, a2]
        q2[s, a2] += alpha * pe
        # Update variance with exponential moving average of squared residuals
        var2[s, a2] = (1.0 - alpha) * var2[s, a2] + alpha * (pe ** 2)

        # Stage-1 MB values from augmented second-stage values (use max over aliens with bonus per planet)
        # Construct augmented per-planet max values
        std_all = np.sqrt(np.maximum(var2, 1e-12))
        q2_aug_all = q2 + bonus_scale * std_all  # use same scale within trial
        max_q2_aug = np.max(q2_aug_all, axis=1)
        q1_mb = transition_matrix @ max_q2_aug

        # Stage-1 hybrid values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c) / np.sum(np.exp(beta * q1c))
        p_choice_1[t] = probs_1[a1]

        # Stage-1 MF update via eligibility trace using the immediate stage-2 PE
        q1_mf[a1] += elig * alpha * pe

        # Optional additional TD correction toward observed second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll