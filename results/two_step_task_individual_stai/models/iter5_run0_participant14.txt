def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated planning weight and forgetting at stage 2.
    
    This model combines model-based (MB) and model-free (MF) values at stage 1.
    The MB component uses the known common transition structure (0.7/0.3) and
    the current best second-stage values. The MF component learns a direct value
    for the first-stage spaceships from second-stage outcomes. Stage 2 uses
    standard Q-learning, with a small forgetting applied to unvisited aliens.
    
    Anxiety (stai) modulates the relative MB weight: higher anxiety increases
    or decreases planning weight depending on parameters, allowing data to decide.
    
    Parameters (model_parameters):
    - alpha_r: learning rate for Q updates (used for both stages), in [0,1]
    - k_forget: forgetting rate for unvisited second-stage Q-values, in [0,1]
    - beta: inverse temperature for both stages' softmax, in [0,10]
    - omega0: baseline MB weight at stage 1, in [0,1]
    - anx_slope: gain mapping anxiety to MB weight, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (float)
    - stai: array-like with a single float in [0,1] (trait anxiety)
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, k_forget, beta, omega0, anx_slope = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known asymmetric transition structure
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])  # rows: [A,U], cols: [X,Y]

    # Action value storage
    q1_mf = np.zeros(2)        # MF values for stage 1: [A, U]
    q2 = np.zeros((2, 2))      # stage-2 values per planet [X,Y] x alien [0,1]

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB weight
    omega = omega0 + anx_slope * (stai - 0.51)
    omega = np.clip(omega, 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)  # best alien per planet: shape (2,)
        q1_mb = T_fixed @ max_q2     # expected value of spaceships

        # Hybrid values
        q1_hyb = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1_hyb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes and learning
        r = reward[t]

        # Stage 2 Q-learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Forget unvisited second-stage actions/planets slightly
        q2[s, 1 - a2] *= (1.0 - k_forget)
        other_s = 1 - s
        q2[other_s, 0] *= (1.0 - k_forget)
        q2[other_s, 1] *= (1.0 - k_forget)

        # Stage 1 MF credit assignment from stage-2 value of chosen alien
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * td1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transitions with anxiety-sharpened expectations (confidence-weighted MB).
    
    This model learns the first-stage transition matrix from experience. Anxiety
    increases a sharpening of the learned transitions, effectively over-trusting
    common transitions and down-weighting rare transitions in planning. Stage 2
    uses MF Q-learning.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for stage-2 Q, in [0,1]
    - alpha_m: learning rate for transition matrix rows (row-wise EWMA), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - phi0: baseline sharpening weight (0=no sharpening, 1=strong), in [0,1]
    - g_anx: anxiety gain controlling increase in sharpening with stai, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (float)
    - stai: array-like with a single float in [0,1] (trait anxiety)
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, alpha_m, beta, phi0, g_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transitions to uninformative
    T = np.ones((2, 2)) * 0.5  # rows: [A,U], cols: [X,Y]
    q2 = np.zeros((2, 2))      # planet x alien values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated sharpening parameter
    phi = np.clip(phi0 + g_anx * (stai - 0.31), 0.0, 1.0)
    # Convert to exponent in [1,5] to sharpen distributions
    exp_pow = 1.0 + 4.0 * phi

    for t in range(n_trials):
        # Compute MB values using sharpened transitions
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        # Sharpen each row of T by elementwise exponent then renormalize
        T_eff = np.empty_like(T)
        for a in range(2):
            row = T[a] ** exp_pow
            row = np.clip(row, 1e-12, None)
            T_eff[a] = row / (np.sum(row) + 1e-12)

        q1_mb = T_eff @ max_q2

        # Stage 1 softmax
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage 2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Transition learning (row-wise update toward observed planet)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_m * (target - T[a1])

        # Keep rows normalized and bounded
        T[a1] = np.clip(T[a1], 1e-9, 1.0)
        T[a1] /= (np.sum(T[a1]) + 1e-12)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Regret-biased model-based choice with anxiety-amplified regret sensitivity.
    
    Stage 1 uses a model-based value computed from the known common transitions
    (0.7/0.3) and current second-stage values. In addition, a dynamic bias
    pushes choices toward the spaceship that commonly reaches the planet with
    currently better potential value (regret for not going there). Regret is the
    difference between the best attainable planet value and the value actually
    obtained on the current trial. This bias decays over trials (trace) and is
    amplified by anxiety.
    
    Stage 2 uses MF Q-learning.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - regret0: baseline sensitivity to regret for bias updates, in [0,1]
    - g_anx: anxiety gain that scales regret sensitivity, in [0,1]
    - trace: persistence of bias across trials (0=no carryover, 1=full), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (float)
    - stai: array-like with a single float in [0,1] (trait anxiety)
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, beta, regret0, g_anx, trace = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure for MB evaluation
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])  # rows: [A,U], cols: [X,Y]

    q2 = np.zeros((2, 2))      # planet x alien values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Directed bias toward A vs U; applied as [+b, -b] to logits
    b = 0.0
    gain = regret0 * np.clip(1.0 + g_anx * (stai - 0.31), 0.0, 2.0)

    for t in range(n_trials):
        # Base MB values from second-stage estimates
        max_q2 = np.max(q2, axis=1)  # [val_X, val_Y]
        q1_mb = T_fixed @ max_q2

        # Apply regret-driven directional bias to logits: prefer A if planet X better, U if Y better
        logits1 = beta * q1_mb + np.array([b, -b])
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage 2 learning
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update regret bias after observing outcome
        # Regret compares the best attainable planet value vs the obtained value
        best_state = 0 if max_q2[0] >= max_q2[1] else 1
        obtained_val = q2[s, a2]
        target_val = max_q2[best_state]
        regret = max(0.0, target_val - obtained_val)

        # Direction: +1 favors A (planet X), -1 favors U (planet Y)
        direction = 1.0 if best_state == 0 else -1.0

        # Decay previous bias and add new regret-driven increment
        b = (1.0 - trace) * b + trace * gain * regret * direction
        # Keep bias bounded to maintain numerical stability
        b = float(np.clip(b, -1.0, 1.0))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll