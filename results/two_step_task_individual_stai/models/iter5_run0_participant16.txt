def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated arbitration with transition learning and eligibility traces.
    
    Mechanism
    - Learns second-stage action values (Q2) with a TD rule.
    - Learns first-stage action values (Q1_MF) via an eligibility trace from stage 2.
    - Learns the transition model T(a->s) and plans model-based values (Q1_MB = T @ max_a Q2).
    - Arbitration between model-based and model-free at stage 1 depends on transition uncertainty
      and anxiety: higher anxiety down-weights model-based control when transitions are uncertain.
    - Anxiety also reduces stage-2 choice determinism and reduces learning of transitions.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int in {0,1}
        Reached planet per trial (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage (alien) choices per trial.
    reward : array-like of float in [0,1]
        Received coins per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values reduce planning and transition learning, increase MF trace.
    model_parameters : iterable of 5 floats
        - alpha_v in [0,1]: learning rate for Q values (both stages).
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2 (scaled down by anxiety).
        - w0_base in [0,1]: baseline model-based arbitration weight.
        - alphaT_base in [0,1]: baseline learning rate for transition probabilities.
    
    Returns
    -------
    float
        Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha_v, beta1, beta2, w0_base, alphaT_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize values
    Q2 = np.zeros((2, 2))           # state x action at stage 2
    Q1_MF = np.zeros(2)             # model-free values at stage 1
    T_est = np.array([[0.7, 0.3],   # learned transition model initialized to task structure
                      [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Stage-2 determinism decreases with anxiety
    beta2_eff = max(0.0, beta2 * (1.0 - 0.35 * stai))
    # Transition learning reduced by anxiety
    alpha_T = alphaT_base * (1.0 - 0.5 * stai)
    # MF eligibility trace increases with anxiety (more credit to recent MF)
    lam = np.clip(0.3 + 0.6 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based value: expected max Q2 over learned transitions
        max_Q2 = np.max(Q2, axis=1)  # size 2 (per state)
        Q1_MB = T_est @ max_Q2       # size 2 (per action)

        # Arbitration weight per action depends on transition uncertainty (entropy) and anxiety
        # Compute row-wise normalized entropy for each action's transition distribution
        H = np.zeros(2)
        for a in range(2):
            p = np.clip(T_est[a], 1e-8, 1.0)
            p = p / np.sum(p)
            H[a] = -np.sum(p * np.log(p))
        H_max = np.log(2.0)
        H_norm = H / (H_max + 1e-12)

        # Base weight is w0_base; reduce weight under anxiety and under uncertainty
        w = np.zeros(2)
        for a in range(2):
            base = w0_base * (1.0 - 0.5 * stai)
            # When entropy is low (predictable), increase MB weight; anxiety dampens this benefit
            incr = (1.0 - H_norm[a]) * (1.0 - stai)
            w[a] = np.clip(base + incr, 0.0, 1.0)

        # Blend MF and MB at stage 1
        Q1 = w * Q1_MB + (1.0 - w) * Q1_MF

        # Stage-1 policy
        logits1 = beta1 * Q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (anxiety-softened)
        s = state[t]
        logits2 = beta2_eff * Q2[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Transition learning for chosen first-stage action from observed state
        # Move T_est[a1] toward one-hot(state)
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - alpha_T) * T_est[a1] + alpha_T * oh
        # Renormalize row
        row_sum = np.sum(T_est[a1])
        if row_sum > 0:
            T_est[a1] = T_est[a1] / row_sum

        # Stage-2 TD update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha_v * delta2

        # Stage-1 MF eligibility-trace update: push toward realized Q2 of reached state/action
        target1 = Q2[s, a2]
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha_v * lam * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric MF-MB hybrid with anxiety-modulated lapse and perseveration.
    
    Mechanism
    - Second-stage values (Q2) are learned model-free with asymmetric learning rates for
      positive vs. negative outcomes. Asymmetry diminishes with higher anxiety.
    - First-stage values use a fixed model-based planner over the known transition matrix (0.7/0.3)
      combined with a model-free cached Q1 via a stai-dependent weight (more anxiety -> more MF).
    - Choice behavior includes an anxiety-inflated lapse rate at both stages and stage-2
      perseveration (stickiness) that increases with anxiety.
    - Shared beta_base is mapped to distinct stage-1/2 temperatures; anxiety reduces stage-2 determinism more.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial.
    reward : array-like of float in [0,1]
        Rewards per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase lapses and perseveration, reduce planning/temperature.
    model_parameters : iterable of 5 floats
        - alpha_base in [0,1]: base learning rate (scaled for valence).
        - beta_base in [0,10]: base inverse temperature for both stages.
        - lapse_base in [0,1]: base lapse probability (uniform random choice).
        - persev_base in [0,1]: base stickiness at stage 2.
        - z_base in [0,1]: valence asymmetry factor (positive > negative); damped by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta_base, lapse_base, persev_base, z_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition matrix (task structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q2 = np.zeros((2, 2))
    Q1_MF = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map parameters with anxiety
    # Valence asymmetry decreases with anxiety
    asym = z_base * (1.0 - stai)
    alpha_pos = np.clip(alpha_base * (1.0 + asym), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 - asym), 0.0, 1.0)

    # Stage temperatures: reduce more at stage 2
    beta1 = max(0.0, beta_base * (1.0 - 0.2 * stai))
    beta2 = max(0.0, beta_base * (1.0 - 0.5 * stai))

    # Lapse increases with anxiety
    lapse = np.clip(lapse_base * (0.5 + 0.5 * stai), 0.0, 0.49)

    # Stage-2 perseveration increases with anxiety
    kappa2 = persev_base * (1.0 + 0.5 * stai)

    prev_a2 = None

    for t in range(n_trials):
        # Model-based Q1 via known transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Anxiety-weighted MF vs MB mixture (no extra param)
        w_mb = np.clip(0.7 * (1.0 - stai), 0.0, 1.0)
        Q1 = w_mb * Q1_MB + (1.0 - w_mb) * Q1_MF

        # Stage-1 softmax with lapse
        logits1 = beta1 * Q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with perseveration and lapse
        s = state[t]
        logits2 = beta2 * Q2[s].copy()
        if prev_a2 is not None:
            logits2[prev_a2] += kappa2
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 asymmetric TD
        pe2 = r - Q2[s, a2]
        if pe2 >= 0:
            Q2[s, a2] += alpha_pos * pe2
        else:
            Q2[s, a2] += alpha_neg * pe2

        # Stage-1 MF bootstrapping toward realized second-stage value
        target1 = Q2[s, a2]
        Q1_MF[a1] += alpha_base * (target1 - Q1_MF[a1])

        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like planning with anxiety-modulated predictive horizon and UCB exploration.
    
    Mechanism
    - Second-stage Q-values learned model-free; exploration bonus (UCB) encourages sampling
      uncertain options. Anxiety increases information-seeking (larger bonus).
    - First-stage planning uses a successor-like transition weighting: the immediate common
      transition is up-weighted by a lambda parameter; higher anxiety shortens predictive horizon
      (larger lambda toward the common state, less averaging over full transition).
    - Learns the transition model T_est from experience; transition learning is reduced by anxiety.
    - Stage-2 choice determinism is reduced by anxiety; stage-1 uses its own temperature.
    
    Parameters
    ----------
    action_1 : array-like of int in {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int in {0,1}
        Reached state (0=X, 1=Y).
    action_2 : array-like of int in {0,1}
        Second-stage choices per trial.
    reward : array-like of float in [0,1]
        Rewards per trial.
    stai : array-like, length 1, float in [0,1]
        Anxiety score; higher values increase exploration bonus and shorten predictive horizon.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for Q2.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: inverse temperature at stage 2 (reduced by anxiety).
        - sr_lambda_base in [0,1]: base weight toward the common transition (predictive myopia).
        - xi_base in [0,1]: base UCB exploration bonus scale at stage 2.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, sr_lambda_base, xi_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize
    Q2 = np.zeros((2, 2))
    counts2 = np.ones((2, 2))  # for uncertainty bonus
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Stronger exploration with anxiety (information-seeking)
    xi = xi_base * (0.5 + 0.5 * stai)
    # Stage-2 temperature reduced by anxiety
    beta2_eff = max(0.0, beta2 * (1.0 - 0.3 * stai))
    # Predictive myopia increases with anxiety (more weight on common transition shortcut)
    sr_lambda = np.clip(sr_lambda_base * (0.5 + 0.5 * stai), 0.0, 1.0)
    # Transition learning reduced by anxiety; derive from alpha
    alpha_T = np.clip(0.5 * alpha * (1.0 - 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # UCB-like bonus for stage-2
        bonus = xi / np.sqrt(counts2 + 1.0)  # state-action specific
        Q2_aug = Q2 + bonus

        # Stage-1 successor-like planning:
        # For action 0 (A), common state is 0; for action 1 (U), common state is 1.
        max_Q2_aug = np.max(Q2_aug, axis=1)
        Q1 = np.zeros(2)
        for a in range(2):
            common_state = a  # mapping: A->X(0), U->Y(1)
            one_hot = np.array([1.0 if i == common_state else 0.0 for i in range(2)])
            p_eff = (1.0 - sr_lambda) * (T_est[a] / np.sum(T_est[a])) + sr_lambda * one_hot
            p_eff = p_eff / (np.sum(p_eff) + 1e-12)
            Q1[a] = np.dot(p_eff, max_Q2_aug)

        # Stage-1 policy
        logits1 = beta1 * Q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with exploration-augmented values
        s = state[t]
        logits2 = beta2_eff * Q2_aug[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and learn
        r = reward[t]
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        counts2[s, a2] += 1.0

        # Update transition model from chosen action and observed state
        oh = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1] = (1.0 - alpha_T) * T_est[a1] + alpha_T * oh
        T_est[a1] = T_est[a1] / (np.sum(T_est[a1]) + 1e-12)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)