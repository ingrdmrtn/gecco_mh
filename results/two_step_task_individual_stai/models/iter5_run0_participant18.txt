def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-modulated exploration.

    Description:
    - Stage-1 values are a weighted hybrid of model-based (using a learned transition matrix) and model-free.
    - The MB weight is shaped by anxiety via a tunable tilt parameter rho_anx: when rho_anx > 0.5,
      lower anxiety increases MB control and higher anxiety reduces it (and vice versa if < 0.5).
    - The transition model is learned with a dedicated transition learning rate tau_trans.
    - Stage-2 policy includes a directed exploration bonus based on outcome uncertainty q*(1-q),
      whose strength is dampened by anxiety: effective bonus = chi_dir * (1 - stai).
    - MF updates use standard TD(0) at stage-2, and eligibility propagation to stage-1
      with an anxiety-dependent trace parameter lambda = (1 - stai).

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - tau_trans: transition learning rate in [0,1]
    - chi_dir: directed exploration bonus weight in [0,1]
    - rho_anx: anxiety-tilt for MB arbitration in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, tau_trans, chi_dir, rho_anx)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, tau_trans, chi_dir, rho_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T_hat (rows: actions A,U; cols: states X,Y)
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Value functions
    Q1_mf = np.zeros(2)       # Stage-1 MF values for actions A/U
    Q2 = np.zeros((2, 2))     # Stage-2 MF values per state and action

    # Track outcome uncertainty at stage-2 via Bernoulli variance proxy q*(1-q)
    # (computed on-the-fly from Q2 which estimates mean reward)
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration: MB weight w_mb
    # rho_anx in [0,1] sets how strongly anxiety tilts arbitration.
    # When rho_anx > 0.5: low anxiety => more MB; high anxiety => less MB.
    w_mb = np.clip(0.5 + (0.5 - st) * (2.0 * rho_anx - 1.0), 0.0, 1.0)

    # Directed exploration effective strength is reduced by anxiety
    dir_bonus_strength = chi_dir * (1.0 - st)

    # Eligibility trace for propagating stage-2 prediction error to stage-1
    lam = 1.0 - st  # higher anxiety -> shorter credit assignment window

    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])

        # Model-based value for stage-1 via learned transitions
        max_Q2 = np.max(Q2, axis=1)       # best value at each state
        Q1_mb = T_hat @ max_Q2

        # Hybrid stage-1 action values
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        pref1 = Q1_hyb
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus based on uncertainty
        # Uncertainty proxy u = q*(1-q) from current Q2 estimates (bounded in [0,0.25])
        u_vec = Q2[s] * (1.0 - Q2[s])
        pref2 = Q2[s] + dir_bonus_strength * u_vec
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # MF learning: stage-2 TD(0)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # MF learning: stage-1 via bootstrapping on reached state/action and eligibility
        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * lam * delta1_boot
        # Additionally propagate a fraction of the immediate outcome PE to stage-1
        Q1_mf[a1] += alpha * lam * delta2

        # Learn transitions for chosen action toward observed state
        # One-hot target vector for observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T_hat[a1] = (1.0 - tau_trans) * T_hat[a1] + tau_trans * target

        # Ensure each row remains a proper probability distribution
        row_sum = np.sum(T_hat[a1])
        if row_sum > 0:
            T_hat[a1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MB/MF with anxiety-driven lapses and decaying perseveration traces.

    Description:
    - Stage-1 uses a fixed model-based lookahead combined with MF values (50-50 baseline),
      then adds a decaying perseveration trace that biases repeating past actions.
    - A lapse parameter injects uniform-random responding; the effective lapse increases with anxiety:
      epsilon_eff = epsilon_base * stai.
    - Perseveration is implemented as action-specific traces at both stages that decay each trial
      with rate 'decay'; its strength is reduced by anxiety: stick_eff = k_stick * (1 - stai).
    - Stage-2 is purely MF in value but includes the same perseveration bias in the policy.
    - MF updates use TD(0) at stage-2 and an eligibility trace to stage-1 with lambda = decay.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - epsilon_base: baseline lapse probability in [0,1]
    - k_stick: perseveration strength in [0,1]
    - decay: trace decay and eligibility parameter in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, epsilon_base, k_stick, decay)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, epsilon_base, k_stick, decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition matrix (common=0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Perseveration traces (action-preference boosts), decaying each trial
    trace1 = np.zeros(2)
    trace2 = np.zeros((2, 2))

    # Effective parameters shaped by anxiety
    epsilon_eff = np.clip(epsilon_base * st, 0.0, 1.0)       # more anxiety -> more lapses
    stick_eff = k_stick * (1.0 - st)                         # more anxiety -> weaker perseveration
    lam = decay                                              # share decay as eligibility parameter

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])

        # Model-based plan: expected best value after each spaceship
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid base: equal MB/MF weighting (kept simple to isolate lapse and stick effects)
        Q1_base = 0.5 * Q1_mb + 0.5 * Q1_mf

        # Add perseveration traces to preferences
        pref1 = Q1_base + stick_eff * trace1

        # Softmax with lapse at stage-1
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - epsilon_eff) * soft1 + epsilon_eff * 0.5  # uniform over 2 actions
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration trace
        pref2 = Q2[s] + stick_eff * trace2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - epsilon_eff) * soft2 + epsilon_eff * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update via bootstrapping and eligibility on immediate outcome
        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * lam * delta1_boot
        Q1_mf[a1] += alpha * lam * delta2

        # Update perseveration traces: decay and reinforce chosen actions
        trace1 *= decay
        trace2 *= decay
        trace1[a1] += 1.0
        trace2[s, a2] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive hybrid with anxiety-shaped utility and surprise-avoidance bias.

    Description:
    - Rewards undergo a concave utility transform u(r) = r^(gamma_eff) to capture risk attitudes;
      anxiety increases concavity by reducing gamma_eff: gamma_eff = gamma_u * (1 - stai).
    - Stage-1 uses a hybrid of MB and MF values with MB weight decreasing as anxiety increases:
      w_mb = omega0 * (1 - stai) + (1 - omega0) * stai.
    - A surprise-avoidance bias on stage-1 preferences penalizes repeating actions that produced
      surprising (rare) transitions on the previous trial. The bias strength scales with both
      nu_surp and stai and decays geometrically with factor (1 - nu_surp).
    - Stage-2 is MF with softmax.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature in [0,10]
    - gamma_u: base utility curvature in [0,1] (smaller => more concave)
    - omega0: base MB weight anchor in [0,1]
    - nu_surp: surprise-bias strength/decay in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, gamma_u, omega0, nu_surp)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, gamma_u, omega0, nu_surp = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Surprise-bias accumulator over stage-1 actions, decays each trial
    bias1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-10

    # Anxiety-shaped parameters
    gamma_eff = max(1e-6, gamma_u * (1.0 - st))  # higher anxiety => more concave utility
    w_mb = np.clip(omega0 * (1.0 - st) + (1.0 - omega0) * st, 0.0, 1.0)

    prev_a1 = None
    prev_s = None

    for t in range(n_trials):
        s = int(state[t])

        # MB plan from fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Hybrid values
        Q1_hyb = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Apply decaying surprise-avoidance bias
        bias1 *= (1.0 - nu_surp)
        pref1 = Q1_hyb + bias1

        # Stage-1 policy
        exp1 = np.exp(beta * (pref1 - np.max(pref1)))
        probs1 = exp1 / (np.sum(exp1) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (MF)
        pref2 = Q2[s]
        exp2 = np.exp(beta * (pref2 - np.max(pref2)))
        probs2 = exp2 / (np.sum(exp2) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward
        r_raw = float(reward[t])
        r = r_raw ** gamma_eff

        # Learning
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1_boot

        # Update surprise-avoidance bias based on the just-experienced transition (for next trial)
        # Compute the probability of the observed transition under the chosen action
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # 0.3 for common, 0.7 for rare
        # Penalize repeating the chosen action proportionally to surprise and anxiety
        # This makes anxious participants more avoidance-biased after surprising outcomes.
        bias1[a1] -= st * nu_surp * surprise
        # Optionally, reward the alternative slightly to keep center-of-mass stable
        bias1[1 - a1] += 0.5 * st * nu_surp * surprise

        prev_a1 = a1
        prev_s = s

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll