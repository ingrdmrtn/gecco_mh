def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid arbitration with anxiety-modulated uncertainty sensitivity, surprise-driven exploration, and perseveration.

    Overview:
    - Stage-2 (planet) action values are learned via TD learning.
    - Stage-1 (spaceship) choice uses a hybrid of model-based and model-free values.
    - Arbitration weight favors model-based control when stage-2 uncertainty (entropy) is low; higher anxiety shifts weight toward model-free control.
    - A surprise bonus (based on running absolute prediction error) encourages exploration at stage-2 and is dampened by anxiety.
    - Perseveration bias at both stages weakens with higher anxiety.

    Parameters (bounds):
    - model_parameters[0] = lr in [0,1]: learning rate for value and surprise traces
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = xi_surprise in [0,1]: weight on surprise-driven bonus at stage 2
    - model_parameters[3] = delta_anx in [0,1]: strength of anxiety's effect on arbitration (higher stai -> more MF)
    - model_parameters[4] = tau_pers in [0,1]: baseline perseveration magnitude; scaled by (1 - stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    lr, beta, xi_surprise, delta_anx, tau_pers = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value tables
    q1_mf = np.zeros(2)          # model-free stage-1 action values
    q2 = np.zeros((2, 2))        # stage-2 state-action values

    # Surprise traces to drive exploration (running abs TD error per state-action)
    s_trace = np.zeros((2, 2))

    # For likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration memory
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Precompute factors
    stick_strength = tau_pers * (1.0 - stai_val)  # less stickiness with higher anxiety

    for t in range(n_trials):
        # Model-based stage-1 values via forward planning with current q2
        max_q2 = np.max(q2, axis=1)            # shape (2,)
        q1_mb = transition_matrix @ max_q2     # shape (2,)

        # Uncertainty-driven arbitration (entropy of stage-2 preferences)
        # Compute normalized entropy per state in [0,1]
        probs_s0 = np.exp(beta * (q2[0] - np.max(q2[0])))
        probs_s0 = probs_s0 / np.sum(probs_s0)
        probs_s1 = np.exp(beta * (q2[1] - np.max(q2[1])))
        probs_s1 = probs_s1 / np.sum(probs_s1)
        def norm_entropy(p):
            eps = 1e-12
            h = -np.sum(p * np.log(p + eps))
            return h / np.log(2.0)  # normalize by log(2) for binary actions
        H_bar = 0.5 * (norm_entropy(probs_s0) + norm_entropy(probs_s1))

        # Arbitration weight: more MB when uncertainty is low; anxiety shifts toward MF (lower w)
        w_mb = 1.0 - H_bar + delta_anx * (0.5 - stai_val)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Stage-1 perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick_strength

        q1_net = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias1
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 decision with surprise-driven exploration and perseveration
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick_strength

        # Surprise bonus: encourage options with larger recent abs TD errors; dampened by anxiety
        bonus = xi_surprise * (1.0 - stai_val) * s_trace[s]  # shape (2,)
        q2_net = q2[s] + bias2 + bonus
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcomes and learning
        r = reward[t]

        # Stage-2 TD error and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr * delta2

        # Update surprise traces toward |delta2|
        s_trace[s, a2] = (1.0 - lr) * s_trace[s, a2] + lr * abs(delta2)

        # Stage-1 MF bootstrapping from obtained second-stage value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr * delta1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition model with anxiety-modulated inverse temperature and eligibility-trace credit assignment.

    Overview:
    - Learns both the transition probabilities (from spaceships to planets) and stage-2 values.
    - Stage-1 values are model-based expectations using the learned transition matrix plus a model-free component updated via an eligibility trace.
    - Anxiety scales decision determinism (inverse temperature): higher anxiety can increase or decrease beta depending on kappa.
    
    Parameters (bounds):
    - model_parameters[0] = lr in [0,1]: learning rate for rewards (stage-2 Q) and for MF stage-1 via eligibility
    - model_parameters[1] = beta0 in [0,10]: baseline inverse temperature
    - model_parameters[2] = kappa_anx_temp in [0,1]: strength of anxiety modulation of beta; beta_eff = clip(beta0 * (1 + kappa_anx_temp * (2*stai - 1)), 1e-3, 10)
    - model_parameters[3] = tau_T in [0,1]: learning rate for transition probabilities
    - model_parameters[4] = lam in [0,1]: eligibility-trace weight for propagating reward to stage-1 MF
    
    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    lr, beta0, kappa_anx_temp, tau_T, lam = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transition matrix (rows: a1 in {0,1}; cols: states {0,1}); start near-agnostic
    T = np.full((2, 2), 0.5)

    # Value tables
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated inverse temperature (same for both stages)
    beta_eff = beta0 * (1.0 + kappa_anx_temp * (2.0 * stai_val - 1.0))
    beta_eff = float(np.clip(beta_eff, 1e-3, 10.0))

    for t in range(n_trials):
        # Stage-1 model-based values from learned transitions
        max_q2 = np.max(q2, axis=1)      # shape (2,)
        q1_mb = T @ max_q2               # shape (2,)

        # Combine MB and MF by simple summation (scales are compatible since both track expected value)
        q1_net = q1_mb + q1_mf
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn transitions via simple delta rule toward observed next state
        one_hot_s = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * one_hot_s
        # Ensure row-normalization (numerical safety)
        T[a1] = T[a1] / np.sum(T[a1])

        # Stage-2 value update
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr * delta2

        # Eligibility-trace update to stage-1 MF values (propagate outcome back)
        q1_mf[a1] += lr * lam * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Loss-aversion utility with anxiety modulation and lapse-biased softmax; model-based planning at stage-1.

    Overview:
    - Stage-2 updates use a utility-transformed outcome: rewards are positive, nonrewards are treated as aversive with anxiety-modulated magnitude.
    - Stage-1 uses model-based planning from a fixed transition structure to the learned utility-based stage-2 values.
    - Action selection includes a lapse component that increases with anxiety.

    Parameters (bounds):
    - model_parameters[0] = lr in [0,1]: learning rate for stage-2 values
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = phi0 in [0,1]: baseline loss-aversion for nonreward (utility of 0 outcome is -phi_eff)
    - model_parameters[3] = phi_anx in [0,1]: anxiety modulation of loss-aversion; phi_eff = clip(phi0 + phi_anx*stai, 0,1)
    - model_parameters[4] = eps0 in [0,1]: baseline lapse rate; effective lapse = eps0 * stai

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0/1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    lr, beta, phi0, phi_anx, eps0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure for planning
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Stage-2 values (in utility space)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion and lapse
    phi_eff = float(np.clip(phi0 + phi_anx * stai_val, 0.0, 1.0))
    eps_lapse = float(np.clip(eps0 * stai_val, 0.0, 1.0))

    for t in range(n_trials):
        # Stage-1 model-based values via forward planning over utility-based q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Lapse-biased softmax at stage 1
        q1c = q1_mb - np.max(q1_mb)
        soft_1 = np.exp(beta * q1c)
        soft_1 = soft_1 / np.sum(soft_1)
        probs_1 = (1.0 - eps_lapse) * soft_1 + eps_lapse * 0.5  # uniform over 2 actions
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        s = state[t]

        # Stage-2 policy with lapse
        q2c = q2[s] - np.max(q2[s])
        soft_2 = np.exp(beta * q2c)
        soft_2 = soft_2 / np.sum(soft_2)
        probs_2 = (1.0 - eps_lapse) * soft_2 + eps_lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility-transformed outcome
        r = reward[t]
        u = r if r > 0 else -phi_eff

        # Stage-2 TD update in utility space
        delta2 = u - q2[s, a2]
        q2[s, a2] += lr * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll