def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety- and uncertainty-modulated arbitration plus MF decay.

    This model blends model-based (MB) and model-free (MF) values at stage 1.
    The arbitration weight w_t is shaped by:
    - baseline w0,
    - state of second-stage uncertainty (smaller alien value differences => higher MB weight),
    - anxiety (higher anxiety reduces MB reliance).
    Stage-2 uses MF Q-learning. MF values decay for unchosen actions to capture forgetting.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF updates at stage 2 and bootstrapped update to stage 1.
    - beta: [0,10] inverse temperature for both stages.
    - w0: [0,1] baseline MB weight at stage 1.
    - k_unc: [0,1] strength of uncertainty-driven increase in MB weight.
    - k_decay: [0,1] decay/forgetting of unupdated MF action values per trial.

    Inputs:
    - action_1: int array in {0,1}, chosen spaceship per trial.
    - state: int array in {0,1}, reached planet per trial.
    - action_2: int array in {0,1}, chosen alien per trial.
    - reward: float array, coins received per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, w0, k_unc, k_decay].

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, w0, k_unc, k_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # action 0 (A): P(X)=0.7, P(Y)=0.3
                  [0.3, 0.7]]) # action 1 (U): P(X)=0.3, P(Y)=0.7

    # MF Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))  # state x action

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Estimate second-stage uncertainty: smaller abs diff => more uncertainty
        diff_x = abs(q2[0, 0] - q2[0, 1])
        diff_y = abs(q2[1, 0] - q2[1, 1])
        # Average uncertainty across planets
        unc = 1.0 - 0.5 * (np.tanh(diff_x) + np.tanh(diff_y))  # in (0,1), saturating

        # Anxiety reduces MB reliance; uncertainty increases MB reliance
        w = w0 * (1.0 - 0.6 * stai) + k_unc * unc
        w = min(1.0, max(0.0, w))

        # Model-based Q for first stage: expected max of second-stage values under T
        max_q2 = np.max(q2, axis=1)  # [X_best, Y_best]
        q1_mb = T @ max_q2

        # Hybrid Q for stage 1
        q1_hybrid = (1.0 - w) * q1_mf + w * q1_mb

        # Stage-1 policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p1[t] = probs1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p2[t] = probs2[a2]

        # Learning updates
        # Stage-2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 MF bootstrapped update toward current second-stage value
        target1 = q2[s, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Decay unchosen MF values (both levels) to capture forgetting
        other_a1 = 1 - a1
        q1_mf[other_a1] *= (1.0 - k_decay)
        # For second stage, decay actions not chosen in each state
        other_a2 = 1 - a2
        q2[s, other_a2] *= (1.0 - k_decay)
        # Also decay the other state's actions slightly to reflect global forgetting
        other_s = 1 - s
        q2[other_s, :] *= (1.0 - 0.5 * k_decay)

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure MB with learned transition reliability and anxiety-shifted priors, plus MF outcome learning.

    This model learns the actionâ†’state transition reliabilities online and uses
    those to plan at stage 1 (pure MB). Transition learning starts from an
    anxiety-shifted prior: higher anxiety assumes less reliable (less common) transitions.
    Stage-2 rewards are learned with MF Q-learning; stage-1 uses the learned transition model.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for stage-2 MF Q-values.
    - beta: [0,10] inverse temperature for both stages.
    - xi: [0,1] transition learning rate toward observed common/rare indicator.
    - anx_shift: [0,1] scales how much anxiety reduces prior commonness (more rare-expectant).
    - decay2: [0,1] slow decay of unvisited second-stage Q-values.

    Inputs:
    - action_1: int array in {0,1}, chosen spaceship per trial.
    - state: int array in {0,1}, reached planet per trial.
    - action_2: int array in {0,1}, chosen alien per trial.
    - reward: float array, coins received per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, xi, anx_shift, decay2].

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, xi, anx_shift, decay2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Prior commonness for each action; anxiety shifts toward rare (lower commonness)
    prior_common = 0.7 - 0.4 * anx_shift * stai
    prior_common = min(0.99, max(0.01, prior_common))
    # Estimated commonness per action (A, U)
    p_common = np.array([prior_common, prior_common], dtype=float)

    # Second-stage MF Q-values
    q2 = np.zeros((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Build current transition matrix from learned p_common
        # For action 0 (A): P(X)=p_common[0], P(Y)=1-p
        # For action 1 (U): P(X)=1-p_common[1], P(Y)=p
        T_hat = np.array([[p_common[0], 1.0 - p_common[0]],
                          [1.0 - p_common[1], p_common[1]]])

        # Model-based Q for stage 1 using current T_hat and current stage-2 values
        max_q2 = np.max(q2, axis=1)  # [X_best, Y_best]
        q1_mb = T_hat @ max_q2

        # Stage-1 policy (pure MB)
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-16)
        p1[t] = probs1[a1]

        # Stage-2 policy (MF)
        logits2 = beta * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-16)
        p2[t] = probs2[a2]

        # Update stage-2 MF Q
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Decay unvisited second-stage values
        other_a2 = 1 - a2
        q2[s, other_a2] *= (1.0 - decay2)
        q2[1 - s, :] *= (1.0 - 0.5 * decay2)

        # Update transition commonness belief for chosen action based on observed state
        # Determine whether transition was common for the chosen action
        # Common if (a1==0 and s==0) or (a1==1 and s==1)
        was_common = 1.0 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.0
        p_common[a1] += xi * (was_common - p_common[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Confidence-weighted lapse with anxiety amplification and first-stage bias.

    This model is MF at both stages but allows dynamic, confidence-dependent lapses.
    Confidence is the absolute value difference between the two aliens' Q-values.
    - Lower confidence -> higher lapse probability (blend with uniform choice).
    - Anxiety amplifies lapses multiplicatively.
    - A first-stage bias term shifts choices toward one spaceship as a linear function of anxiety.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for MF Q updates (both stages via bootstrapping to stage 1).
    - beta: [0,10] inverse temperature for both stages.
    - phi_conf: [0,1] strength of confidence-to-lapse mapping (larger -> more lapses when uncertain).
    - theta_anx: [0,1] multiplicative amplification of lapse by anxiety.
    - bias_side: [0,1] strength of anxiety-driven bias toward spaceship A at stage 1.

    Inputs:
    - action_1: int array in {0,1}, chosen spaceship per trial.
    - state: int array in {0,1}, reached planet per trial.
    - action_2: int array in {0,1}, chosen alien per trial.
    - reward: float array, coins received per trial.
    - stai: array-like (length 1), anxiety score in [0,1].
    - model_parameters: list/array [alpha, beta, phi_conf, theta_anx, bias_side].

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, phi_conf, theta_anx, bias_side = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure for expected confidence at stage 1
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # MF Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Compute per-planet confidence from current q2
        conf_state = np.array([abs(q2[0, 0] - q2[0, 1]),
                               abs(q2[1, 0] - q2[1, 1])])
        # Bound confidence to [0,1] by a saturating transform
        conf_state = np.tanh(conf_state)

        # Stage-2 policy with confidence-weighted lapse
        base_logits2 = beta * q2[s, :]
        base_logits2 -= np.max(base_logits2)
        base_probs2 = np.exp(base_logits2)
        base_probs2 /= (np.sum(base_probs2) + 1e-16)
        # Lapse probability decreases with confidence; amplify by anxiety
        p_lapse2 = phi_conf * (1.0 - conf_state[s])
        p_lapse2 *= (1.0 + theta_anx * stai)
        p_lapse2 = min(1.0, max(0.0, p_lapse2))
        probs2 = (1.0 - p_lapse2) * base_probs2 + p_lapse2 * 0.5
        p2[t] = probs2[a2]

        # Stage-1 policy with confidence- and anxiety-driven lapse and bias
        # Expected confidence for each action: expectation over next-state confidence under T
        exp_conf_a0 = T[0, :] @ conf_state
        exp_conf_a1 = T[1, :] @ conf_state
        exp_conf = np.array([exp_conf_a0, exp_conf_a1])

        base_logits1 = beta * q1
        # Anxiety-driven directional bias toward spaceship A (action 0)
        # Adds +bias_side*stai to A and -bias_side*stai to U
        base_logits1[0] += bias_side * stai
        base_logits1[1] -= bias_side * stai
        base_logits1 -= np.max(base_logits1)
        base_probs1 = np.exp(base_logits1)
        base_probs1 /= (np.sum(base_probs1) + 1e-16)

        p_lapse1 = phi_conf * (1.0 - 0.5 * (exp_conf[0] + exp_conf[1]))
        p_lapse1 *= (1.0 + theta_anx * stai)
        p_lapse1 = min(1.0, max(0.0, p_lapse1))
        probs1 = (1.0 - p_lapse1) * base_probs1 + p_lapse1 * 0.5
        p1[t] = probs1[a1]

        # Learning (MF Q-learning with eligibility-like bootstrapping)
        # Stage-2 update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 update toward current second-stage chosen value
        target1 = q2[s, a2]
        delta1 = target1 - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll