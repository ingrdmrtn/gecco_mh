def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-gated model-based control.
    
    This model mixes model-free (MF) and model-based (MB) action values at the first stage.
    The MB weight is gated by anxiety: higher anxiety increases/decreases MB control depending
    on parameters. Second-stage values are learned with MF updates. First-stage MF values
    are backed up from the realized second-stage action value, with an anxiety-scaled learning gain.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1) within the visited state.
    reward : array-like of float
        Received reward each trial (e.g., 0/1 or probabilistic).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [lr, beta, theta_mb0, chi_anx]
        - lr in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax.
        - theta_mb0 in [0,1]: baseline weight on MB control at stage 1.
        - chi_anx in [0,1]: how strongly anxiety shifts the MB weight.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, theta_mb0, chi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)      # MF values at stage 1
    q2 = np.zeros((2, 2))    # MF values at stage 2 (state, action)

    # Storage for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-gated MB weight, clipped to [0,1]
    omega = theta_mb0 + chi_anx * s
    if omega < 0.0:
        omega = 0.0
    elif omega > 1.0:
        omega = 1.0

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based action values at stage 1 from second-stage values
        max_q2 = np.max(q2, axis=1)     # best attainable value in each second-stage state
        q1_mb = T @ max_q2              # expectation under transition model

        # Hybrid action values and softmax policy for stage 1
        q1_hybrid = (1.0 - omega) * q1_mf + omega * q1_mb
        prefs1 = beta * (q1_hybrid - np.max(q1_hybrid))
        exp1 = np.exp(prefs1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        prefs2 = beta * (q2[st] - np.max(q2[st]))
        exp2 = np.exp(prefs2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 MF update
        pe2 = r - q2[st, a2]
        q2[st, a2] += lr * pe2

        # Stage-1 MF update with anxiety-scaled gain (higher anxiety -> larger gain)
        gain1 = lr * (0.5 + 0.5 * s)  # in [0.5*lr, lr]
        backup = q2[st, a2]
        pe1 = backup - q1_mf[a1]
        q1_mf[a1] += gain1 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive MF learning with anxiety-modulated risk and stickiness.
    
    Rewards are transformed by a power utility u(r) = r^(1 - xi) before learning.
    Anxiety increases or decreases risk aversion via xi(s) = xi0 + xi_stai * s.
    Policies at both stages include a choice stickiness bias that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha, beta, xi0, xi_stai, tau_stick]
        - alpha in [0,1]: MF learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - xi0 in [0,1]: baseline risk-aversion parameter in utility transform.
        - xi_stai in [0,1]: how much anxiety changes risk-aversion.
        - tau_stick in [0,1]: base stickiness magnitude; effective stickiness is tau_stick * stai.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, xi0, xi_stai, tau_stick = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Effective parameters
    xi = xi0 + xi_stai * s
    if xi < 0.0:
        xi = 0.0
    elif xi > 1.0:
        xi = 1.0
    kappa = tau_stick * s  # anxiety-scaled stickiness

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stickiness memory
    last_a1 = None
    last_a2 = {0: None, 1: None}

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stickiness biases
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        bias2 = np.zeros(2)
        if last_a2[st] is not None:
            bias2[last_a2[st]] += kappa

        # Policies
        prefs1 = q1 + bias1
        prefs1 = beta * (prefs1 - np.max(prefs1))
        exp1 = np.exp(prefs1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        prefs2 = q2[st] + bias2
        prefs2 = beta * (prefs2 - np.max(prefs2))
        exp2 = np.exp(prefs2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Utility-transformed reward (risk sensitivity)
        # Ensure r in [0,1]; power transform is well-defined.
        u = (r ** (1.0 - xi)) if r >= 0 else -((-r) ** (1.0 - xi))

        # MF updates
        pe2 = u - q2[st, a2]
        q2[st, a2] += alpha * pe2

        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Update stickiness memory
        last_a1 = a1
        last_a2[st] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with learned transitions and anxiety-modulated mixing.
    
    The agent learns both second-stage MF values and first-stage transition probabilities.
    First-stage decisions use a mixture of MF and model-based values computed from the learned
    transition model. Anxiety shifts the MF/MB mixing weight.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (0/1).
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [lr, beta, eta_T, w_mix0, delta_anx]
        - lr in [0,1]: MF learning rate for rewards.
        - beta in [0,10]: inverse temperature for softmax.
        - eta_T in [0,1]: learning rate for transition probabilities.
        - w_mix0 in [0,1]: baseline weight on MB control at stage 1.
        - delta_anx in [0,1]: how strongly anxiety shifts the MB weight.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, eta_T, w_mix0, delta_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize learned transition model close to uniform
    T = np.full((2, 2), 0.5)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        st = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Compute MB values from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Anxiety-modulated mixing weight
        w = w_mix0 + delta_anx * s
        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

        q1_hybrid = (1.0 - w) * q1_mf + w * q1_mb

        # Policies
        prefs1 = beta * (q1_hybrid - np.max(q1_hybrid))
        exp1 = np.exp(prefs1)
        probs1 = exp1 / np.sum(exp1)
        p_choice_1[t] = probs1[a1]

        prefs2 = beta * (q2[st] - np.max(q2[st]))
        exp2 = np.exp(prefs2)
        probs2 = exp2 / np.sum(exp2)
        p_choice_2[t] = probs2[a2]

        # Outcome learning: stage-2 then stage-1 MF
        pe2 = r - q2[st, a2]
        q2[st, a2] += lr * pe2

        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += lr * pe1

        # Transition learning for chosen first-stage action
        # Update T[a1] toward the observed state (one-hot), keep row normalized
        target = np.array([1.0, 0.0]) if st == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - eta_T) * T[a1] + eta_T * target
        # Ensure numerical normalization
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll