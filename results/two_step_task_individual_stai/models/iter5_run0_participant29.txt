def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free learner with learned transitions and risk-sensitive utility.
    Anxiety reduces model-based reliance and increases perceived transition volatility.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — model-free learning rate for action values
        beta: [0,10] — inverse temperature for both stages
        mb_mix: [0,1] — base weight of model-based control at stage 1
        risk_aversion: [0,1] — curvature of utility (larger => more concave utility)
        trans_volatility: [0,1] — scales transition learning rate; anxiety amplifies it

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.

    Notes
    - Transition probabilities for spaceships to planets are learned online.
    - Utility transformation u(r) = r^(1 - risk_aversion * (1 + stai)), shaping both learning targets and choice.
    - Effective MB weight decreases with anxiety: mb_eff = mb_mix * (1 - 0.5 * stai).
    """
    alpha, beta, mb_mix, risk_aversion, trans_volatility = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    eps = 1e-12

    # Initialize learned transition matrix T[a, s] and ensure rows sum to 1
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Model-free Q-values
    q1_mf = np.zeros(2)        # stage-1 MF values (spaceships)
    q2 = np.zeros((2, 2))      # stage-2 MF values (aliens within states)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Utility transform
    def util(r):
        curvature = 1.0 - risk_aversion * (1.0 + stai_val)
        curvature = np.clip(curvature, 0.01, 1.0)
        return np.power(np.clip(r, 0.0, 1.0), curvature)

    for t in range(n_trials):
        # Stage-2 decision in observed state
        s = state[t]
        # Use utility-shaped values for policy only (do not bias learning target beyond q2 update)
        logits2 = util(q2[s, :])
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Compute model-based Q1 via current learned transitions and utility-shaped Q2 maxima
        max_q2_util = np.max(util(q2), axis=1)  # per state
        q1_mb = T @ max_q2_util

        # Combine MB and MF at stage 1 with anxiety-reduced MB weight
        mb_eff = mb_mix * (1.0 - 0.5 * stai_val)
        q1_combined = mb_eff * q1_mb + (1.0 - mb_eff) * q1_mf

        # Stage-1 decision
        logits1 = q1_combined - np.max(q1_combined)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Outcome
        r = reward[t]
        u_r = util(r)

        # Learn transitions: simple delta rule toward one-hot next state
        # Anxiety increases transition learning rate (perceived volatility)
        trans_lr = np.clip(0.2 + 0.6 * trans_volatility * (1.0 + stai_val), 0.0, 1.0)
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - trans_lr) * T[a1, :] + trans_lr * target
        # Renormalize to avoid drift
        T[a1, :] = T[a1, :] / (np.sum(T[a1, :]) + eps)

        # Stage-2 MF update
        pe2 = u_r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF update toward realized second-stage value (bootstrapped)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free learner with anxiety-gated credit assignment and transition-surprise amplification.
    Rare transitions increase credit assignment to the first-stage action,
    and anxiety amplifies this effect. Adds an anxiety-modulated choice inertia.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for MF updates at both stages
        beta: [0,10] — inverse temperature
        kappa_transition: [0,1] — strength of transition-surprise gating for stage-1 learning
        choice_inertia: [0,1] — base bias to repeat the last action
        anx_credit: [0,1] — scales how anxiety increases surprise gating and inertia

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.

    Notes
    - Surprise is computed relative to the fixed common transitions: P(common)=0.7
      surprise = 1 for rare transitions and 0 for common transitions.
    - Effective stage-1 learning rate: alpha1_eff = alpha * (0.5 + 0.5*kappa_transition*surprise*(1+anx_credit*stai))
    - Choice inertia is increased by anxiety for both stages.
    """
    alpha, beta, kappa_transition, choice_inertia, anx_credit = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])
    eps = 1e-12

    # Fixed transition structure (used only to compute surprise)
    # A->X common, U->Y common
    common_prob = 0.7

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_a2 = [None, None]

    # Anxiety-modulated inertia used as additive bias on logits
    inertia_strength1 = choice_inertia * (1.0 + anx_credit * stai_val)
    inertia_strength2 = 0.5 * choice_inertia * (1.0 + anx_credit * stai_val)

    for t in range(n_trials):
        # Stage 1 policy with inertia
        logits1 = q1.copy()
        if last_a1 is not None:
            logits1[last_a1] += inertia_strength1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with inertia, in reached state
        s = state[t]
        logits2 = q2[s, :].copy()
        if last_a2[s] is not None:
            logits2[last_a2[s]] += inertia_strength2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Compute transition surprise (1 for rare, 0 for common)
        # Determine the "common" next state for chosen a1
        common_state = 0 if a1 == 0 else 1
        prob_common = common_prob
        is_common = 1 if s == common_state else 0
        surprise = 1.0 - is_common  # 1 if rare, 0 if common

        # Anxiety-gated amplification of credit assignment at stage 1
        gate = 0.5 + 0.5 * kappa_transition * surprise * (1.0 + anx_credit * stai_val)
        gate = np.clip(gate, 0.0, 1.0)

        # Stage 2 update (MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 update (MF toward realized second-stage value), gated by surprise/anxiety
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += (alpha * gate) * pe1

        # Update inertia memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman-like uncertainty tracking with anxiety-amplified exploration bonus and adaptive MB weighting.
    The agent tracks both expected reward and uncertainty for each alien using a simple
    scalar Kalman filter. Uncertainty yields an exploration bonus at stage 2,
    which is amplified by anxiety, and propagated to stage 1 via model-based planning.
    Stage 1 also has a model-free component; MB/MF mix is adaptively set by anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A,1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X,1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within state.
    - reward: array-like of floats in [0,1]. Coins received.
    - stai: array-like of one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — additional step-size blending with the Kalman update (helps track drift)
        beta: [0,10] — inverse temperature
        volatility: [0,1] — process noise scale for uncertainty dynamics
        unc_bonus: [0,1] — base weight of uncertainty bonus in valuation
        anx_unc_gain: [0,1] — how strongly anxiety amplifies uncertainty bonus

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.

    Notes
    - Measurement noise is fixed; process noise scales with 'volatility'.
    - MB/MF mix at stage 1 is set as: mb_weight = 0.5 + 0.5*(1 - stai), i.e., higher anxiety => less MB.
    """
    alpha, beta, volatility, unc_bonus, anx_unc_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])
    eps = 1e-12

    # Fixed transition matrix (task structure known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Reward mean and uncertainty (variance) for each alien (state, action)
    q2_mean = np.zeros((2, 2))
    q2_var = np.ones((2, 2)) * 0.1  # initial uncertainty

    # Stage-1 model-free values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Kalman parameters
    # Process noise increases with volatility
    process_q = 0.01 + 0.19 * volatility  # in [0.01, 0.20]
    meas_r = 0.25  # fixed observation noise for Bernoulli-like rewards

    for t in range(n_trials):
        # Exploration bonus from uncertainty
        bonus_scale = unc_bonus * (1.0 + anx_unc_gain * stai_val)

        # Stage-2 policy in observed state
        s = state[t]
        bonus2 = bonus_scale * np.sqrt(np.maximum(q2_var[s, :], 1e-8))
        q2_aug = q2_mean[s, :] + bonus2

        logits2 = q2_aug - np.max(q2_aug)
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB values via expected max value per state (including bonus)
        bonus2_all = bonus_scale * np.sqrt(np.maximum(q2_var, 1e-8))
        q2_aug_all = q2_mean + bonus2_all
        max_q2_per_state = np.max(q2_aug_all, axis=1)
        q1_mb = T @ max_q2_per_state

        # MB/MF mixture weight depends on anxiety (higher anxiety => lower MB)
        mb_weight = 0.5 + 0.5 * (1.0 - stai_val)
        q1_combined = mb_weight * q1_mb + (1.0 - mb_weight) * q1_mf

        # Stage-1 policy
        logits1 = q1_combined - np.max(q1_combined)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Observe reward and update Kalman filter for the chosen alien
        r = reward[t]
        # Time update (increase uncertainty due to process noise)
        q2_var[s, a2] = q2_var[s, a2] + process_q
        # Measurement update
        S = q2_var[s, a2] + meas_r
        K = q2_var[s, a2] / (S + eps)  # Kalman gain in [0,1)
        pred = q2_mean[s, a2]
        pe = r - pred
        # Blend a small additional step-size alpha to track non-stationarity robustly
        effective_update = K * pe
        q2_mean[s, a2] = pred + (alpha * pe + (1.0 - alpha) * effective_update)
        q2_var[s, a2] = (1.0 - K) * q2_var[s, a2]

        # Stage-1 MF update toward realized stage-2 (mean) value without bonus
        target1 = q2_mean[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)