def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated model-based control with anxiety and perseveration.

    Mechanism:
    - Stage-2 values Q2(s2, a2) learned with TD(0).
    - A transition model T(a1 -> s2) is learned online (EMA) and used for model-based (MB) evaluation.
    - The MB weight at stage-1 is dynamically gated by transition uncertainty (row entropy of T) and by anxiety.
      Higher uncertainty reduces MB control; anxiety amplifies uncertainty sensitivity.
    - Perseveration bias applied to both stages.

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}; first-stage choices
    - state:    int array (n_trials,), values in {0,1}; reached second-stage state
    - action_2: int array (n_trials,), values in {0,1}; second-stage choices
    - reward:   float array (n_trials,), rewards in [0,1]
    - stai:     float array with single element in [0,1]; participant anxiety
    - model_parameters: tuple/list with five params:
        lr    in [0,1]: learning rate for Q2 and for the transition model T (EMA)
        invt  in [0,10]: inverse temperature (softmax) at both stages
        gate0 in [0,1]: baseline MB gating parameter (higher => more MB)
        unc_s in [0,1]: sensitivity to transition uncertainty in gating (scales with anxiety)
        pers  in [0,1]: perseveration strength added to the previously chosen action at each stage

    Returns:
    - Negative log-likelihood of the observed first- and second-stage actions.
    """
    lr, invt, gate0, unc_s, pers = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Initialize learned transition model T(a1, s2)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value function for stage-2: Q2[state, action]
    Q2 = np.zeros((2, 2))
    # Stage-1 model-free cache (SARSA-style backup of last reached Q2)
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = -1
    eps = 1e-12

    def row_entropy(p_row):
        p_row = np.clip(p_row, eps, 1.0)
        p_row = p_row / np.sum(p_row)
        return -np.sum(p_row * np.log(p_row))

    for t in range(n_trials):
        # Compute MB projection using current T and Q2
        v2 = np.max(Q2, axis=1)           # value of each second-stage state
        Q1_mb = T @ v2                    # MB action values at stage-1

        # Uncertainty of each action's transition row (entropy in nats, max ~ ln(2))
        H0 = row_entropy(T[0])
        H1 = row_entropy(T[1])
        H = np.array([H0, H1])

        # Dynamic MB weight per action (bounded in [0,1])
        # Anxiety amplifies the impact of uncertainty: effective penalty = unc_s * anx * H
        gate_penalty = unc_s * anx * H
        omega = np.clip(gate0 - gate_penalty, 0.0, 1.0)

        # Combine MB and MF per action
        Q1 = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Perseveration features
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Stage-1 policy
        logits1 = invt * Q1 + pers * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p1[t] = probs1[a1]

        # Stage-2 policy (with perseveration)
        s2 = state[t]
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = invt * Q2[s2] + pers * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions via EMA toward one-hot of observed state
        target_row = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        T[a1] = (1.0 - lr) * T[a1] + lr * target_row
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Stage-2 TD(0) update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += lr * delta2

        # Stage-1 MF cache updated by value of reached state-action and reward
        target1 = Q2[s2, a2]  # bootstrap on current Q2 after update
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += lr * delta1
        # Optional direct reward backup (small extra MF kick consistent with SARSA(1)-ish)
        Q1_mf[a1] += lr * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric valence learning with anxiety-weighted rare-transition perseveration.

    Mechanism:
    - Pure model-free learner:
      * Stage-2 Q2(s2, a2) updated with separate learning rates for positive vs. negative TD errors.
      * Stage-1 Q1(a1) backed up from the realized Q2 (SARSA-style), also with valence asymmetry.
    - After rare transitions, a transient perseveration bias increases the tendency to repeat the
      chosen first-stage action; this effect scales with anxiety.
    - Known transition structure defines "common" vs. "rare" (0.7/0.3).

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}
    - state:    int array (n_trials,), values in {0,1}
    - action_2: int array (n_trials,), values in {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with single element in [0,1]
    - model_parameters: tuple/list with five params:
        lr_pos in [0,1]: learning rate when TD error >= 0
        lr_neg in [0,1]: learning rate when TD error <  0
        invt   in [0,10]: inverse temperature for softmax at both stages
        rare_b in [0,1]: strength of rare-transition-induced perseveration at stage-1
        stick  in [0,1]: baseline perseveration strength (both stages)

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, invt, rare_b, stick = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])
    eps = 1e-12

    # Known transition matrix (for rarity classification)
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1 = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    prev_a1 = -1
    prev_a2 = -1

    def lr_sel(d):
        return lr_pos if d >= 0.0 else lr_neg

    for t in range(n_trials):
        # Policies with perseveration
        s2 = state[t]

        # Stage-2
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = invt * Q2[s2] + stick * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p2[t] = probs2[a2]

        # Stage-1 action values are current cache Q1
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Determine if current transition (given chosen a1) is rare, to modulate extra perseveration
        a1 = action_1[t]
        # We don't know the next state until after choice, but likelihood needs policy at choice time.
        # We therefore compute the rare-bias on the previous trial's outcome (standard approach),
        # applying a bias to repeating the previous action if the previous transition was rare.
        # For the current trial, compute an additive bias to the action that equals prev_a1.
        # To maintain online causality, we keep a flag from previous trial:
        # We'll implement via storing a variable prev_rare; initialize before loop.
        # However within this structure, we compute bias using stored variable updated at end.
        # To keep code consistent, we initialize and update below outside this block.
        # The variable 'rare_flag' will be set before softmax; handle this by setting at first iteration to 0.

        # We'll set rare_flag via closure variable; initialize before loop if not exists.
        try:
            rare_flag  # noqa: F823
        except NameError:
            rare_flag = 0.0

        extra_bias = rare_b * anx * rare_flag  # applied to repeating previous a1
        bias_vec = stick * stick1 + extra_bias * stick1

        logits1 = invt * Q1 + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Observe reward
        r = reward[t]

        # TD updates
        # Stage-2
        d2 = r - Q2[s2, a2]
        Q2[s2, a2] += lr_sel(d2) * d2

        # Stage-1 SARSA-style backup from realized Q2 for the chosen a1
        target1 = Q2[s2, a2]
        d1 = target1 - Q1[a1]
        Q1[a1] += lr_sel(d1) * d1
        # Optional direct reward influence
        Q1[a1] += lr_sel(d2) * d2

        # Update rare_flag for the NEXT trial, based on what just happened
        # A transition is "common" if reached state is the more likely one under chosen a1.
        common_state = 0 if T_known[a1, 0] >= 0.5 else 1
        is_common = 1.0 if s2 == common_state else 0.0
        rare_flag = 1.0 - is_common  # 1 if rare, else 0

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility and lapse noise with anxiety-modulated curvature, hybrid MB/MF.

    Mechanism:
    - Stage-2 Q2(s2, a2) learned with TD(0).
    - Utility curvature transforms expected values before choice:
        u(x) = x^(phi_eff), where phi_eff mixes a risk parameter with anxiety
        (higher anxiety -> more concave utility, i.e., risk aversion).
    - Stage-1 combines model-based (using fixed transitions) and model-free caches with a mixing weight.
    - A small lapse probability mixes the softmax with a uniform distribution at both stages.

    Parameters and bounds:
    - action_1: int array (n_trials,), values in {0,1}
    - state:    int array (n_trials,), values in {0,1}
    - action_2: int array (n_trials,), values in {0,1}
    - reward:   float array (n_trials,), in [0,1]
    - stai:     float array with single element in [0,1]
    - model_parameters: tuple/list with five params:
        lr   in [0,1]: learning rate for Q2 and stage-1 MF cache
        invt in [0,10]: inverse temperature for softmax
        risk in [0,1]: baseline risk/curvature parameter (0=extreme concavity, 1=linear)
        mix  in [0,1]: weight on model-based values at stage-1 (rest on MF)
        lps  in [0,1]: lapse probability (uniform mixing in action probabilities)

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, invt, risk, mix, lps = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])
    eps = 1e-12

    # Fixed transition structure for MB projection
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-modulated curvature: higher anxiety -> more concave (phi smaller)
    # phi_eff in (0,1]: ensure at least small curvature>0
    phi_eff = np.clip(0.1 + 0.9 * (risk * (1.0 - 0.5 * anx)), 0.05, 1.0)

    def util(x):
        x = np.clip(x, 0.0, 1.0)
        return np.power(x, phi_eff)

    for t in range(n_trials):
        # Stage-2 policy: transform action values by utility before softmax
        s2 = state[t]
        u_Q2 = util(Q2[s2])
        logits2 = invt * u_Q2
        logits2 -= np.max(logits2)
        p_soft2 = np.exp(logits2)
        p_soft2 = p_soft2 / (np.sum(p_soft2) + eps)
        # Mix with lapse
        p2_mix = (1.0 - lps) * p_soft2 + lps * 0.5
        a2 = action_2[t]
        p2[t] = p2_mix[a2]

        # Stage-1 MB projection uses maximum u(Q2) for each s2
        u_v2 = np.max(util(Q2), axis=1)
        Q1_mb = T @ u_v2

        # Combine MB and MF at stage-1
        Q1 = mix * Q1_mb + (1.0 - mix) * util(Q1_mf)

        logits1 = invt * Q1
        logits1 -= np.max(logits1)
        p_soft1 = np.exp(logits1)
        p_soft1 = p_soft1 / (np.sum(p_soft1) + eps)
        p1_mix = (1.0 - lps) * p_soft1 + lps * 0.5
        a1 = action_1[t]
        p1[t] = p1_mix[a1]

        # Learning updates
        r = reward[t]
        # Stage-2 TD(0)
        d2 = r - Q2[s2, a2]
        Q2[s2, a2] += lr * d2

        # Stage-1 MF cache updated toward utility of realized second-stage value
        target1 = Q2[s2, a2]
        d1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += lr * d1
        # Small additional reward-consistent update
        Q1_mf[a1] += lr * d2

    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return nll