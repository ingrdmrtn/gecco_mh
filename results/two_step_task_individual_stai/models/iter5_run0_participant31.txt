def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-modulated hybrid control with anxiety-sensitive learning-rate gating.

    Overview
    --------
    This model blends model-based (MB) and model-free (MF) control at the first stage.
    Second-stage values are learned via TD. A dynamically tracked uncertainty signal
    (from unsigned RPE) gates both:
      - the effective learning rate (higher uncertainty => faster learning),
      - the MB/MF arbitration (higher uncertainty and higher anxiety => shift toward MF).

    Anxiety use
    -----------
    The participant's anxiety (stai) increases the gain on the learning rate when
    uncertainty is high and reduces the MB weight under uncertainty.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0/1; e.g., W/S on X, P/H on Y).
    reward : array-like of float
        Rewards received (e.g., 0/1).
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1].
    model_parameters : array-like of floats, length 5
        [alpha0, beta, tau_h0, upsilon, k_anx_eta]
        - alpha0 in [0,1]: base learning rate for value updates.
        - beta in [0,10]: inverse temperature for softmax.
        - tau_h0 in [0,1]: baseline MB weight at stage 1.
        - upsilon in [0,1]: update rate for uncertainty (unsigned RPE) tracker.
        - k_anx_eta in [0,1]: anxiety gain on learning rate under uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha0, beta, tau_h0, upsilon, k_anx_eta = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: actions A/U; cols: states X/Y

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Action values
    q1_mf = np.zeros(2)        # MF first-stage Q
    q2 = np.zeros((2, 2))      # second-stage Q per state

    # Uncertainty tracker (unsigned RPE EMA)
    uncert = 0.0

    for t in range(n_trials):
        # Model-based Q from transition model and current second-stage values
        max_q2 = np.max(q2, axis=1)                 # length 2 over states
        q1_mb = transition_matrix @ max_q2          # length 2 over first-stage actions

        # Anxiety-uncertainty arbitration:
        # - learning rate scales with uncertainty and anxiety
        # - MB weight is reduced under uncertainty more for anxious individuals
        # Learning-rate gain: base scaled by uncertainty (0.5+uncert) and anxiety gain around 0.5
        eta = alpha0 * (0.5 + uncert) * (1.0 + k_anx_eta * (stai - 0.5) * 2.0)
        # Clamp to [0,1]
        if eta < 0.0:
            eta = 0.0
        if eta > 1.0:
            eta = 1.0

        # Hybrid weight: baseline minus anxiety-weighted uncertainty
        tau_h = tau_h0 - (stai * uncert * 0.5)
        if tau_h < 0.0:
            tau_h = 0.0
        if tau_h > 1.0:
            tau_h = 1.0

        q1_h = tau_h * q1_mb + (1.0 - tau_h) * q1_mf

        # First-stage policy
        logits1 = beta * q1_h
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1_sum = np.sum(p1) + 1e-12
        p1 /= p1_sum
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2_sum = np.sum(p2) + 1e-12
        p2 /= p2_sum
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcomes
        r = reward[t]

        # TD updates
        # Stage-2 TD error
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta * delta2

        # Stage-1 MF bootstrapping towards the chosen second-stage action value
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta * delta1

        # Update uncertainty (unsigned RPE EMA from stage-2)
        uncert = (1.0 - upsilon) * uncert + upsilon * abs(delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-driven aversion to surprising transitions and negative-outcome emphasis.

    Overview
    --------
    A purely model-free learner updates first- and second-stage Q values via SARSA(0).
    Two biases shape policy/value construction:
      1) Surprise bias at stage 1: common transitions receive a bonus; rare transitions a penalty.
         This bias is stronger when anxiety is high.
      2) Negative-outcome emphasis: zero reward is treated as a negative utility whose magnitude
         increases with anxiety (captures anxious sensitivity to non-reward).

    Anxiety use
    -----------
    - Reduces inverse temperature (more exploration).
    - Increases surprise bias magnitude.
    - Increases negative weighting of zero outcomes.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within state.
    reward : array-like of float
        Obtained rewards (e.g., 0/1).
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1].
    model_parameters : array-like of floats, length 5
        [alpha, beta, kappa0, zeta_tr, psi_anx_exp]
        - alpha in [0,1]: TD learning rate.
        - beta in [0,10]: base inverse temperature.
        - kappa0 in [0,1]: baseline disutility magnitude for zero reward.
        - zeta_tr in [0,1]: magnitude of transition surprise bias in first-stage logits.
        - psi_anx_exp in [0,1]: anxiety effect on exploration and surprise weighting.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, kappa0, zeta_tr, psi_anx_exp = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective temperature: higher anxiety => more exploration
    beta_eff = beta * (1.0 - 0.7 * psi_anx_exp * stai)
    if beta_eff < 0.0:
        beta_eff = 0.0
    if beta_eff > 10.0:
        beta_eff = 10.0

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Surprise indicator: common=+1, rare=-1 based on fixed task structure
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        surprise_bias = 1.0 if is_common else -1.0

        # Anxiety-weighted transition bias applied to first-stage logits
        tr_bias_mag = zeta_tr * (1.0 + stai * psi_anx_exp)
        bias_vec = np.zeros(2)
        # Apply bias in favor of the action that would make the observed transition common
        # For action A (0), common -> X (0); for U (1), common -> Y (1).
        # We nudge the chosen action's logit according to whether current transition was common/rare.
        bias_vec[a1] = tr_bias_mag * surprise_bias

        # Stage-1 policy
        logits1 = beta_eff * q1 + bias_vec
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= (np.sum(p1) + 1e-12)
        p_choice_1[t] = p1[a1]

        # Stage-2 policy
        logits2 = beta_eff * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= (np.sum(p2) + 1e-12)
        p_choice_2[t] = p2[a2]

        # Subjective outcome: treat zero as negative utility with anxiety-dependent magnitude
        kappa = kappa0 * (0.5 + stai)
        subj_r = r if r > 0.0 else -kappa

        # TD updates
        delta2 = subj_r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-driven meta-control: dynamic temperature and arbitration with anxiety coupling.

    Overview
    --------
    This hybrid model adapts both the exploration level (beta) and the arbitration
    between model-based and model-free values (phi) based on an online volatility
    estimate of the reward environment. Volatility is tracked as the EMA of the
    change in the signed RPE across trials. Anxiety increases sensitivity to
    volatility, pushing the agent toward more exploration and more MF control.

    Mechanics
    ---------
    - Volatility v_t = (1 - k_vol) * v_{t-1} + k_vol * |delta2_t - delta2_{t-1}|
    - Effective beta_t = beta0 * (1 + (0.5 - stai) * rho_stai) / (1 + v_t)
    - Hybrid weight phi_t = clip(phi0 - rho_stai * stai * v_t, 0, 1)

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices.
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float
        Rewards obtained (e.g., 0/1).
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1].
    model_parameters : array-like of floats, length 5
        [alpha, beta0, phi0, k_vol, rho_stai]
        - alpha in [0,1]: TD learning rate for values.
        - beta0 in [0,10]: baseline inverse temperature.
        - phi0 in [0,1]: baseline MB weight at stage 1.
        - k_vol in [0,1]: update rate for volatility tracker.
        - rho_stai in [0,1]: strength of anxiety coupling to meta-control.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta0, phi0, k_vol, rho_stai = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0
    prev_delta2 = 0.0

    for t in range(n_trials):
        # Compute MB first-stage Q
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Volatility-driven meta-control and exploration
        beta_t = beta0 * (1.0 + (0.5 - stai) * rho_stai)
        beta_t = beta_t / (1.0 + v)
        if beta_t < 0.0:
            beta_t = 0.0
        if beta_t > 10.0:
            beta_t = 10.0

        phi_t = phi0 - rho_stai * stai * v
        if phi_t < 0.0:
            phi_t = 0.0
        if phi_t > 1.0:
            phi_t = 1.0

        q1_h = phi_t * q1_mb + (1.0 - phi_t) * q1_mf

        # First-stage policy
        logits1 = beta_t * q1_h
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 /= (np.sum(p1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = state[t]
        logits2 = beta_t * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 /= (np.sum(p2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = p2[a2]

        # Outcome and TD updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update volatility from RPE change and carry over for next trial
        v = (1.0 - k_vol) * v + k_vol * abs(delta2 - prev_delta2)
        prev_delta2 = delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll