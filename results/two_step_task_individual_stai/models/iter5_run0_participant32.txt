Below are three standalone cognitive models as Python functions. Each returns the negative log-likelihood of the observed choices. All parameters are used, respect the requested bounds, and stai is used meaningfully in each model. No imports are included; assume numpy as np is available.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with learned transitions and anxiety-modulated planning weight.

    Core idea:
    - Second-stage (aliens) values are learned model-free.
    - First-stage values are a hybrid of model-based planning (using a learned transition model)
      and model-free bootstrapping from second-stage values.
    - The model-based weighting is modulated by anxiety: higher stai can shift reliance between MB and MF.
    - Transition probabilities from each spaceship to planets are learned via Rescorla-Wagner updating.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial (0/1: two aliens within the reached state).
    reward : array-like of float
        Reward obtained each trial (e.g., 0/1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha2, beta, w_mb0, kappa_anx, tau_trans]
        Bounds:
        - alpha2: [0,1] learning rate for MF value updates (both stages).
        - beta:   [0,10] inverse temperature for both stages.
        - w_mb0:  [0,1] baseline weight on model-based value at stage 1.
        - kappa_anx: [0,1] strength with which stai shifts the MB weight (positive pushes toward MB).
        - tau_trans: [0,1] learning rate for updating transition probabilities.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha2, beta, w_mb0, kappa_anx, tau_trans = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Q-values
    q1_mf = np.zeros(2)        # model-free first-stage action values
    q2 = np.zeros((2, 2))      # second-stage values: state x action

    # Learned transition model: rows = action (A/U), cols = state (X/Y)
    # Initialize to neutral 0.5/0.5
    T = np.ones((2, 2)) * 0.5

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Anxiety-modulated MB weight: logistic transform to keep in [0,1]
        # Effective logit is shifted by kappa_anx*(stai-0.5) around baseline w_mb0.
        eps_clip = 1e-8
        w0 = np.clip(w_mb0, eps_clip, 1 - eps_clip)
        logit_w0 = np.log(w0 / (1 - w0))
        w_mb = 1.0 / (1.0 + np.exp(-(logit_w0 + (kappa_anx * (stai - 0.5) * 4.0))))
        w_mb = np.clip(w_mb, 0.0, 1.0)

        # Model-based action values at stage 1: expectation over learned transitions of max Q2 at each state
        max_q2_per_state = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2_per_state           # shape (2,)

        # Hybrid value for stage 1
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Policy stage 1
        logits1 = beta * (q1_hybrid - np.max(q1_hybrid))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within reached state)
        s = state[t]
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values (standard MF)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Update first-stage MF value toward the obtained second-stage value (bootstrapping)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update transitions for the chosen action: RW learning toward observed state
        # T[a1, :] moves toward one-hot(s)
        for st in range(2):
            target = 1.0 if st == s else 0.0
            T[a1, st] += tau_trans * (target - T[a1, st])

        # Re-normalize rows to guard against numerical drift
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive second stage with learned variance and anxiety-modulated uncertainty bonus.

    Core idea:
    - Learn second-stage action values and their outcome variance.
    - Choice at stage 2 uses a mean-variance utility: U = mean - xi_risk * variance.
    - Add an uncertainty bonus to prefer options with higher estimated variance; the bonus is
      amplified with higher anxiety (exploration under anxiety).
    - First-stage values are computed model-based from fixed transitions (A->X common, U->Y common),
      using the max utility in each state.
    - Standard MF backup from stage 2 to stage 1.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward (e.g., 0/1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha, beta, xi_risk, zeta_unc, phi_anx]
        Bounds:
        - alpha: [0,1] learning rate for updating means and variance (variance via squared PE).
        - beta: [0,10] inverse temperature for both stages (applied to utilities).
        - xi_risk: [0,1] weight on variance penalty in utility (0 risk-neutral; 1 risk-averse).
        - zeta_unc: [0,1] base weight of uncertainty (variance) bonus.
        - phi_anx: [0,1] anxiety modulation strength on the uncertainty bonus.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, xi_risk, zeta_unc, phi_anx = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Values and variance estimates for second stage
    q2_mean = np.zeros((2, 2))
    q2_var = np.zeros((2, 2))   # running estimate of outcome variance

    # First-stage MF values for bootstrapping
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Fixed transition structure (common: 0.7, rare: 0.3)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    for t in range(n_trials):
        # Anxiety-modulated uncertainty bonus weight (increases with anxiety)
        unc_weight = zeta_unc * (1.0 + phi_anx * stai)
        # Construct risk-sensitive utilities at second stage for each state
        utilities_state = np.zeros((2, 2))
        for s in range(2):
            # mean-variance utility
            utilities_state[s, :] = q2_mean[s, :] - xi_risk * np.clip(q2_var[s, :], 0.0, 1.0)
            # add exploration bonus based on estimated variance
            utilities_state[s, :] += unc_weight * np.sqrt(np.clip(q2_var[s, :], 0.0, 1.0))

        # Stage 1 model-based values from fixed transitions and utilities
        maxU_per_state = np.max(utilities_state, axis=1)
        q1_mb = T_fixed @ maxU_per_state

        # Hybrid choice at stage 1: we use pure MB for policy but include MF backup in learning.
        q1_policy = q1_mb

        # Stage 1 policy
        logits1 = beta * (q1_policy - np.max(q1_policy))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in reached state using utilities
        s = state[t]
        u2 = utilities_state[s, :].copy()
        logits2 = beta * (u2 - np.max(u2))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates from realized outcome
        r = reward[t]

        # Update second-stage mean and variance (EWMA of squared PE)
        pe2 = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha * pe2
        # Variance update: v <- (1-alpha)*v + alpha*(pe^2)
        q2_var[s, a2] = (1.0 - alpha) * q2_var[s, a2] + alpha * (pe2 ** 2)

        # MF backup to stage 1: move toward the updated mean value (not utility to avoid double-counting bonuses)
        target1 = q2_mean[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Transition-outcome-dependent perseveration with anxiety-modulated lapse.

    Core idea:
    - Model-free learner at both stages with bootstrap from stage 2 to stage 1.
    - First-stage decision includes a stay/switch bias that depends on whether the previous
      trial's transition was common vs. rare and whether it was rewarded (a classic MB signature).
      The strength of this bias is a parameter.
    - Additionally, an anxiety-modulated lapse rate injects random choice at both stages,
      capturing increased choice noise with higher anxiety.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward (e.g., 0/1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha, beta, kappa_tr, lapse0, anx_gain]
        Bounds:
        - alpha: [0,1] learning rate for MF updates (both stages).
        - beta: [0,10] inverse temperature for both stages.
        - kappa_tr: [0,1] strength of transition-outcome-dependent stay/switch bias at stage 1.
        - lapse0: [0,1] baseline lapse rate mixed with uniform choice; capped at 0.5 internally.
        - anx_gain: [0,1] scales how much anxiety increases lapse.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, kappa_tr, lapse0, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition structure to classify common vs rare
    # Common if (A->X) or (U->Y); rare otherwise.
    def is_common(a, s):
        return (a == 0 and s == 0) or (a == 1 and s == 1)

    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # Anxiety-modulated lapse (increase with stai). Cap at 0.5 to retain identifiability.
        lapse = np.clip(lapse0 * (1.0 + anx_gain * stai), 0.0, 0.5)

        # Transition-outcome-dependent stay/switch bias
        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            # Compute MB-consistent stay tendency:
            # If previous was common and rewarded -> stay; rare and rewarded -> switch (i.e., anti-stay).
            # If unrewarded, reverse: common+loss -> switch; rare+loss -> stay.
            prev_common = is_common(prev_a1, prev_s)
            rewarded = (prev_r > 0.0)

            # MB stay signal (+1 favors repeating prev_a1; -1 favors switching)
            mb_stay = 0.0
            if rewarded and prev_common:
                mb_stay = 1.0
            elif rewarded and (not prev_common):
                mb_stay = -1.0
            elif (not rewarded) and prev_common:
                mb_stay = -1.0
            else:  # not rewarded and rare
                mb_stay = 1.0

            # Apply to logits as a bias favoring previous action
            bias_strength = kappa_tr * mb_stay
            bias_vec[prev_a1] = bias_strength
            bias_vec[1 - prev_a1] = -bias_strength

        # Stage 1 policy: softmax over Q1 with bias, then lapse mixture
        logits1 = beta * (q1 - np.max(q1)) + bias_vec
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (within reached state) with lapse
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Bootstrap to stage 1
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Cache for next trial
        prev_a1 = a1
        prev_s = s
        prev_r = r

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik