def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated perseveration and lapse with MF-MB blending.

    This model blends model-free (MF) and model-based (MB) action values at stage 1,
    learns second-stage Q-values with a TD rule, and propagates MF value to stage 1 via
    bootstrapping. Anxiety increases choice perseveration and lapse, and reduces MB reliance.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], obtained reward
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: list/tuple of 5 parameters (bounds in brackets):
        alpha2 [0,1]: learning rate for second-stage MF Q-values
        beta  [0,10]: inverse temperature for softmax at both stages
        w0    [0,1]: baseline weight of MB vs MF at stage 1
        kappa_stay [0,1]: baseline perseveration strength (bias to repeat previous action)
        eps0  [0,1]: baseline lapse probability (uniform random choice component)

    Anxiety usage
    - MB reliance decreases with anxiety: w_mb = clip(w0 * (1 - 0.4*stai))
    - Perseveration increases with anxiety: kappa = kappa_stay * (0.5 + stai)
    - Lapse increases with anxiety: epsilon = clip(0.5, eps0 * (0.5 + 0.8*stai))

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, w0, kappa_stay, eps0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: rows = actions [A,U], cols = states [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.zeros((2, 2))      # stage-2 MF Q(s, a2)
    q1_mf = np.zeros(2)        # stage-1 MF Q(a1)

    # Choice probability storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration traces (for previous actions)
    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated control parameters
    w_mb = min(1.0, max(0.0, w0 * (1.0 - 0.4 * stai)))
    kappa = kappa_stay * (0.5 + stai)
    epsilon = min(0.5, max(0.0, eps0 * (0.5 + 0.8 * stai)))

    for t in range(n_trials):
        # Model-based stage-1 values by planning to second-stage max Q
        max_q2 = np.max(q2, axis=1)       # [X, Y]
        q1_mb = T @ max_q2                # [A, U]

        # Combine MB and MF, add perseveration bias on previous action
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        if prev_a1 is not None:
            bias = np.array([0.0, 0.0])
            bias[prev_a1] += kappa
            q1 = q1 + bias

        # Softmax with lapse: pi = (1-eps)*softmax(beta*q) + eps*uniform
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (per-state softmax with perseveration on last a2 in that state)
        s = int(state[t])
        q2_state = q2[s].copy()
        if prev_a2_by_state[s] is not None:
            q2_state[prev_a2_by_state[s]] += kappa
        q2_centered = q2_state - np.max(q2_state)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Stage-2 TD learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF bootstrapping from experienced second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use same learning rate as stage-2 for simplicity and parsimony
        q1_mf[a1] += alpha2 * pe1

        # Update perseveration traces
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-weighted uncertainty bonus via Beta-Bernoulli with leaky counts.

    This model maintains Beta posteriors over each alien's payout probability and uses
    the posterior mean plus an exploration bonus proportional to posterior uncertainty.
    The bonus magnitude scales up with anxiety. Stage-1 values are model-based projections
    of second-stage action values, blended with a simple MF stage-1 value learned via a
    leaky bootstrap. No free learning-rate for second-stage is needed because we update
    exact Beta counts.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) within the state
    - reward: array-like of floats in [0,1], reward obtained
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: list/tuple of 4 parameters (bounds in brackets):
        beta        [0,10]: inverse temperature for softmax policies
        kappa_unc   [0,1]: base coefficient for the uncertainty (std) exploration bonus
        w_mb        [0,1]: weight on model-based values at stage 1 (vs MF)
        gamma_leak  [0,1]: leak rate toward priors for Beta counts and MF stage-1 value

    Anxiety usage
    - Uncertainty-driven exploration increases with anxiety:
        bonus_scale = kappa_unc * (0.5 + stai)
      so higher anxiety yields larger directed exploration toward uncertain aliens.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    beta, kappa_unc, w_mb, gamma_leak = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition matrix (fixed, known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Beta-Bernoulli parameters for each alien: alpha_succ, beta_fail
    # Initialize to (1,1) uniform prior
    succ = np.ones((2, 2), dtype=float)
    fail = np.ones((2, 2), dtype=float)

    # Stage-1 MF value (leaky bootstrap)
    q1_mf = np.zeros(2, dtype=float)

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    bonus_scale = kappa_unc * (0.5 + stai)

    for t in range(n_trials):
        # Posterior means and stds for each alien
        mean = succ / (succ + fail)
        # Posterior variance for Beta(a,b): ab / [(a+b)^2 (a+b+1)]
        a = succ
        b = fail
        var = (a * b) / ((a + b) ** 2 * (a + b + 1.0))
        std = np.sqrt(var)

        # Stage-2 target values with directed exploration bonus
        q2 = mean + bonus_scale * std

        # Model-based stage-1 value: expected best alien per planet under T
        max_q2 = np.max(q2, axis=1)   # best alien value on X and Y
        q1_mb = T @ max_q2

        # Stage-1 decision values: blend MB and MF
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = int(state[t])
        q2_state = q2[s]
        q2_centered = q2_state - np.max(q2_state)
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update Beta counts with leak toward prior (1,1)
        r = float(reward[t])
        # Leak toward prior: counts <- (1 - gamma)*counts + gamma*1
        succ = (1.0 - gamma_leak) * succ + gamma_leak * 1.0
        fail = (1.0 - gamma_leak) * fail + gamma_leak * 1.0
        # Add fractional Bernoulli evidence from outcome
        succ[s, a2] += r
        fail[s, a2] += (1.0 - r)

        # Update stage-1 MF value by leaky bootstrap from the realized second-stage chosen value (posterior mean)
        target1 = mean[s, a2]
        q1_mf = (1.0 - gamma_leak) * q1_mf
        q1_mf[a1] += gamma_leak * target1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning with anxiety-weighted surprise gating of model-based control.

    This model learns the transition structure online, updates second-stage MF values
    from reward, and blends MB and MF at stage 1. High anxiety reduces reliance on MB
    control on surprising (rare) transitions by down-weighting the MB contribution when
    the experienced transition was unexpected.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions
    - reward: array-like of floats in [0,1], received reward
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: list/tuple of 5 parameters (bounds in brackets):
        alpha2     [0,1]: learning rate for second-stage MF Q-values
        beta       [0,10]: inverse temperature for softmax at both stages
        eta_trans  [0,1]: learning rate for transition probabilities T_hat
        gate_gain  [0,1]: strength of anxiety-weighted surprise gating on MB weight
        mb_base    [0,1]: baseline MB mixture weight at stage 1

    Anxiety usage
    - Surprise gating of MB weight:
        surprise_t = 1 - T_hat[a1, s]  (unexpectedness of reached state)
        w_mb_t = clip(mb_base * (1 - gate_gain * stai * surprise_t))
      So rare/unexpected transitions reduce MB reliance more strongly with higher anxiety.

    Returns
    - Negative log-likelihood over observed choices.
    """
    alpha2, beta, eta_trans, gate_gain, mb_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T_hat close to the canonical structure
    T_hat = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Ensure rows are valid probabilities at all times
    def normalize_rows(M):
        row_sums = np.sum(M, axis=1, keepdims=True)
        # avoid divide by zero
        row_sums[row_sums == 0.0] = 1.0
        return M / row_sums

    # Value functions
    q2 = np.zeros((2, 2), dtype=float)  # stage-2 MF Q(s, a2)
    q1_mf = np.zeros(2, dtype=float)    # stage-1 MF Q(a1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Model-based stage-1 from current T_hat and q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Compute policy for stage 1 with surprise-gated MB mixture
        a1 = int(action_1[t])
        s = int(state[t])

        # Surprise of observed transition under current T_hat
        surprise = 1.0 - T_hat[a1, s]
        w_mb_t = mb_base * (1.0 - gate_gain * stai * surprise)
        # Clip mixture weight to [0,1]
        w_mb_t = min(1.0, max(0.0, w_mb_t))

        q1 = w_mb_t * q1_mb + (1.0 - w_mb_t) * q1_mf
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        q2_centered = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]
        # Update second-stage MF values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Propagate MF value to stage 1 for the chosen action
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update learned transition probabilities T_hat for the chosen action
        # Move probability mass toward the observed state
        for sp in (0, 1):
            if sp == s:
                T_hat[a1, sp] += eta_trans * (1.0 - T_hat[a1, sp])
            else:
                T_hat[a1, sp] += eta_trans * (0.0 - T_hat[a1, sp])
        # Re-normalize the updated row to maintain a valid distribution
        T_hat[a1] = T_hat[a1] / np.sum(T_hat[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll