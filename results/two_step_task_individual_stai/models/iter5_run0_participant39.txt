def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid RL with learned transitions and anxiety-modulated information bonus.

    The agent learns:
      - Second-stage values model-free from reward.
      - First-stage transition probabilities for each spaceship.
      - First-stage action values as a hybrid of model-based (via learned transitions)
        and model-free values. An information bonus encourages or discourages choosing
        uncertain transitions, with anxiety modulating the direction/strength.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array (n_trials,), observed second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; alien choice)
    - reward: np.array (n_trials,), outcomes (e.g., 0/1 coins)
    - stai: np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        lr_r:   [0,1] learning rate for second-stage Q-values
        beta:   [0,10] inverse temperature for both stages
        lr_T0:  [0,1] baseline learning rate for transition probabilities
        info0:  [0,1] baseline weight on information bonus at stage 1
        mix0:   [0,1] baseline weight of model-based over model-free at stage 1

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    lr_r, beta, lr_T0, info0, mix0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize values
    q1_mf = np.zeros(2)        # model-free values for spaceships
    q2 = np.zeros((2, 2))      # second-stage Q-values: state x action
    # Learned transition probabilities p(s'|a1). Start uninformative 0.5.
    p_trans = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Effective transition learning rate: higher anxiety -> faster updating (hypervigilance)
    lr_T = np.clip(lr_T0 * (0.5 + 0.5 * stai0), 0.0, 1.0)
    # Effective info weight: high anxiety turns bonus into uncertainty aversion
    # (sign flip around zero when stai>0.5).
    info_w = info0 * (1.0 - 2.0 * stai0)
    # Effective MB mixing weight: high anxiety reduces planning reliance
    w_mb = np.clip(mix0 * (1.0 - 0.5 * stai0), 0.0, 1.0)

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q for stage 1 using learned transitions and current q2
        max_q2 = np.max(q2, axis=1)  # size 2 for states X/Y
        q1_mb = np.array([
            p_trans[0, 0] * max_q2[0] + p_trans[0, 1] * max_q2[1],
            p_trans[1, 0] * max_q2[0] + p_trans[1, 1] * max_q2[1]
        ])

        # Uncertainty per action (Bernoulli variance)
        uncert = p_trans[:, 0] * (1.0 - p_trans[:, 0])  # same as for state 1
        q1_aug = (w_mb * q1_mb + (1.0 - w_mb) * q1_mf) + info_w * uncert

        # Stage 1 policy
        q1s = q1_aug - np.max(q1_aug)
        probs1 = np.exp(beta * q1s)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * q2s)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn transitions for chosen a1 toward observed state s2
        # For binary state, we update p(a1->s2) toward 1 and the other toward 0
        p_old = p_trans[a1, s2]
        p_trans[a1, s2] = p_old + lr_T * (1.0 - p_old)
        other = 1 - s2
        p_old_other = p_trans[a1, other]
        p_trans[a1, other] = p_old_other + lr_T * (0.0 - p_old_other)

        # Stage 2 value update
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += lr_r * pe2

        # Stage 1 model-free update toward realized second-stage action value
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += lr_r * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric win/loss learning with anxiety-modulated transition-based perseveration.

    The agent learns second-stage values with separate learning rates for wins vs losses.
    First-stage choices include a perseveration bias that depends on whether the last
    transition was common or rare. Anxiety strengthens rare-transition-driven biases.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array (n_trials,), observed second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        lr_win0: [0,1] baseline learning rate when reward=1
        lr_loss0: [0,1] baseline learning rate when reward=0
        beta: [0,10] inverse temperature for both stages
        stick0: [0,1] baseline perseveration after common transitions
        rare0: [0,1] baseline perseveration after rare transitions

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    lr_win0, lr_loss0, beta, stick0, rare0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure for common/rare check (A->X, U->Y commonly)
    common_prob = 0.7

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_common = None

    # Anxiety modulation:
    # - High anxiety lowers win learning and increases loss learning (negativity bias).
    lr_win = np.clip(lr_win0 * (1.0 - 0.5 * stai0), 0.0, 1.0)
    lr_loss = np.clip(lr_loss0 * (0.5 + 0.5 * stai0), 0.0, 1.0)
    # - Perseveration: high anxiety reduces common-stick but increases rare-driven bias
    stick_common = stick0 * (1.0 - stai0)
    stick_rare = rare0 * (0.5 + 0.5 * stai0)

    eps = 1e-10
    for t in range(n_trials):
        # First-stage softmax with transition-dependent perseveration
        bias = np.zeros(2)
        if prev_a1 is not None:
            if prev_common:
                bias[prev_a1] += stick_common
            else:
                # After rare transitions, anxiety-weighted bias to repeat previous a1
                bias[prev_a1] += stick_rare
        q1_eff = q1_mf + bias
        q1s = q1_eff - np.max(q1_eff)
        probs1 = np.exp(beta * q1s)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second stage policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * q2s)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage Q with asymmetric learning rates
        alpha2 = lr_win if r > 0.0 else lr_loss
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha2 * pe2

        # Stage-1 MF update toward the realized second-stage action value
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        # Use an averaged alpha from win/loss to reflect affective bias at stage 1
        alpha1 = 0.5 * (lr_win + lr_loss)
        q1_mf[a1] += alpha1 * pe1

        # Determine if the current transition was common vs rare for bias next trial
        if a1 == 0:
            # spaceship A commonly goes to X (0)
            was_common = (s2 == 0)
        else:
            # spaceship U commonly goes to Y (1)
            was_common = (s2 == 1)

        prev_a1 = a1
        prev_common = was_common

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-based planning with dual temperatures, reward sensitivity, and anxiety-driven decay.

    The agent:
      - Learns second-stage values model-free with reward sensitivity (utility scaling).
      - Plans at stage 1 using the fixed transition model (common=0.7, rare=0.3).
      - Uses separate inverse temperatures for stages 1 and 2.
      - Applies anxiety-modulated decay (forgetting) of second-stage values toward 0.5.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state: np.array (n_trials,), observed second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward: np.array (n_trials,), outcomes (0/1)
    - stai: np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha: [0,1] learning rate for second-stage Q-values
        beta1_base: [0,10] baseline inverse temperature at stage 1
        beta2_base: [0,10] baseline inverse temperature at stage 2
        decay0: [0,1] baseline decay rate toward 0.5 at second stage
        rho0: [0,1] baseline reward sensitivity (scales reward before learning)

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta1_base, beta2_base, decay0, rho0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Transition matrix known
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation:
    # - Higher anxiety -> more noise at stage 1, sharper focus at stage 2 (vigilant local policy)
    beta1 = max(0.0, beta1_base * (1.0 - 0.7 * stai0))
    beta2 = max(0.0, beta2_base * (0.6 + 0.4 * stai0))
    # - Higher anxiety -> stronger decay (forgetting/volatility tracking)
    decay = np.clip(decay0 * (0.5 + 0.5 * stai0), 0.0, 1.0)
    # - Reward sensitivity: high anxiety blunts utility impact
    rho = rho0 * (1.0 - 0.5 * stai0)

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q for stage 1 from current second-stage max values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Stage 1 policy
        q1s = q1_mb - np.max(q1_mb)
        probs1 = np.exp(beta1 * q1s)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s2 = state[t]
        q2s = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta2 * q2s)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Apply decay toward 0.5 baseline before updating (anxiety -> more decay)
        q2 = (1.0 - decay) * q2 + decay * 0.5

        # Reward-sensitive update at second stage
        r = reward[t]
        util = rho * r + (1.0 - rho) * 0.5  # interpolates raw reward with neutral 0.5
        pe2 = util - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # No explicit stage-1 MF term; first-stage learning is purely model-based

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll