def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-gated arbitration and action bias.

    Overview
    - Stage-2 values (Q2) learned via Rescorla–Wagner (alpha).
    - Stage-1 choice values are a weighted mixture of model-based (MB) and model-free (MF) values.
    - The arbitration weight w_t shifts toward MB when transitions are expected and away when surprise occurs;
      anxiety amplifies the impact of transition surprise on reducing MB control.
    - A static first-stage action bias term depends on anxiety (approach/avoid tendency).

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:     [0,1]   Learning rate for Q updates (both stages).
    - beta:      [0,10]  Inverse temperature for softmax at both stages.
    - w0:        [0,1]   Initial model-based weight at stage 1.
    - alpha_w:   [0,1]   Update rate for arbitration weight based on transition surprise.
    - kappa_bias:[0,1]   Magnitude of first-stage static bias; sign set by (stai - 0.5).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, w0, alpha_w, kappa_bias].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, w0, alpha_w, kappa_bias = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Task transition structure (fixed, known)
    # Rows: actions (A=0, U=1). Cols: states (X=0, Y=1).
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    q1_mf = np.zeros(2) + 0.5         # stage-1 model-free action values
    q2 = np.zeros((2, 2)) + 0.5       # stage-2 values per state-action

    # Arbitration weight
    w = float(w0)

    # Anxiety-dependent static bias at stage 1 (favor A vs U depending on sign)
    # Bias vector applied as additive logits shift.
    bias_mag = kappa_bias * (stai - 0.5)  # in [-0.5*kappa_bias, 0.5*kappa_bias]
    bias1 = np.array([+bias_mag, -bias_mag])

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1 from current Q2
        v2 = np.max(q2, axis=1)                 # value of each state (best alien)
        q1_mb = T @ v2

        # Mixture of MB and MF, plus anxiety-dependent static bias
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1_mix + bias1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Model-free update at stage 1 toward the realized second-stage value
        delta1 = (q2[s, a2] - q1_mf[a1])
        q1_mf[a1] += alpha * delta1

        # Arbitration weight update: reduce MB weight when transition is surprising,
        # with anxiety scaling this effect.
        p_obs = T[a1, s]                 # probability of observed transition given chosen action
        trans_surprise = 1.0 - p_obs     # 0 for common (0.7), 0.7 for rare (0.3)
        target_w = 1.0 - stai * trans_surprise
        target_w = min(1.0, max(0.0, target_w))
        w = (1.0 - alpha_w) * w + alpha_w * target_w

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman-Rescorla hybrid with anxiety-inflated observation noise and stage-2 choice kernel.

    Overview
    - Stage-2 values are learned with an uncertainty-adaptive learning rate (Kalman-like):
      alpha_t = Var / (Var + sigma_obs_eff), updated per state-action.
    - Anxiety increases effective observation noise, reducing learning from outcomes.
    - A stage-2 choice kernel biases perseveration, strengthened by anxiety.
    - Stage-1 is purely model-based using the known transition matrix and current Q2.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - beta:        [0,10]  Inverse temperature for softmax (both stages).
    - sigma_obs:   [0,1]   Base observation noise for outcome (Bernoulli-like).
    - kappa_proc:  [0,1]   Process noise; increases uncertainty over time when options are sampled.
    - rho_anx:     [0,1]   Scales how much anxiety inflates observation noise: sigma_eff = sigma_obs*(1 + rho_anx*stai).
    - tau_ck2:     [0,1]   Learning rate/strength for stage-2 choice kernel (perseveration).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [beta, sigma_obs, kappa_proc, rho_anx, tau_ck2].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    beta, sigma_obs, kappa_proc, rho_anx, tau_ck2 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Stage-2 value means and variances
    q2 = np.zeros((2, 2)) + 0.5
    v2 = np.zeros((2, 2)) + 0.25  # initial uncertainty

    # Stage-2 choice kernel (per-state)
    ck2 = np.zeros((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-inflated observation noise
    sigma_eff = sigma_obs * (1.0 + rho_anx * stai)
    sigma_eff = max(1e-6, min(1.0, sigma_eff))

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1: pure MB from current Q2
        v_state = np.max(q2, axis=1)
        q1_mb = T @ v_state

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Stage-2 policy with choice kernel; kernel gain increases with anxiety
        kernel_gain = 1.0 + stai
        logits2 = beta * q2[s] + ck2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Kalman-like update at stage 2 for chosen (s, a2)
        # Adaptive learning rate
        alpha_t = v2[s, a2] / (v2[s, a2] + sigma_eff)
        alpha_t = max(0.0, min(1.0, float(alpha_t)))

        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * pe2

        # Update uncertainty: posterior variance and process noise injection
        v2[s, a2] = (1.0 - alpha_t) * v2[s, a2] + kappa_proc
        v2[s, a2] = max(1e-6, min(1.0, v2[s, a2]))

        # Update choice kernel (perseveration) with anxiety-scaled increment
        ck2[s] = (1.0 - tau_ck2) * ck2[s]
        ck2[s, a2] += tau_ck2 * kernel_gain

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB with anxiety-weighted risk sensitivity and stage-1 stickiness.

    Overview
    - Learns the state transition probabilities online (per action) via a simple delta rule.
    - Stage-1 model-based values incorporate a risk penalty: V_state = E[Q2] - zeta*stai*Var,
      where Var is approximated by q*(1-q) for the best action in each state.
    - Stage-2 values learned via Rescorla–Wagner (alpha).
    - Stage-1 stickiness (choice kernel) biases repeating the previous stage-1 choice,
      with magnitude scaled by anxiety.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:        [0,1]   Learning rate for Q2 updates.
    - beta:         [0,10]  Inverse temperature for softmax at both stages.
    - zeta_risk:    [0,1]   Strength of risk penalty at stage 1, scaled by anxiety.
    - phi_trans:    [0,1]   Learning rate for updating transition probabilities T_hat.
    - kappa_stick:  [0,1]   Stage-1 stickiness (choice kernel) learning rate/strength.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, zeta_risk, phi_trans, kappa_stick].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, zeta_risk, phi_trans, kappa_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix T_hat to be agnostic (0.5/0.5 per action)
    T_hat = np.ones((2, 2)) * 0.5

    # Stage-2 values
    q2 = np.zeros((2, 2)) + 0.5

    # Stage-1 choice kernel (stickiness)
    ck1 = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]
        r = reward[t]

        # Risk-adjusted state values: V_state = max_a q2[s,a] - zeta*stai*Var
        q2_max = np.max(q2, axis=1)
        q2_argmax = np.argmax(q2, axis=1)
        # Approximate Bernoulli variance for the best action in each state
        best_probs = q2[np.arange(2), q2_argmax]
        var_state = best_probs * (1.0 - best_probs)
        risk_penalty = zeta_risk * stai * var_state
        v_state = q2_max - risk_penalty

        # Stage-1 model-based Q using learned transitions
        q1_mb = T_hat @ v_state

        # Add stage-1 stickiness (anxiety scales its effective strength)
        stick_gain = 0.5 + 0.5 * stai
        logits1 = beta * q1_mb + ck1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = soft1[a1]

        # Stage-2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = soft2[a2]

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stickiness kernel
        ck1 = (1.0 - kappa_stick) * ck1
        ck1[a1] += kappa_stick * stick_gain

        # Transition learning for chosen action: move probability toward observed state
        # Ensure row sums to 1 by updating both columns.
        T_hat[a1, s] += phi_trans * (1.0 - T_hat[a1, s])
        other = 1 - s
        T_hat[a1, other] = 1.0 - T_hat[a1, s]

        # Keep probabilities in [0,1]
        T_hat[a1, s] = min(1.0, max(0.0, T_hat[a1, s]))
        T_hat[a1, other] = 1.0 - T_hat[a1, s]

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)