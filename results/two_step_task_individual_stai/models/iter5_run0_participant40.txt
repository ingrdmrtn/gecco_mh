def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB–MF with anxiety-gated arbitration, decay-forgetting, and choice perseveration.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, omega_base, kappa_rep, mu_forget)
        - alpha2 in [0,1]: learning rate for second-stage MF values and stage-1 MF bootstrapping.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - omega_base in [0,1]: baseline MB arbitration weight at stage 1.
        - kappa_rep in [0,1]: choice perseveration strength (applied to both stages).
        - mu_forget in [0,1]: global forgetting factor applied to all second-stage Q-values.
          Effective forgetting scales with anxiety: decay = mu_forget * stai.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Transition structure is known: A→X and U→Y with prob 0.7 (rare=0.3).
    - Stage-1 policy is hybrid: Q1 = (1 - omega_eff)*Q1_MF + omega_eff*(T @ max(Q2)).
      Anxiety reduces reliance on model-based control: omega_eff = omega_base * (1 - stai).
    - Second-stage forgetting is stronger with higher anxiety: Q2 ← (1 - mu_forget*stai) * Q2 each trial.
    - Perseveration biases logits toward repeating the last chosen action; bias magnitude increases
      mildly with anxiety: bias = kappa_rep * (0.5 + 0.5*stai) on the previously chosen action.
    """
    alpha2, beta, omega_base, kappa_rep, mu_forget = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities of observed actions
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q2 = np.zeros((2, 2))      # second-stage MF values: Q2[state, action]
    Q1_MF = np.zeros(2)        # first-stage MF values

    prev_a1 = None
    prev_a2 = None

    # Anxiety-gated arbitration and forgetting
    omega_eff = omega_base * (1.0 - stai_val)
    decay = mu_forget * stai_val

    for t in range(n_trials):
        # Model-based contribution to stage-1 values
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Hybrid Q at stage 1
        Q1 = (1.0 - omega_eff) * Q1_MF + omega_eff * Q1_MB

        # Perseveration biases (increase with anxiety)
        bias_mag = kappa_rep * (0.5 + 0.5 * stai_val)
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += bias_mag

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        bias2 = np.zeros(2)
        if prev_a2 is not None:
            bias2[prev_a2] += bias_mag
        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Learning: stage-2 TD
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 MF bootstrapping from realized second-stage chosen value
        delta1 = Q2[s2, a2] - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1

        # Anxiety-scaled forgetting across all Q2 values
        if decay > 0.0:
            Q2 *= (1.0 - decay)

        prev_a1 = a1
        prev_a2 = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free control with transition-surprise–gated learning and anxiety-modulated decay.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha_base, beta, kappa1, chi_surprise, gamma_decay)
        - alpha_base in [0,1]: base MF learning rate.
        - beta in [0,10]: inverse temperature (both stages).
        - kappa1 in [0,1]: first-stage stickiness strength.
        - chi_surprise in [0,1]: multiplicative boost of learning rate on rare transitions.
        - gamma_decay in [0,1]: decay factor for unchosen values (stronger with anxiety).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Transition structure is known (common=0.7, rare=0.3), but control is model-free.
      Surprise from rare transitions increases the learning rate:
      alpha_eff = alpha_base * (1 + chi_surprise * surprise * (1 + stai)),
      where surprise=1 for rare transitions, 0 for common.
    - Unchosen values decay toward 0 with strength gamma_decay * stai, capturing
      anxiety-related uncertainty and value erosion.
    - Stage-1 and stage-2 policies are softmax over MF values with stickiness at stage 1.
    """
    alpha_base, beta, kappa1, chi_surprise, gamma_decay = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known common transitions: A->X, U->Y
    def is_rare(a1, s2):
        # Returns 1 if rare, else 0
        return int((a1 == 0 and s2 == 1) or (a1 == 1 and s2 == 0))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))  # MF values at stage 2
    Q1 = np.zeros(2)       # MF values at stage 1

    prev_a1 = None
    stickiness_bias = 0.0

    for t in range(n_trials):
        # Stage-1 softmax with stickiness
        a1 = int(action_1[t])
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            # Stickiness does not depend on the observed transition; anxiety reduces consistency
            # Here we assume higher anxiety weakens the stickiness expression
            bias1[prev_a1] = kappa1 * (1.0 - 0.5 * stai_val)

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Surprise-gated learning rate
        surprise = is_rare(a1, s2)
        alpha_eff = alpha_base * (1.0 + chi_surprise * surprise * (1.0 + stai_val))

        # TD updates
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha_eff * delta2

        # Stage-1 bootstrapping from Q2 chosen value
        delta1 = Q2[s2, a2] - Q1[a1]
        Q1[a1] += alpha_eff * delta1

        # Anxiety-modulated decay of unchosen values
        decay = gamma_decay * stai_val
        if decay > 0.0:
            # Decay other action at the visited state
            other_a2 = 1 - a2
            Q2[s2, other_a2] *= (1.0 - decay)
            # Decay both actions at the unvisited state
            other_s2 = 1 - s2
            Q2[other_s2, :] *= (1.0 - decay)
            # Decay unchosen first-stage action
            Q1[1 - a1] *= (1.0 - decay)

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive hybrid with asymmetric learning and anxiety-weighted utility.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Obtained reward each trial.
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, rho)
        - alpha_pos in [0,1]: learning rate for positive utility prediction errors.
        - alpha_neg in [0,1]: learning rate for negative utility prediction errors.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - rho in [0,1]: risk sensitivity exponent applied to utility transform.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Utility transform: u(r) = sign(r) * |r|^rho, with anxiety amplifying losses and dampening gains:
      u_eff = u(r) * (1 + stai) if r < 0, and u_eff = u(r) * (1 - 0.5*stai) if r >= 0.
    - Asymmetric learning: alpha_pos for positive utility PE, alpha_neg for negative utility PE.
    - Stage-1 policy is hybrid using known transitions (A→X, U→Y with 0.7):
      Q1 = (1 - omega(stai))*Q1_MF + omega(stai)*(T @ max(Q2)),
      where omega(stai) = 0.7 - 0.5*stai (more anxiety -> less model-based control).
    """
    alpha_pos, alpha_neg, beta, rho = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))
    Q1_MF = np.zeros(2)

    # Anxiety-based arbitration: higher anxiety -> lower MB weight
    omega = max(0.0, min(1.0, 0.7 - 0.5 * stai_val))

    def utility(r):
        # Risk-sensitive utility with anxiety-weighted valence
        mag = np.abs(r) ** rho
        u = np.sign(r) * mag
        if r < 0:
            u *= (1.0 + stai_val)
        else:
            u *= (1.0 - 0.5 * stai_val)
        return u

    for t in range(n_trials):
        # Model-based Q1 from max over aliens
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2
        Q1 = (1.0 - omega) * Q1_MF + omega * Q1_MB

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Outcome utility
        u = utility(reward[t])

        # Stage-2 TD with asymmetric learning
        pe2 = u - Q2[s2, a2]
        alpha2_eff = alpha_pos if pe2 >= 0.0 else alpha_neg
        Q2[s2, a2] += alpha2_eff * pe2

        # Stage-1 MF bootstrapping from second stage
        pe1 = Q2[s2, a2] - Q1_MF[a1]
        alpha1_eff = alpha_pos if pe1 >= 0.0 else alpha_neg
        Q1_MF[a1] += alpha1_eff * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll