def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-gated MB/MF arbitration with learned transitions and lapse.

    Core ideas
    - Learn the transition matrix P(planet | spaceship) online (alpha_tr).
    - First-stage policy mixes model-based (MB) and model-free (MF) values with
      an arbitration weight that decreases with transition uncertainty and with anxiety.
    - Second-stage values learned via TD (alpha_mf).
    - A lapse component increases with anxiety, adding choice noise.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher anxiety reduces MB arbitration weight and increases lapse.
    model_parameters : array-like of float
        [alpha_mf, beta, k_mb0, alpha_tr, lapse0]
        - alpha_mf in [0,1]: TD learning rate for MF Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - k_mb0 in [0,1]: baseline MB arbitration weight (scaled by uncertainty and anxiety).
        - alpha_tr in [0,1]: learning rate for transition probabilities.
        - lapse0 in [0,1]: baseline lapse rate blended with softmax; increases with anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_mf, beta, k_mb0, alpha_tr, lapse0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix; start near canonical structure but uncertain
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # MF action values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    # Lapse increases with anxiety
    lapse_eff = np.clip(lapse0 * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute transition uncertainty (entropy averaged across actions)
        row_ent = []
        for a in range(2):
            p_row = np.clip(T[a], eps, 1.0)
            h = -np.sum(p_row * np.log(p_row))
            row_ent.append(h)
        unc = 0.5 * (row_ent[0] + row_ent[1])  # higher = more uncertain
        # Normalize uncertainty to [0, ln2]; convert to [0,1]
        unc_norm = np.clip(unc / np.log(2.0), 0.0, 1.0)

        # MB estimate for first stage
        max_q2 = np.max(q2, axis=1)  # best available at each planet
        q1_mb = T @ max_q2

        # Arbitration weight: lower when transitions are uncertain or anxiety is high
        w_mb = np.clip(k_mb0 * (1.0 - 0.6 * unc_norm) * (1.0 - 0.5 * st), 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy: softmax with lapse
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p1[t] = probs1[a1]

        # Second-stage policy: softmax with the same lapse
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p2[t] = probs2[a2]

        # Update learned transitions using observed outcome of first-stage action
        # Move the chosen row toward the observed state
        T[a1] *= (1.0 - alpha_tr)
        T[a1, s] += alpha_tr
        # Ensure rows sum to 1, stay within bounds
        T[a1] = np.clip(T[a1], 0.0, 1.0)
        T[a1] /= (np.sum(T[a1]) + eps)

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * pe2

        # Stage-1 MF update bootstrapped from obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_mf * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Asymmetric learning with anxiety amplification and perseveration bias.

    Core ideas
    - Stage-2 learning uses different learning rates for wins and losses, and anxiety
      amplifies loss learning and dampens win learning.
    - First-stage value is a hybrid: MB weight is a simple function of anxiety (no parameter),
      MF learned via TD from stage-2.
    - Perseveration bias (same-choice tendency) is present at both stages and grows with anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase perseveration and loss-focused learning.
    model_parameters : array-like of float
        [alpha_win, alpha_lose, beta, kappa_pers]
        - alpha_win in [0,1]: learning rate when reward prediction error is positive.
        - alpha_lose in [0,1]: learning rate when reward prediction error is non-positive.
        - beta in [0,10]: inverse temperature for both stages.
        - kappa_pers in [0,1]: baseline perseveration strength.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_win, alpha_lose, beta, kappa_pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure according to task description
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # MF values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    # Perseveration kernels (one-hot of last choice)
    last_a1 = None
    last_a2 = [None, None]  # per state

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-gated components
    # MB weight increases when anxiety is low
    w_mb = np.clip(0.7 * (1.0 - st), 0.0, 1.0)
    # Effective perseveration grows with anxiety
    kappa_eff = np.clip(kappa_pers * (0.5 + 0.5 * st), 0.0, 5.0)  # cap bias magnitude

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-attenuated reward impact (reduced utility under high anxiety)
        r_eff = (0.8 - 0.3 * st) * r

        # MB first-stage values from current second-stage values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add perseveration bias to first stage
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa_eff

        logits1 = beta * (q1 - np.max(q1)) + bias1 - np.max(bias1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Second-stage policy with state-dependent perseveration
        bias2 = np.zeros(2)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += kappa_eff

        logits2 = beta * (q2[s] - np.max(q2[s])) + bias2 - np.max(bias2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Stage-2 asymmetric learning with anxiety amplification
        pe2 = r_eff - q2[s, a2]
        if pe2 > 0:
            alpha2 = alpha_win * (1.0 - 0.5 * st)
        else:
            alpha2 = alpha_lose * (0.5 + 0.5 * st)
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF update from obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Use a moderate learning rate derived from the two, scaled by anxiety
        alpha1 = (0.5 * (alpha_win + alpha_lose)) * (0.8 + 0.2 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Kalman-like reward tracking with anxiety-driven volatility and uncertainty bonus.

    Core ideas
    - Track each second-stage action's mean reward and uncertainty (variance).
    - Volatility/process noise increases with anxiety, yielding higher Kalman gains.
    - Stage-2 policy includes a directed exploration bonus proportional to uncertainty,
      attenuated by anxiety.
    - Stage-1 uses a hybrid of MB (via expected values from the transition model)
      and MF with small forgetting.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase volatility and reduce directed exploration.
    model_parameters : array-like of float
        [beta, vol0, omega_mix, theta_decay]
        - beta in [0,10]: inverse temperature for both stages.
        - vol0 in [0,1]: baseline process noise for reward dynamics (higher -> more volatile).
        - omega_mix in [0,1]: baseline MB weight at stage 1.
        - theta_decay in [0,1]: MF stage-1 forgetting toward 0 (per trial).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    beta, vol0, omega_mix, theta_decay = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure as per task
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 estimates: mean and variance for each planet-alien pair
    m2 = 0.5 * np.ones((2, 2))
    v2 = 0.25 * np.ones((2, 2))  # initial uncertainty
    v_min, v_max = 1e-6, 0.25

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    # Anxiety effects
    # Volatility increases with anxiety
    q_proc = np.clip(vol0 * (0.5 + 0.5 * st), 0.0, 1.0)
    # Directed exploration bonus scale decreases with anxiety
    bonus_scale = np.clip(0.5 * (1.0 - st), 0.0, 1.0)
    # MB weight decreases with anxiety
    w_mb = np.clip(omega_mix * (1.0 - 0.5 * st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Build uncertainty-bonus-augmented values at stage 2
        bonus = bonus_scale * np.sqrt(np.clip(v2, v_min, v_max))
        q2_aug = m2 + bonus

        # First-stage MB values: expectation over next-state max augmented value
        max_q2_aug = np.max(q2_aug, axis=1)
        q1_mb = T @ max_q2_aug

        # Combine MB and MF for stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Policies
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        logits2 = beta * (q2_aug[s] - np.max(q2_aug[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Kalman-like update for the chosen second-stage action
        # Predict step: increase variance by process noise
        v_pred = np.clip(v2[s, a2] + q_proc, v_min, v_max)
        m_pred = m2[s, a2]

        # Observation noise for Bernoulli reward approximated by p(1-p)
        rvar = np.clip(m_pred * (1.0 - m_pred), 1e-4, 0.25)

        K = v_pred / (v_pred + rvar)  # Kalman gain in [0,1]
        pe = r - m_pred
        m2[s, a2] = m_pred + K * pe
        v2[s, a2] = (1.0 - K) * v_pred
        v2[s, a2] = np.clip(v2[s, a2], v_min, v_max)

        # Stage-1 MF: decay then TD update toward observed second-stage mean value
        q1_mf = (1.0 - theta_decay) * q1_mf
        target1 = m2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Effective learning rate mirrors the current Kalman gain (confidence in update)
        q1_mf[a1] += K * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)