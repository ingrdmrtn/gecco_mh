def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive hybrid MB/MF with anxiety-modulated reward curvature.

    Overview:
    - Stage 2 values learned via TD(0).
    - Stage 1 choice values are a mixture of model-based (using known transitions) and model-free values.
    - Rewards are transformed by a concave/convex power utility u(r) = sign(r)*|r|^gamma_eff.
    - Trait anxiety (stai) modulates curvature: gamma_eff = clip(gamma0 + k_gamma*stai, [0,1]).
      Lower gamma => more risk/loss aversion; higher gamma => more risk seeking in magnitude.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for value updates (both stages).
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1.
    - gamma0: [0,1] base utility curvature.
    - k_gamma: [0,1] how strongly anxiety shifts curvature (gamma_eff = gamma0 + k_gamma*stai).

    Inputs:
    - action_1: array-like of ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like of ints {0,1}, second-stage states (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage choices (aliens indices).
    - reward: array-like of floats, coins received (can be negative/zero/positive).
    - stai: array-like with a single float in [0,1], trait anxiety.
    - model_parameters: tuple/list of 5 params (alpha, beta, w, gamma0, k_gamma).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, gamma0, k_gamma = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Initialize values
    Q2 = np.zeros((2, 2))   # stage-2 Q(s, a2)
    Q1_mf = np.zeros(2)     # stage-1 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated utility curvature
    gamma_eff = gamma0 + k_gamma * stai
    if gamma_eff < 0.0:
        gamma_eff = 0.0
    if gamma_eff > 1.0:
        gamma_eff = 1.0

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Mixture MB/MF
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage 1 choice probability
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probability
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Utility transform of reward with anxiety-modulated curvature
        r = reward[t]
        if r >= 0:
            u = (abs(r)) ** gamma_eff
        else:
            u = - (abs(r)) ** gamma_eff

        # TD update stage 2
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF bootstrapping from updated stage-2 value
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated surprise-seeking intrinsic motivation.

    Overview:
    - Stage 2: standard TD(0) learning on rewards.
    - Stage 1: mixture of model-based and model-free values.
    - Additionally, an intrinsic "surprise" signal is maintained per first-stage action:
        surprise_t = -log P(observed_state | chosen_ship) under the known transition model.
      A running trace of surprise per action S[a] is updated and added as a bonus to stage-1 values.
    - Trait anxiety scales the strength of the surprise bonus:
        z_eff = clip(z0 + k_z * stai, [0,1]).
      Higher z_eff increases preference for actions that recently produced surprising transitions.

    Parameters (model_parameters):
    - alpha: [0,1] learning rate for Q-value updates and surprise trace updates.
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1.
    - z0: [0,1] base weight of surprise bonus.
    - k_z: [0,1] how strongly anxiety increases surprise bonus (z_eff = z0 + k_z*stai).

    Inputs:
    - action_1: array-like of ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like of ints {0,1}, second-stage states (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage choices.
    - reward: array-like of floats.
    - stai: array-like with a single float in [0,1].
    - model_parameters: tuple/list of 5 params (alpha, beta, w, z0, k_z).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, z0, k_z = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions for surprise computation
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    Q2 = np.zeros((2, 2))   # stage-2
    Q1_mf = np.zeros(2)     # stage-1 MF
    S = np.zeros(2)         # surprise trace per first-stage action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    z_eff = z0 + k_z * stai
    if z_eff < 0.0:
        z_eff = 0.0
    if z_eff > 1.0:
        z_eff = 1.0

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Add intrinsic surprise bonus to stage-1 values
        # The bonus is added additively to the MB/MF mixture.
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf + z_eff * S

        # Stage 1 choice probability
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probability
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Reward learning at stage 2
        r = reward[t]
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage 1 MF bootstrapping
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Update surprise trace for the chosen first-stage action using observed transition
        # surprise = -log P(s | a1) under known T
        p_s_given_a = T[a1, s]
        # Prevent log(0)
        if p_s_given_a < 1e-8:
            p_s_given_a = 1e-8
        surprise = -np.log(p_s_given_a)
        # Running exponential average with learning rate alpha
        S[a1] += alpha * (surprise - S[a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-amplified RPE-adaptive learning rates.

    Overview:
    - Stage 2 values learned via TD(0), but the learning rate is adapted each trial:
        alpha_eff = clip(alpha + v_eff * |RPE|, [0,1]),
      where RPE is the stage-2 reward prediction error and
        v_eff = clip(v0 + k_v * stai, [0,1]).
      Thus, higher anxiety increases sensitivity to surprising outcomes, speeding learning.
    - Stage 1 MF values are updated via bootstrapping using the same adaptive alpha_eff.
    - Stage 1 decisions combine model-based and model-free values.

    Parameters (model_parameters):
    - alpha: [0,1] base learning rate.
    - beta: [0,10] inverse temperature for both stages.
    - w: [0,1] weight on model-based control at stage 1.
    - v0: [0,1] base gain that scales learning rate with |RPE|.
    - k_v: [0,1] anxiety modulation of adaptive gain (v_eff = v0 + k_v*stai).

    Inputs:
    - action_1: array-like of ints {0,1}, first-stage choices (0=A, 1=U).
    - state: array-like of ints {0,1}, second-stage states (0=X, 1=Y).
    - action_2: array-like of ints {0,1}, second-stage choices.
    - reward: array-like of floats.
    - stai: array-like with a single float in [0,1].
    - model_parameters: tuple/list of 5 params (alpha, beta, w, v0, k_v).

    Returns:
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w, v0, k_v = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transitions
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    Q2 = np.zeros((2, 2))   # stage-2 values
    Q1_mf = np.zeros(2)     # stage-1 MF

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated adaptive gain
    v_eff = v0 + k_v * stai
    if v_eff < 0.0:
        v_eff = 0.0
    if v_eff > 1.0:
        v_eff = 1.0

    eps = 1e-12

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2

        # Mixture at stage 1
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage 1 choice probability
        q1c = Q1 - np.max(Q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage 2 choice probability
        s = state[t]
        q2c = Q2[s] - np.max(Q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Compute RPE and adaptive learning rate
        r = reward[t]
        delta2 = r - Q2[s, a2]
        alpha_eff = alpha + v_eff * abs(delta2)
        if alpha_eff < 0.0:
            alpha_eff = 0.0
        if alpha_eff > 1.0:
            alpha_eff = 1.0

        # Update stage 2 with adaptive LR
        Q2[s, a2] += alpha_eff * delta2

        # Stage 1 MF bootstrapping with adaptive LR
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha_eff * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll