def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """SRâ€“MB arbitration with anxiety-modulated model-based control.
    
    Summary
    -------
    This model blends a model-based (MB) planner with a learned successor-like
    representation (SR) of first-stage actions predicting second-stage states.
    Anxiety reduces reliance on the MB planner in favor of the SR cache.
    
    Mechanics
    ---------
    - Stage 2: Learn Q-values for aliens via delta-rule.
    - SR: Learn a mapping M[a1, s2] via a delta-rule toward the reached state.
    - Stage 1 values:
        MB: E[max_a2 Q2 | transition_matrix]
        SR: SR-predicted occupancy dot max_a2 Q2
      Q1 = w_mb * MB + (1 - w_mb) * SR, where w_mb decreases with anxiety.
    - Policies: softmax with inverse temperature beta at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1]. Higher anxiety reduces MB weight.
    model_parameters : iterable of floats
        [alpha_r, beta, w0, alpha_sr, kappa_anx]
        - alpha_r: learning rate for stage-2 Q-values [0,1]
        - beta: inverse temperature for both stages [0,10]
        - w0: baseline MB weight in arbitration [0,1]
        - alpha_sr: learning rate for SR mapping [0,1]
        - kappa_anx: strength by which anxiety lowers MB reliance [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, w0, alpha_sr, kappa_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure (common transitions)
    T_mb = np.array([[0.7, 0.3],  # From A to [X, Y]
                     [0.3, 0.7]], # From U to [X, Y]
                    dtype=float)

    # Initialize tracking
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values (planets x aliens), initialized to 0.5
    q2 = np.zeros((2, 2)) + 0.5

    # Successor-like mapping: M[a1, s2] ~ P(s2 | a1) learned from experience
    M = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Effective MB weight, reduced by anxiety
    w_mb = w0 * (1.0 - kappa_anx * st)
    w_mb = np.clip(w_mb, 0.0, 1.0)

    for t in range(n_trials):
        # Compute MB and SR action values for stage 1
        vmax2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T_mb @ vmax2        # model-based
        q1_sr = M @ vmax2           # SR-based
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_sr

        # Stage 1 choice probability
        logits1 = q1 - np.max(q1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy within reached planet
        s2 = state[t]
        logits2 = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        # Update stage-2 Q
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # Update SR mapping from chosen a1 toward the reached state
        target = np.array([0.0, 0.0])
        target[s2] = 1.0
        M[a1] = (1.0 - alpha_sr) * M[a1] + alpha_sr * target
        # Normalize row to stay a distribution
        row_sum = M[a1, 0] + M[a1, 1]
        if row_sum > 0:
            M[a1] = M[a1] / row_sum

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration via leaky Bayesian counts, scaled by anxiety.
    
    Summary
    -------
    This model encourages sampling uncertain aliens by adding an uncertainty bonus
    to second-stage values, with leaky accumulation of successes and counts to track
    slowly drifting probabilities. Anxiety increases the directed exploration bonus and
    slightly reduces perseveration bias.
    
    Mechanics
    ---------
    - Maintain leaky counts N[s,a] and successes S[s,a].
    - Estimated mean: m = S / (N + tiny); uncertainty: u = 1 / (N + 1).
    - Stage 2 value: Q2 = m + phi(stai) * u, where phi increases with anxiety.
    - Stage 1 value: MB projection of max Q2 via known transitions.
    - Perseveration at stage 1 reduced by anxiety.
    - Policies: softmax with inverse temperature beta at both stages.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage alien choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1]. Higher anxiety increases exploration bonus.
    model_parameters : iterable of floats
        [rho_decay, beta, phi0, pi0]
        - rho_decay: leaky decay factor applied each trial to counts and successes [0,1]
                     (larger means slower forgetting; 0 = full reset, 1 = no decay)
        - beta: inverse temperature for both stages [0,10]
        - phi0: baseline directed exploration bonus weight [0,1]
        - pi0: baseline perseveration bias magnitude at stage 1 [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    rho_decay, beta, phi0, pi0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Leaky Bayesian counts for each alien
    N = np.zeros((2, 2)) + 1.0  # start with weak prior count to avoid zero
    S = np.zeros((2, 2)) + 0.5  # weak prior successes for ~0.5 mean

    prev_a1 = None

    # Anxiety-modulated exploration bonus and perseveration
    phi = phi0 * (0.5 + st)           # higher anxiety -> larger exploration bonus
    persev = pi0 * (1.0 - 0.5 * st)   # higher anxiety -> less perseveration

    for t in range(n_trials):
        # Leak counts each trial
        N *= rho_decay
        S *= rho_decay

        # Compute posterior means and uncertainty
        m = S / (N + 1e-8)
        u = 1.0 / (N + 1.0)

        # Stage 2 action values with uncertainty bonus
        q2 = m + phi * u

        # Stage 1 MB projection
        vmax2 = np.max(q2, axis=1)
        q1 = T @ vmax2

        # Add perseveration bias at stage 1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] += persev

        logits1 = q1 + bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(beta * logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in reached planet
        s2 = state[t]
        logits2 = q2[s2] - np.max(q2[s2])
        probs2 = np.exp(beta * logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Update counts with leaky accumulation using observed outcome
        r = reward[t]
        N[s2, a2] += 1.0
        S[s2, a2] += r

        prev_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated arbitration with anxiety and lapse, plus eligibility trace to Q1.
    
    Summary
    -------
    This hybrid model mixes model-based (MB) and model-free (MF) control at stage 1.
    The MB weight is reduced after rare transitions and further reduced by anxiety.
    A simple eligibility trace assigns second-stage prediction errors to first-stage
    Q-values. A small lapse probability allows random responding.
    
    Mechanics
    ---------
    - Stage 2: Learn Q2 via delta-rule.
    - Stage 1 MF: Learn Q1 via TD with eligibility lambda from second-stage PE.
    - Stage 1 MB: E[max_a2 Q2 | known transitions].
    - Arbitration weight: w_t = clip(omega0 * (1 - stai * rare_t), 0, 1),
      where rare_t=1 if the observed transition was rare for the chosen spaceship.
    - Policies: softmax at both stages with inverse temperature beta, mixed with
      lapse epsilon toward uniform choice.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0 = A, 1 = U).
    state : array-like of int (0 or 1)
        Reached second-stage planet (0 = X, 1 = Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached planet.
    reward : array-like of float
        Trial outcomes (e.g., 0/1 coins).
    stai : array-like of float
        Anxiety score(s); uses stai[0] in [0,1]. Higher anxiety lowers MB control after rare transitions.
    model_parameters : iterable of floats
        [nu_r, beta, omega0, lam, epsilon]
        - nu_r: learning rate for value updates (Q2 and Q1) [0,1]
        - beta: inverse temperature for both stages [0,10]
        - omega0: baseline MB weight in arbitration [0,1]
        - lam: eligibility trace from stage-2 PE to Q1 update [0,1]
        - epsilon: lapse probability for random responding at each stage [0,1]
    
    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    nu_r, beta, omega0, lam, epsilon = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known common transitions: A->X, U->Y are common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize values
    q2 = np.zeros((2, 2)) + 0.5         # stage-2 values
    q1_mf = np.zeros(2) + 0.0           # stage-1 model-free values

    for t in range(n_trials):
        # Model-based projection
        vmax2 = np.max(q2, axis=1)
        q1_mb = T @ vmax2

        # Arbitration weight depends on whether last transition was rare
        # For the current trial's choice, we won't know rare/common until after observing state,
        # but for action selection we use expected weight based on common transitions.
        # We'll approximate by using baseline omega0 (before seeing the transition).
        w = np.clip(omega0, 0.0, 1.0)

        # Combine MF and MB for policy
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Stage 1 policy with lapse
        logits1 = q1 - np.max(q1)
        probs1_soft = np.exp(beta * logits1)
        probs1_soft = probs1_soft / np.sum(probs1_soft)
        probs1 = (1.0 - epsilon) * probs1_soft + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with lapse
        s2 = state[t]
        logits2 = q2[s2] - np.max(q2[s2])
        probs2_soft = np.exp(beta * logits2)
        probs2_soft = probs2_soft / np.sum(probs2_soft)
        probs2 = (1.0 - epsilon) * probs2_soft + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update Q2
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += nu_r * pe2

        # Determine if the observed transition was rare for the chosen a1
        # Common mapping: A->X (0->0), U->Y (1->1)
        common_state = a1  # because of the mapping above
        rare = 1.0 if s2 != common_state else 0.0

        # Anxiety- and surprise-gated weight to update MF/MB arbitration going forward:
        # Reduce MB weight when transition was rare, more so for higher anxiety.
        w_post = np.clip(omega0 * (1.0 - st * rare), 0.0, 1.0)

        # Update stage-1 MF value using eligibility trace from stage-2 PE
        # TD target uses the chosen second-stage action value
        td1 = q2[s2, a2] - q1_mf[a1]
        q1_mf[a1] += nu_r * lam * td1

        # Optionally, incorporate the updated arbitration into q1_mf as a bias toward MB target
        # (keeps all parameters used meaningfully without altering policy already evaluated)
        blend_target = w_post * q1_mb[a1] + (1.0 - w_post) * q1_mf[a1]
        q1_mf[a1] = 0.5 * q1_mf[a1] + 0.5 * blend_target  # gentle alignment

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll