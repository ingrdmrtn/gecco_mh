def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-tilted arbitration and transition stickiness.

    Bounds for parameters:
    - alpha: [0,1] learning rate for second-stage values
    - beta: [0,10] inverse temperature for both stages
    - w0: [0,1] baseline model-based weight at stage 1
    - kappa_trans: [0,1] transition-stickiness bias toward the ship commonly leading to last reached planet
    - lambda_elig: [0,1] eligibility trace strength from stage 2 PE to stage 1 MF value
    - anx_gain: [0,1] scales how anxiety (stai) shifts arbitration weight toward MB

    Inputs
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U)
    - state: array-like (n_trials,), reached planet (0=X, 1=Y)
    - action_2: array-like (n_trials,), second-stage choices (0 or 1)
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1)
    - stai: array-like (1,), anxiety score in [0,1]
    - model_parameters: [alpha, beta, w0, kappa_trans, lambda_elig, anx_gain]

    Model summary
    - Stage-2 values learned with a simple delta rule.
    - Stage-1 action values are a convex combination of model-based (MB) and model-free (MF) values.
      The MB/MF arbitration weight is shifted by anxiety: w_mb = clip(w0 + anx_gain*(stai-0.5), 0,1).
    - Transition stickiness: after visiting a planet, the agent is biased toward the ship that commonly goes to that planet
      (i.e., if last state was X, bias ship A; if Y, bias ship U) by kappa_trans.
    - Eligibility trace: stage-2 prediction error propagates to the MF value of the chosen first-stage action scaled by lambda_elig.

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, w0, kappa_trans, lambda_elig, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clamp parameters to their bounds
    alpha = min(1.0, max(0.0, alpha))
    beta = min(10.0, max(1e-6, beta))
    w0 = min(1.0, max(0.0, w0))
    kappa_trans = min(1.0, max(0.0, kappa_trans))
    lambda_elig = min(1.0, max(0.0, lambda_elig))
    anx_gain = min(1.0, max(0.0, anx_gain))

    # Fixed transition structure (common 0.7, rare 0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value structures
    q2 = np.zeros((2, 2), dtype=float)     # stage-2 Q(s,a2)
    q1_mf = np.zeros(2, dtype=float)       # stage-1 MF Q(a1)

    # Choice probabilities for likelihood
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Last visited planet to implement transition stickiness
    last_state = None

    # Anxiety-tilted arbitration
    w_mb = w0 + anx_gain * (stai - 0.5)
    w_mb = min(1.0, max(0.0, w_mb))

    for t in range(n_trials):

        # Model-based stage-1 values via expected max stage-2 values
        v2_max = np.max(q2, axis=1)                 # value per planet
        q1_mb = T @ v2_max                          # expected by each ship

        # Transition stickiness bias
        bias = np.zeros(2, dtype=float)
        if last_state is not None:
            # ship 0 commonly -> state 0; ship 1 commonly -> state 1
            bias[last_state] += kappa_trans  # adds to ship matching last visited planet

        # Combine MB and MF
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb + bias

        # Stage-1 choice policy
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy
        s = state[t]
        q2_s = q2[s]
        q2s_c = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2s_c)
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observed reward
        r = reward[t]

        # Stage-2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace to stage-1 MF
        q1_mf[a1] += alpha * lambda_elig * pe2

        # Update last visited state for next trial stickiness
        last_state = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Second-stage uncertainty bonus via adaptive variance, with anxiety-amplified exploration and first-stage perseveration.

    Bounds for parameters:
    - eta_base: [0,1] base scaling of adaptive learning (influences gain via variance)
    - beta: [0,10] inverse temperature for both stages
    - phi_ucb: [0,1] base UCB bonus weight on uncertainty (variance)
    - anx_ucb: [0,1] scales UCB bonus with anxiety
    - chi_decay: [0,1] decay/smoothing of variance estimate after PE
    - stick1: [0,1] perseveration bias to repeat last first-stage action

    Inputs
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U)
    - state: array-like (n_trials,), reached planet (0=X, 1=Y)
    - action_2: array-like (n_trials,), second-stage choices (0 or 1)
    - reward: array-like (n_trials,), reward per trial
    - stai: array-like (1,), anxiety score in [0,1]
    - model_parameters: [eta_base, beta, phi_ucb, anx_ucb, chi_decay, stick1]

    Model summary
    - Maintains stage-2 Q-values and an uncertainty (variance-like) tracker per state-action.
    - Trial-wise learning gain k_t is proportional to current uncertainty: k_t = clip(eta_base * var, 0, 1).
    - UCB-like bonus added to stage-2 decision values: bonus = (phi_ucb * (1 + anx_ucb*stai)) * sqrt(var).
      Anxiety increases the exploration bonus linearly.
    - First-stage values are purely model-based, using expected max of (Q2 + bonus).
    - Perseveration at stage 1 biases repeating the previous ship choice.

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    eta_base, beta, phi_ucb, anx_ucb, chi_decay, stick1 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clamp parameters
    eta_base = min(1.0, max(0.0, eta_base))
    beta = min(10.0, max(1e-6, beta))
    phi_ucb = min(1.0, max(0.0, phi_ucb))
    anx_ucb = min(1.0, max(0.0, anx_ucb))
    chi_decay = min(1.0, max(0.0, chi_decay))
    stick1 = min(1.0, max(0.0, stick1))

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 value and variance trackers
    q2 = np.zeros((2, 2), dtype=float)
    var2 = np.ones((2, 2), dtype=float) * 0.5  # start with moderate uncertainty

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Perseveration memory
    last_a1 = None

    # Anxiety-scaled UCB gain
    ucb_gain = phi_ucb * (1.0 + anx_ucb * stai)

    for t in range(n_trials):

        # Compute UCB bonuses for both states
        bonus = ucb_gain * np.sqrt(np.maximum(var2, 1e-8))

        # Stage-1 MB values: expectation over max (Q2 + bonus) at each planet
        v2_aug = np.max(q2 + bonus, axis=1)  # per planet
        q1_mb = T @ v2_aug

        # Add perseveration bias to last chosen action
        q1 = q1_mb.copy()
        if last_a1 is not None:
            q1[last_a1] += stick1

        # Stage-1 choice policy
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        probs1 = exp_q1 / np.sum(exp_q1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice policy using augmented values at visited state
        s = state[t]
        q2_aug_s = q2[s] + bonus[s]
        q2s_c = q2_aug_s - np.max(q2_aug_s)
        exp_q2 = np.exp(beta * q2s_c)
        probs2 = exp_q2 / np.sum(exp_q2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Prediction error on raw Q (no bonus in learning)
        pe = r - q2[s, a2]

        # Learning gain proportional to uncertainty, scaled by eta_base
        k_t = eta_base * var2[s, a2]
        k_t = min(1.0, max(0.0, k_t))

        # Update value
        q2[s, a2] += k_t * pe

        # Update variance (higher with surprise, decays otherwise)
        var2[s, a2] = (1.0 - chi_decay) * var2[s, a2] + chi_decay * abs(pe)
        # Keep variance within [0,1]
        var2[s, a2] = min(1.0, max(0.0, var2[s, a2]))

        # Perseveration memory
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Pearce-Hall associability with anxiety-weighted loss sensitivity, eligibility trace, and lapse.

    Bounds for parameters:
    - alpha0: [0,1] base learning rate scaling (modulated by associability)
    - beta: [0,10] inverse temperature for both stages
    - epsilon: [0,1] lapse probability mixed with uniform choice
    - lambda_elig: [0,1] eligibility trace strength to stage-1 MF
    - phi_ph: [0,1] Pearce-Hall associability update rate
    - rho_loss: [0,1] loss sensitivity factor; anxiety increases effective loss aversion
    - w_base: [0,1] baseline model-based weight in stage-1 arbitration (modulated by associability and anxiety)

    Inputs
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U)
    - state: array-like (n_trials,), reached planet (0=X, 1=Y)
    - action_2: array-like (n_trials,), second-stage choices (0 or 1)
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1)
    - stai: array-like (1,), anxiety score in [0,1]
    - model_parameters: [alpha0, beta, epsilon, lambda_elig, phi_ph, rho_loss, w_base]

    Model summary
    - Stage-2 learning uses a Pearce-Hall associability per state-action that tracks absolute PE.
      Learning rate_t = alpha0 * associability_t * (1 + stai), allowing higher anxiety to amplify reactivity.
    - Outcomes are transformed with anxiety-weighted loss sensitivity:
        u(r) = 1 for r=1; u(r) = -rho_loss*(1+stai) for r=0.
    - Stage-1 values combine MF and MB with a dynamic weight:
        w_t = clip(w_base + (assoc_mean - 0.5) * (1 - 2*stai) * 0.5, 0, 1),
      meaning higher surprise shifts arbitration toward MB when anxiety is low; with higher anxiety, this shift is dampened or reversed.
    - Eligibility trace propagates stage-2 PE to the MF value of the chosen first-stage action.
    - Choice policies include an epsilon lapse that mixes softmax with uniform (0.5 at both stages).

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha0, beta, epsilon, lambda_elig, phi_ph, rho_loss, w_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clamp parameters
    alpha0 = min(1.0, max(0.0, alpha0))
    beta = min(10.0, max(1e-6, beta))
    epsilon = min(1.0, max(0.0, epsilon))
    lambda_elig = min(1.0, max(0.0, lambda_elig))
    phi_ph = min(1.0, max(0.0, phi_ph))
    rho_loss = min(1.0, max(0.0, rho_loss))
    w_base = min(1.0, max(0.0, w_base))

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and associabilities
    q2 = np.zeros((2, 2), dtype=float)
    assoc = np.ones((2, 2), dtype=float) * 0.5  # start at moderate associability
    q1_mf = np.zeros(2, dtype=float)

    # Likelihood holders
    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):

        # MB stage-1 values via expected max stage-2 values
        v2_max = np.max(q2, axis=1)
        q1_mb = T @ v2_max

        # Dynamic arbitration weight from associability and anxiety
        assoc_mean = np.mean(assoc)
        w_t = w_base + (assoc_mean - 0.5) * (1.0 - 2.0 * stai) * 0.5
        w_t = min(1.0, max(0.0, w_t))

        # Final stage-1 values
        q1 = (1.0 - w_t) * q1_mf + w_t * q1_mb

        # Stage-1 policy with epsilon lapse
        q1_c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1_c)
        soft1 = exp_q1 / np.sum(exp_q1)
        probs1 = (1.0 - epsilon) * soft1 + epsilon * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with epsilon lapse
        s = state[t]
        q2_s = q2[s]
        q2s_c = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2s_c)
        soft2 = exp_q2 / np.sum(exp_q2)
        probs2 = (1.0 - epsilon) * soft2 + epsilon * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Anxiety-weighted loss-sensitive utility
        r = reward[t]
        u = 1.0 if r > 0.0 else -rho_loss * (1.0 + stai)

        # Stage-2 PE and associability-based learning rate
        pe2 = u - q2[s, a2]
        lr2 = alpha0 * assoc[s, a2] * (1.0 + stai)
        lr2 = min(1.0, max(0.0, lr2))
        q2[s, a2] += lr2 * pe2

        # Update associability (Pearce-Hall): move toward |PE|
        assoc[s, a2] = (1.0 - phi_ph) * assoc[s, a2] + phi_ph * abs(pe2)
        assoc[s, a2] = min(1.0, max(0.0, assoc[s, a2]))

        # Eligibility trace to stage-1 MF
        q1_mf[a1] += lr2 * lambda_elig * pe2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll