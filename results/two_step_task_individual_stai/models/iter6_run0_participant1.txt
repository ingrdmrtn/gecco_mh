def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with uncertainty-effort arbitration, Dirichlet transition confidence, and MF eligibility.
    
    This model blends model-based (MB) and model-free (MF) control at the first stage via an arbitration
    signal that increases with transition-confidence (low entropy of learned transition rows) and decreases
    with cognitive effort costs that rise with anxiety. Second-stage values are learned with MF TD and
    an eligibility trace propagates reward back to first-stage MF values.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaQ: MF learning rate for Q-values at both stages (0..1).
    - beta: inverse temperature for both stages (0..10).
    - kappa1: first-stage perseveration bias (adds to logit for repeating previous spaceship; 0..1).
    - c_effort: baseline effort cost of MB planning (0..1).
    - zeta_eff_anx: anxiety scaling of effort cost; effective effort = c_effort * (1 + zeta_eff_anx*stai) (0..1).
    - lam: eligibility trace for propagating second-stage PE to first-stage MF Q (0..1).

    Args:
        action_1: 1D array of first-stage actions (0=A, 1=U).
        state: 1D array of encountered second-stage states (0=X, 1=Y).
        action_2: 1D array of second-stage actions within the encountered state (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: participant's anxiety score.
        model_parameters: [alphaQ, beta, kappa1, c_effort, zeta_eff_anx, lam]
    
    Returns:
        Negative log-likelihood of observed choices across both stages.
    """
    alphaQ, beta, kappa1, c_effort, zeta_eff_anx, lam = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition counts with informative priors (reflecting common transitions)
    # Dirichlet pseudocounts: A->X common, U->Y common
    counts = np.array([[7.0, 3.0],   # for action 0 (A): counts over states [X, Y]
                       [3.0, 7.0]])  # for action 1 (U): counts over states [X, Y]
    # MF values
    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = -1  # for perseveration
    prev_a2 = np.array([-1, -1], dtype=int)  # per state

    for t in range(n_trials):
        # Transition estimate and its confidence (low entropy -> high confidence)
        T = counts / (np.sum(counts, axis=1, keepdims=True) + eps)  # shape (2,2)
        # entropy per action row in bits
        H = -np.sum(T * (np.log(T + eps) / np.log(2.0)), axis=1)
        H_norm = H / 1.0  # max entropy in 2-outcome is 1 bit
        confidence_MB = 1.0 - np.mean(H_norm)  # in [0,1]

        # Anxiety-adjusted effort cost
        effort = c_effort * (1.0 + zeta_eff_anx * stai)
        # Arbitration weight favoring MB: increase with confidence, decrease with effort
        w = confidence_MB - effort
        # clip to [0,1]
        if w < 0.0:
            w = 0.0
        elif w > 1.0:
            w = 1.0

        # Model-based first-stage values via transition model and current second-stage values
        max_q2 = np.max(Q2, axis=1)  # value of each second-stage state
        Q1_mb = T @ max_q2  # shape (2,)

        # Combine MB and MF for first-stage action values
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Add first-stage perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        # First-stage policy
        logits1 = beta * Q1 + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with simple perseveration within state
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa1 * 0.5  # reuse kappa1 at half-strength for stage 2 to keep params ≤7

        logits2 = beta * Q2[s] + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Second-stage TD
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaQ * pe2

        # First-stage MF with eligibility trace driven by second-stage PE
        Q1_mf[a1] += alphaQ * lam * pe2

        # Update transition counts with the observed transition
        counts[a1, s] += 1.0

        # Update perseverations
        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive MB at stage 1 with anxiety-amplified loss aversion and volatility-adaptive learning.
    
    First-stage choices are model-based using the fixed transition structure (common 0.7/0.3).
    Second-stage learning uses a dynamic learning rate that increases with unsigned prediction error
    (Pearce–Hall style). Anxiety amplifies both volatility sensitivity and loss aversion for omitted rewards.
    Perseveration at stage 2 captures habitual repetition within a state.

    Parameters (all in [0,1] except beta in [0,10]):
    - alpha0: base learning rate for second-stage Q (0..1).
    - beta: inverse temperature for both stages (0..10).
    - phi_loss: baseline loss-aversion parameter; anxiety scales the penalty for r=0 (0..1).
    - kappa_vol: volatility sensitivity; alpha_t = clip(alpha0 + kappa_vol*stai*|PE|) (0..1).
    - kappa2: second-stage perseveration bias (0..1).
    - epsilon: lapse rate applied to both stages (0..1).

    Args:
        action_1: 1D array of first-stage actions (0=A, 1=U).
        state: 1D array of encountered second-stage states (0=X, 1=Y).
        action_2: 1D array of second-stage actions within the encountered state (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: participant's anxiety score.
        model_parameters: [alpha0, beta, phi_loss, kappa_vol, kappa2, epsilon]
    
    Returns:
        Negative log-likelihood of observed choices across both stages.
    """
    alpha0, beta, phi_loss, kappa_vol, kappa2, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = np.array([-1, -1], dtype=int)

    for t in range(n_trials):
        # Stage 1: model-based evaluation from current Q2
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Optional small perseveration at stage 1 reusing kappa2 minimally
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += 0.5 * kappa2

        logits1 = beta * Q1_mb + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        # Lapse
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2: within encountered state, include perseveration bias
        s = state[t]
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += kappa2

        logits2 = beta * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome utility with anxiety-amplified loss aversion for r=0
        r = reward[t]
        # r in {0,1}. Penalize omissions: utility u = r - (phi_loss*stai)*(1-r)
        u = r - (phi_loss * stai) * (1.0 - r)

        # TD update with volatility-adaptive learning rate
        pe2 = u - Q2[s, a2]
        alpha_t = alpha0 + kappa_vol * stai * abs(pe2)
        # clip alpha_t
        if alpha_t < 0.0:
            alpha_t = 0.0
        elif alpha_t > 1.0:
            alpha_t = 1.0
        Q2[s, a2] += alpha_t * pe2

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid with surprise-boosted model-based weight and anxiety-driven credit misassignment on rare transitions.
    
    The model uses a fixed transition structure for MB evaluation. The first-stage weight on MB control is
    dynamically modulated by transition surprise: rare transitions increase MB reliance. Anxiety reduces base MB
    reliance but also promotes generalized credit assignment across states after rare transitions (worry-driven
    misassignment), partially updating the mirror action in the unvisited state.

    Parameters (all in [0,1] except beta in [0,10]):
    - alphaR: learning rate for second-stage Q-values (0..1).
    - beta: inverse temperature for both stages (0..10).
    - omega0: baseline MB weight (0..1); transformed via logit to center the dynamic weight.
    - psi_surprise: strength of surprise boost to MB weight on rare transitions (0..1).
    - chi_misassign: strength of anxiety-driven credit misassignment across states on rare transitions (0..1).
    - kappa1: first-stage perseveration bias (0..1).
    - epsilon: lapse rate for both stages (0..1).

    Args:
        action_1: 1D array of first-stage actions (0=A, 1=U).
        state: 1D array of second-stage states (0=X, 1=Y).
        action_2: 1D array of second-stage actions within the encountered state (0/1).
        reward: 1D array of rewards (0/1).
        stai: 1D array with one element in [0,1]: participant's anxiety score.
        model_parameters: [alphaR, beta, omega0, psi_surprise, chi_misassign, kappa1, epsilon]
    
    Returns:
        Negative log-likelihood of observed choices across both stages.
    """
    alphaR, beta, omega0, psi_surprise, chi_misassign, kappa1, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (common = 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = np.array([-1, -1], dtype=int)

    # Helper for logistic transform
    def logit(p):
        p = min(max(p, 1e-6), 1.0 - 1e-6)
        return np.log(p / (1.0 - p))

    base_logit = logit(omega0)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]

        # Determine whether the observed transition was common or rare
        # Common if (a1==0 and s==0) or (a1==1 and s==1)
        is_common = 1.0 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.0
        surprise = 1.0 - is_common  # 1 for rare, 0 for common

        # Dynamic MB weight: base tendency decreased by anxiety, boosted by surprise
        z = base_logit - stai + psi_surprise * surprise
        w = 1.0 / (1.0 + np.exp(-z))  # in (0,1)

        # MB first-stage values
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Combine MB and MF for stage 1
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf

        # Stage 1 policy with perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] += kappa1

        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        probs1 = (1.0 - epsilon) * probs1 + epsilon * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (no extra bias beyond optional mild perseveration)
        bias2 = np.zeros(2)
        if prev_a2[s] != -1:
            bias2[prev_a2[s]] += 0.5 * kappa1

        logits2 = beta * Q2[s] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        probs2 = (1.0 - epsilon) * probs2 + epsilon * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn second-stage values (experienced state-action)
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alphaR * pe2

        # Anxiety-driven misassignment on rare transitions:
        # Partially update the "mirror" action in the unvisited state with the same observed outcome.
        if surprise > 0.5:
            s_other = 1 - s
            a2_mirror = a2  # generalize to the same index in the other state
            alpha_gen = alphaR * chi_misassign * stai
            if alpha_gen > 0.0:
                pe2_other = r - Q2[s_other, a2_mirror]
                Q2[s_other, a2_mirror] += alpha_gen * pe2_other

        # Update MF first-stage with a simple TD using max second-stage value of reached state
        Q1_mf[a1] += alphaR * (np.max(Q2[s]) - Q1_mf[a1])

        prev_a1 = a1
        prev_a2[s] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll