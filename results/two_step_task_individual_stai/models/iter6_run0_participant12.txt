def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and learned transitions.
    
    This model blends a model-based (MB) and model-free (MF) controller at the first stage.
    The MB component uses a learned transition model T(s1->s2), updated trial-by-trial.
    The MF component backs up second-stage rewards to first-stage values.
    Anxiety (stai) shifts the arbitration weight toward or away from MB control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration toward MB control.
    model_parameters : list or array
        [alpha, beta, omega_base, stai_alpha, alpha_trans]
        Bounds:
          alpha in [0,1]        : reward learning rate
          beta in [0,10]        : inverse temperature
          omega_base in [0,1]   : baseline MB weight at stage 1
          stai_alpha in [0,1]   : sensitivity of MB weight to STAI
          alpha_trans in [0,1]  : transition-learning rate for T
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, omega_base, stai_alpha, alpha_trans = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize value functions
    q_stage1_mf = np.zeros(2)        # MF values at stage 1 (A vs U)
    q_stage2 = np.zeros((2, 2))      # Second-stage Q-values: states X/Y x aliens (0/1)

    # Initialize transition model (row = first-stage action, col = second-stage state)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight toward MB control
    omega = np.clip(omega_base + stai_alpha * (stai - 0.5), 0.0, 1.0)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1 using learned transitions
        max_q2 = np.max(q_stage2, axis=1)         # value of best alien in each state
        q1_mb = T @ max_q2                        # expected value of each first-stage action
        q1 = (1.0 - omega) * q_stage1_mf + omega * q1_mb

        # Stage 1 policy
        q1_centered = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy for the visited state
        q2_s = q_stage2[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning: second stage TD
        pe2 = r - q_stage2[s, a2]
        q_stage2[s, a2] += alpha * pe2

        # MF backup to stage 1
        q_stage1_mf[a1] += alpha * pe2

        # Learn transitions from observed (a1 -> s)
        # Move the transition probabilities of row a1 toward the observed state s
        T[a1, :] = (1.0 - alpha_trans) * T[a1, :]
        T[a1, s] += alpha_trans
        # Numerical safety: renormalize the row
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive value transformation with anxiety-modulated curvature and lapse.
    
    This model is primarily model-based at stage 1 using fixed common-rare transitions.
    Decision values at both stages are transformed by a power-law utility curvature applied
    to learned expected values, capturing risk sensitivity. Anxiety (stai) modulates both the
    curvature and a small lapse (stimulus-independent) choice noise at each stage.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Modulates risk curvature and lapse probability.
    model_parameters : list or array
        [alpha, beta, curv_base, stai_gain, eps_base]
        Bounds:
          alpha in [0,1]      : reward learning rate
          beta in [0,10]      : inverse temperature
          curv_base in [0,1]  : baseline curvature exponent on Q (lower => more risk-averse)
          stai_gain in [0,1]  : how strongly STAI shifts curvature and lapse
          eps_base in [0,1]   : baseline lapse rate mixed with uniform choice
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, curv_base, stai_gain, eps_base = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Learned second-stage Q-values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated curvature and lapse
    curv = np.clip(curv_base + stai_gain * (stai - 0.5), 0.0, 1.0)
    lapse = np.clip(eps_base + 0.5 * stai_gain * (stai - 0.5), 0.0, 1.0)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Model-based Q at stage 1 by propagating best alien values
        max_q2 = np.max(q2, axis=1)        # expected values per second-stage state
        q1_mb = T @ max_q2

        # Apply curvature to decision values (risk sensitivity on expected values)
        # Use small epsilon to avoid 0**0 ambiguity
        epsv = 1e-8
        q1_val = np.power(np.clip(q1_mb, 0.0, 1.0) + epsv, curv)
        q2_val = np.power(np.clip(q2[s], 0.0, 1.0) + epsv, curv)

        # Softmax with lapse at stage 1
        q1_centered = q1_val - np.max(q1_val)
        p1_soft = np.exp(beta * q1_centered)
        p1_soft = p1_soft / np.sum(p1_soft)
        p1 = (1.0 - lapse) * p1_soft + lapse * 0.5
        p_choice_1[t] = p1[a1]

        # Softmax with lapse at stage 2
        q2_centered = q2_val - np.max(q2_val)
        p2_soft = np.exp(beta * q2_centered)
        p2_soft = p2_soft / np.sum(p2_soft)
        p2 = (1.0 - lapse) * p2_soft + lapse * 0.5
        p_choice_2[t] = p2[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MF with transition-outcome interaction bias modulated by anxiety and an eligibility trace.
    
    This model is primarily model-free but adds a first-stage bias term that captures the
    canonical transition x outcome interaction (repeat after common+reward or rare+no-reward;
    switch after common+no-reward or rare+reward). Anxiety modulates the strength of this bias.
    A stage-2 to stage-1 eligibility trace propagates reward to the chosen first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=spaceship A, 1=spaceship U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within state (0=first alien, 1=second alien).
    reward : array-like of float
        Reward outcomes per trial (0 or 1).
    stai : array-like of float
        Anxiety score array of length 1. Modulates strength of the interaction bias.
    model_parameters : list or array
        [alpha, beta, zeta_base, delta_stai, omega_trace]
        Bounds:
          alpha in [0,1]        : reward learning rate
          beta in [0,10]        : inverse temperature
          zeta_base in [0,1]    : baseline strength of transition-outcome interaction bias
          delta_stai in [0,1]   : how strongly STAI shifts the bias
          omega_trace in [0,1]  : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, zeta_base, delta_stai, omega_trace = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated bias strength
    zeta = np.clip(zeta_base + delta_stai * (stai - 0.5), 0.0, 1.0)

    prev_a1 = None
    prev_state = None
    prev_reward = None

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Build transition-outcome interaction bias on first-stage values
        bias_vec = np.zeros(2)
        if prev_a1 is not None:
            # common if A->X or U->Y; rare otherwise
            was_common = int((prev_a1 == 0 and prev_state == 0) or (prev_a1 == 1 and prev_state == 1))
            was_rewarded = 1 if prev_reward > 0 else 0

            # Interaction signal: +1 for repeat-favoring (common&reward or rare&no-reward), -1 otherwise
            interact = 1 if (was_common == was_rewarded) else -1

            # Apply symmetric bias to the previous action vs the alternative
            bias_vec[prev_a1] += zeta * interact
            bias_vec[1 - prev_a1] -= zeta * interact

        # Stage 1 policy (MF values plus bias)
        q1 = q1_mf + bias_vec
        q1_centered = q1 - np.max(q1)
        p1_soft = np.exp(beta * q1_centered)
        p1_soft = p1_soft / np.sum(p1_soft)
        p_choice_1[t] = p1_soft[a1]

        # Stage 2 policy
        q2_s = q2[s].copy()
        q2_centered = q2_s - np.max(q2_s)
        p2_soft = np.exp(beta * q2_centered)
        p2_soft = p2_soft / np.sum(p2_soft)
        p_choice_2[t] = p2_soft[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace: propagate stage-2 PE to chosen first-stage action
        q1_mf[a1] += alpha * omega_trace * pe2

        # Optional MF bootstrapping toward current second-stage value
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Store for next-trial bias
        prev_a1 = a1
        prev_state = s
        prev_reward = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll