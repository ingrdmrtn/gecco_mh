def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and lapse exploration.

    Core ideas:
    - Stage-2 values (Q2) learned via model-free TD(0).
    - Stage-1 uses a hybrid of model-based (MB) and model-free (MF) values.
      MB values come from known transitions (common=0.7, rare=0.3) and max Q2 per planet.
      MF values are learned via bootstrapping from Q2.
    - Anxiety (stai) increases: 
        (a) the arbitration weight on MB planning (w), and 
        (b) a lapse-like random exploration rate (epsilon).
      This reflects higher anxiety shifting decisions toward planning yet noisier exploration.

    Parameters (model_parameters):
    - lr: learning rate for Q-value updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - w_base: base MB arbitration weight (before anxiety), in [0,1]
    - k_anx_w: anxiety gain on MB weight (positive pushes toward MB), in [0,1]
    - k_explore: anxiety gain on lapse rate (random choice probability), in [0,1]

    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on that planet (0/1)
    - reward: array of rewards per trial, typically 0/1
    - stai: array-like with single float [0-1] anxiety score
    - model_parameters: array-like of parameters as above

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    lr, beta, w_base, k_anx_w, k_explore = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known (instructed) transition structure: rows=spaceships [A,U], cols=planets [X,Y]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value tables
    q2 = np.zeros((2, 2))   # per planet s in {X,Y} and alien action a2 in {0,1}
    q1_mf = np.zeros(2)     # MF values for spaceships {A,U}

    # Prob tracking for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight (sigmoid mapping to [0,1])
    # Reference point at medium-high boundary (.51)
    w_raw = w_base + k_anx_w * (st - 0.51)
    w_mb = 1.0 / (1.0 + np.exp(-10.0 * (w_raw - 0.5)))  # sharpened sigmoid to keep within [0,1]

    # Anxiety-modulated lapse (epsilon-greedy-like mixture with uniform)
    eps1 = np.clip(k_explore * st, 0.0, 1.0)
    eps2 = np.clip(0.5 * k_explore * st, 0.0, 1.0)  # slightly smaller lapse at stage 2

    for t in range(n_trials):
        # Model-based Q at stage 1 from current Q2
        max_q2 = np.max(q2, axis=1)    # best alien per planet
        q1_mb = T @ max_q2             # expectation over planets given spaceships

        # Hybrid stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 policy with softmax + lapse mixture
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        soft1 = exp1 / (np.sum(exp1) + 1e-12)
        probs1 = (1.0 - eps1) * soft1 + eps1 * 0.5  # uniform over 2 actions

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (planet-specific)
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + 1e-12)
        probs2 = (1.0 - eps2) * soft2 + eps2 * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes and learning
        r = reward[t]

        # Stage-2 TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2

        # Stage-1 MF bootstrapping toward the attained stage-2 value
        boot = q2[s, a2]
        pe1 = boot - q1_mf[a1]
        q1_mf[a1] += lr * pe1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-scaled associability (Pearce–Hall) with MB planning and lapse.

    Core ideas:
    - Stage 1 uses model-based evaluation from instructed transitions (0.7/0.3).
    - Stage 2 uses model-free TD learning with a dynamic, PE-driven associability
      that sets the learning rate per trial and per state-action.
    - Associability increases with unsigned prediction error (|PE|) and is
      further amplified by anxiety (stai), enabling faster adaptation under
      higher anxiety when outcomes are surprising.
    - A global lapse rate increases with anxiety and mixes softmax with uniform.

    Parameters (model_parameters):
    - alpha0: base learning-rate floor, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - phi_anx: anxiety gain on PE-driven associability, in [0,1]
    - lapse0: base lapse probability, in [0,1]
    - g_lapse: anxiety gain on lapse, in [0,1]

    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on that planet (0/1)
    - reward: array of rewards per trial, typically 0/1
    - stai: array-like with single float [0-1] anxiety score
    - model_parameters: array-like of parameters as above

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha0, beta, phi_anx, lapse0, g_lapse = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Instructed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and associability per (state, action2)
    q2 = np.zeros((2, 2))
    assoc = np.ones((2, 2)) * alpha0  # initialize associability near base rate

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated lapse used at both stages
    lapse = np.clip(lapse0 + g_lapse * st, 0.0, 1.0)

    for t in range(n_trials):
        # Stage-1 MB evaluation from current Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Policy stage 1 (softmax + lapse)
        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        soft1 = exp1 / (np.sum(exp1) + 1e-12)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy on reached planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        soft2 = exp2 / (np.sum(exp2) + 1e-12)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with anxiety-scaled associability
        r = reward[t]
        pe2 = r - q2[s, a2]

        # Trial- and option-specific learning rate from associability
        # alpha_t = clip(alpha0 + phi_anx * stai * |PE|, 0, 1)
        alpha_t = np.clip(alpha0 + phi_anx * st * abs(pe2), 0.0, 1.0)

        # Update Q2 with this dynamic rate
        q2[s, a2] += alpha_t * pe2

        # Update associability itself toward |PE| (Pearce–Hall style)
        # assoc <- (1 - k) * assoc + k * |PE|, with k tied to anxiety for reactivity
        k = np.clip(phi_anx * st, 0.0, 1.0)
        assoc[s, a2] = (1.0 - k) * assoc[s, a2] + k * abs(pe2)
        assoc[s, a2] = np.clip(assoc[s, a2], 0.0, 1.0)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MF with anxiety-amplified forgetting and transition-contingent choice bias.

    Core ideas:
    - Purely model-free values at both stages with inter-trial forgetting (decay).
    - Decay is stronger under higher anxiety, modeling rumination-related drift.
    - A transition-contingent bonus at stage 1 biases repeating/switching the
      spaceship depending on whether the previous trial had a common/rare
      transition and whether it was rewarded (captures canonical stay/switch
      interaction patterns).
    - Stage 2 uses MF TD(0) learning.

    Parameters (model_parameters):
    - lr: reward learning rate (MF), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - tbonus: magnitude of the transition-contingent bias at stage 1, in [0,1]
    - forget: base forgetting rate per trial, in [0,1]
    - g_anx_forget: anxiety gain on forgetting, in [0,1]

    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on that planet (0/1)
    - reward: array of rewards per trial, typically 0/1
    - stai: array-like with single float [0-1] anxiety score
    - model_parameters: array-like of parameters as above

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    lr, beta, tbonus, forget, g_anx_forget = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition structure to define common vs rare for bias computation
    # Common if (A->X) or (U->Y)
    def is_common(a1, s):
        return (a1 == 0 and s == 0) or (a1 == 1 and s == 1)

    # Values
    q1 = np.zeros(2)        # MF first-stage spaceship values
    q2 = np.zeros((2, 2))   # MF second-stage values per planet

    # Likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-amplified forgetting
    decay = np.clip(forget * (1.0 + g_anx_forget * st), 0.0, 1.0)

    # For transition-contingent bias
    prev_a1 = None
    prev_s = None
    prev_r = None

    for t in range(n_trials):
        # Apply decay before decision (values drift toward zero)
        q1 *= (1.0 - decay)
        q2 *= (1.0 - decay)

        # Stage-1 bias from previous trial's transition-outcome structure
        bias = np.zeros(2)
        if prev_a1 is not None:
            prev_common = 1.0 if is_common(prev_a1, prev_s) else 0.0
            # Classic interaction: stay after rewarded+common OR unrewarded+rare; switch otherwise
            sign = 1.0 if ((prev_r > 0.0 and prev_common == 1.0) or (prev_r <= 0.0 and prev_common == 0.0)) else -1.0
            # Push toward repeating previous choice if sign>0, away if sign<0
            bias[prev_a1] += tbonus * sign
            bias[1 - prev_a1] -= tbonus * sign

        # Stage 1 policy (MF values + bias)
        logits1 = beta * q1 + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy on reached planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2

        # Stage-1 MF bootstrapping toward attained Q2
        boot = q2[s, a2]
        pe1 = boot - q1[a1]
        q1[a1] += lr * pe1

        # Store for next trial's bias
        prev_a1 = a1
        prev_s = s
        prev_r = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll