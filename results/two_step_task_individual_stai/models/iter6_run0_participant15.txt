def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """MB-MF mixture with anxiety-shifted planning weight and uncertainty bonus.
    
    Idea:
    - Stage-2 values learned with TD(0).
    - Stage-1 uses a mixture of model-based (MB) values and model-free (MF) values.
    - An uncertainty-driven exploration bonus (UCB-like) is added to Stage-2 values when planning,
      scaled upward with anxiety.
    - Anxiety also shifts the balance between MB and MF control.

    Parameters (all used; total=5):
    - lr0: [0,1] Base learning rate for Stage-2 TD updates.
    - beta: [0,10] Inverse temperature applied at both stages.
    - omega0: [0,1] Baseline MB weight at Stage-1 (mixing with MF).
    - chi: [0,1] Strength of anxiety-driven shift of MB weight: omega = clip(omega0 + chi*(stai - 0.5)).
    - xi_e: [0,1] Base weight of uncertainty bonus; effective weight scales with anxiety (xi_eff = xi_e * stai).

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (spaceships).
    - state: array of ints in {0,1}, second-stage states (planets).
    - action_2: array of ints in {0,1}, second-stage choices (aliens).
    - reward: array of floats in [0,1], obtained coins.
    - stai: array-like of length 1 with participant's anxiety score in [0,1].
    - model_parameters: iterable [lr0, beta, omega0, chi, xi_e].

    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    lr0, beta, omega0, chi, xi_e = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure (common=0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Probabilities of observed choices
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value tables
    q2 = 0.5 * np.ones((2, 2))     # Stage-2 MF values
    q1_mf = np.zeros(2)            # Stage-1 MF values (initialized at 0)

    # Anxiety-modulated weights
    omega = omega0 + chi * (stai - 0.5)  # shift MB weight towards higher MB if stai>0.5 (if chi>0)
    omega = min(1.0, max(0.0, omega))
    xi_eff = xi_e * stai                 # more uncertainty-driven exploration under anxiety

    prev_q2_for_update = None  # optional helper (not strictly needed but clarifies ordering)

    for t in range(n_trials):
        s = state[t]

        # Compute uncertainty bonus per second-stage state-action as: u = 1 - 2*|q - 0.5|
        # Larger when values are closer to 0.5 (uncertain); in [0,1].
        u_bonus = 1.0 - 2.0 * np.abs(q2 - 0.5)
        u_bonus = np.maximum(0.0, u_bonus)  # guard numerical

        # For planning, add bonus to q2
        q2_plan = q2 + xi_eff * u_bonus

        # Model-based Stage-1 action values: expected max over second stage
        mb_q1 = transition_matrix @ np.max(q2_plan, axis=1)

        # Mixture with Stage-1 model-free values
        q1 = omega * mb_q1 + (1.0 - omega) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited state
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes
        r = reward[t]

        # TD update Stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr0 * pe2

        # Stage-1 MF update toward the realized stage-2 value (semi-gradient)
        # Use the chosen second-stage action's current value (after learning) as target
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr0 * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Structure-learning MB planner with anxiety-modulated transition learning and stage-specific temperature.
    
    Idea:
    - Learns the transition structure online from data (Dirichlet-like exponential recency via eta_t).
    - Plans at Stage-1 using the learned transition matrix and current Stage-2 values.
    - Stage-2 values are learned via TD(0).
    - Anxiety reduces Stage-1 decisiveness (lower beta1) and increases Stage-2 decisiveness (higher beta2),
      and slows transition learning.

    Parameters (all used; total=5):
    - eta_r: [0,1] Learning rate for Stage-2 TD updates.
    - beta1: [0,10] Base inverse temperature at Stage-1.
    - beta2: [0,10] Base inverse temperature at Stage-2.
    - eta_t: [0,1] Base learning rate for updating the transition matrix from experienced transitions.
    - kappa_s: [0,1] Strength of anxiety modulation of temperatures and transition learning:
        beta1_eff = beta1 * (1 - kappa_s*stai)
        beta2_eff = beta2 * (1 + kappa_s*stai), clipped to [0,10]
        eta_t_eff = eta_t * (1 - kappa_s*stai)

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices (spaceships).
    - state: array of ints in {0,1}, second-stage states (planets).
    - action_2: array of ints in {0,1}, second-stage choices (aliens).
    - reward: array of floats in [0,1], obtained coins.
    - stai: array-like of length 1 with participant's anxiety score in [0,1].
    - model_parameters: iterable [eta_r, beta1, beta2, eta_t, kappa_s].

    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta_r, beta1, beta2, eta_t, kappa_s = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix with weak prior toward common transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))

    # Anxiety modulations
    beta1_eff = beta1 * (1.0 - kappa_s * stai)
    beta2_eff = beta2 * (1.0 + kappa_s * stai)
    beta1_eff = min(10.0, max(0.0, beta1_eff))
    beta2_eff = min(10.0, max(0.0, beta2_eff))
    eta_t_eff = eta_t * (1.0 - kappa_s * stai)

    for t in range(n_trials):
        a1 = action_1[t]
        s = state[t]
        a2 = action_2[t]
        r = reward[t]

        # Stage-1 policy via learned transitions
        mb_q1 = T @ np.max(q2, axis=1)
        z1 = beta1_eff * (mb_q1 - np.max(mb_q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in realized state
        z2 = beta2_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Stage-2 TD learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_r * pe2

        # Transition learning: Update the row of T corresponding to chosen a1 toward observed state s
        # One-hot target for observed transition from a1 to s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1, :] = (1.0 - eta_t_eff) * T[a1, :] + eta_t_eff * target
        # Ensure row normalization and numerical safety
        row_sum = np.sum(T[a1, :])
        if row_sum > 0:
            T[a1, :] /= row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive MB-MF planner with anxiety-amplified risk aversion and perseveration.
    
    Idea:
    - Stage-2 values learned via TD(0) with learning rate mu.
    - Stage-1 values combine MB planning and MF (cached) values.
    - Risk sensitivity implemented by penalizing high-variance outcomes: q' = q - rho_eff * q*(1-q).
      Since rewards are Bernoulli-like, q*(1-q) approximates outcome variance.
    - Anxiety increases risk sensitivity and perseveration bias.

    Parameters (all used; total=5):
    - mu: [0,1] Learning rate for Stage-2 TD and Stage-1 MF updates.
    - beta: [0,10] Inverse temperature for both stages.
    - rho_risk: [0,1] Baseline risk penalty weight; effective rho_eff = rho_risk * stai.
    - phi_pers: [0,1] Baseline perseveration strength; effective stick = phi_pers * stai.
    - w_plan: [0,1] Baseline MB weight in Stage-1 mixture; effective weight shifts with anxiety:
        w_eff = clip( w_plan*(1 - stai) + (1 - w_plan)*stai ), biasing away from MB under low anxiety if w_plan<0.5.

    Inputs:
    - action_1: array of ints in {0,1}, first-stage choices.
    - state: array of ints in {0,1}, second-stage states.
    - action_2: array of ints in {0,1}, second-stage choices.
    - reward: array of floats in [0,1].
    - stai: array-like of length 1 with participant's anxiety score in [0,1].
    - model_parameters: iterable [mu, beta, rho_risk, phi_pers, w_plan].

    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    mu, beta, rho_risk, phi_pers, w_plan = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))  # Stage-2 MF values
    q1_mf = np.zeros(2)         # Stage-1 MF values

    # Effective parameters with anxiety
    rho_eff = rho_risk * stai
    stick_eff = phi_pers * stai
    w_eff = w_plan * (1.0 - stai) + (1.0 - w_plan) * stai
    w_eff = min(1.0, max(0.0, w_eff))

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]

        # Risk-sensitive transform of second-stage values for planning: penalize variance q*(1 - q)
        variance_proxy = q2 * (1.0 - q2)
        q2_risk_adj = q2 - rho_eff * variance_proxy

        # MB planning uses adjusted values
        mb_q1 = transition_matrix @ np.max(q2_risk_adj, axis=1)

        # Mixture with Stage-1 MF values
        base_q1 = w_eff * mb_q1 + (1.0 - w_eff) * q1_mf

        # Add perseveration bias toward previous Stage-1 choice
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        z1 = beta * (base_q1 + bias - np.max(base_q1 + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (no explicit risk penalty at choice; penalty is expressed via planning only)
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += mu * pe2

        # Stage-1 MF update toward the realized Stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += mu * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll