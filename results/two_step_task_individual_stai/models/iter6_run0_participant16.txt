def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated eligibility, perseveration, and transition-outcome bias.

    This model combines:
      - Model-free updates at stage 2 with TD learning.
      - An eligibility trace from stage 2 to stage 1 MF values, scaled up by anxiety.
      - A model-based (MB) planner using the known transition matrix.
      - Perseveration (stickiness) at stage 1 that increases with anxiety.
      - A transition-outcome bias that promotes stay after common+reward or rare+no-reward,
        and switch after common+no-reward or rare+reward; its strength increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (0 or 1 within the reached state).
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with a single float in [0,1]
        Anxiety score. Higher values increase eligibility trace, perseveration, and transition-outcome bias,
        and reduce reliance on model-based planning.
    model_parameters : iterable of 5 floats
        - eta in [0,1]: learning rate for MF action values (stage 2) and eligibility at stage 1.
        - b1 in [0,10]: inverse temperature at stage 1.
        - b2 in [0,10]: inverse temperature at stage 2.
        - pi_base in [0,1]: baseline perseveration strength at stage 1.
        - chi_base in [0,1]: baseline weight for transition-outcome interaction (stay/switch bias).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta, b1, b2, pi_base, chi_base = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],  # A -> (X,Y)
                  [0.3, 0.7]]) # U -> (X,Y)

    # Action values
    q2 = np.zeros((2, 2))       # stage-2 MF values per state and action
    q1_mf = np.zeros(2)         # stage-1 MF cached values

    # Anxiety-dependent elements
    lam = np.clip(0.4 + 0.5 * stai, 0.0, 1.0)                 # eligibility trace
    pi = np.clip(pi_base * (1.0 + 0.8 * stai), 0.0, 5.0)      # perseveration bias added to previous action logit
    chi = np.clip(chi_base * (0.5 + 0.5 * stai), 0.0, 5.0)    # transition-outcome interaction weight
    w_mb = np.clip(0.7 - 0.4 * stai, 0.0, 1.0)                # anxiety reduces MB reliance

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_sigma = 0.0  # transition-outcome signal from previous trial

    for t in range(n_trials):
        # Compute MB values from current q2
        max_q2 = np.max(q2, axis=1)         # value per state
        q1_mb = T @ max_q2                  # plan with transition structure

        # Hybrid Q at stage 1
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 logits with biases
        logits1 = b1 * q1
        # Perseveration
        if prev_a1 is not None:
            logits1[prev_a1] += pi
            # Transition-outcome interaction: bias to repeat vs switch
            # sigma from previous trial: +1 promotes repetition, -1 promotes switching
            logits1[prev_a1] += chi * prev_sigma

        # Softmax for stage 1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        logits2 = b2 * q2[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # TD update at stage 2
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta * delta2

        # Eligibility trace to stage 1 MF
        q1_mf[a1] += eta * lam * delta2

        # Update previous signals for next trial biases
        # Compute transition commonality for current trial
        common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Transition-outcome interaction signal:
        # +1 for (common & reward) or (rare & no reward)
        # -1 for (common & no reward) or (rare & reward)
        sigma = 1 if (common and r > 0.5) or ((1 - common) and r <= 0.5) else -1
        prev_sigma = float(sigma)
        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated uncertainty-bonus exploration with model-based planning.

    The agent maintains MF estimates of second-stage action values and augments them
    with an exploration bonus that decays with visit count. Anxiety reduces the bonus,
    shifting behavior from directed exploration to exploitation. Stage-1 planning is
    model-based over bonus-augmented state values.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with a single float in [0,1]
        Anxiety score. Higher values reduce the uncertainty bonus and lower stage-2 determinism.
    model_parameters : iterable of 5 floats
        - alpha in [0,1]: learning rate for second-stage MF values.
        - beta1 in [0,10]: inverse temperature at stage 1.
        - beta2 in [0,10]: baseline inverse temperature at stage 2.
        - bonus_base in [0,1]: baseline magnitude of directed exploration bonus.
        - phi in [0,1]: curvature of anxiety-to-bonus mapping.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta1, beta2, bonus_base, phi = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Q-values and counts
    q2 = np.zeros((2, 2))
    n_sa = np.zeros((2, 2))  # visit counts per state-action

    # Anxiety-modulated parameters
    # Directed exploration weight decreases with anxiety; phi controls curvature
    bonus_weight = np.clip(bonus_base * (1.0 - stai) ** (0.5 + 0.5 * phi), 0.0, 5.0)
    beta2_eff = max(0.0, beta2 * (1.0 - 0.4 * stai))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute exploration bonus per state-action
        bonus = bonus_weight / np.sqrt(n_sa + 1.0)

        # Stage-1 planning over bonus-augmented values
        max_aug = np.max(q2 + bonus, axis=1)
        q1_mb = T @ max_aug

        # Stage-1 policy
        logits1 = beta1 * q1_mb
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy within reached state using augmented values
        s = state[t]
        logits2 = beta2_eff * (q2[s] + bonus[s])
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = reward[t]
        n_sa[s, a2] += 1.0
        delta = r - q2[s, a2]
        q2[s, a2] += alpha * delta

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-structure arbitration with anxiety-dependent lapse.

    The agent learns:
      - Second-stage MF action values (q2).
      - A first-stage MF cache via eligibility from stage 2.
      - An empirical transition model (T_hat) via incremental updates.

    First-stage choices mix MF and structure-based planning:
      Q1 = (1 - w_sr) * Q1_MF + w_sr * (T_hat @ V),
    where V(s) = max_a q2[s,a]. Anxiety increases the reliance on the structure-based
    component (w_sr) and adds a lapse (uniform mixing) at stage 1.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Reached state (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices.
    reward : array-like of float in [0,1]
        Received rewards.
    stai : array-like with a single float in [0,1]
        Anxiety score. Higher values increase arbitration weight toward structure-based planning
        and increase a lapse component at stage 1; also mildly reduces stage-2 determinism.
    model_parameters : iterable of 5 floats
        - lr_r in [0,1]: learning rate for second-stage MF values and stage-1 eligibility.
        - lr_M in [0,1]: learning rate for transition model (from action to state).
        - beta in [0,10]: inverse temperature used at both stages.
        - w_sr0 in [0,1]: baseline arbitration weight for structure-based component.
        - lapse0 in [0,1]: baseline lapse rate mixed with uniform at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_r, lr_M, beta, w_sr0, lapse0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize transition model from actions to states, learned online
    T_hat = np.ones((2, 2)) * 0.5  # start uninformative

    # Values
    q2 = np.zeros((2, 2))     # stage-2 MF values
    q1_mf = np.zeros(2)       # stage-1 MF cache

    # Anxiety mappings
    w_sr = np.clip(w_sr0 + (1.0 - w_sr0) * stai, 0.0, 1.0)     # more anxious -> more structure-based
    lapse = np.clip(lapse0 * stai, 0.0, 1.0)                   # lapse increases with anxiety
    beta2 = max(0.0, beta * (1.0 - 0.3 * stai))                # anxiety reduces stage-2 determinism
    lam = np.clip(0.3 + 0.5 * stai, 0.0, 1.0)                  # eligibility trace to stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute structure-based Q1 using learned transitions
        V = np.max(q2, axis=1)          # state values
        q1_struct = T_hat @ V

        # Hybrid Q1
        q1 = (1.0 - w_sr) * q1_mf + w_sr * q1_struct

        # Stage-1 softmax with lapse
        logits1 = beta * q1
        maxl1 = np.max(logits1)
        probs1 = np.exp(logits1 - maxl1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5  # mix with uniform
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in reached state
        s = state[t]
        logits2 = beta2 * q2[s]
        maxl2 = np.max(logits2)
        probs2 = np.exp(logits2 - maxl2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Update second-stage MF values
        delta2 = r - q2[s, a2]
        q2[s, a2] += lr_r * delta2

        # Eligibility update to stage-1 MF
        q1_mf[a1] += lr_r * lam * delta2

        # Update learned transition model T_hat for the taken action a1
        # Move the row for action a1 toward the observed one-hot state
        target = np.array([1.0 if s == 0 else 0.0, 1.0 if s == 1 else 0.0])
        T_hat[a1] = (1.0 - lr_M) * T_hat[a1] + lr_M * target
        # Keep rows normalized (should already hold by construction)
        row_sum = T_hat[a1].sum()
        if row_sum > 0:
            T_hat[a1] = T_hat[a1] / row_sum

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)