def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Rare-transition-gated hybrid control with valence-asymmetric learning modulated by anxiety.

    Overview:
    - Stage-2 Q-values are learned with separate learning rates for positive vs. negative prediction errors.
    - Stage-1 choice uses a hybrid of model-based and model-free values.
    - The model-based weight is dynamically gated by whether the previous trial produced a rare vs. common transition,
      and this gating is amplified or dampened by the participant's anxiety.
    - Intuition: more anxious participants rely less on rare-transition-driven re-planning (reduced gating effect).

    Parameters (bounds):
    - model_parameters[0] = alpha_pos in [0,1]: learning rate for positive stage-2 prediction errors (r - Q > 0)
    - model_parameters[1] = alpha_neg in [0,1]: learning rate for negative stage-2 prediction errors (r - Q < 0)
    - model_parameters[2] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[3] = gate0 in [0,1]: baseline model-based weight at stage 1
    - model_parameters[4] = anx_gate in [0,1]: anxiety sensitivity of rare-transition gating

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0 or 1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of the 5 parameters defined above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha_pos, alpha_neg, beta, gate0, anx_gate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known task transition structure: rows are actions (0=A,1=U), columns are states (0=X,1=Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Model-free and model-based components
    q1_mf = np.zeros(2)        # stage-1 model-free Q-values
    q2 = np.zeros((2, 2))      # stage-2 Q-values for each state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track whether previous transition was rare relative to the chosen action's common destination
    prev_is_rare = 0.0  # initialize as common for the first trial

    for t in range(n_trials):
        # Compute model-based stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)            # best available at each state
        q1_mb = transition_matrix @ max_q2     # expected value of each first-stage action

        # Anxiety-gated hybridization: when previous trial was rare, the gating term activates.
        # Effective MB weight for this trial:
        # gate_effect = (prev_is_rare - 0.5) ranges in {-0.5, 0.5}; amplify rare, attenuate common.
        # Anxiety reduces amplification: larger stai -> smaller adjustment around gate0.
        gate_adjust = (prev_is_rare - 0.5) * anx_gate * (1.0 - stai_val)
        omega_t = np.clip(gate0 + gate_adjust, 0.0, 1.0)

        q1_net = omega_t * q1_mb + (1.0 - omega_t) * q1_mf

        # Stage-1 policy
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (no stickiness here; purely value-based)
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with valence-asymmetric learning rates
        delta2 = r - q2[s, a2]
        lr2 = alpha_pos if delta2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * delta2

        # Stage-1 model-free update by bootstrapping from updated stage-2 value
        # Use the same valence-dependent rate but based on the sign of the effective TD at stage-1
        delta1 = q2[s, a2] - q1_mf[a1]
        lr1 = alpha_pos if delta1 >= 0.0 else alpha_neg
        q1_mf[a1] += lr1 * delta1

        # Determine whether the current transition was rare to gate the next trial
        # A transition is "common" if s equals argmax transition prob for chosen a1
        common_state = 0 if transition_matrix[a1, 0] >= transition_matrix[a1, 1] else 1
        prev_is_rare = 1.0 if s != common_state else 0.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-adaptive learning with anxiety-amplified volatility and softmax choice.

    Overview:
    - Maintains a latent volatility signal v_t that tracks recent unsigned prediction errors.
    - Learning rate adapts to volatility: alpha_t = clip(alpha0 + vol_gain * (v_t + anx_vol * stai), 0, 1).
      Thus, higher anxiety increases perceived volatility and speeds learning.
    - Both stages use the same adaptive learning rate at each trial for simplicity.
    - Stage-1 choice is model-based via transition structure combined with learned stage-2 values only
      (no explicit model-free first-stage cache).

    Parameters (bounds):
    - model_parameters[0] = alpha0 in [0,1]: baseline learning rate
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = vol_gain in [0,1]: scaling from volatility to learning rate
    - model_parameters[3] = anx_vol in [0,1]: anxiety contribution to perceived volatility
    - model_parameters[4] = kappa in [0,1]: volatility update rate (how fast v_t tracks unsigned errors)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0 or 1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of the 5 parameters defined above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha0, beta, vol_gain, anx_vol, kappa = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # stage-2 values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    v = 0.0  # latent volatility proxy

    for t in range(n_trials):
        # Effective trial-wise learning rate
        alpha_t = alpha0 + vol_gain * (v + anx_vol * stai_val)
        alpha_t = 0.0 if alpha_t < 0.0 else (1.0 if alpha_t > 1.0 else alpha_t)

        # Stage-1 model-based values from current Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 learning with adaptive learning rate
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * delta2

        # Update volatility proxy v_t from unsigned prediction error, with leak (1 - kappa)
        v = (1.0 - kappa) * v + kappa * abs(delta2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility and novelty-seeking bonus with anxiety modulation.

    Overview:
    - Stage-2 values learn from a transformed utility rather than raw reward:
      u(1) = 1, u(0) = -lambda_eff * 0.5, capturing stronger aversion to omissions when lambda_eff > 1.
    - lambda_eff is driven by anxiety: lambda_eff = 0.5 + 1.5 * clip(lambda0 + anx_risk * stai, 0, 1),
      so higher anxiety increases loss sensitivity.
    - An intrinsic novelty bonus encourages exploration at stage 2 and propagates to stage 1 via planning.
      Bonus decays with visit count and is reduced by anxiety.
    - Policies at both stages use softmax over augmented values.

    Parameters (bounds):
    - model_parameters[0] = alpha in [0,1]: learning rate for stage-2 values
    - model_parameters[1] = beta in [0,10]: inverse temperature for softmax at both stages
    - model_parameters[2] = lambda0 in [0,1]: baseline (bounded) loss-sensitivity seed
    - model_parameters[3] = anx_risk in [0,1]: anxiety weight on loss sensitivity
    - model_parameters[4] = bonus_scale in [0,1]: scale of the novelty bonus

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like of ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like of ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like of floats (typically 0 or 1), received coins per trial
    - stai: array-like with one float in [0,1], anxiety score
    - model_parameters: list/array of the 5 parameters defined above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    alpha, beta, lambda0, anx_risk, bonus_scale = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # learned values (expected utility)
    visits = np.zeros((2, 2))  # visitation counts for novelty bonus

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss sensitivity mapped to [0.5, 2.0]
    lam_eff = np.clip(lambda0 + anx_risk * stai_val, 0.0, 1.0)
    lam_eff = 0.5 + 1.5 * lam_eff

    # Anxiety reduces novelty drive
    bonus_gain = bonus_scale * (1.0 - stai_val)

    for t in range(n_trials):
        # Compute novelty bonuses for current trial
        # b[s,a] = bonus_gain / sqrt(1 + visits[s,a])
        b = np.zeros((2, 2))
        for s_idx in (0, 1):
            for a_idx in (0, 1):
                b[s_idx, a_idx] = bonus_gain / np.sqrt(1.0 + visits[s_idx, a_idx])

        # Stage-1 model-based planning with bonuses propagated via max over stage-2
        max_q2_b = np.max(q2 + b, axis=1)
        q1_mb = transition_matrix @ max_q2_b

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with novelty bonus
        s = state[t]
        q2_net = q2[s] + b[s]
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Risk-sensitive utility transform
        u = 1.0 if r >= 1.0 else (-lam_eff * 0.5)

        # Stage-2 learning on utility
        delta2 = u - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visitation counts after experiencing the action
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll