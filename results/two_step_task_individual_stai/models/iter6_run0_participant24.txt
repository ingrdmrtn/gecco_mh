def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration via global stage-2 entropy.

    The first-stage policy blends model-based (MB) and model-free (MF) action values.
    The blend weight is dynamically adjusted by the current decision entropy at stage 2
    and by trait anxiety. Higher entropy pushes the system toward MB control, while
    higher anxiety shifts the arbitration toward MF control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used to bias arbitration toward MF.
    model_parameters : array-like
        [alpha, beta, omega0, chi_ent, xi_anx]
        - alpha in [0,1]: learning rate for both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - omega0 in [0,1]: baseline MB weight when entropy is neutral and stai=0.
        - chi_ent in [0,1]: sensitivity of MB weight to average stage-2 entropy.
        - xi_anx in [0,1]: anxiety penalty on MB weight (higher stai reduces MB).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, omega0, chi_ent, xi_anx = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # a1=0 (A): P(X)=0.7, P(Y)=0.3
                  [0.3, 0.7]]) # a1=1 (U): P(X)=0.3, P(Y)=0.7

    # Values
    q1_mf = np.zeros(2)        # MF values at stage 1
    q2 = np.zeros((2, 2))      # MF values at stage 2 (state x action)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper for entropy of a 2-action softmax given preferences v
    def softmax_probs(v, b):
        v_stable = v - np.max(v)
        ev = np.exp(b * v_stable)
        return ev / np.sum(ev)

    def entropy_of_probs(p):
        eps = 1e-12
        p = np.clip(p, eps, 1 - eps)
        return -np.sum(p * np.log(p))

    # Transform baseline omega0 to logit then adjust; finally squash with sigmoid
    def logit(x):
        eps = 1e-8
        x = np.clip(x, eps, 1 - eps)
        return np.log(x) - np.log(1 - x)

    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    for t in range(n_trials):
        # Compute MB first-stage values from current q2
        max_q2_by_state = np.max(q2, axis=1)  # [X, Y]
        q1_mb = T @ max_q2_by_state

        # Entropy averaged over both second-stage states
        p2_x = softmax_probs(q2[0], beta)
        p2_y = softmax_probs(q2[1], beta)
        H_avg = 0.5 * (entropy_of_probs(p2_x) + entropy_of_probs(p2_y)) / np.log(2)  # normalize to [0,1]

        # Arbitration weight toward MB
        arb_input = logit(omega0) + chi_ent * H_avg - xi_anx * s_anx
        w_mb = sigmoid(arb_input)

        # First-stage policy
        q1_blend = w_mb * q1_mb + (1 - w_mb) * q1_mf
        probs1 = softmax_probs(q1_blend, beta)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        st = state[t]
        probs2 = softmax_probs(q2[st], beta)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 MF update
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2
        # Stage-1 MF update via bootstrap from chosen second-stage action
        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Directed exploration at stage 2 with anxiety-dampened exploration bonus and value decay.

    This model is purely MF for learning but adds an uncertainty bonus to the policy.
    An exploration bonus is added to second-stage action preferences based on the
    estimated outcome uncertainty of each alien (higher variance, fewer samples => larger bonus).
    Anxiety reduces the size of this directed exploration bonus. Second-stage values
    also decay slightly toward 0.5 to capture nonstationarity tracking.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; higher scores reduce the exploration bonus.
    model_parameters : array-like
        [alpha, beta, nu0, nu_anx, xi_decay]
        - alpha in [0,1]: MF learning rate for both stages.
        - beta in [0,10]: inverse temperature.
        - nu0 in [0,1]: baseline scale of the uncertainty (directed exploration) bonus.
        - nu_anx in [0,1]: how strongly anxiety suppresses the bonus.
        - xi_decay in [0,1]: decay toward 0.5 for visited second-stage Q-values per trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, nu0, nu_anx, xi_decay = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transitions (used for forming first-stage policy from expected second-stage values)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))      # MF values at stage 2
    n_visits = np.ones((2, 2)) # start with 1 to avoid zero-division; acts like a mild prior

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    def softmax_probs(v, b):
        v_stable = v - np.max(v)
        ev = np.exp(b * v_stable)
        return ev / np.sum(ev)

    # Anxiety-dampened exploration weight
    bonus_scale = nu0 * (1 - nu_anx * s_anx)

    for t in range(n_trials):
        # Compute uncertainty bonus for each state-action from Bernoulli variance estimate
        # u = sqrt(p*(1-p)/(n+1)), where p is current mean (q2) and n is visit count
        p_est = np.clip(q2, 1e-6, 1 - 1e-6)
        var = p_est * (1 - p_est)
        u = np.sqrt(var / (n_visits + 1.0))

        # Second-stage policy (directed exploration)
        st = state[t]
        prefs2 = q2[st] + bonus_scale * u[st]
        probs2 = softmax_probs(prefs2, beta)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage expected values use MB expectation over max second-stage total preferences
        # Use bonus-augmented preferences to reflect exploration-aware planning
        max_total_by_state = np.max(q2 + bonus_scale * u, axis=1)
        q1_mb = T @ max_total_by_state
        probs1 = softmax_probs(q1_mb, beta)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning updates
        r = reward[t]

        # Decay visited state's Q-values slightly toward 0.5 to track drift
        q2[st] = (1 - xi_decay) * q2[st] + xi_decay * 0.5

        # MF updates
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Update counts
        n_visits[st, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB controller with surprise-weighted MF credit assignment modulated by anxiety.

    The model learns the first-stage transition structure online and uses it for MB planning.
    Trial-by-trial transition surprise reduces reliance on the MB planner on the next trial
    and increases MF credit assignment on the current trial. Anxiety strengthens the impact
    of surprise on MF credit assignment and weakens MB reliance.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; higher scores amplify surprise effects.
    model_parameters : array-like
        [alpha_q, beta, alpha_tr, psi0, psi_anx]
        - alpha_q in [0,1]: learning rate for Q-values at both stages.
        - beta in [0,10]: inverse temperature.
        - alpha_tr in [0,1]: learning rate for transition probabilities.
        - psi0 in [0,1]: baseline strength of surprise weighting.
        - psi_anx in [0,1]: increase of surprise weighting due to anxiety.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_q, beta, alpha_tr, psi0, psi_anx = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transition matrix T_hat[a1, s]
    T_hat = np.full((2, 2), 0.5)  # start uncertain: 0.5 to each state

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_surprise = 0.0  # used to modulate arbitration for the next first-stage choice

    def softmax_probs(v, b):
        v_stable = v - np.max(v)
        ev = np.exp(b * v_stable)
        return ev / np.sum(ev)

    # Surprise weighting parameter (stronger with anxiety)
    psi = psi0 + psi_anx * s_anx
    psi = np.clip(psi, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based first-stage values from learned transitions
        max_q2_by_state = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2_by_state

        # Arbitration weight: surprise on previous trial reduces MB reliance
        w_mb = np.clip(1.0 - psi * prev_surprise, 0.0, 1.0)
        q1_blend = w_mb * q1_mb + (1 - w_mb) * q1_mf

        # First-stage policy
        probs1 = softmax_probs(q1_blend, beta)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        st = state[t]
        probs2 = softmax_probs(q2[st], beta)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Compute and apply transition learning for the chosen first-stage action
        # One-hot target for observed transition
        s_obs = st
        # Surprise: 1 - predicted probability of the observed transition
        surprise_t = 1.0 - T_hat[a1, s_obs]

        # Update transition probabilities toward the observed outcome
        # Move probability mass toward observed state and away from the other state
        T_hat[a1, s_obs] += alpha_tr * (1.0 - T_hat[a1, s_obs])
        other = 1 - s_obs
        T_hat[a1, other] += alpha_tr * (0.0 - T_hat[a1, other])

        # Stage-2 MF update
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_q * pe2

        # Stage-1 MF update with surprise-weighted eligibility
        # More unexpected transitions (higher surprise) produce stronger MF credit assignment.
        elig = (1 - psi) + psi * surprise_t
        pe1 = q2[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_q * elig * pe1

        # Update previous surprise for next-trial arbitration
        prev_surprise = surprise_t

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll