def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-weighted surprise gating of planning.
    The agent blends model-free (MF) and model-based (MB) value at stage 1. The MB weight is dynamically
    modulated by the recent frequency of rare transitions (surprise), and anxiety increases the suppression
    of planning following surprising transitions. Stage 2 is learned model-free.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within the reached state.
    - reward: array-like of floats in [0,1]. Coins received on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for MF values (also used to update surprise memory)
        beta: [0,10] — inverse temperature for both stages
        blend0: [0,1] — baseline MB mixing weight (after logit transform to avoid edge saturation)
        phi_surprise: [0,1] — sensitivity of MB weight to surprise (rare transitions)
        anx_blend: [0,1] — how strongly anxiety amplifies surprise-driven reduction in MB weight

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, blend0, phi_surprise, anx_blend = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed task transition structure (common: A->X, U->Y)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Value stores
    q1_mf = np.zeros(2)        # model-free values for stage 1 actions
    q2 = np.zeros((2, 2))      # model-free values for stage 2 (per state, per alien)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Surprise memory: running estimate of "rarity" on previous trial
    prev_rare_est = 0.5  # neutral start

    # Helper: stable softmax
    eps = 1e-12

    # Precompute logit for blend0
    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    base_logit = logit(blend0)

    for t in range(n_trials):
        # Compute current MB weight based on previous surprise estimate and anxiety
        # More surprise => reduce MB weight, effect amplified by anxiety.
        surpr_term = (2.0 * prev_rare_est - 1.0)  # -1 (common) to +1 (rare)
        w_mb = inv_logit(base_logit - phi_surprise * (1.0 + anx_blend * stai_val) * surpr_term)

        # Model-based Q at stage 1: expected max Q2 under known transition matrix
        max_q2 = np.max(q2, axis=1)                 # shape (2,)
        q1_mb = transition_matrix @ max_q2          # shape (2,)

        # Blend MB and MF values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy given reached state
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Determine whether the observed transition was rare
        is_rare = 1 if ((a1 == 0 and s == 1) or (a1 == 1 and s == 0)) else 0

        # Learning updates
        # Stage 2 MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward observed second-stage value (SARSA(0) target)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update running surprise estimate using same alpha (keeps params <=5)
        prev_rare_est = (1.0 - alpha) * prev_rare_est + alpha * is_rare

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with uncertainty aversion and an anxiety-modulated choice kernel.
    The agent dislikes uncertain second-stage options: both action selection and learning
    penalize options with higher reward uncertainty (estimated from current Q2 via p*(1-p)).
    Anxiety increases uncertainty aversion. Additionally, a simple choice kernel captures
    repetition tendencies at both stages, with strength growing slightly with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within the reached state.
    - reward: array-like of floats in [0,1]. Coins received on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for MF values
        beta: [0,10] — inverse temperature for both stages
        kernel_lr: [0,1] — learning/decay rate of the choice kernels
        risk_penalty: [0,1] — base penalty weight for uncertainty
        anx_risk_gain: [0,1] — how strongly anxiety amplifies uncertainty aversion and kernel strength

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, kernel_lr, risk_penalty, anx_risk_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Choice kernels (stage 1, and per-state stage 2)
    k1 = np.zeros(2)
    k2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective strengths
    lam = risk_penalty * (1.0 + anx_risk_gain * stai_val)
    kernel_strength = kernel_lr * (1.0 + 0.5 * anx_risk_gain * stai_val)

    eps = 1e-12

    for t in range(n_trials):
        # Uncertainty estimate from current Q2: var ~ p*(1-p), use sqrt for scaling
        var2 = q2 * (1.0 - q2)
        unc2 = np.sqrt(np.clip(var2, 0.0, 0.25))  # in [0, 0.5]

        # Stage 1 logits: MF Q1 plus choice kernel bias
        # Kernel contribution: map kernel in [0,1] to [-1,1] via (2k - 1)
        logits1 = q1 + kernel_strength * (2.0 * k1 - 1.0)
        logits1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 logits: MF Q2 penalized by uncertainty + choice kernel
        s = state[t]
        logits2 = (q2[s] - lam * unc2[s]) + kernel_strength * (2.0 * k2[s] - 1.0)
        logits2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Stage 2: subjective reward penalizes uncertainty of the chosen option
        var_chosen = q2[s, a2] * (1.0 - q2[s, a2])
        unc_chosen = np.sqrt(max(0.0, min(0.25, var_chosen)))
        r_subj = r - lam * unc_chosen
        pe2 = r_subj - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1: bootstrap toward updated second-stage value (no explicit MB)
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update choice kernels: decay and reinforce chosen action
        k1 = (1.0 - kernel_lr) * k1
        k1[a1] += kernel_lr

        k2[s] = (1.0 - kernel_lr) * k2[s]
        k2[s, a2] += kernel_lr

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-boosted transition updating and Q2 decay.
    The agent learns its own first-stage transition model T(a->state) and uses it for model-based
    planning at stage 1 (MB), while also maintaining model-free values (MF). Stage 1 choice uses a
    dynamic hybrid: Q1 = 0.5*(MF + MB). Second-stage values slowly decay toward 0.5 to capture
    non-stationarity, while anxiety accelerates transition learning.

    Parameters
    - action_1: array-like of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array-like of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}. Second-stage choices within the reached state.
    - reward: array-like of floats in [0,1]. Coins received on each trial.
    - stai: array-like with one float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha: [0,1] — learning rate for MF values
        beta: [0,10] — inverse temperature for both stages
        decay2: [0,1] — per-trial decay of Q2 toward 0.5 (captures drifting rewards)
        trans_learn: [0,1] — base learning rate for transition probabilities
        anx_trans_gain: [0,1] — how strongly anxiety increases transition learning

    Returns
    - Negative log-likelihood of the observed action_1 and action_2 sequences.
    """
    alpha, beta, decay2, trans_learn, anx_trans_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Initialize learned transition model T[a, s'], start neutral
    T = np.full((2, 2), 0.5)  # rows sum to 1 with updates

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12

    for t in range(n_trials):
        # Compute MB Q at stage 1 from learned transitions and current stage-2 values
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_mb = T @ max_q2

        # Hybrid: simple average to keep params <=5
        q1 = 0.5 * (q1_mf + q1_mb)

        # Stage 1 policy
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learning updates
        # Transition learning: update only the row for the chosen action
        tlr = trans_learn * (1.0 + anx_trans_gain * stai_val)
        # Move probability mass toward the observed state
        T[a1] = (1.0 - tlr) * T[a1]
        T[a1, s] += tlr
        # Normalize defensively (should remain normalized)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

        # Stage 2 decay toward 0.5 to model drifting payoffs
        q2 = (1.0 - decay2) * q2 + decay2 * 0.5

        # Stage 2 MF update with reward
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward observed second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)