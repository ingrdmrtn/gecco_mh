def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-gated model-based control with anxiety-amplified arbitration and stickiness.

    Mechanism overview:
    - Stage-2 values Q2(s2, a2) learned with a single learning rate.
    - Stage-1 hybrid action values combine a model-based (MB) projection using a learned
      transition model T(a1->s2) and a model-free (MF) backup from the last reached Q2.
    - A Dirichlet transition posterior is tracked from observed transitions; the entropy of T
      indexes transition uncertainty. Higher uncertainty increases MB reliance, and this
      effect is amplified by anxiety (stai).
    - A single perseveration parameter k1 acts at both stages; its effect is attenuated
      at stage-2 by anxiety (higher anxiety reduces stage-2 stickiness).

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; reached second-stage planet (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        rho_v   in [0,1]: value learning rate for Q2 and MF backup to Q1
        beta    in [0,10]: inverse temperature for softmax at both stages
        k1      in [0,1]: perseveration strength (shared); anxiety scales its stage-2 effect
        omega0  in [0,1]: baseline weight on model-based control at stage-1
        xi_unc  in [0,1]: strength of uncertainty-driven boost to MB weight

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    rho_v, beta, k1, omega0, xi_unc = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialization
    q2 = np.zeros((2, 2), dtype=float)   # Q2[s2, a2]
    q1_mf = np.zeros(2, dtype=float)     # model-free Q at stage-1
    # Dirichlet posterior over transitions per action: counts[a1, s2]
    trans_counts = np.ones((2, 2), dtype=float)  # symmetric prior -> starts at 0.5/0.5
    eps = 1e-12

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Current transition probs from Dirichlet posterior
        T = trans_counts / (np.sum(trans_counts, axis=1, keepdims=True) + eps)

        # Transition uncertainty via mean entropy across actions
        # H(p) = -sum p log p, normalized by log(2) to be in [0,1]
        ent = -np.sum(T * (np.log(T + eps)), axis=1) / np.log(2 + eps)
        unc = 0.5 * (ent[0] + ent[1])  # scalar in [0,1]

        # MB projection
        max_q2 = np.max(q2, axis=1)        # value of each second-stage state
        q1_mb = T @ max_q2                 # MB value for each first-stage action

        # Anxiety-amplified arbitration: more uncertainty -> more MB, amplified by anxiety
        omega_eff = omega0 + xi_unc * unc * (0.5 + 0.5 * s_anx)
        omega_eff = float(np.clip(omega_eff, 0.0, 1.0))

        # Hybrid Q at stage-1
        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Stickiness vectors
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Stage-1 policy
        logits1 = beta * q1 + k1 * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (anxiety reduces stickiness at stage-2)
        s2 = int(state[t])
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k1 * (1.0 - 0.7 * s_anx)  # stronger anxiety -> less stage-2 perseveration
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Transition posterior update (Dirichlet counts)
        trans_counts[a1, s2] += 1.0  # simple Bayesian counting update

        # Value learning at stage-2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += rho_v * delta2

        # MF backup to stage-1: learn from reached Q2 and obtained reward
        target1 = q2[s2, a2]  # could include immediate r via delta2 already embedded
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += rho_v * delta1
        # Optional additional TD(1) style boost with the stage-2 TD error
        q1_mf[a1] += rho_v * delta2

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-driven arousal model: anxiety scales how transition surprise lowers beta.

    Mechanism overview:
    - Q2 learned with a single learning rate; Q1 uses a hybrid MB+MF value.
    - Transition model T(a1->s2) is learned incrementally.
    - Trial-by-trial transition surprise (absolute deviation between observed s2 and predicted T)
      modulates the next-trial softmax temperature: higher surprise reduces beta (more randomness).
      This arousal-like effect is multiplicatively scaled by anxiety (stai).
    - A stage-2 perseveration term is included.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - model_parameters: tuple/list with five params:
        alpha   in [0,1]: learning rate for Q-values
        beta    in [0,10]: baseline inverse temperature (when no surprise)
        mu_T    in [0,1]: learning rate for the transition model T
        phi_s   in [0,1]: strength of surprise-induced beta reduction
        kappa2  in [0,1]: perseveration at stage-2; stage-1 has no stickiness here

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, mu_T, phi_s, kappa2 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])
    eps = 1e-12

    # Initialize values and transitions
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # start from nominal task structure

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a2 = -1
    # Surprise from the previous transition; initialize to zero (no arousal initially)
    prev_surprise = 0.0

    for t in range(n_trials):
        # Surprise-adjusted inverse temperature (lower beta -> more random)
        beta_t = beta / (1.0 + phi_s * s_anx * prev_surprise)

        # Model-based projection
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid Q1: 50-50 MB/MF blend (beta_t already captures arousal/surprise)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy (no stickiness term here)
        logits1 = beta_t * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with perseveration
        s2 = int(state[t])
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        logits2 = beta_t * q2[s2] + kappa2 * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Transition learning with delta rule
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)])
        T[a1] = (1.0 - mu_T) * T[a1] + mu_T * onehot_s
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Compute surprise for next trial arousal modulation
        # Surprise = 1 - predicted probability of observed state
        prev_surprise = 1.0 - float(T[a1, s2])

        # Q-learning at stage-2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * delta2

        # MF backup to Q1 using reached second-stage value
        target1 = q2[s2, a2]
        q1_mf[a1] += alpha * (target1 - q1_mf[a1])
        # Optional extra with TD error
        q1_mf[a1] += alpha * delta2

        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with eligibility trace and anxiety-modulated forgetting.

    Mechanism overview:
    - Subjective utility u(r) = r^gamma with gamma <= 1 capturing risk aversion for probabilities.
      Anxiety increases risk aversion by reducing gamma (stronger concavity).
    - TD learning on Q2 uses utility u(r).
    - An eligibility trace at stage-1 propagates the stage-2 TD error back to Q1 (Î»-like backup).
      Anxiety increases the effective trace length.
    - Global forgetting pulls unchosen Q-values toward a neutral baseline (0.5) each trial;
      anxiety reduces forgetting (more rigid values).

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - model_parameters: tuple/list with five params:
        eta        in [0,1]: base learning rate for value updates
        beta       in [0,10]: inverse temperature for both stages
        lam_elig   in [0,1]: eligibility-trace strength (backups to Q1)
        psi_risk   in [0,1]: risk-sensitivity strength (larger -> more concavity)
        nu_forget  in [0,1]: forgetting strength toward 0.5 baseline for unchosen actions

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eta, beta, lam_elig, psi_risk, nu_forget = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])
    eps = 1e-12

    # Initialize values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Fixed transition structure (task-known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    # Risk exponent gamma in (0,1]; stronger anxiety -> smaller gamma (more concave)
    gamma = 1.0 - 0.7 * psi_risk * s_anx
    gamma = float(np.clip(gamma, 0.05, 1.0))
    # Eligibility increased by anxiety
    lam_eff = float(np.clip(lam_elig * (1.0 + 0.5 * s_anx), 0.0, 1.0))
    # Forgetting reduced by anxiety
    forget_eff = nu_forget * (1.0 - 0.7 * s_anx)

    for t in range(n_trials):
        # Stage-1 MB projection from current Q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Combine MB and MF equally (focus of this model is utility/trace/forgetting)
        q1 = 0.5 * q1_mb + 0.5 * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Subjective utility
        r = float(reward[t])
        u = r ** gamma

        # TD at stage-2 (with utility)
        delta2 = u - q2[s2, a2]
        q2[s2, a2] += eta * delta2

        # Eligibility trace backup to stage-1 MF value
        # Two components: (i) bootstrapped from reached Q2, (ii) direct TD error propagation
        target1 = q2[s2, a2]
        q1_mf[a1] += eta * (target1 - q1_mf[a1])
        q1_mf[a1] += eta * lam_eff * delta2

        # Forgetting toward neutral baseline (0.5) for unchosen actions
        # Stage-2: decay all non-visited state-action pairs this trial
        for s_idx in (0, 1):
            for a_idx in (0, 1):
                if not (s_idx == s2 and a_idx == a2):
                    q2[s_idx, a_idx] += forget_eff * (0.5 - q2[s_idx, a_idx])

        # Stage-1: decay the unchosen action
        unchosen_a1 = 1 - a1
        q1_mf[unchosen_a1] += forget_eff * (0.5 - q1_mf[unchosen_a1])

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)