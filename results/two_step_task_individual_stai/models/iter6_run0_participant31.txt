def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-modulated learning rate and second-stage lapse.

    Idea:
    - First-stage policy blends a model-free Q1 with a model-based plan computed from the
      known transition structure (70/30). The mixture weight is xi_mb.
    - Anxiety increases or decreases the learning rate (alpha) via k_anx_alpha.
    - Second-stage choice includes an epsilon-like lapse (lapse2): with probability lapse2,
      choice is random; otherwise it follows softmax.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher scores modulate learning rate.
    model_parameters : array-like of floats, length 5
        [alpha, beta, xi_mb, k_anx_alpha, lapse2]
        - alpha in [0,1]: base learning rate for Q updates.
        - beta in [0,10]: inverse temperature for softmax.
        - xi_mb in [0,1]: weight of model-based Q in first-stage decision values.
        - k_anx_alpha in [0,1]: how strongly anxiety modulates learning rate.
                                alpha_eff = alpha * (1 + (stai - 0.5) * 2 * k_anx_alpha).
        - lapse2 in [0,1]: second-stage lapse probability (random choice fraction).

    Returns
    -------
    float
        Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, xi_mb, k_anx_alpha, lapse2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated learning rate
    alpha_eff = alpha * (1.0 + (stai_val - 0.5) * 2.0 * k_anx_alpha)
    if alpha_eff < 0.0:
        alpha_eff = 0.0
    if alpha_eff > 1.0:
        alpha_eff = 1.0

    # Fixed transitions: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])  # rows: A/U; cols: X/Y

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    q1_mf = np.zeros(2)        # A/U
    q2_mf = np.zeros((2, 2))   # state X/Y x action (0/1)

    for t in range(n_trials):
        # Model-based first-stage values: plan via max over second-stage Q
        max_q2 = np.max(q2_mf, axis=1)  # values of planets X and Y
        q1_mb = transition_matrix @ max_q2  # expected value of choosing A or U

        # Hybrid value
        q1_hybrid = (1.0 - xi_mb) * q1_mf + xi_mb * q1_mb

        # First-stage policy
        logits1 = beta * q1_hybrid
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage policy with lapse
        s = int(state[t])
        logits2 = beta * q2_mf[s]
        logits2 -= np.max(logits2)
        p2_soft = np.exp(logits2)
        p2_soft = p2_soft / (np.sum(p2_soft) + 1e-12)
        p2 = (1.0 - lapse2) * p2_soft + lapse2 * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        delta2 = r - q2_mf[s, a2]
        q2_mf[s, a2] += alpha_eff * delta2

        # Stage-1 TD update toward second-stage action value (SARSA-like bootstrap)
        delta1 = q2_mf[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_eff * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with risk-sensitive second-stage choice and anxiety-amplified risk aversion, plus second-stage perseveration.

    Idea:
    - Maintain MF Q-values at both stages with a single learning rate.
    - Maintain an exponential estimate of outcome variance at the second stage (per state-action).
      The policy penalizes actions with higher estimated variance (risk).
    - Anxiety increases effective risk-aversion.
    - Include second-stage perseveration (stickiness) that biases repeating the last second-stage choice.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher anxiety => stronger risk penalty.
    model_parameters : array-like of floats, length 5
        [alpha, beta, k_risk0, k_anx_loss, pers2]
        - alpha in [0,1]: learning rate for Q and variance estimates.
        - beta in [0,10]: inverse temperature for softmax.
        - k_risk0 in [0,1]: baseline weight for risk penalty on second-stage values.
        - k_anx_loss in [0,1]: scales how anxiety amplifies risk penalty:
                               k_risk_eff = k_risk0 * (1 + k_anx_loss * stai).
        - pers2 in [0,1]: second-stage perseveration strength added to last chosen action's logit.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, k_risk0, k_anx_loss, pers2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    k_risk_eff = k_risk0 * (1.0 + k_anx_loss * stai_val)
    if k_risk_eff < 0.0:
        k_risk_eff = 0.0
    if k_risk_eff > 1.0:
        k_risk_eff = 1.0

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)              # MF Q at stage 1
    q2 = np.zeros((2, 2))         # MF Q at stage 2
    v2 = np.zeros((2, 2))         # running variance estimate per state-action

    prev_a2 = [None, None]        # track last action within each second-stage state

    for t in range(n_trials):
        # First-stage softmax (MF only)
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage policy with risk penalty and perseveration
        s = int(state[t])

        # Perseveration bias for this state
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] = pers2

        # Risk-adjusted logits
        risk_penalty = k_risk_eff * v2[s]  # subtract variance-weighted penalty
        logits2 = beta * (q2[s] - risk_penalty) + bias2
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Learning
        r = reward[t]

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update variance estimate with squared prediction error
        v2[s, a2] += alpha * (delta2 * delta2 - v2[s, a2])

        # Stage-1 TD update towards q2 value (SARSA-like)
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

        prev_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated forgetting and model-based heuristic bias toward common transitions.

    Idea:
    - Use MF Q-learning at both stages with per-trial forgetting/decay of all Q-values.
    - Anxiety increases the effective decay (forgetting), capturing higher volatility beliefs.
    - First-stage decision includes a heuristic bias that favors the spaceship whose common
      destination currently has higher estimated value (based on max second-stage Q for each planet).
      This uses the known transition structure without full planning.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states encountered (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the encountered state (0/1).
    reward : array-like of float
        Rewards obtained on each trial.
    stai : array-like of float
        Anxiety score; uses stai[0] in [0,1]. Higher anxiety => stronger forgetting.
    model_parameters : array-like of floats, length 5
        [alpha, beta, decay, k_anx_decay, tr_bias]
        - alpha in [0,1]: learning rate.
        - beta in [0,10]: inverse temperature.
        - decay in [0,1]: baseline forgetting rate applied each trial to all Q-values.
        - k_anx_decay in [0,1]: scales how anxiety increases effective decay:
                                decay_eff = min(1, decay + k_anx_decay * stai).
        - tr_bias in [0,1]: strength of heuristic transition bias added to first-stage logits.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay, k_anx_decay, tr_bias = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Anxiety-modulated decay
    decay_eff = decay + k_anx_decay * stai_val
    if decay_eff < 0.0:
        decay_eff = 0.0
    if decay_eff > 1.0:
        decay_eff = 1.0
    keep = 1.0 - decay_eff

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    for t in range(n_trials):
        # Heuristic transition bias: compare planet values
        v_planet = np.max(q2, axis=1)  # [vX, vY]
        # Map to spaceship bias: A commonly -> X; U commonly -> Y
        # Add +tr_bias * (vX - vY) to A, and - that to U
        bias1 = np.array([tr_bias * (v_planet[0] - v_planet[1]),
                          -tr_bias * (v_planet[0] - v_planet[1])])

        # First-stage policy
        logits1 = beta * q1 + bias1
        logits1 -= np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + 1e-12)
        a1 = int(action_1[t])
        p_choice_1[t] = p1[a1]

        # Second-stage policy
        s = int(state[t])
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + 1e-12)
        a2 = int(action_2[t])
        p_choice_2[t] = p2[a2]

        # Learning with forgetting
        r = reward[t]

        # Apply decay to all Q-values before update
        q1 *= keep
        q2 *= keep

        # Stage-2 TD update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Stage-1 TD update toward second-stage chosen value
        delta1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * delta1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll