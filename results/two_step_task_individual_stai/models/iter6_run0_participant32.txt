def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MB/MF with learned transition model and anxiety-modulated arbitration and eligibility trace.

    Core idea:
    - Learn second-stage rewards (model-free) and first-stage model-free values.
    - Learn first-stage transition probabilities online.
    - Combine model-based and model-free action values at stage 1 via an arbitration weight that grows with
      transition certainty and shrinks with anxiety.
    - Anxiety also reduces the eligibility trace that propagates reward to the first-stage model-free system.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial (0/1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha_r, beta, alpha_T, w_base, kappa_anx2arb]
        Bounds:
        - alpha_r: [0,1] learning rate for rewards (both stages).
        - beta: [0,10] inverse temperature (both stages).
        - alpha_T: [0,1] learning rate for transitions P(state | first-stage action).
        - w_base: [0,1] baseline model-based weight at stage 1.
        - kappa_anx2arb: [0,1] strength by which anxiety reduces the MB weight.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, alpha_T, w_base, kappa_anx2arb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model P(s | a1). Start neutral (0.5/0.5).
    P = np.ones((2, 2)) * 0.5

    # Second-stage Q-values and first-stage model-free values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Helper for numerical stability
    def softmax_logits(vals, beta_val):
        v = vals - np.max(vals)
        logits = beta_val * v
        ex = np.exp(logits - np.max(logits))
        return ex / np.sum(ex)

    # Eligibility trace is reduced by anxiety (less credit propagation when anxious)
    lam = np.clip(0.8 - 0.6 * stai, 0.0, 1.0)

    for t in range(n_trials):

        # Model-based Q at stage 1 from current transition model and second-stage values
        max_q2 = np.max(q2, axis=1)  # [X, Y]
        q1_mb = P @ max_q2  # shape (2,)

        # Compute uncertainty of transitions (mean normalized entropy across actions)
        # Entropy normalized by log(2) to be in [0,1].
        eps_ent = 1e-12
        ent_a0 = -np.sum(P[0] * np.log(P[0] + eps_ent)) / np.log(2.0)
        ent_a1 = -np.sum(P[1] * np.log(P[1] + eps_ent)) / np.log(2.0)
        mean_entropy = 0.5 * (ent_a0 + ent_a1)  # in [0,1]

        # Arbitration weight: more MB when transitions are certain (low entropy), less with higher anxiety
        w_eff = w_base + (1.0 - mean_entropy) - kappa_anx2arb * stai
        w_eff = np.clip(w_eff, 0.0, 1.0)

        q1_combined = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage 1 policy
        probs1 = softmax_logits(q1_combined, beta)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = int(state[t])
        probs2 = softmax_logits(q2[s], beta)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Second-stage update (model-free)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # First-stage model-free updates:
        # 1) TD from the reached second-stage value (bootstrapped)
        pe1_boot = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * (1.0 - lam) * pe1_boot
        # 2) Eligibility-trace-like direct reward credit
        pe1_rew = r - q1_mf[a1]
        q1_mf[a1] += alpha_r * lam * pe1_rew

        # Transition learning for chosen first-stage action toward observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        P[a1] = (1.0 - alpha_T) * P[a1] + alpha_T * target

        # Keep P rows normalized (should already be, but guard numerical error)
        P[a1] = P[a1] / np.sum(P[a1])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive utility with anxiety-modulated risk aversion and temperature, plus stickiness.

    Core idea:
    - Maintain running estimates of mean and variance of rewards for each second-stage option.
    - Choose using a risk-adjusted utility: U = mean - theta_risk * sqrt(variance).
    - Anxiety increases risk aversion and reduces inverse temperature (more noise).
    - Add choice stickiness at both stages, amplified by anxiety.
    - Stage 1 uses model-based expected utility via fixed transitions.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial (0/1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha_m, alpha_v, beta0, theta_risk0, psi_stick]
        Bounds:
        - alpha_m: [0,1] learning rate for mean rewards.
        - alpha_v: [0,1] learning rate for reward variance.
        - beta0: [0,10] baseline inverse temperature.
        - theta_risk0: [0,1] baseline risk-aversion coefficient.
        - psi_stick: [0,1] baseline stickiness weight.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_m, alpha_v, beta0, theta_risk0, psi_stick = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # P(X|A), P(Y|A)
                  [0.3, 0.7]]) # P(X|U), P(Y|U)

    # Mean and variance estimates for each state-action
    m = np.zeros((2, 2))
    v = np.ones((2, 2)) * 0.25  # start with some uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2 = {0: None, 1: None}

    def softmax_with_stick(vals, beta_val, stick_weight, prev_act):
        base = vals - np.max(vals)
        logits = beta_val * base
        if prev_act is not None:
            bias = np.zeros_like(vals)
            bias[prev_act] = 1.0
            logits = logits + stick_weight * bias
        ex = np.exp(logits - np.max(logits))
        return ex / np.sum(ex)

    # Anxiety effects
    theta_risk = theta_risk0 * (0.5 + 0.5 * stai)  # more anxious => more risk averse
    beta_eff = max(1e-6, beta0 * (1.0 - 0.5 * stai))  # more anxious => lower beta (noisier)
    stick_eff = psi_stick * (1.0 + stai)  # amplify persistence with anxiety

    for t in range(n_trials):

        # Risk-adjusted utilities at stage 2
        U2 = m - theta_risk * np.sqrt(v + 1e-8)

        # Stage 1 model-based expected utilities
        max_U2 = np.max(U2, axis=1)  # per state
        q1_mb = T @ max_U2

        # Stage 1 choice with stickiness
        probs1 = softmax_with_stick(q1_mb, beta_eff, stick_eff, prev_a1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice with stickiness
        s = int(state[t])
        probs2 = softmax_with_stick(U2[s], beta_eff, stick_eff, prev_a2[s])
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Update mean and variance estimates for the chosen second-stage option
        r = reward[t]
        m_prev = m[s, a2]
        # Mean update
        m[s, a2] = m_prev + alpha_m * (r - m_prev)
        # Variance update (around the previous mean; tracks second moment)
        err2 = (r - m_prev) ** 2
        v[s, a2] = v[s, a2] + alpha_v * (err2 - v[s, a2])

        # Update stickiness memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Uncertainty-bonus exploration with anxiety-dampened novelty and common-transition reward bias.

    Core idea:
    - Second-stage values learned with standard TD.
    - Add an Upper-Confidence-Bound (UCB) exploration bonus at stage 2 that decreases with experience.
    - Anxiety diminishes the exploration bonus (more anxious => less novelty seeking).
    - Stage 1 uses model-based values from fixed transitions applied to augmented (value + bonus) second-stage options.
    - Add a first-stage bias to repeat the previous spaceship if the previous rewarded transition was common,
      scaled up by anxiety.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (alien) per trial (0/1).
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1].
    model_parameters : list or array-like of float
        [alpha, beta, c_ucb, zeta_anx, rho_comm]
        Bounds:
        - alpha: [0,1] learning rate for second-stage Q-values.
        - beta: [0,10] inverse temperature (both stages).
        - c_ucb: [0,1] base strength of the exploration bonus.
        - zeta_anx: [0,1] scaling of how much anxiety suppresses exploration bonus.
        - rho_comm: [0,1] base bias to repeat previous first-stage choice after common rewarded transitions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, c_ucb, zeta_anx, rho_comm = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # P(X|A), P(Y|A)
                  [0.3, 0.7]]) # P(X|U), P(Y|U)

    # Second-stage Q-values and visit counts
    q2 = np.zeros((2, 2))
    N = np.zeros((2, 2))  # visit counts per state-action

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous trial info for common-transition rewarded bias
    prev_a1 = None
    prev_trans_common = False
    prev_reward_pos = False

    # Anxiety-dampened exploration
    c_ucb_eff = c_ucb * (1.0 - zeta_anx * stai)
    c_ucb_eff = max(0.0, c_ucb_eff)

    for t in range(n_trials):

        # Compute UCB bonuses
        bonus = c_ucb_eff * np.sqrt(1.0 / (1.0 + N))

        # Augmented second-stage action values
        aug2 = q2 + bonus

        # Model-based first-stage values using augmented second-stage values
        max_aug2 = np.max(aug2, axis=1)
        q1_mb = T @ max_aug2

        # Add common-rewarded repetition bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None and prev_trans_common and prev_reward_pos:
            # Favor repeating the previous first-stage choice; stronger with anxiety
            rho_eff = rho_comm * (0.5 + 0.5 * stai)
            bias1[prev_a1] += rho_eff

        # Stage 1 policy
        vals1 = q1_mb + bias1
        v1 = vals1 - np.max(vals1)
        probs1 = np.exp(beta * v1 - np.max(beta * v1))
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy at reached state
        s = int(state[t])
        vals2 = aug2[s]
        v2 = vals2 - np.max(vals2)
        probs2 = np.exp(beta * v2 - np.max(beta * v2))
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update second-stage values and counts
        r = reward[t]
        N[s, a2] += 1.0
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update prev-trial markers for the next trial
        # Determine whether the observed transition was common
        # A->X (s=0) and U->Y (s=1) are common; the other pairings are rare.
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        prev_trans_common = is_common
        prev_reward_pos = r > 0.0
        prev_a1 = a1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik