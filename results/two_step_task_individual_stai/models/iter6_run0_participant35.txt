Below are three standalone cognitive models, each as a Python function that returns the negative log-likelihood of the observed choices. Each model uses the participantâ€™s anxiety (stai) in a distinct, theoretically motivated way. All parameters are used, constrained conceptually to [0,1] (and beta in [0,10]) as specified.

Note: Assume numpy is available as np.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Kalman-TD with anxiety-modulated volatility, uncertainty-weighted learning, and eligibility propagation.

    Summary
    - Second-stage values are learned via a Kalman filter per state-action (uncertainty-adaptive learning).
    - Anxiety increases assumed environmental volatility and observation noise, raising Kalman gain when outcomes vary.
    - First-stage values are updated via an eligibility-like propagation of second-stage PEs, with a small model-based blend.
    - Model-based influence is reduced with anxiety.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1) on the reached planet
    - reward: array-like of floats in [0,1], coin outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters (all in [0,1] except beta in [0,10]):
        beta          (float in [0,10]): inverse temperature for both stages
        w_plan_base   (float in [0,1]) : baseline model-based weight at stage 1
        eta_elig      (float in [0,1]) : eligibility strength to propagate PE2 to stage-1 MF value
        sigma0        (float in [0,1]) : baseline observation noise scale for Kalman update
        phi_vol       (float in [0,1]) : baseline process noise (volatility) for Kalman update

    Anxiety usage
    - Effective process noise q_eff increases with anxiety: q_eff = q0 + kq * phi_vol * (0.5 + 0.5*stai)
    - Effective observation noise r_eff increases with anxiety: r_eff = r0 + kr * sigma0 * (0.5 + 0.5*stai)
    - Model-based weight decreases with anxiety: w_mb = clip(w_plan_base * (1 - 0.4*stai), 0, 1)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    beta, w_plan_base, eta_elig, sigma0, phi_vol = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-1 model-free values and stage-2 Kalman means/variances
    q1_mf = np.zeros(2)
    q2_mean = np.zeros((2, 2))
    q2_var = np.full((2, 2), 0.2)  # initial uncertainty

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated noise terms for Kalman updates
    # Keep within reasonable [~0.01, ~0.5] range to avoid degenerate gains
    q0, kq = 0.01, 0.49
    r0, kr = 0.01, 0.49
    anxiety_scale = 0.5 + 0.5 * stai
    q_eff = q0 + kq * phi_vol * anxiety_scale
    r_eff = r0 + kr * sigma0 * anxiety_scale

    # MB weight reduced by anxiety
    w_mb = w_plan_base * (1.0 - 0.4 * stai)
    w_mb = max(0.0, min(1.0, w_mb))

    for t in range(n_trials):
        # Model-based estimate from stage-2 values
        max_q2 = np.max(q2_mean, axis=1)
        q1_mb = T @ max_q2

        # Decision values for stage 1
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        v1 = q1_dec - np.max(q1_dec)
        pi1 = np.exp(beta * v1)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy at the reached state
        s = int(state[t])
        v2 = q2_mean[s] - np.max(q2_mean[s])
        pi2 = np.exp(beta * v2)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Kalman update at stage 2
        r = reward[t]
        pe2 = r - q2_mean[s, a2]
        prior_var = q2_var[s, a2] + q_eff
        K = prior_var / (prior_var + r_eff)  # Kalman gain in (0,1)
        q2_mean[s, a2] += K * pe2
        q2_var[s, a2] = (1.0 - K) * prior_var

        # Eligibility-like propagation to stage-1 MF value
        # Use the same PE2 as a teaching signal for chosen first-stage action
        q1_mf[a1] += eta_elig * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning MB/MF blend with anxiety-sensitive rare-transition switching bias.

    Summary
    - Learns second-stage Q-values with a standard learning rate.
    - Learns transition probabilities online (action-specific rows of T) via a simple delta rule.
    - Stage-1 decisions blend MB and MF values using learned transitions.
    - Anxiety reduces MB reliance and increases an adaptive switch-bias after rare transitions.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters (all in [0,1] except beta in [0,10]):
        alpha_s   (float in [0,1]) : learning rate for second-stage values
        beta      (float in [0,10]): inverse temperature for both stages
        eta_T     (float in [0,1]) : learning rate for transitions
        w_mb0     (float in [0,1]) : baseline model-based weight for stage 1
        zeta_rare (float in [0,1]) : magnitude of rare-transition-induced switch bias

    Anxiety usage
    - MB weight decreases with anxiety: w_mb = clip(w_mb0 * (1 - 0.5*stai), 0, 1)
    - Rare-transition switch bias is amplified by anxiety:
        if previous trial was rare, add +b to the action that switches and -b to the action that stays,
        where b = zeta_rare * stai.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_s, beta, eta_T, w_mb0, zeta_rare = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix (rows sum to 1; start near canonical)
    T = np.array([[0.65, 0.35],
                  [0.35, 0.65]], dtype=float)

    # Stage-1 model-free values and stage-2 values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    w_mb = w_mb0 * (1.0 - 0.5 * stai)
    w_mb = max(0.0, min(1.0, w_mb))

    prev_a1 = None
    prev_rare = False

    for t in range(n_trials):
        # Model-based from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Rare-transition switch bias based on previous trial
        bias = np.zeros(2)
        if prev_a1 is not None and prev_rare:
            b = zeta_rare * stai
            # Encourage switching away from previous action
            bias[1 - prev_a1] += b
            bias[prev_a1]     -= b

        # Stage-1 decision values
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias
        v1 = q1_dec - np.max(q1_dec)
        pi1 = np.exp(beta * v1)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy
        s = int(state[t])
        v2 = q2[s] - np.max(q2[s])
        pi2 = np.exp(beta * v2)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        r = reward[t]

        # Update second-stage values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s * pe2

        # Update stage-1 MF by bootstrapping from realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_s * pe1

        # Learn transitions via simple delta rule toward observed state
        # For chosen action, move probability mass toward observed state s
        # Keep row normalized by complementary update
        row = T[a1].copy()
        for sp in (0, 1):
            target = 1.0 if sp == s else 0.0
            row[sp] += eta_T * (target - row[sp])
        # Normalize row to sum to 1 and keep probabilities within [0.01, 0.99] to avoid degeneracy
        row = np.clip(row, 1e-3, 1.0)
        row /= np.sum(row)
        T[a1] = row

        # Determine if current transition was rare using the canonical structure for rarity detection
        # (A->Y or U->X = rare). This is used only for the bias mechanism.
        rare_now = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)
        prev_a1 = a1
        prev_rare = rare_now

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-based arbitration with anxiety-modulated forgetting and lapses.

    Summary
    - Stage-2 MF learning with a standard learning rate.
    - Stage-1 arbitration between MB and MF based on relative uncertainty (entropy) of each system.
    - Anxiety increases forgetting (value leak toward 0) and increases lapse rate.
    - Baseline MB tendency sets the center of arbitration; arbitration shifts with uncertainty contrast.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1]
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters (all in [0,1] except beta in [0,10]):
        alpha        (float in [0,1]) : learning rate for stage-2 values and stage-1 MF bootstrapping
        beta         (float in [0,10]): inverse temperature
        mu_mb0       (float in [0,1]) : baseline MB weighting at stage 1
        delta_forget (float in [0,1]) : forgetting/leak rate (toward 0) applied each trial
        eps_lapse    (float in [0,1]) : baseline lapse probability mixed with uniform choice

    Anxiety usage
    - Forgetting increased by anxiety: leak_eff = delta_forget * (0.5 + 0.5*stai)
    - Lapse increased by anxiety: eps_eff = min(1, eps_lapse * (1 + stai))
    - Arbitration weight:
        w_mb = clip(mu_mb0 + c*(U_MF - U_MB) - 0.3*stai, 0, 1)
      where U_MF and U_MB are uncertainty proxies (entropies) for the MF and MB policies at stage 1,
      and c = 0.5 scales sensitivity to uncertainty contrast. Anxiety reduces w_mb baseline.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, mu_mb0, delta_forget, eps_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix for MB component
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    def softmax_policy(q, temp):
        v = q - np.max(q)
        p = np.exp(temp * v)
        p /= np.sum(p)
        return p

    def entropy(p):
        eps = 1e-12
        x = np.clip(p, eps, 1.0)
        return -np.sum(x * np.log(x))

    for t in range(n_trials):
        # Apply anxiety-modulated forgetting to values
        leak_eff = delta_forget * (0.5 + 0.5 * stai)
        q2 *= (1.0 - leak_eff)
        q1_mf *= (1.0 - leak_eff)

        # Compute MB and MF first-stage action values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1_mf_now = q1_mf.copy()

        # Build proxy policies to compute uncertainty (entropy)
        pi1_mf = softmax_policy(q1_mf_now, beta)
        pi1_mb = softmax_policy(q1_mb, beta)
        U_MF = entropy(pi1_mf)
        U_MB = entropy(pi1_mb)

        # Arbitration weight: higher when MB is more certain than MF; reduced by anxiety
        c = 0.5
        w_mb = mu_mb0 + c * (U_MF - U_MB) - 0.3 * stai
        w_mb = max(0.0, min(1.0, w_mb))

        # Stage-1 decision with lapse
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf_now
        pi1 = softmax_policy(q1_dec, beta)
        eps_eff = min(1.0, eps_lapse * (1.0 + stai))
        pi1 = (1.0 - eps_eff) * pi1 + eps_eff * 0.5  # mix with uniform over 2 actions
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 decision
        s = int(state[t])
        pi2 = softmax_policy(q2[s], beta)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Learning updates
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll