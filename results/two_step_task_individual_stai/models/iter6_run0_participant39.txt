def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning hybrid with anxiety-shifted MF reliance and second-stage stickiness.
    
    Mechanism:
    - Learns second-stage action values model-free.
    - Learns first-stage transition probabilities online; uses them to compute
      model-based first-stage values as T @ max(Q2).
    - First-stage policy is a hybrid of model-based values and model-free Q1,
      but anxiety (stai) increases reliance on model-free control.
    - Second-stage policy includes an action stickiness term that is stronger with anxiety.
    - Anxiety also slightly reduces choice precision (lower beta) to capture exploratory tendency.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; e.g., W/S in X or P/H in Y)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha_r:       [0,1] learning rate for second-stage rewards (Q2)
        beta_base:     [0,10] inverse temperature base for both stages
        tau_T_base:    [0,1] transition learning rate (for updating T)
        w_mf_base:     [0,1] baseline weight on model-free values at stage 1
        kappa2_base:   [0,1] stickiness strength at stage 2
    Returns
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha_r, beta_base, tau_T_base, w_mf_base, kappa2_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition matrix (rows: actions A/U, cols: states X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value tables
    q1_mf = np.zeros(2)           # model-free values for first-stage actions
    q2 = np.zeros((2, 2))         # second-stage action values per state

    # Likelihood accumulators
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective parameters modulated by anxiety
    # Anxiety increases MF reliance at stage 1
    w_mf_eff = np.clip(w_mf_base + stai0 * (1.0 - w_mf_base), 0.0, 1.0)
    # Anxiety reduces precision slightly (more exploration)
    beta = beta_base * (1.0 - 0.25 * stai0)
    # Anxiety increases second-stage stickiness
    kappa2 = kappa2_base * (1.0 + stai0)
    # Anxiety increases vigilance to transitions
    tau_T = np.clip(tau_T_base * (1.0 + 0.5 * stai0), 0.0, 1.0)

    # Second-stage stickiness: last action chosen within each state
    prev_a2 = [None, None]

    eps = 1e-10
    for t in range(n_trials):
        # Model-based Q1 from current transition estimates and max Q2 per state
        max_q2 = np.max(q2, axis=1)              # shape (2,)
        q1_mb = T @ max_q2                       # shape (2,)

        # Hybrid first-stage action values (anxiety shifts weight toward MF)
        q1_hyb = w_mf_eff * q1_mf + (1.0 - w_mf_eff) * q1_mb

        # First-stage choice probabilities
        q1_centered = q1_hyb - np.max(q1_hyb)
        probs_1 = np.exp(beta * q1_centered)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage choice probabilities with stickiness
        s2 = state[t]
        q2_s = q2[s2].copy()
        if prev_a2[s2] is not None:
            stick = np.zeros(2)
            stick[prev_a2[s2]] = 1.0
            q2_s = q2_s + kappa2 * stick

        q2_centered = q2_s - np.max(q2_s)
        probs_2 = np.exp(beta * q2_centered)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Observe reward and update values
        r = reward[t]

        # Update second-stage Q-values (model-free)
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha_r * pe2

        # Update first-stage MF toward realized second-stage value
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1  # reuse alpha_r for simplicity; bounded in [0,1]

        # Update transition matrix for the chosen first-stage action based on observed state
        # Ensure row sums to 1
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s2 else 0.0
            T[a1, s_idx] += tau_T * (target - T[a1, s_idx])
        # Normalize for numerical stability
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

        # Update stickiness memory
        prev_a2[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric MF with anxiety-modulated eligibility and rare-transition credit.
    
    Mechanism:
    - Purely model-free values at both stages.
    - Second-stage learning uses separate learning rates for wins vs losses.
    - First-stage credit assignment uses an eligibility trace from the second-stage
      prediction error. Eligibility is amplified on rare transitions, with amplification
      increasing with anxiety (sensitivity to surprising events).
    - Anxiety also tilts learning: reduced win learning, increased loss learning.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        a_win:        [0,1] base learning rate when outcome is better than expected (positive PE)
        a_loss:       [0,1] base learning rate when outcome is worse than expected (negative PE)
        beta:         [0,10] inverse temperature (both stages)
        trace0:       [0,1] baseline eligibility strength for propagating PE to stage 1
        rare_boost0:  [0,1] baseline multiplicative boost on eligibility for rare transitions
    Returns
    - Negative log-likelihood of observed choices at both stages.
    """
    a_win, a_loss, beta, trace0, rare_boost0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition structure to classify common vs rare
    # A->X and U->Y are common
    def is_common(a1, s2):
        return (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    # Value tables
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Anxiety: lower win learning, higher loss learning
    alpha_pos = np.clip(a_win * (1.0 - 0.5 * stai0), 0.0, 1.0)
    alpha_neg = np.clip(a_loss * (1.0 + 0.5 * stai0), 0.0, 1.0)
    # Eligibility trace, stronger with anxiety
    lam = np.clip(trace0 * (1.0 + stai0), 0.0, 1.0)
    # Rare-transition boost, stronger with anxiety
    rare_boost = rare_boost0 * (1.0 + stai0)

    eps = 1e-10
    for t in range(n_trials):
        # First-stage policy (pure MF)
        q1_c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2_s = q2[s2] - np.max(q2[s2])
        probs_2 = np.exp(beta * q2_s)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Second-stage learning with valence asymmetry
        pe2 = r - q2[s2, a2]
        alpha2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s2, a2] += alpha2 * pe2

        # First-stage credit assignment via eligibility; amplify if rare
        rare = not is_common(a1, s2)
        lam_eff = lam * (1.0 + (rare_boost if rare else 0.0))
        # Propagate the same valence-dependent PE to Q1
        q1[a1] += lam_eff * alpha2 * pe2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation hybrid with directed exploration bonus and anxiety effects.
    
    Mechanism:
    - Second-stage values learned model-free; track uncertainty (running variance of PE)
      per state-action and add an exploration bonus proportional to uncertainty.
    - First-stage values are a hybrid: SR-based model-based values (using fixed transitions)
      combined with MF first-stage values. Anxiety reduces SR weighting and directed exploration.
    - Includes first-stage perseveration (choice kernel).

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        beta:           [0,10] inverse temperature for both stages
        decay_u_base:   [0,1] PE-variance decay for uncertainty tracking at stage 2
        xi_bonus_base:  [0,1] exploration bonus weight on sqrt(uncertainty)
        kappa1_base:    [0,1] perseveration strength at stage 1
        omega_sr_base:  [0,1] baseline weight on SR (model-based) control at stage 1
    Returns
    - Negative log-likelihood of observed choices.
    """
    beta, decay_u_base, xi_bonus_base, kappa1_base, omega_sr_base = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed SR for first-stage actions: expected visitation of second-stage states given choice
    # Equivalent to the known common transition structure
    SR = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)

    # Value and uncertainty tables
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    var2 = np.zeros((2, 2))  # running PE variance proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Anxiety reduces SR reliance and exploration bonus; increases perseveration slightly
    omega_sr = np.clip(omega_sr_base * (1.0 - stai0), 0.0, 1.0)
    xi_bonus = xi_bonus_base * (1.0 - stai0)
    kappa1 = kappa1_base * (1.0 + 0.5 * stai0)
    decay_u = np.clip(decay_u_base, 0.0, 1.0)
    beta_eff = beta * (1.0 - 0.2 * stai0)

    prev_a1 = None
    eps = 1e-10

    for t in range(n_trials):
        # SR-based evaluation uses max Q2 per state
        max_q2 = np.max(q2, axis=1)  # shape (2,)
        q1_sr = SR @ max_q2          # shape (2,)
        q1_hyb = omega_sr * q1_sr + (1.0 - omega_sr) * q1_mf

        # Add perseveration bias at stage 1
        if prev_a1 is not None:
            bias = np.zeros(2)
            bias[prev_a1] = 1.0
            q1_hyb = q1_hyb + kappa1 * bias

        # First-stage softmax
        q1_c = q1_hyb - np.max(q1_hyb)
        probs_1 = np.exp(beta_eff * q1_c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage softmax with directed exploration bonus
        s2 = state[t]
        bonus = xi_bonus * np.sqrt(np.maximum(var2[s2], 0.0))
        q2_bonus = q2[s2] + bonus
        q2_c = q2_bonus - np.max(q2_bonus)
        probs_2 = np.exp(beta_eff * q2_c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome and learning
        r = reward[t]
        pe2 = r - q2[s2, a2]
        # Update Q2
        q2[s2, a2] += decay_u * pe2  # reuse decay_u as a bounded learning rate
        # Update uncertainty proxy (running variance of PEs)
        var2[s2, a2] = (1.0 - decay_u) * var2[s2, a2] + decay_u * (pe2 ** 2)

        # Update Q1 MF toward realized second-stage value (SARSA(0)-like bootstrap)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += decay_u * pe1  # same bounded step size

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll