def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-weighted model-based arbitration with asymmetric learning and transition-sensitive credit.

    Overview
    - Stage-2 values learned via asymmetric Rescorla–Wagner (separate alpha for positive/negative PE).
    - Stage-1 choice uses a dynamic arbitration between model-based (MB) and model-free (MF) values.
      The MB weight increases when anxiety is low and decreases when anxiety is high.
    - Transition-sensitive credit assignment to stage-1 MF values:
      learning from rare transitions is down-weighted, especially under higher anxiety.
      This uses the same anxiety sensitivity parameter that controls MB arbitration.
    - No explicit transition input is needed; common vs. rare is inferred from (action_1, state).

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha_pos: [0,1]       Learning rate for positive PE at stage-2.
    - alpha_neg: [0,1]       Learning rate for negative PE at stage-2.
    - beta:      [0,10]      Inverse temperature for softmax choices (both stages).
    - w_base:    [0,1]       Baseline model-based weight for stage-1 arbitration.
    - psi_anx:   [0,1]       Anxiety sensitivity: higher values amplify the effect of anxiety on
                             reducing MB weight and reducing learning from rare transitions.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha_pos, alpha_neg, beta, w_base, psi_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha_pos, alpha_neg, beta, w_base, psi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    # We'll use it for MB evaluation and to infer common/rare after the fact.
    transition_matrix = np.array([[0.7, 0.3],  # from A to [X,Y]
                                  [0.3, 0.7]]) # from U to [X,Y]

    # Value functions
    q2 = np.zeros((2, 2)) + 0.5    # stage-2 Q-values per state and action (aliens)
    q1_mf = np.zeros(2) + 0.0      # stage-1 MF cache

    # Likelihood accumulators
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-modulated arbitration weight for MB (higher anxiety => lower MB)
    # w_mb_t = clip(w_base * (1 - psi_anx * stai), 0, 1)
    w_mb = max(0.0, min(1.0, w_base * (1.0 - psi_anx * stai)))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy: MB evaluation via transition matrix times max stage-2 Q
        max_q2 = np.max(q2, axis=1)                  # [X_max, Y_max]
        q1_mb = transition_matrix @ max_q2           # value of [A, U] from MB
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb     # arbitration

        # Softmax for stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy for reached state
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Stage-2 learning with asymmetric rates
        pe2 = r - q2[s, a2]
        alpha = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += alpha * pe2

        # Transition-sensitive credit to stage-1 MF:
        # common if A->X or U->Y; rare otherwise.
        common = 1 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0
        # Anxiety reduces credit on rare transitions using psi_anx
        rare_weight = 1.0 - psi_anx * stai
        credit = 1.0 if common == 1 else max(0.0, rare_weight)
        # MF update uses stage-2 PE scaled by credit
        q1_mf[a1] += alpha * credit * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Control-cost regularization with anxiety-weighted uncertainty and stay bias.

    Overview
    - Stage-2 values learned via standard Rescorla–Wagner.
    - A control-cost term increases exploration when uncertainty is high; anxiety amplifies this effect.
      We implement this by reducing effective inverse temperature based on a running uncertainty estimate.
    - Uncertainty is tracked per state as a running average of absolute prediction errors (uses alpha).
    - A simple stay bias (perseveration) at both stages encourages repeating previous choices.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:      [0,1]   Learning rate for Q updates and uncertainty tracking.
    - beta:       [0,10]  Base inverse temperature for softmax.
    - omega_cost: [0,1]   Strength by which uncertainty reduces effective beta (control cost).
    - rho_stay:   [0,1]   Strength of stay bias on both stages (applied as additive logits term).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, omega_cost, rho_stay].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, omega_cost, rho_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q1 = np.zeros(2) + 0.0
    q2 = np.zeros((2, 2)) + 0.5

    # Uncertainty per state (running mean absolute PE), initialized small
    u_state = np.zeros(2) + 0.05

    # Stay biases (logit boosts for repeating)
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)  # per state

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Effective beta reduced by control cost of exerting precise control under uncertainty
        # beta_eff = beta / (1 + omega_cost * stai * u) ; for stage-1 use average across states
        u_bar = 0.5 * (u_state[0] + u_state[1])
        beta1 = beta / (1.0 + omega_cost * stai * u_bar)
        beta2 = beta / (1.0 + omega_cost * stai * u_state[s])
        beta1 = max(1e-3, beta1)
        beta2 = max(1e-3, beta2)

        # Stay bias logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_stay

        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_stay

        # Stage-1 policy (pure MF here)
        logits1 = beta1 * (q1 - np.max(q1)) + bias1
        # subtract max again for numerical stability after adding bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 policy
        q2_s = q2[s]
        logits2 = beta2 * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Stage-2 learning and uncertainty update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2
        u_state[s] = (1.0 - alpha) * u_state[s] + alpha * abs(pe2)

        # Stage-1 MF update via eligibility trace from stage-2 PE
        q1[a1] += alpha * pe2

        # Update stay memory
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty bonus (UCB-like) with anxiety-modulated optimism and exploration.

    Overview
    - Stage-2 values learned via Rescorla–Wagner.
    - An uncertainty bonus encourages exploration (Upper Confidence Bound style) using a running
      estimate of uncertainty per state-action from absolute PEs.
    - Anxiety modulates both the optimism toward uncertain options (bonus strength) and the
      effective exploration temperature (reducing inverse temperature when anxious and uncertain).
    - Stage-1 MF cache is backed up from stage-2 PE; stage-1 decision uses an MB look-ahead
      (via known transitions) of uncertainty-augmented stage-2 values.

    Parameters (all used; bounds in [0,1] except beta in [0,10])
    - alpha:      [0,1]   Learning rate for Q updates and uncertainty tracking.
    - beta:       [0,10]  Base inverse temperature for softmax.
    - phi_u:      [0,1]   Smoothing for uncertainty estimate from absolute PEs (higher -> faster).
    - zeta_bonus: [0,1]   Base strength of uncertainty bonus added to Q.
    - xi_anx:     [0,1]   Anxiety sensitivity: scales how much anxiety boosts exploration bonus
                          and reduces effective beta under uncertainty.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, phi_u, zeta_bonus, xi_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, phi_u, zeta_bonus, xi_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure for MB look-ahead
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values and uncertainties
    q2 = np.zeros((2, 2)) + 0.5
    u2 = np.zeros((2, 2)) + 0.05  # running uncertainty per state-action
    q1_mf = np.zeros(2) + 0.0

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-adjusted uncertainty bonus strength and effective betas
        # More anxious -> larger bonus and lower beta when uncertainty is high
        bonus_strength = zeta_bonus * (1.0 + xi_anx * stai)

        # Stage-2 augmented values with bonus
        q2_aug = q2 + bonus_strength * u2

        # Effective beta reduced by average uncertainty weighted by anxiety
        u_bar = float(np.mean(u2))
        beta_eff1 = beta / (1.0 + xi_anx * stai * u_bar)
        beta_eff2 = beta / (1.0 + xi_anx * stai * float(np.mean(u2[s])))
        beta_eff1 = max(1e-3, beta_eff1)
        beta_eff2 = max(1e-3, beta_eff2)

        # Stage-1 MB look-ahead using augmented stage-2 values
        max_q2_aug = np.max(q2_aug, axis=1)      # best augmented value per planet
        q1_mb = T @ max_q2_aug                   # MB value for [A, U]
        q1 = 0.5 * q1_mf + 0.5 * q1_mb           # fixed 50/50 arbitration to keep params bounded

        # Stage-1 softmax
        logits1 = beta_eff1 * (q1 - np.max(q1))
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p1[t] = probs1[a1]

        # Stage-2 softmax at reached state using augmented values
        logits2 = beta_eff2 * (q2_aug[s] - np.max(q2_aug[s]))
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p2[t] = probs2[a2]

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty (running abs PE)
        u2[s, a2] = (1.0 - phi_u) * u2[s, a2] + phi_u * abs(pe2)

        # Stage-1 MF update via eligibility from stage-2 PE
        q1_mf[a1] += alpha * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(nll)