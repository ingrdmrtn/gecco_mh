def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free SARSA(λ) with anxiety-asymmetric learning, lapse, and repetition bias.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Outcome on each trial (gold coins; can be negative).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha, beta, elig, stick, lapse)
        - alpha in [0,1]: base learning rate.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - elig in [0,1]: eligibility trace transferring credit to stage-1 action.
        - stick in [0,1]: repetition bias strength added to logits for repeating last action at each stage.
        - lapse in [0,1]: choice lapse; with prob lapse, choices are random at each stage.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Anxiety modulates learning asymmetry: effective alpha is larger for negative outcomes
      and smaller for positive outcomes as stai increases:
          alpha_pos = alpha * (1 - 0.5 * stai)
          alpha_neg = alpha * (1 + 0.5 * stai)
      Updates use alpha_pos if TD error >= 0, else alpha_neg.
    - Purely model-free SARSA(λ): stage-1 value updated toward stage-2 action value and reward,
      with eligibility trace parameter 'elig'.
    - Repetition bias is applied at both stages (separate previous-action memories).
    - Lapse produces mixture of softmax choice and uniform random choice.
    """
    alpha, beta, elig, stick, lapse = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values: stage 1 has 2 actions; stage 2 has 2 states x 2 actions
    Q1 = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Previous actions for repetition bias
    prev_a1 = None
    prev_a2 = [None, None]  # one per state

    # Anxiety-modulated learning rates
    alpha_pos = alpha * (1.0 - 0.5 * stai_val)
    alpha_neg = alpha * (1.0 + 0.5 * stai_val)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])
        r = reward[t]

        # Stage-1 policy: softmax over Q1 with repetition bias
        logits1 = beta * Q1.copy()
        if prev_a1 is not None:
            logits1[prev_a1] += stick
        # Softmax
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        # Lapse mixture with uniform
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax over Q2[s2] with repetition bias
        logits2 = beta * Q2[s2].copy()
        if prev_a2[s2] is not None:
            logits2[prev_a2[s2]] += stick
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # TD errors
        # SARSA target at stage 2 is immediate reward (terminal after second choice)
        delta2 = r - Q2[s2, a2]
        # Anxiety-asymmetric learning rate
        a2_lr = alpha_pos if delta2 >= 0 else alpha_neg
        Q2[s2, a2] += a2_lr * delta2

        # Stage-1 TD error uses bootstrapped value of chosen stage-2 action (post-update)
        delta1 = Q2[s2, a2] - Q1[a1]
        a1_lr = alpha_pos if delta1 >= 0 else alpha_neg
        # Eligibility trace moves a fraction of the stage-2 update to stage-1
        Q1[a1] += a1_lr * (elig * delta2 + (1.0 - elig) * delta1)

        # Update previous choices
        prev_a1 = a1
        prev_a2[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid arbitration with surprise- and anxiety-modulated model-based weight; MF stage-2 learning.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Outcome on each trial (gold coins).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, mix0, psi, stick2)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values (model-free).
        - beta in [0,10]: inverse temperature for softmax.
        - mix0 in [0,1]: baseline weight on model-based control at stage 1.
        - psi in [0,1]: sensitivity of arbitration to transition surprise.
        - stick2 in [0,1]: repetition bias at stage 2 (bias to repeat last alien in a state).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Model-based values use the known transition structure (common 0.7, rare 0.3).
    - The arbitration weight w_t is updated each trial via:
          w_t = sigmoid( logit(mix0) + psi_eff * surpr_t )
      where surpr_t = +1 for rare transitions, -1 for common transitions.
    - Anxiety reduces surprise sensitivity: psi_eff = psi * (1 - stai).
    - Stage-1 action values are a convex combination:
          Q1 = (1 - w_t) * Q1_MF + w_t * Q1_MB
      where Q1_MF is the cached MF value bootstrapped from Q2 and updated by TD.
    - Stage-2 policy is MF softmax with repetition bias stick2; Q2 learned with alpha2.
    """
    alpha2, beta, mix0, psi, stick2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    Q2 = np.zeros((2, 2))
    Q1_MF = np.zeros(2)

    # Helpers
    def logit(p):
        p = min(max(p, 1e-6), 1 - 1e-6)
        return np.log(p / (1 - p))

    def sigmoid(x):
        # Stable sigmoid
        if x >= 0:
            z = np.exp(-x)
            return 1.0 / (1.0 + z)
        else:
            z = np.exp(x)
            return z / (1.0 + z)

    # Baseline arbitration in logit space
    base_logit = logit(mix0)
    psi_eff = psi * (1.0 - stai_val)

    # Track previous stage-2 action per state for stickiness
    prev_a2 = [None, None]

    # Track previous stage-1 action for MF bootstrapping (SARSA-style)
    prev_a1 = None
    prev_s2 = None
    prev_a2_taken = None

    for t in range(n_trials):

        # Compute model-based Q1 from current Q2 and known transitions
        max_Q2 = np.max(Q2, axis=1)  # best alien value on each planet
        Q1_MB = T @ max_Q2

        # Arbitration weight from previous trial's surprise (for t>0)
        if t == 0:
            w_t = mix0
        else:
            # Surprise from the last transition
            was_common = (prev_a1 == prev_s2)  # common if A->X (0->0) or U->Y (1->1)
            surpr = -1.0 if was_common else 1.0
            w_t = sigmoid(base_logit + psi_eff * surpr)

        # Combine MB and MF for current choice
        Q1 = (1.0 - w_t) * Q1_MF + w_t * Q1_MB

        # Stage-1 choice probability
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability (with repetition bias)
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2].copy()
        if prev_a2[s2] is not None:
            logits2[prev_a2[s2]] += stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning updates
        r = reward[t]

        # Update stage-2 MF values
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update stage-1 MF values toward current max Q2 of reached state
        # (cached MF that learns which spaceship leads to better second-stage returns)
        target1 = np.max(Q2[s2])
        delta1 = target1 - Q1_MF[a1]
        Q1_MF[a1] += alpha2 * delta1  # share same alpha2 for simplicity/parsimonious parameters

        # Bookkeeping
        prev_a2[s2] = a2
        prev_a1 = a1
        prev_s2 = s2
        prev_a2_taken = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-Representation stage-1 control with anxiety-modulated planning depth and forgetting.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on the visited planet).
    reward : array-like of float
        Outcome on each trial (gold coins).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : tuple/list
        (alpha2, beta, delta, fade, biasc)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax.
        - delta in [0,1]: SR discount controlling planning depth from stage 1 to stage 2.
        - fade in [0,1]: forgetting rate applied to Q2 each trial (toward zero baseline).
        - biasc in [0,1]: constant bias favoring spaceship A at stage 1; scaled down by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes
    -----
    - Successor representation for stage 1: M = I + delta_eff * T, where T is the known transition
      matrix from actions to states; Q1_SR = M_row_action · V where V = max_a Q2[state, a].
    - Anxiety reduces effective planning depth: delta_eff = delta * (1 - stai).
    - Forgetting: each trial Q2 <- (1 - fade_eff) * Q2, where fade_eff = fade * (0.5 + 0.5*stai),
      making higher anxiety accelerate forgetting of second-stage values.
    - A small constant bias toward spaceship A is added to logits, attenuated by anxiety:
      bias1 = [biasc * (1 - stai), 0].
    """
    alpha2, beta, delta, fade, biasc = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition matrix: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize storage for choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Q-values for stage 2
    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    delta_eff = delta * (1.0 - stai_val)
    fade_eff = fade * (0.5 + 0.5 * stai_val)

    for t in range(n_trials):
        # Apply forgetting to Q2 before computing policy
        if fade_eff > 0.0:
            Q2 *= (1.0 - fade_eff)

        # Compute SR-based Q1
        # M = I + delta_eff * T for a one-step augmented occupancy
        I2 = np.eye(2)
        M = I2 + delta_eff * T
        V2 = np.max(Q2, axis=1)  # value of each state given best alien
        Q1_SR = M @ V2  # 2 actions

        # Stage-1 policy with bias toward spaceship A (attenuated by anxiety)
        bias_vec = np.array([biasc * (1.0 - stai_val), 0.0])
        logits1 = beta * Q1_SR + bias_vec
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Update Q2 with outcome
        r = reward[t]
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll