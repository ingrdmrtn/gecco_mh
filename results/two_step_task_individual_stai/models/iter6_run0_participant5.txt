def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Reliability-adaptive model-based arbitration with learned transitions.

    Core idea:
    - Learn second-stage action values (Q2) via TD learning.
    - Learn the transition model T from experience.
    - Compute a model-based (MB) plan using the learned T and a model-free (MF) Q1.
    - Arbitration weight for MB depends on (i) how reliable the learned transitions are,
      and (ii) anxiety: higher anxiety down-weights MB planning.
    
    Parameters (all in [0,1] except beta in [0,10]):
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (0/1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce reliance on MB planning in proportion to learned transition reliability.
    model_parameters : array-like of float
        [learn_r, beta, learn_T, conf_gain]
        - learn_r in [0,1]: learning rate for Q2 and Q1 MF updates.
        - beta in [0,10]: inverse temperature for both stages.
        - learn_T in [0,1]: learning rate for the transition matrix T.
        - conf_gain in [0,1]: step size for updating a running transition-reliability signal.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    learn_r, beta, learn_T, conf_gain = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize
    # Transition model T[action, state]; start uncertain (0.5/0.5)
    T = 0.5 * np.ones((2, 2))
    # Running "confidence" per action: how deterministic the learned transitions are
    conf = np.zeros(2)
    # Stage-2 action values (aliens in each planet)
    Q2 = 0.5 * np.ones((2, 2))
    # Stage-1 model-free values for spaceships
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based evaluation from learned transitions
        max_Q2 = np.max(Q2, axis=1)           # value of best alien on each planet
        Q1_mb = T @ max_Q2                    # expected value for each spaceship

        # Anxiety- and reliability-gated arbitration weight
        # Determinism per action: |T[a,0]-0.5| scaled to [0,1]
        det = np.abs(T[:, 0] - 0.5) * 2.0
        # Update confidence only for chosen action (EMA)
        conf[a1] = (1.0 - conf_gain) * conf[a1] + conf_gain * det[a1]
        conf_mean = 0.5 * (conf[0] + conf[1])

        # Weight MB more when transitions are reliable and anxiety is low
        w_mb = np.clip(conf_mean * (1.0 - st), 0.0, 1.0)

        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Stage-1 policy
        logits1 = beta * (Q1 - np.max(Q1))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        # Stage-2 policy
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Update transitions T for the chosen action toward observed state
        # Row a1 moves toward one-hot of the observed state
        T[a1, s] += learn_T * (1.0 - T[a1, s])
        T[a1, 1 - s] += learn_T * (0.0 - T[a1, 1 - s])

        # TD learning at stage 2
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += learn_r * pe2

        # Backpropagate value to stage 1 (MF)
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += learn_r * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxious surprise-weighted outcomes and action bias at stage 1.

    Core idea:
    - Stage 2 values learned from an outcome augmented by transition "surprise":
      rare transitions boost the effective outcome more when anxiety is high.
      Common transitions mildly dampen outcomes when anxiety is low.
    - Stage 1 uses a hybrid MB/MF policy with fixed known transition structure (0.7 common).
    - A bias toward spaceship A at stage 1 increases with anxiety (action bias modulation).

    Parameters (all in [0,1] except beta in [0,10]):
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher anxiety amplifies the positive impact of rare transitions on learning
        and strengthens a bias toward spaceship A.
    model_parameters : array-like of float
        [alpha, beta, biasA0, omega_surprise]
        - alpha in [0,1]: learning rate for Q updates.
        - beta in [0,10]: inverse temperature for both stages.
        - biasA0 in [0,1]: baseline additive bias for choosing spaceship A at stage 1,
          scaled upward by anxiety.
        - omega_surprise in [0,1]: weight on rare-transition surprise added to the observed outcome.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, biasA0, omega_surprise = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed known transition structure for MB planning (A->X common, U->Y common)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    # MB weight increases when anxiety is low (not a free parameter)
    w_mb = np.clip(0.3 + 0.4 * (1.0 - st), 0.0, 1.0)
    # Anxiety-modulated bias toward spaceship A (action 0)
    biasA = biasA0 * (0.5 + 0.5 * st)  # more bias with higher anxiety

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Determine rarity of observed transition under the fixed model
        # Rare if A->Y or U->X
        rare = 1 if ((a1 == 0 and s == 1) or (a1 == 1 and s == 0)) else 0

        # Surprise-weighted effective outcome for learning at stage 2
        # Rare transitions add a positive boost (stronger with anxiety).
        # Common transitions get a small dampening when anxiety is low.
        boost_rare = omega_surprise * (0.5 + 0.5 * st) * rare
        damp_common = omega_surprise * (0.3 * (1.0 - st)) * (1 - rare)
        r_eff = r + boost_rare - damp_common

        # Model-based evaluation using fixed transitions
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_fixed @ max_Q2

        # Combine MB and MF; add anxiety-modulated action bias for A
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        Q1_biased = np.array([Q1[0] + biasA, Q1[1]])

        # Stage 1 policy
        logits1 = beta * (Q1_biased - np.max(Q1_biased))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        # Stage 2 policy
        logits2 = beta * (Q2[s] - np.max(Q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Stage 2 learning with surprise-weighted outcome
        pe2 = r_eff - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Stage 1 MF update toward realized stage-2 value (using r_eff)
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-gated exploration at stage 2 and credit assignment to stage 1.

    Core idea:
    - Stage 2 exploration increases with anxiety (lower beta at stage 2 only).
    - Credit assignment from stage 2 to stage 1 is reduced by anxiety
      via an eligibility-like credit parameter.
    - Stage 1 uses a hybrid MB/MF policy where MB weight increases when anxiety is low.

    Parameters (all in [0,1] except beta_base in [0,10]):
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (0/1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce stage-2 beta (more exploration)
        and diminish credit assignment to stage 1.
    model_parameters : array-like of float
        [alpha, beta_base, credit0, beta_boost, mix_base]
        - alpha in [0,1]: learning rate for Q updates.
        - beta_base in [0,10]: baseline inverse temperature.
        - credit0 in [0,1]: base credit from stage 2 to stage 1 (eligibility-like).
        - beta_boost in [0,1]: scales the reduction of stage-2 beta with anxiety.
        - mix_base in [0,1]: baseline MB weight at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta_base, credit0, beta_boost, mix_base = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure for MB evaluation
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    Q2 = 0.5 * np.ones((2, 2))
    Q1_mf = np.zeros(2)

    # Anxiety-gated parameters
    w_mb = np.clip(mix_base * (0.5 + 0.5 * (1.0 - st)), 0.0, 1.0)
    beta1 = np.clip(beta_base, 0.0, 10.0)
    # Reduce beta2 with anxiety to increase exploration
    beta2 = np.clip(beta_base * (1.0 - 0.5 * beta_boost * st), 0.0, 10.0)
    credit = np.clip(credit0 * (1.0 - st), 0.0, 1.0)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB evaluation for stage 1
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf

        # Policies
        logits1 = beta1 * (Q1 - np.max(Q1))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        logits2 = beta2 * (Q2[s] - np.max(Q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Stage 2 TD update
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2

        # Credit assignment from stage 2 to stage 1 MF value
        pe1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * credit * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)