def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-gated arbitration and entropy-sensitive control; MF credit via eligibility from second stage.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (0/1 or continuous in [0,1]).
    - stai: array-like (1,), anxiety score in [0,1]; modulates arbitration.
    - model_parameters: 6 parameters, all used:
        alpha2        [0,1]  - second-stage reward learning rate.
        beta          [0,10] - inverse temperature used at both stages.
        w_base        [0,1]  - baseline weight on model-based control.
        lambda_elig   [0,1]  - eligibility trace: back-propagation from stage-2 PE to stage-1 MF values.
        phi_ent       [0,1]  - arbitration sensitivity to average second-stage action entropy (higher entropy -> more MB if positive effect).
        anx_gain      [0,1]  - how strongly anxiety (stai) pushes toward MB control.
    
    Model summary
    - Stage-2 Q-values (q2) are learned with alpha2.
    - Stage-1 MB values use the fixed transition structure (0.7/0.3).
    - Stage-1 MF values receive credit from the stage-2 prediction error via lambda_elig.
    - Arbitration weight w is clipped to [0,1] and increases with anxiety and with average second-stage entropy:
        w = clip(w_base + anx_gain*stai + phi_ent * H_bar, 0, 1),
      where H_bar is the mean entropy across the two planet policies implied by current q2 and beta.
    - Policy at each stage uses softmax with beta.
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, w_base, lambda_elig, phi_ent, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Stage-2 Q-values and Stage-1 MF values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Precompute average second-stage entropy at each trial from current q2
    def softmax_probs(q):
        z = q - np.max(q)
        e = np.exp(beta * z)
        return e / np.sum(e)

    def entropy(p):
        eps = 1e-12
        return -np.sum(p * np.log(p + eps))

    for t in range(n_trials):

        # Arbitration signal: average entropy across planet policies implied by current q2
        pX = softmax_probs(q2[0])
        pY = softmax_probs(q2[1])
        H_bar = 0.5 * (entropy(pX) + entropy(pY))

        # MB values: expectation of max q2 under transitions
        mb_component = T @ np.max(q2, axis=1)

        # Combine MB and MF with anxiety- and entropy-gated arbitration
        w = w_base + anx_gain * stai + phi_ent * H_bar
        w = max(0.0, min(1.0, w))
        q1 = w * mb_component + (1.0 - w) * q1_mf

        # First-stage choice probability
        a1 = action_1[t]
        q1_cent = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_cent)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability (given observed state)
        s = state[t]
        a2 = action_2[t]
        q2_cent = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2_cent)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Rewards and learning
        r = reward[t]

        # Stage-2 PE and update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * delta2

        # Stage-1 MF credit assignment via eligibility from stage-2 PE
        q1_mf[a1] += lambda_elig * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with anxiety-amplified exploration bonus at stage 2 and rare-transition-sensitive perseveration at stage 1.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (0/1 or continuous in [0,1]).
    - stai: array-like (1,), anxiety score in [0,1]; scales exploration.
    - model_parameters: 6 parameters, all used:
        alpha_r        [0,1]  - learning rate for second-stage reward expectations.
        beta           [0,10] - inverse temperature at both stages.
        ucb_base       [0,1]  - baseline scale for optimistic exploration bonus (UCB-like).
        anx_ucb_slope  [0,1]  - slope by which anxiety increases exploration bonus.
        pers1          [0,1]  - first-stage perseveration to repeat the last chosen ship.
        zeta_rt        [0,1]  - attenuation of perseveration on rare transitions (0=no attenuation, 1=full attenuation).
    
    Model summary
    - Stage-2 Q-values are learned with alpha_r.
    - An exploration bonus b(s,a) = scale / sqrt(visit_count(s,a) + 1) is added to stage-2 values,
      where scale = ucb_base * (1 + anx_ucb_slope * stai). Higher anxiety -> larger bonus -> more exploration.
    - Stage-1 values are model-based by projecting the max (Q2 + bonus) through the fixed transitions (0.7/0.3).
    - First-stage perseveration is applied to the last chosen action; it is attenuated after rare transitions by factor (1 - zeta_rt).
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, ucb_base, anx_ucb_slope, pers1, zeta_rt = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = np.zeros((2, 2), dtype=float)
    n_visits = np.zeros((2, 2), dtype=float)

    last_a1 = None
    last_rare = 0

    # exploration bonus scale
    bonus_scale = ucb_base * (1.0 + anx_ucb_slope * stai)

    for t in range(n_trials):

        # Exploration bonuses for each planet-action
        bonus = bonus_scale / np.sqrt(n_visits + 1.0)

        # MB first-stage values based on max over (Q2 + bonus)
        max_stage2 = np.max(q2 + bonus, axis=1)
        q1 = T @ max_stage2

        # Perseveration on first-stage repeating, attenuated if last transition was rare
        if last_a1 is not None:
            persev = pers1 * (1.0 - zeta_rt * last_rare)
            q1[last_a1] += persev

        # First-stage policy
        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy includes exploration bonus
        s = state[t]
        a2 = action_2[t]
        q2_eff = q2[s] + bonus[s]
        q2c = q2_eff - np.max(q2_eff)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        n_visits[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update rare/common flag for next trial
        common_dest = a1  # 0->X(0), 1->Y(1)
        last_rare = int(s != common_dest)
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-sensitive learning with anxiety amplification, hybrid MB/MF arbitration, and state-specific second-stage perseveration.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial.
    - stai: array-like (1,), anxiety score in [0,1]; scales volatility impact and arbitration.
    - model_parameters: 7 parameters, all used:
        alpha0        [0,1]  - base learning rate for second-stage values.
        beta          [0,10] - inverse temperature at both stages.
        kappa_vol     [0,1]  - gain by which local volatility inflates the learning rate.
        tau_leak      [0,1]  - leak for volatility estimate (higher -> faster adaptation).
        stick2        [0,1]  - state-specific perseveration on repeating the same second-stage action.
        w0            [0,1]  - baseline weight on model-based control at the first stage.
        rho_anx_mb    [0,1]  - anxiety slope increasing MB weight and volatility sensitivity.
    
    Model summary
    - Each planet maintains a volatility estimate z(s) tracking absolute prediction error; it is updated by a leaky integrator.
    - Effective learning rate at the visited planet s is:
        alpha_eff(s) = clip(alpha0 + kappa_vol * z(s) * (1 + rho_anx_mb * stai), 0, 1).
      Higher volatility and anxiety increase learning speed.
    - First-stage values are a MB/MF mixture: Q1 = w * MB + (1-w) * MF, with
        w = clip(w0 + rho_anx_mb * stai, 0, 1).
      MB uses fixed transition probabilities (0.7/0.3). MF credit uses the stage-2 PE.
    - Second-stage policy includes state-specific perseveration: at each planet, repeating the last chosen alien gets +stick2.
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha0, beta, kappa_vol, tau_leak, stick2, w0, rho_anx_mb = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    # Volatility estimates per planet
    z = np.zeros(2, dtype=float)

    # Track last second-stage action per state for perseveration
    last_a2 = np.array([-1, -1], dtype=int)

    # Anxiety-modulated MB weight
    w = w0 + rho_anx_mb * stai
    w = max(0.0, min(1.0, w))

    for t in range(n_trials):

        # MB component via current q2 and fixed transitions
        mb_component = T @ np.max(q2, axis=1)
        q1 = w * mb_component + (1.0 - w) * q1_mf

        # First-stage policy
        a1 = action_1[t]
        q1c = q1 - np.max(q1)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with state-specific perseveration
        s = state[t]
        q2_eff = q2[s].copy()
        if last_a2[s] != -1:
            q2_eff[last_a2[s]] += stick2

        q2c = q2_eff - np.max(q2_eff)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q2[s, a2]

        # Update volatility estimate (leaky integration of absolute PE)
        z[s] = (1.0 - tau_leak) * z[s] + tau_leak * abs(delta2)

        # Anxiety- and volatility-modulated learning rate
        alpha_eff = alpha0 + kappa_vol * z[s] * (1.0 + rho_anx_mb * stai)
        alpha_eff = max(0.0, min(1.0, alpha_eff))

        # Update second-stage values
        q2[s, a2] += alpha_eff * delta2

        # MF credit to first stage from stage-2 PE
        q1_mf[a1] += delta2

        # Update last chosen second-stage action for perseveration
        last_a2[s] = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll