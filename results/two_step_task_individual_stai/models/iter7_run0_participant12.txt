def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety- and volatility-gated arbitration and eligibility trace.
    
    Idea
    ----
    First-stage action values are a convex combination of model-free (MF) and model-based (MB)
    controllers. The arbitration weight dynamically depends on:
      - Anxiety (stai): higher anxiety reduces reliance on MB planning.
      - Recent reward volatility (via an entropy tracker): higher entropy reduces MB reliance.
    Second-stage values are purely model-free and are backed up to stage 1 with an eligibility trace.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial within the planet (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Rewards received per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to gate arbitration.
    model_parameters : list or array
        [alpha_q, beta, mix_base, ent_sens, lambda_e]
        Bounds:
          alpha_q   in [0,1]   : learning rate for Q-updates at stage 2
          beta      in [0,10]  : inverse temperature for both stages
          mix_base  in [0,1]   : baseline MB weight (at minimal anxiety/volatility)
          ent_sens  in [0,1]   : sensitivity to reward entropy (volatility gate)
          lambda_e  in [0,1]   : eligibility trace from stage 2 to stage 1
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_q, beta, mix_base, ent_sens, lambda_e = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure (common A->X and U->Y)
    transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])  # rows: action (A,U), cols: state (X,Y)

    # Initialize values
    q_stage1_mf = np.zeros(2)          # MF values at stage 1
    q_stage2_mf = np.zeros((2, 2))     # MF values per planet and alien

    # For likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track recent reward entropy (exponential moving estimate of Bernoulli parameter)
    p_r = 0.5
    # Decay rate for the moving estimate reuses ent_sens (more sensitivity -> faster tracking)
    alpha_h = max(1e-6, ent_sens)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based stage-1 values: expect max Q on each planet
        max_q2 = np.max(q_stage2_mf, axis=1)  # per state
        q1_mb = transition_matrix @ max_q2

        # Model-free stage-1 values already in q_stage1_mf
        # Arbitration weight: baseline reduced by anxiety and by entropy
        # - Anxiety gate: g_stai in [0,1], where higher stai reduces MB weight
        g_stai = 1.0 - stai  # low anxiety -> closer to 1, high anxiety -> closer to 0
        # - Entropy gate: compute current entropy of reward distribution estimate
        eps = 1e-12
        p_r = (1 - alpha_h) * p_r + alpha_h * reward[t]  # update with current outcome
        H = -(p_r * np.log(p_r + eps) + (1 - p_r) * np.log(1 - p_r + eps)) / np.log(2.0)  # normalized to [0,1]
        g_ent = 1.0 - H                                 # low entropy => more MB

        omega = mix_base * g_stai * (ent_sens * g_ent + (1 - ent_sens) * 1.0)
        omega = np.clip(omega, 0.0, 1.0)

        # Hybrid Q for stage 1
        q1_hybrid = (1.0 - omega) * q_stage1_mf + omega * q1_mb

        # Stage-1 policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c); probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (purely MF)
        q2 = q_stage2_mf[s].copy()
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c); probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Learning at stage 2
        pe2 = reward[t] - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha_q * pe2

        # Eligibility-backed update to stage 1
        q_stage1_mf[a1] += lambda_e * alpha_q * pe2

        # Optional direct TD correction towards realized state-action value
        td1 = np.max(q_stage2_mf[s]) - q_stage1_mf[a1]
        q_stage1_mf[a1] += (1.0 - lambda_e) * alpha_q * td1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned-transition MB with anxiety-slowed transition learning and stickiness.
    
    Idea
    ----
    The agent learns the transition function P(planet | spaceship) online. Anxiety reduces the
    effective transition learning rate, slowing adaptation to rare transitions. Action stickiness
    at stage 1 is scaled up by anxiety, capturing perseveration under stress. Values at stage 2
    are model-free and inform the MB planner.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to slow transition learning and increase stickiness.
    model_parameters : list or array
        [alpha_q, beta, alpha_T0, anx_slope, stick_base]
        Bounds:
          alpha_q   in [0,1]   : learning rate for second-stage values
          beta      in [0,10]  : inverse temperature
          alpha_T0  in [0,1]   : baseline transition learning rate
          anx_slope in [0,1]   : strength of anxiety effects on alpha_T and stickiness
          stick_base in [0,1]  : baseline first-stage stickiness
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_q, beta, alpha_T0, anx_slope, stick_base = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix P(s | a); start near the instructed structure
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))  # second-stage MF values
    q1_mf = np.zeros(2)    # optional MF cache from eligibility

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation
    alpha_T = alpha_T0 * (1.0 - anx_slope * stai)           # higher stai -> smaller alpha_T
    alpha_T = np.clip(alpha_T, 1e-6, 1.0)
    stick_eff = stick_base * (1.0 + anx_slope * stai)       # higher stai -> stronger stickiness

    prev_a1 = None

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based first-stage values from learned T and current q2
        mb_q1 = T @ np.max(q2, axis=1)

        # Add stickiness bias to last chosen a1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        # Combine MB with small MF cache (0.5 weight for stability)
        q1 = mb_q1 * 0.8 + q1_mf * 0.2 + bias

        # Stage 1 policy
        q1c = q1 - np.max(q1)
        prob1 = np.exp(beta * q1c); prob1 = prob1 / np.sum(prob1)
        p_choice_1[t] = prob1[a1]

        # Stage 2 policy
        q2c = q2[s] - np.max(q2[s])
        prob2 = np.exp(beta * q2c); prob2 = prob2 / np.sum(prob2)
        p_choice_2[t] = prob2[a2]

        # Update second-stage MF values
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Small eligibility backup to stage 1 MF cache
        q1_mf[a1] += 0.5 * alpha_q * pe2

        # Learn transition function T for executed a1 toward observed s
        # Move row a1 toward one-hot of observed state s
        target = np.array([0.0, 0.0]); target[s] = 1.0
        T[a1] = (1 - alpha_T) * T[a1] + alpha_T * target
        # Renormalize to guard against drift
        T[a1] = T[a1] / np.sum(T[a1])

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus planner with anxiety-modulated exploration and lapse.
    
    Idea
    ----
    The agent augments second-stage Q-values with an uncertainty bonus proportional to the
    estimated reward variance m*(1-m), where m is the learned mean reward for each alien.
    Anxiety modulates the strength and sign of this bonus: higher anxiety reduces (or can
    reverse) directed exploration. A small lapse mixes policies with uniform choice noise.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Rewards received per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to modulate uncertainty-driven exploration and lapse.
    model_parameters : list or array
        [alpha_q, beta, uncert_base, lapse_base, anx_gain]
        Bounds:
          alpha_q     in [0,1]   : learning rate for mean reward estimates
          beta        in [0,10]  : inverse temperature
          uncert_base in [0,1]   : baseline uncertainty-bonus strength
          lapse_base  in [0,1]   : baseline lapse probability
          anx_gain    in [0,1]   : how strongly STAI modulates uncertainty bonus and lapse
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_q, beta, uncert_base, lapse_base, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition matrix for planning
    T = np.array([[0.7, 0.3], [0.3, 0.7]], dtype=float)

    # Reward mean estimates (Q-like) and induced uncertainty (from mean)
    m = np.full((2, 2), 0.5)  # initial mean reward per alien
    # Stage-1 MF cache to allow quick backup from stage 2 (helps fit stay/switch effects)
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated uncertainty bonus and lapse
    # Map stai in [0,1] to a signed modulation in [-1,1] via centered scaling
    mod = (2.0 * stai - 1.0) * anx_gain
    # Effective uncertainty coefficient: positive for directed exploration, negative for ambiguity aversion
    uncert_coef = uncert_base * (1.0 - 2.0 * stai * anx_gain) + mod * 0.0  # keep in [-uncert_base, uncert_base]
    uncert_coef = np.clip(uncert_coef, -uncert_base, uncert_base)

    lapse = np.clip(lapse_base * (1.0 + 0.5 * mod), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute uncertainty bonus per alien using variance proxy var = m*(1-m)
        var = m * (1.0 - m)
        bonus = uncert_coef * np.sqrt(var + 1e-6)

        # Augmented second-stage values
        q2_aug = m + bonus

        # Model-based evaluation of first-stage actions using augmented second-stage values
        mb_q1 = T @ np.max(q2_aug, axis=1)

        # Combine with MF cache (small weight)
        q1 = 0.85 * mb_q1 + 0.15 * q1_mf

        # Stage-1 softmax
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta * q1c); pi1 = pi1 / np.sum(pi1)
        # Apply lapse (mixture with uniform over 2 actions)
        pi1 = (1.0 - lapse) * pi1 + lapse * 0.5
        p_choice_1[t] = pi1[a1]

        # Stage-2 softmax using augmented values in the visited state
        q2s = q2_aug[s]
        q2c = q2s - np.max(q2s)
        pi2 = np.exp(beta * q2c); pi2 = pi2 / np.sum(pi2)
        pi2 = (1.0 - lapse) * pi2 + lapse * 0.5
        p_choice_2[t] = pi2[a2]

        # Learning: update mean reward for the chosen alien
        pe = reward[t] - m[s, a2]
        m[s, a2] += alpha_q * pe

        # Eligibility-like backup to stage-1 MF cache
        q1_mf[a1] += 0.5 * alpha_q * pe

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll