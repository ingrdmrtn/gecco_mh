def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive-volatility hybrid MB/MF with anxiety-scaled learning and eligibility.
    
    Idea:
    - Second-stage Q-values are updated with an adaptive learning rate that increases
      with surprise |delta|. Anxiety scales the volatility sensitivity, so higher STAI
      increases the adjustment of learning rate by surprise.
    - First-stage choice values are a hybrid: w_mb * model-based + (1 - w_mb) * model-free.
      The model-free stage-1 values are updated via an eligibility trace from the stage-2 TD error.
      The trace strength is tied to anxiety (more anxious -> stronger credit assignment to the
      chosen spaceship).
    - Transitions are assumed known and stable (common = 0.7, rare = 0.3).
    
    Parameters (model_parameters):
    - lr0: base learning rate for Q2 and MF Q1 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - vol0: base volatility sensitivity scaling surprise into learning rate, in [0,1]
    - anx_vol: anxiety gain on volatility sensitivity, in [0,1]
    - w_mb: weight on model-based value at stage 1, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0.0/1.0)
    - stai: array-like with single float in [0,1]
    - model_parameters: array-like [lr0, beta, vol0, anx_vol, w_mb]
    
    Returns:
    - Negative log-likelihood of both stage choices under the model.
    """
    lr0, beta, vol0, anx_vol, w_mb = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure (rows: [A,U], cols: [X,Y])
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    q2 = np.zeros((2, 2))      # planet x alien values
    q1_mf = np.zeros(2)        # model-free spaceship values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated learning-rate adaptivity
    vol_eff = np.clip(vol0 + anx_vol * stai, 0.0, 1.0)
    # Eligibility trace strength increases with anxiety (credit assignment to a1 from a2 outcome)
    lam = np.clip(0.1 + 0.8 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based Q at stage 1 from current q2
        max_q2 = np.max(q2, axis=1)     # best alien per planet
        q1_mb = T @ max_q2

        # Hybrid value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited planet
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q2[s, a2]

        # Adaptive learning rate from surprise, scaled by anxiety
        alpha_adapt = np.clip(lr0 * (1.0 + vol_eff * abs(delta2)), 0.0, 1.0)

        # Update second-stage value
        q2[s, a2] += alpha_adapt * delta2

        # Update model-free stage-1 via eligibility from stage-2 TD error
        q1_mf[a1] += lr0 * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with anxiety and directed exploration at stage 2.
    
    Idea:
    - Rewards are transformed by a concave/convex utility u(r) = r^eta, where
      eta = risk0 + anx_risk * stai. Higher anxiety can bias toward risk aversion
      (eta<1) or risk seeking (eta>1), depending on fitted signs within [0,1].
    - Stage-2 choice incorporates a directed exploration bonus proportional to an
      uncertainty trace u_trace[s,a] (running absolute TD errors). Anxiety speeds up
      the updating of this trace, increasing exploration under high uncertainty.
    - Stage-1 uses model-based values from fixed transitions and the risk-transformed
      expected Q2, no MF component here to keep the mechanism focused on risk/explore.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2 updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - risk0: base risk exponent for utility, in [0,1] (eta base)
    - anx_risk: anxiety gain on risk exponent, in [0,1] (eta = risk0 + anx_risk*stai, clipped to [0,1])
    - k_ucb: weight of uncertainty bonus at stage 2, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0.0/1.0)
    - stai: array-like with single float in [0,1]
    - model_parameters: array-like [alpha_r, beta, risk0, anx_risk, k_ucb]
    
    Returns:
    - Negative log-likelihood of both stage choices.
    """
    alpha_r, beta, risk0, anx_risk, k_ucb = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Risk exponent from anxiety
    eta = np.clip(risk0 + anx_risk * stai, 0.0, 1.0)

    q2 = np.zeros((2, 2))
    u_trace = np.zeros((2, 2))  # uncertainty trace per planet-alien

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety increases how quickly uncertainty traces adapt to new errors
    kappa_u = np.clip(0.3 + 0.7 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Stage-1: MB values computed from risk-transformed q2
        # Use the max over aliens per planet
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2: include uncertainty bonus
        s = state[t]
        bonus = k_ucb * u_trace[s]
        logits2 = beta * (q2[s] + bonus)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome with risk-sensitive utility
        r_raw = reward[t]
        r_util = r_raw ** eta

        # TD update
        delta2 = r_util - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update uncertainty trace from absolute TD errors (directed exploration)
        u_trace[s, a2] = (1.0 - kappa_u) * u_trace[s, a2] + kappa_u * abs(delta2)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transitions with anxiety-driven confusion (uniform pull) and MF credit.
    
    Idea:
    - The agent learns the transition matrix from experience (row-wise updates).
    - Anxiety induces a "transition confusion" bias: the effective transition used
      for planning is a convex combination of the learned T and a uniform matrix,
      T_eff = (1 - tau_eff) * T + tau_eff * 0.5, where tau_eff increases with STAI.
      This captures difficulty trusting the learned structure under higher anxiety.
    - Stage-2 uses MF Q-learning. Stage-1 combines MB planning (via T_eff) and MF
      credit assignment from the stage-2 TD error through an eligibility trace
      whose strength grows with anxiety.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for Q2, in [0,1]
    - alpha_t: transition learning rate, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - tau0: base confusion strength toward uniform transitions, in [0,1]
    - g_tau: anxiety gain on confusion, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices on the planet (0/1)
    - reward: array of rewards per trial (e.g., 0.0/1.0)
    - stai: array-like with single float in [0,1]
    - model_parameters: array-like [alpha_r, alpha_t, beta, tau0, g_tau]
    
    Returns:
    - Negative log-likelihood of both stage choices.
    """
    alpha_r, alpha_t, beta, tau0, g_tau = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transitions to uniform and Q2 to zeros
    T = np.ones((2, 2)) * 0.5  # rows sum to 1
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-dependent confusion toward uniform
    tau_eff = np.clip(tau0 + g_tau * stai, 0.0, 1.0)
    # Eligibility trace strength increases with anxiety
    lam = np.clip(0.2 + 0.7 * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Effective transition matrix used for planning
        T_eff = (1.0 - tau_eff) * T + tau_eff * 0.5

        # MB values at stage 1 from T_eff and q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_eff @ max_q2

        # Combine MB and MF (simple sum; MF will be updated via eligibility)
        q1 = q1_mb + q1_mf

        # Stage-1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy at visited state
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward and updates
        r = reward[t]
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update learned transitions row for chosen a1 toward observed state s
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_t * (target - T[a1])

        # Normalize to keep a valid row-stochastic matrix
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        # Model-free credit assignment to stage 1 via eligibility
        q1_mf[a1] += alpha_r * lam * delta2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll