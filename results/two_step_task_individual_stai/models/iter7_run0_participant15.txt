def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-arbitrated MB/MF learner with anxiety-gated arbitration.
    
    Idea:
    - Stage-2 (aliens) values learned with TD(0) and a moving estimate of uncertainty (mean squared PE).
    - Arbitration between Model-Based (MB) and Model-Free (MF) control at Stage-1 depends on current uncertainty
      about Stage-2 values and the participant's anxiety.
        w_t = clip( a0*(1 - stai) + kappa * mean_uncertainty )
      So MB control increases with uncertainty (exploration/planning demand), but baseline MB is lower
      when anxiety is higher (since 1 - stai scales a0).
    - Stage-1 MF values are updated via eligibility (backing up Stage-2 value).
    
    Parameters (all used; total = 5):
    - lr_mf: [0,1] Learning rate for MF updates (Stage-2 TD and Stage-1 MF eligibility backup).
    - lr_mb: [0,1] Learning rate for the uncertainty estimator (moving average of squared PEs).
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - a0: [0,1] Baseline arbitration term favoring MB when stai is low.
    - kappa: [0,1] Weight of online uncertainty driving MB arbitration.
    
    Inputs:
    - action_1: int array in {0,1}; first-stage spaceship choices (0=A, 1=U).
    - state: int array in {0,1}; second-stage planet (0=X, 1=Y).
    - action_2: int array in {0,1}; second-stage alien choices.
    - reward: float array in [0,1]; coins received.
    - stai: array-like with one float in [0,1]; anxiety score.
    - model_parameters: iterable [lr_mf, lr_mb, beta, a0, kappa].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    lr_mf, lr_mb, beta, a0, kappa = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value and uncertainty estimates
    q2 = 0.5 * np.ones((2, 2))   # Stage-2 MF values per planet x alien
    u2 = 0.0 * np.ones((2, 2))   # Moving estimate of squared PE (uncertainty proxy)
    q1_mf = np.zeros(2)          # Stage-1 MF values for spaceships

    for t in range(n_trials):
        s = state[t]

        # Risk-neutral MB evaluation based on current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)

        # Uncertainty-driven arbitration (with anxiety gate)
        mean_unc = np.mean(u2)  # [0, ~1]
        w_t = a0 * (1.0 - stai) + kappa * mean_unc
        # clip to [0,1]
        if w_t < 0.0:
            w_t = 0.0
        elif w_t > 1.0:
            w_t = 1.0

        q1_combined = w_t * mb_q1 + (1.0 - w_t) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1_combined - np.max(q1_combined))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcomes
        r = reward[t]

        # Stage-2 update: TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += lr_mf * pe2

        # Update uncertainty proxy with moving average of squared PEs
        u2[s, a2] = (1.0 - lr_mb) * u2[s, a2] + lr_mb * (pe2 * pe2)

        # Stage-1 MF eligibility backup toward observed Stage-2 value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += lr_mf * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-sensitive credit misassignment with anxiety scaling and perseveration.
    
    Idea:
    - Pure MF action values at Stage-1; Stage-2 values learned by TD(0).
    - After rare transitions, some credit is shifted from the chosen spaceship to the unchosen one
      (misassignment), scaled by anxiety. This captures model-free behavior that is sensitive to the
      transition structure but in a maladaptive way that increases with anxiety.
    - Perseveration bias also scales with anxiety.
    
    Parameters (all used; total = 5):
    - eta2: [0,1] Learning rate for Stage-2 TD and Stage-1 MF updates.
    - beta: [0,10] Inverse temperature for both stages.
    - lam_ca: [0,1] Strength of credit passed to the unchosen action after rare transitions.
    - xi_trans: [0,1] Scales transition-based misassignment; effective xi_eff = xi_trans * stai.
    - stick0: [0,1] Baseline perseveration; effective stick = stick0 * stai.
    
    Inputs:
    - action_1: int array in {0,1}; first-stage choices (0=A, 1=U).
    - state: int array in {0,1}; second-stage planet (0=X, 1=Y).
    - action_2: int array in {0,1}; second-stage choices.
    - reward: float array in [0,1]; coins received.
    - stai: array-like with one float in [0,1]; anxiety score.
    - model_parameters: iterable [eta2, beta, lam_ca, xi_trans, stick0].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta2, beta, lam_ca, xi_trans, stick0 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))  # Stage-2 MF values
    q1_mf = np.zeros(2)         # Stage-1 MF values

    prev_a1 = None
    stick_eff = stick0 * stai
    xi_eff = xi_trans * stai
    lam_eff = lam_ca * (1.0 - 0.5 * stai)  # anxiety slightly reduces how much is shifted

    for t in range(n_trials):
        s = state[t]

        # Perseveration bias at Stage-1
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        # Stage-1 policy (pure MF with bias)
        q1b = q1_mf + bias
        z1 = beta * (q1b - np.max(q1b))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # TD update at Stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta2 * pe2

        # Determine whether the transition was common or rare for chosen spaceship
        # common if (A->X) or (U->Y)
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Stage-1 MF credit assignment
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]

        if is_common:
            # Normal credit to chosen after common transition
            q1_mf[a1] += eta2 * pe1
        else:
            # Rare transition: shift some credit to unchosen, scaled by anxiety
            q1_mf[a1] += eta2 * (1.0 - xi_eff) * pe1
            q1_mf[1 - a1] += eta2 * lam_eff * xi_eff * pe1

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Entropy- and surprise-modulated MB/MF mixture with anxiety shaping exploration and switching.
    
    Idea:
    - Stage-2 values learned via TD(0) with learning rate lr.
    - Stage-1 values combine MB planning and MF cached values with a weight that decreases with anxiety.
      w_eff = w_mix * (1 - stai).
    - Exploration/temperature: an entropy-like exploration factor lowers effective beta when anxiety is low:
      beta_eff = beta * (1 - zeta_ent * (1 - stai)).
    - Surprise-driven switching: after a surprising transition on the previous trial, low-anxiety agents
      tend to switch at Stage-1, high-anxiety agents tend to stick. Implemented as a bias on current Stage-1 policy
      toward the alternative action proportional to surprise_prev * kappa_surp * (0.5 - stai).
    
    Parameters (all used; total = 5):
    - lr: [0,1] Learning rate for TD updates at Stage-2 and Stage-1 MF eligibility.
    - beta: [0,10] Base inverse temperature for both stages.
    - zeta_ent: [0,1] Strength of entropy-based exploration; reduces beta when anxiety is low.
    - kappa_surp: [0,1] Weight on surprise-driven switching bias across trials.
    - w_mix: [0,1] Baseline MB mixture weight; effective weight decreases with anxiety.
    
    Inputs:
    - action_1: int array in {0,1}; first-stage choices.
    - state: int array in {0,1}; second-stage states.
    - action_2: int array in {0,1}; second-stage choices.
    - reward: float array in [0,1].
    - stai: array-like with one float in [0,1].
    - model_parameters: iterable [lr, beta, zeta_ent, kappa_surp, w_mix].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    lr, beta, zeta_ent, kappa_surp, w_mix = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    q2 = 0.5 * np.ones((2, 2))  # Stage-2 values
    q1_mf = np.zeros(2)         # Stage-1 MF values

    # Effective parameters shaped by anxiety
    beta1_eff = beta * (1.0 - zeta_ent * (1.0 - stai))
    beta2_eff = beta * (1.0 - zeta_ent * (1.0 - stai))
    if beta1_eff < 1e-6:
        beta1_eff = 1e-6
    if beta2_eff < 1e-6:
        beta2_eff = 1e-6
    w_eff = w_mix * (1.0 - stai)

    prev_a1 = None
    prev_is_common = None

    for t in range(n_trials):
        s = state[t]

        # Model-based Stage-1 values from current Stage-2 values
        mb_q1 = transition_matrix @ np.max(q2, axis=1)
        q1_base = w_eff * mb_q1 + (1.0 - w_eff) * q1_mf

        # Surprise-driven switching bias based on previous trial's transition
        bias = np.zeros(2)
        if prev_a1 is not None and prev_is_common is not None:
            p_trans = 0.7 if prev_is_common else 0.3
            surprise_prev = -np.log(max(p_trans, 1e-8))  # higher for rare
            # Positive for low anxiety (switch), negative for high anxiety (stick)
            bias_to_alt = kappa_surp * (0.5 - stai) * surprise_prev
            bias[1 - prev_a1] += bias_to_alt

        # Stage-1 policy
        z1 = beta1_eff * (q1_base + bias - np.max(q1_base + bias))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (entropy-modulated)
        z2 = beta2_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and updates
        r = reward[t]

        pe2 = r - q2[s, a2]
        q2[s, a2] += lr * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr * pe1

        # Store info for next-trial surprise bias
        prev_a1 = a1
        prev_is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll