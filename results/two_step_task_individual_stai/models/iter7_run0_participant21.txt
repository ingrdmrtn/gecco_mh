def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid planner with learned transitions and anxiety-reduced trust in planning.

    Overview:
    - Stage-2 (alien) values are learned via a delta-rule.
    - Transition probabilities from spaceships to planets are learned online.
    - Stage-1 action values blend model-based (using learned transitions) and model-free values.
    - Anxiety reduces the trust/weight placed on model-based planning.

    Parameters (bounds):
    - model_parameters[0] = eta_r (0 to 1): learning rate for stage-2 values and for bootstrapping stage-1 MF values
    - model_parameters[1] = beta (0 to 10): inverse temperature used for both stages
    - model_parameters[2] = tau_tr (0 to 1): learning rate for transition probabilities (per chosen spaceship)
    - model_parameters[3] = trust0 (0 to 1): baseline weight on model-based value at stage 1
    - model_parameters[4] = anx_trust (0 to 1): how strongly anxiety reduces the model-based weight
        Effective MB weight: w_MB = clip(trust0 * (1 - anx_trust * stai), 0, 1)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, chosen alien per trial
    - reward: array-like of floats, received coins per trial
    - stai: array-like with one float in [0,1], anxiety score for this participant
    - model_parameters: list/array of five parameters as specified above

    Returns:
    - Negative log-likelihood of the observed sequence of choices at both stages.
    """
    eta_r, beta, tau_tr, trust0, anx_trust = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize with the task's common/rare structure as a prior; learn from experience
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: a1 in {0,1}, cols: states in {0,1}

    q1_mf = np.zeros(2)           # model-free stage-1 values
    q2 = np.zeros((2, 2))         # stage-2 values: states x actions

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced trust in planning
    w_MB = np.clip(trust0 * (1.0 - anx_trust * stai_val), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based stage-1 values from learned transitions
        max_q2_by_state = np.max(q2, axis=1)          # shape (2,)
        q1_mb = T @ max_q2_by_state                   # shape (2,)

        # Hybrid value for stage 1
        q1 = w_MB * q1_mb + (1.0 - w_MB) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at the reached state
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 value
        delta2 = r - q2[s, a2]
        q2[s, a2] += eta_r * delta2

        # Bootstrap stage-1 model-free value from observed stage-2 value (SARSA-style)
        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += eta_r * delta1

        # Update learned transition probabilities for the chosen spaceship
        # Move the row T[a1] toward a one-hot on the observed state s
        target_row = np.array([0.0, 0.0])
        target_row[s] = 1.0
        T[a1] = (1.0 - tau_tr) * T[a1] + tau_tr * target_row
        # Renormalize to avoid numerical drift
        T[a1] = T[a1] / np.sum(T[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with anxiety-tilted learning rates and bi-stage stickiness.

    Overview:
    - Stage-2 values are learned with separate learning rates for positive vs. negative prediction errors.
    - Anxiety tilts the balance: higher anxiety increases sensitivity to negative outcomes and reduces sensitivity to positive ones.
    - Stage-1 uses pure model-based planning from the fixed transition structure.
    - Perseveration (choice stickiness) applies at both stages and weakens with higher anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha_pos0 (0 to 1): baseline learning rate for positive PE at stage 2
    - model_parameters[1] = alpha_neg0 (0 to 1): baseline learning rate for negative PE at stage 2
    - model_parameters[2] = beta (0 to 10): inverse temperature for both stages
    - model_parameters[3] = anx_shift (0 to 1): strength of anxiety-induced tilt of learning rates
        Effective rates: alpha_pos = clip(alpha_pos0 * (1 - anx_shift * stai), 0, 1)
                         alpha_neg = clip(alpha_neg0 * (1 + anx_shift * stai), 0, 1)
    - model_parameters[4] = stick0 (0 to 1): baseline perseveration magnitude (reduced by anxiety)
        Effective stickiness: stick = stick0 * (1 - stai)

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, chosen alien per trial
    - reward: array-like of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of five parameters as specified above

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha_pos0, alpha_neg0, beta, anx_shift, stick0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (task known)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))  # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_a2_by_state = [None, None]

    # Anxiety-modulated parameters
    alpha_pos = np.clip(alpha_pos0 * (1.0 - anx_shift * stai_val), 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg0 * (1.0 + anx_shift * stai_val), 0.0, 1.0)
    stick = stick0 * (1.0 - stai_val)

    for t in range(n_trials):
        # Model-based stage-1 values from fixed transitions
        max_q2_by_state = np.max(q2, axis=1)          # shape (2,)
        q1_mb = T @ max_q2_by_state                   # shape (2,)

        # Add perseveration bias at stage 1
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += stick

        q1_net = q1_mb + bias1

        # Stage-1 policy
        q1c = q1_net - np.max(q1_net)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with perseveration at the reached state
        s = int(state[t])
        bias2 = np.zeros(2)
        if prev_a2_by_state[s] is not None:
            bias2[prev_a2_by_state[s]] += stick

        q2_net = q2[s] + bias2
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Stage-2 update with valence asymmetry
        pe = r - q2[s, a2]
        lr = alpha_pos if pe >= 0.0 else alpha_neg
        q2[s, a2] += lr * pe

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2_by_state[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Count-based exploration bonus with anxiety-curbed novelty seeking.

    Overview:
    - Stage-2 values are learned via a delta-rule.
    - An exploration bonus encourages choosing less-visited aliens (uncertainty bonus).
    - Anxiety curbs novelty seeking: higher anxiety reduces the exploration bonus.
    - Stage-1 is model-based using the task transition structure while propagating the exploration bonus.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 values
    - model_parameters[1] = beta (0 to 10): inverse temperature for both stages
    - model_parameters[2] = bonus0 (0 to 1): baseline magnitude of the exploration bonus
    - model_parameters[3] = anx_curb (0 to 1): strength by which anxiety reduces the bonus
        Effective bonus scale: b_eff = bonus0 * (1 - anx_curb * stai)
    - model_parameters[4] = decay_u (0 to 1): controls how fast uncertainty decays with visits

    Inputs:
    - action_1: array-like of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, chosen alien per trial
    - reward: array-like of floats
    - stai: array-like with one float in [0,1]
    - model_parameters: list/array of five parameters as specified above

    Returns:
    - Negative log-likelihood of the observed choices at both stages.
    """
    alpha, beta, bonus0, anx_curb, decay_u = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))          # stage-2 values
    visits = np.zeros((2, 2))      # visit counts for state-action pairs

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-curbed exploration bonus scale
    b_eff = bonus0 * (1.0 - anx_curb * stai_val)
    b_eff = max(0.0, b_eff)

    for t in range(n_trials):
        # Compute per-state uncertainty bonuses from visit counts
        # u = 1 / sqrt(1 + decay_u * N) ensures [0,1] and decreasing with visits and decay_u
        u_state = 1.0 / np.sqrt(1.0 + decay_u * visits)  # shape (2,2)
        bonus = b_eff * u_state

        # Stage-1 model-based values propagate the bonus-augmented stage-2 values
        max_q2_bonus = np.max(q2 + bonus, axis=1)        # shape (2,)
        q1_mb = T @ max_q2_bonus                         # shape (2,)

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy at reached state with exploration bonus
        s = int(state[t])
        q2_net = q2[s] + bonus[s]
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update stage-2 value
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update visit counts after observing the choice
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll