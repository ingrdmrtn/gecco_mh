def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB–MF with anxiety-modulated arbitration and eligibility trace from stage 2 to stage 1.
    
    This model learns model-free values at both stages with an eligibility trace that
    propagates second-stage value back to the first-stage action. The first-stage
    policy mixes a model-based (transition-structured) value with the MF value.
    The MB/MF mixing weight is modulated by the participant's anxiety (stai).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage state reached (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Trait anxiety score in [0,1]; we use stai[0].
    model_parameters : array-like
        [alpha2, beta, mix0, anx_mix, lam]
        - alpha2 in [0,1]: learning rate for stage-2 MF values and the stage-1 backprop update.
        - beta in [0,10]: softmax inverse temperature at both stages.
        - mix0 in [0,1]: baseline MB weight at stage 1 (when stai=0.5).
        - anx_mix in [0,1]: strength with which anxiety shifts MB weight (positive -> higher stai increases MB reliance).
        - lam in [0,1]: eligibility trace strength from stage 2 to stage 1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, mix0, anx_mix, lam = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure (common=0.7; A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)       # MF values for first-stage actions
    q2 = np.zeros((2, 2))     # MF values for second-stage actions by state

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated MB/MF mixing weight (kept in [0,1])
    # Center stai at 0.5 so mix0 is the midpoint baseline.
    omega = mix0 + anx_mix * (s - 0.5)
    omega = min(1.0, max(0.0, omega))

    for t in range(n_trials):
        st = state[t]

        # Model-based Q at stage 1 from transition expectations and current stage-2 values
        v2 = np.max(q2, axis=1)         # V(s) = max_a Q2(s,a)
        q1_mb = T @ v2                  # MB action values

        # Mixed policy at stage 1
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf
        pref1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_s = q2[st]
        pref2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD update
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha2 * pe2

        # Backpropagate with eligibility trace to stage 1 (MF)
        # Using the realized stage-2 action-value as the bootstrap target.
        target1 = q2[st, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha2 * lam) * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like first-stage evaluation with anxiety-modulated planning horizon.
    
    This model learns:
    - A first-stage state-occupancy predictor (a light-weight successor representation) M[a, s2]
      equal to the probability of landing in each second-stage state given first-stage action a.
      It is updated from observed transitions.
    - Second-stage MF action values Q2(s2, a2) from rewards.
    
    The first-stage action values are computed as a discounted expectation of second-stage
    state values under M. Anxiety modulates the effective planning horizon via a discount
    factor gamma_eff: higher gamma puts more weight on future value; anxiety can increase
    or decrease gamma depending on parameters.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices (X:0=W,1=S; Y:0=P,1=H).
    reward : array-like of float
        Rewards per trial (0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha_sr, beta, gamma0, gamma_anx]
        - alpha_sr in [0,1]: learning rate for both M and Q2.
        - beta in [0,10]: softmax inverse temperature for both stages.
        - gamma0 in [0,1]: baseline planning discount applied to first-stage values.
        - gamma_anx in [0,1]: how much anxiety shifts the discount (positive -> higher stai increases discount).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_sr, beta, gamma0, gamma_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize M with the known common/rare structure as a prior
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated planning discount
    gamma_eff = gamma0 + gamma_anx * s
    gamma_eff = min(1.0, max(0.0, gamma_eff))

    for t in range(n_trials):
        st = state[t]

        # Second-stage policy
        q2_s = Q2[st]
        pref2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # First-stage policy: discounted expected second-stage value under learned M
        V2 = np.max(Q2, axis=1)       # state values
        Q1 = gamma_eff * (M @ V2)     # discounted expectation
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning updates
        r = reward[t]

        # Update M row for chosen action toward the observed next state (one-hot)
        oh = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        M[a1] += alpha_sr * (oh - M[a1])

        # Update Q2 at the visited state/action
        pe2 = r - Q2[st, a2]
        Q2[st, a2] += alpha_sr * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Arbitrated MB–MF with learned transitions and anxiety-modulated surprise gating.
    
    This model learns:
    - Second-stage MF action values Q2(s2, a2) from reward.
    - First-stage MF values Q1_mf(a1) from bootstrapping onto realized second-stage values.
    - Action-specific transition models P(s2 | a1) from observed transitions.
    
    The first-stage decision value is a mixture of MB and MF components. The MB weight
    is dynamically reduced by transition surprise and by anxiety: on trials where the
    transition is surprising (relative to the learned P), the model relies more on MF.
    Higher anxiety further down-weights MB control.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the reached state.
    reward : array-like of float
        Received reward per trial (0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; uses stai[0].
    model_parameters : array-like
        [alpha_q, beta, kappa_surprise0, anx_gain]
        - alpha_q in [0,1]: learning rate for both Q2 and the transition model.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa_surprise0 in [0,1]: baseline trust in MB on low-surprise trials (higher -> more MB).
        - anx_gain in [0,1]: how much anxiety down-weights MB control (higher -> more MF with higher stai).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_q, beta, kappa_surprise0, anx_gain = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize transition model with the known common/rare prior
    P = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        st = state[t]

        # MB action values from current transition model and second-stage values
        V2 = np.max(Q2, axis=1)
        Q1_mb = P @ V2

        # Surprise w.r.t. the chosen action's predicted next state probabilities
        a1 = action_1[t]
        # Surprise as 1 - predicted probability of the observed state
        surpr = 1.0 - P[a1, st]

        # Anxiety- and surprise-modulated MB weight
        # Start from kappa_surprise0 and reduce with surprise and anxiety
        w_mb = kappa_surprise0 * (1.0 - surpr)
        w_mb *= (1.0 - anx_gain * s)
        w_mb = min(1.0, max(0.0, w_mb))

        # First-stage policy from arbitrated value
        Q1 = w_mb * Q1_mb + (1.0 - w_mb) * Q1_mf
        pref1 = beta * (Q1 - np.max(Q1))
        probs1 = np.exp(pref1) / np.sum(np.exp(pref1))
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        a2 = action_2[t]
        q2_s = Q2[st]
        pref2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(pref2) / np.sum(np.exp(pref2))
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Update transition model for chosen action toward the observed state
        oh = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        P[a1] += alpha_q * (oh - P[a1])

        # Update Q2
        pe2 = r - Q2[st, a2]
        Q2[st, a2] += alpha_q * pe2

        # Update Q1_mf by bootstrapping to realized second-stage value
        target1 = Q2[st, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll