def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MF/MB with an uncertainty-seeking exploration bonus attenuated by anxiety.
    
    Idea
    - Stage-2 values (Q2) are learned model-freely.
    - A running, leaky visit-count per state-action estimates uncertainty; fewer recent visits => higher uncertainty.
    - An intrinsic information bonus (added to Q2 for choice and propagated to stage-1 via planning) promotes exploring uncertain aliens.
      This bonus is attenuated by anxiety: higher STAI reduces the weight on uncertainty-driven exploration.
    - Stage-1 choices are a convex combination of MF Q1 and MB plan (via known transition structure), mixed by omega.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array of ints in {0,1}. First-stage choices (0=A, 1=U).
    - state: array of ints in {0,1}. Second-stage state reached (0=X, 1=Y).
    - action_2: array of ints in {0,1}. Second-stage choices (0=left alien on that planet, 1=right alien).
    - reward: array of floats in [0,1]. Coins received.
    - stai: array-like with single float in [0,1]. Anxiety score (higher = more anxious).
    - model_parameters: iterable of 5 floats
        alpha        — [0,1] learning rate for MF values
        beta         — [0,10] inverse temperature for both stages
        omega        — [0,1] mixing weight of model-based value at stage 1
        leak         — [0,1] leak for visit-counts (higher => faster forgetting, more sustained uncertainty)
        info_bonus   — [0,1] base weight of the uncertainty bonus (attenuated by anxiety)
    
    Returns
    - Negative log-likelihood of observed action_1 and action_2.
    """
    alpha, beta, omega, leak, info_bonus = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # shape (a1, state)

    # MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Leaky visit counts for uncertainty
    counts = np.zeros((2, 2))  # per (state, action)
    
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-attenuated information bonus weight
    # Higher anxiety -> weaker bonus
    bonus_w = info_bonus * (1.0 - stai_val)

    for t in range(n_trials):
        # Uncertainty from leaky counts: unc = 1 / sqrt(1 + counts)
        unc2 = 1.0 / np.sqrt(1.0 + counts)
        # Information bonus bounded roughly in (0,1]
        bonus2 = bonus_w * unc2

        # Model-based plan at stage 1 uses future best (Q2 + bonus)
        max_q2_bonus = np.max(q2 + bonus2, axis=1)  # per state
        q1_mb = T @ max_q2_bonus  # shape (2,)

        # Hybrid action values for stage 1
        q1 = (1.0 - omega) * q1_mf + omega * q1_mb

        # Softmax for stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in reached state with bonus
        s = state[t]
        q2_aug = q2[s] + bonus2[s]
        logits2 = beta * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update leaky visit counts (uncertainty estimator)
        counts = (1.0 - leak) * counts
        counts[s, a2] += 1.0

        # MF learning at stage 1 toward obtained second-stage value (without bonus in learning)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated model-based arbitration with anxiety-modulated sensitivity.
    
    Idea
    - The agent learns second-stage MF values (Q2) and a simple estimate of the transition matrix from experience.
    - On each trial, the degree of model-based control (w_mb) increases with the surprise of the observed transition
      (1 - current predicted probability to the reached state). Anxiety modulates sensitivity to surprise:
      higher anxiety down-weights surprise-based arbitration.
    - First-stage values are a hybrid of MF Q1 and MB plan using learned transitions and current Q2.
    - An eligibility trace propagates second-stage prediction errors back to Q1.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array of ints in {0,1}. First-stage choices.
    - state: array of ints in {0,1}. Reached second-stage state.
    - action_2: array of ints in {0,1}. Second-stage choices.
    - reward: array of floats in [0,1].
    - stai: array-like with single float in [0,1]. Anxiety score.
    - model_parameters: iterable of 5 floats
        alpha        — [0,1] learning rate for MF values (both stages)
        beta         — [0,10] inverse temperature
        phi_surprise — [0,1] base sensitivity of arbitration to surprise (also used as transition-learning rate scale)
        lambda_elig  — [0,1] eligibility for backing up stage-2 PE to stage 1
        anx_gate     — [0,1] strength by which anxiety reduces surprise sensitivity (effective sensitivity multiplied by (1 - anx_gate*stai))
    
    Returns
    - Negative log-likelihood of observed action_1 and action_2.
    """
    alpha, beta, phi_surprise, lambda_elig, anx_gate = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize MF values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Learnable transition model T_hat[a1, s]
    T_hat = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Effective surprise sensitivity after anxiety modulation
    sens = phi_surprise * (1.0 - anx_gate * stai_val)

    for t in range(n_trials):
        # MB plan from learned transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_hat @ max_q2

        # Compute surprise for current choice and reached state (before updating T_hat)
        a1 = action_1[t]
        s = state[t]
        pred_p = T_hat[a1, s]
        surprise = 1.0 - pred_p  # larger when transition was unlikely under current belief

        # Surprise-gated arbitration weight
        w_mb = np.clip(sens * surprise, 0.0, 1.0)

        # Hybrid action values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Softmax stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage 2 softmax
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # MF learning stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility-based update to Q1 MF using stage-2 PE
        q1_mf[a1] += lambda_elig * alpha * pe2

        # Update transition belief for chosen action using a scaled delta rule
        # T_hat[a1] moves toward a one-hot vector for reached state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T_hat[a1, s_idx] += phi_surprise * (target - T_hat[a1, s_idx])

        # Renormalize row to stay within simplex (guarding numerical drift)
        row_sum = T_hat[a1, 0] + T_hat[a1, 1]
        if row_sum > 0:
            T_hat[a1, :] /= row_sum

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Valence-asymmetric learning with an anxiety-modulated Pavlovian approach/avoid bias at stage 2.
    
    Idea
    - Stage-2 MF learning uses separate learning rates for positive vs negative prediction errors.
    - A Pavlovian approach bias favors a default alien within each state (index 0 in each planet), and is
      attenuated by anxiety (more anxious -> weaker approach, more neutral choices).
    - Stage-1 MF values are updated toward the obtained stage-2 value with the same valence-asymmetric rates.
    
    Parameters (bounds: alpha_* and biases in [0,1], beta in [0,10])
    - action_1: array of ints in {0,1}.
    - state: array of ints in {0,1}.
    - action_2: array of ints in {0,1}.
    - reward: array of floats in [0,1].
    - stai: array-like with single float in [0,1].
    - model_parameters: iterable of 5 floats
        alpha_pos   — [0,1] learning rate when PE >= 0
        alpha_neg   — [0,1] learning rate when PE < 0
        beta        — [0,10] inverse temperature
        pav_bias    — [0,1] base magnitude of approach bias toward default alien (index 0) at stage 2
        anx_valence — [0,1] how strongly anxiety attenuates the Pavlovian bias and enhances loss learning
    
    Returns
    - Negative log-likelihood of observed action_1 and action_2.
    """
    alpha_pos, alpha_neg, beta, pav_bias, anx_valence = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety modulation:
    # - attenuate approach bias: higher anxiety -> smaller effective Pavlovian bias
    pav_eff = pav_bias * (1.0 - anx_valence * stai_val)
    # - increase sensitivity to negative outcomes in learning by scaling alpha_neg
    alpha_neg_eff = np.clip(alpha_neg * (1.0 + anx_valence * stai_val), 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 policy (pure MF)
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with Pavlovian bias toward alien index 0
        s = state[t]
        pav_vec = np.array([+pav_eff, -pav_eff])  # approach 0, avoid 1 (state-independent sign)
        logits2 = beta * ((q2[s] + pav_vec) - np.max(q2[s] + pav_vec))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage 2 learning with valence asymmetry
        pe2 = r - q2[s, a2]
        if pe2 >= 0.0:
            q2[s, a2] += alpha_pos * pe2
            alpha1 = alpha_pos
        else:
            q2[s, a2] += alpha_neg_eff * pe2
            alpha1 = alpha_neg_eff

        # Stage 1 learning: move toward obtained stage-2 value with the same valence rate
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)