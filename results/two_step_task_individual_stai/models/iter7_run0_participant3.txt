def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Dual-rate valence learning with anxiety-tuned model-based weighting and value decay.

    Mechanism:
    - Stage-2 Q-values (Q2[s2,a2]) are updated with separate learning rates for positive vs.
      negative prediction errors (alpha_plus, alpha_minus).
    - Stage-1 action values are a hybrid of model-based (MB) and model-free (MF) values.
      The MB component uses a fixed transition structure (common=0.7) to project the best
      second-stage value from each action. The MF component is learned by backing up the
      reached Q2 value.
    - Anxiety (stai) increases reliance on MB control and accelerates value decay, modeling
      anxious discounting/forgetting of old outcomes.
    - A small value decay/forgetting pulls Q2 toward a neutral prior (0.5).

    Parameters and bounds:
    - model_parameters = (alpha_plus, alpha_minus, beta, w0, phi_decay)
        alpha_plus in [0,1]: learning rate for positive Q2 prediction errors
        alpha_minus in [0,1]: learning rate for negative Q2 prediction errors
        beta in [0,10]: inverse temperature for softmax policies (both stages)
        w0 in [0,1]: baseline MB weight at stage-1
        phi_decay in [0,1]: base decay strength; anxiety scales it upward

    Inputs:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; second-stage planet reached (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - Returns negative log-likelihood of the observed choices.
    """
    alpha_plus, alpha_minus, beta, w0, phi_decay = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed known transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float) + 0.5  # initialize neutral expectation
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12
    # Anxiety-tuned parameters
    # MB weight increases with anxiety; clip to [0,1]
    w_mb = float(np.clip(w0 + (s_anx - 0.5) * (0.8), 0.0, 1.0))
    # Value decay increases with anxiety
    decay = float(np.clip(phi_decay * (0.5 + 0.5 * s_anx), 0.0, 1.0))

    for t in range(n_trials):
        # Stage-1 policy: hybrid of MB and MF
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        logits1 = beta * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax over Q2 in reached state
        s2 = int(state[t])
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 learning with valence asymmetry
        pe2 = r - q2[s2, a2]
        lr2 = alpha_plus if pe2 >= 0.0 else alpha_minus
        q2[s2, a2] += lr2 * pe2

        # Value decay toward 0.5 (neutral) for all Q2 entries
        q2 = (1.0 - decay) * q2 + decay * 0.5

        # Stage-1 MF update by backing up the reached Q2 value (SARSA-style)
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        # Use mean of valence rates to update MF for stability; anxiety scales learning slightly
        lr1 = 0.5 * (alpha_plus + alpha_minus) * (0.75 + 0.25 * s_anx)
        q1_mf[a1] += lr1 * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transitions with confidence-weighted arbitration and anxiety-shaped exploration and stickiness.

    Mechanism:
    - The transition model T(a1->s2) is learned via a simple delta rule (eta_T).
    - Stage-2 values Q2(s2, a2) are learned with a single learning rate (eta_V = eta_T).
    - Stage-1 action values hybridize MB and MF values. The MB weight increases with
      transition confidence (1 - entropy(T)); confidence scaling starts at omega0.
    - Separate perseveration (stickiness) for stage-1 and stage-2 (k1, k2).
    - Anxiety reduces effective beta (more exploration) and attenuates stage-2 stickiness.

    Parameters and bounds:
    - model_parameters = (eta_T, beta, k1, k2, omega0)
        eta_T in [0,1]: learning rate for transitions and Q2 values
        beta in [0,10]: inverse temperature
        k1 in [0,1]: stage-1 perseveration strength
        k2 in [0,1]: stage-2 perseveration strength
        omega0 in [0,1]: base weight scaling confidence into MB arbitration

    Inputs:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - Returns negative log-likelihood of observed choices.
    """
    eta_T, beta, k1, k2, omega0 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize transition model to uniform uncertainty (0.5/0.5)
    T = np.ones((2, 2), dtype=float) * 0.5
    q2 = np.zeros((2, 2), dtype=float)
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = -1
    prev_a2 = -1

    eps = 1e-12

    for t in range(n_trials):
        # Transition confidence via normalized entropy
        ent = -np.sum(T * (np.log(T + eps)), axis=1) / np.log(2 + eps)  # in [0,1]
        conf = 1.0 - 0.5 * (ent[0] + ent[1])  # average confidence in [0,1]

        # Arbitration weight increases with confidence; anxiety amplifies this effect
        omega = omega0 * (1.0 + 0.5 * s_anx * conf)
        omega = float(np.clip(omega, 0.0, 1.0))

        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stickiness vectors
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Anxiety reduces effective beta (more exploration)
        beta_eff1 = beta * (1.0 - 0.4 * s_anx)
        logits1 = beta_eff1 * q1 + k1 * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])

        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0

        # Anxiety attenuates stage-2 stickiness
        k2_eff = k2 * (1.0 - 0.6 * s_anx)
        beta_eff2 = beta * (1.0 - 0.3 * s_anx)
        logits2 = beta_eff2 * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Transition learning: delta rule toward observed outcome
        # Move probability mass for chosen action toward the observed state
        for s_alt in (0, 1):
            target = 1.0 if s_alt == s2 else 0.0
            T[a1, s_alt] += eta_T * (target - T[a1, s_alt])
        # Renormalize to protect against drift
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Value learning at stage-2
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += eta_T * pe2

        # Model-free backup to stage-1
        target1 = q2[s2, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_T * pe1

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Utility-transformed rewards with anxiety-shaped sensitivity and surprise-seeking bias.

    Mechanism:
    - Stage-2 learning uses a utility-transformed reward u(r) that flattens or sharpens the
      impact of outcomes depending on risk/sensitivity parameter and anxiety.
    - Stage-1 values combine model-based projections with a surprise-seeking bonus assigned
      to actions that recently produced rare transitions (relative to a fixed transition).
    - A static choice bias toward spaceship A (bias_side) operates at stage-1 and extends
      to the lower stage (alien 0) to capture idiosyncratic preferences; anxiety modulates
      the expression of this bias.

    Parameters and bounds:
    - model_parameters = (alpha, beta, theta_sense, psi_surprise, bias_side)
        alpha in [0,1]: learning rate for Q2
        beta in [0,10]: inverse temperature
        theta_sense in [0,1]: baseline reward sensitivity shaping; higher -> flatter utility
        psi_surprise in [0,1]: weight of surprise-seeking bonus at stage-1
        bias_side in [0,1]: baseline bias toward action index 0 at both stages

    Inputs:
    - action_1: int array (n_trials,) in {0,1}
    - state:    int array (n_trials,) in {0,1}
    - action_2: int array (n_trials,) in {0,1}
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]
    - Returns negative log-likelihood of the observed choices.
    """
    alpha, beta, theta_sense, psi_surprise, bias_side = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed transition structure to evaluate surprise
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    # Keep track of whether last observed transition was rare, by action
    last_surprise = np.zeros(2, dtype=float)  # action-indexed bonus memory

    for t in range(n_trials):
        # Compute MB projection using fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Surprise-seeking bonus at stage-1, decays implicitly by overwrite
        # Anxiety amplifies the weight of surprise seeking
        surprise_weight = psi_surprise * (0.5 + 0.5 * s_anx)
        bonus1 = surprise_weight * last_surprise.copy()

        # Static side bias toward action 0, scaled by anxiety expression
        side_bias1 = np.array([1.0, 0.0]) * (2.0 * bias_side - 1.0) * (0.5 + 0.5 * s_anx)

        logits1 = beta * (q1_mb + bonus1) + side_bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        s2 = int(state[t])

        # Stage-2 policy with analogous side bias toward alien 0
        side_bias2 = np.array([1.0, 0.0]) * (2.0 * bias_side - 1.0) * (0.4 + 0.6 * s_anx)
        logits2 = beta * q2[s2] + side_bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Utility transformation of reward: compress with theta_sense and anxiety
        # Higher anxiety + higher theta_sense -> more compression (diminished sensitivity)
        gamma = 1.0 - 0.6 * theta_sense * (0.5 + 0.5 * s_anx)  # in (0.4,1]
        u = r**gamma  # preserves ordering; compresses when gamma<1

        # Stage-2 learning on utility
        pe2 = u - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Update surprise memory: 1 if rare transition occurred, else 0
        # Rare if observed state is the low-prob branch for chosen action
        prob_common = T_fixed[a1, s2]
        is_rare = 1.0 if prob_common < 0.5 else 0.0
        last_surprise = np.zeros(2, dtype=float)
        last_surprise[a1] = is_rare

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)