def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated pessimistic planning with repetition bias.

    Overview
    - Second-stage values are learned model-free.
    - First-stage uses a mixture of model-based (MB) and model-free (MF) values.
    - Anxiety increases pessimism in planning by applying a concave utility transform
      to second-stage values when computing MB values.
    - Anxiety also scales a repetition bias that favors repeating the previous first-stage action.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U), length n_trials
    - state: array-like of ints in {0,1}, second-stage state reached (0=X, 1=Y), length n_trials
    - action_2: array-like of ints in {0,1}, second-stage actions, length n_trials
    - reward: array-like of floats in [0,1], obtained reward, length n_trials
    - stai: array-like length-1 with scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: second-stage learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        phi_risk: anxiety coupling for pessimistic (concave) utility in [0,1]
        w_mb0: baseline model-based weight at stage 1 in [0,1]
        bias_repeat: baseline repetition bias magnitude in [0,1]

    Bounds
    - alpha2, phi_risk, w_mb0, bias_repeat in [0,1]
    - beta in [0,10]

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, phi_risk, w_mb0, bias_repeat = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)        # model-free first-stage action values
    q2 = np.zeros((2, 2))      # second-stage action values for states X=0, Y=1

    # Likelihood storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated pessimistic utility exponent for MB planning
    # Higher stai -> smaller gamma -> more concave utility -> pessimism
    gamma = 1.0 - 0.5 * phi_risk * stai
    gamma = max(0.2, min(1.0, gamma))  # keep a sensible range

    # Anxiety reduces reliance on MB planning
    w_mb = w_mb0 * (1.0 - 0.4 * stai)
    w_mb = min(1.0, max(0.0, w_mb))

    # Repetition bias scaled by anxiety (stronger with higher stai)
    rep_scale = (0.5 + 0.5 * stai)  # in [0.5, 1]
    last_a1 = None

    for t in range(n_trials):
        # Compute MB stage-1 values using pessimistic utility on q2
        max_q2 = np.max(q2, axis=1)
        # Map values to a "utility" space for planning (concave transform)
        max_q2_u = np.power(np.clip(max_q2, 0.0, 1.0), gamma)
        q1_mb = T @ max_q2_u

        # Combine MB and MF for decision values at stage 1
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Add repetition bias (to previously chosen first-stage action)
        if last_a1 is not None:
            bias_vec = np.zeros(2)
            bias_vec[last_a1] = bias_repeat * rep_scale
            q1_dec = q1_dec + bias_vec

        # Softmax for stage 1
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # MF bootstrapping for stage 1 toward realized second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1  # same alpha for simplicity and parsimony

        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-sensitive transition learning with rare-switch bias.

    Overview
    - Learns the transition model T_est online from experience.
    - Anxiety increases transition learning "instability" (faster adaptation),
      making planning more sensitive to recent transitions.
    - First-stage decisions blend model-based values computed via T_est with MF values.
    - Anxiety also increases a bias to switch first-stage choices after rare transitions
      and/or poor outcomes.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U), length n_trials
    - state: array-like of ints in {0,1}, second-stage state (0=X, 1=Y), length n_trials
    - action_2: array-like of ints in {0,1}, second-stage actions, length n_trials
    - reward: array-like of floats in [0,1], obtained reward, length n_trials
    - stai: array-like length-1 with scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: second-stage learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        zeta_Tinst: scales how much anxiety increases transition learning rate in [0,1]
        w_plan0: baseline planning weight in [0,1]
        xi_switch: baseline rare/outcome-driven switch bias in [0,1]

    Bounds
    - alpha2, zeta_Tinst, w_plan0, xi_switch in [0,1]
    - beta in [0,10]

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, zeta_Tinst, w_plan0, xi_switch = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize estimated transitions near canonical structure
    T_est = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Value functions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated transition learning rate (instability)
    alpha_T = min(1.0, max(0.0, 0.1 + 0.7 * zeta_Tinst * stai))
    # Anxiety-modulated planning weight (can increase with anxiety)
    w_plan = min(1.0, max(0.0, w_plan0 * (1.0 + 0.3 * stai)))

    # For rare detection relative to canonical structure (A->X, U->Y common)
    def is_rare(a, s):
        return (a == 0 and s == 1) or (a == 1 and s == 0)

    last_a1 = None
    last_rare = 0.0
    last_bad = 0.0

    for t in range(n_trials):
        # Compute MB values using current T_est
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_est @ max_q2

        # Switch bias based on last trial's circumstances, scaled by anxiety
        # Bias pushes away from last action when last trial was rare or bad
        bias_vec = np.zeros(2)
        if last_a1 is not None:
            switch_drive = (last_rare + last_bad) * xi_switch * stai
            bias_vec[1 - last_a1] += switch_drive

        # Combine MB and MF values
        q1_dec = w_plan * q1_mb + (1.0 - w_plan) * q1_mf + bias_vec

        # Softmax stage 1
        q1c = q1_dec - np.max(q1_dec)
        probs1 = np.exp(beta * q1c)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcomes
        r = reward[t]

        # Learn transitions for the chosen first-stage action toward observed state
        onehot = np.array([1.0 if i == s else 0.0 for i in range(2)])
        T_est[a1, :] = T_est[a1, :] + alpha_T * (onehot - T_est[a1, :])
        # Keep each row normalized (should be already, but clip for numerical stability)
        T_est[a1, :] = np.clip(T_est[a1, :], 1e-6, 1.0)
        T_est[a1, :] /= np.sum(T_est[a1, :])

        # Learn second-stage MF values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # Stage-1 MF bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update rare/bad markers for next-trial switching bias
        last_rare = 1.0 if is_rare(a1, s) else 0.0
        last_bad = 1.0 if r < 0.5 else 0.0
        last_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Confidence-trace arbitration with anxiety-modulated volatility and lapses.

    Overview
    - Second-stage values are learned model-free; first-stage blends MB and MF values.
    - Arbitration weight for MB at stage 1 is a leaky running estimate of second-stage
      choice confidence from the previous trial(s).
    - Anxiety increases the leak rate (more volatile arbitration, tracking recent confidence).
    - Anxiety also increases lapse probability (random choice mixture at both stages).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U), length n_trials
    - state: array-like of ints in {0,1}, second-stage state (0=X, 1=Y), length n_trials
    - action_2: array-like of ints in {0,1}, second-stage actions, length n_trials
    - reward: array-like of floats in [0,1], obtained reward, length n_trials
    - stai: array-like length-1 with scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha2: second-stage learning rate in [0,1]
        beta: inverse temperature for both stages in [0,10]
        theta_conf0: baseline arbitration strength in [0,1] (scales the confidence trace)
        gamma_leak: anxiety coupling for confidence leak rate in [0,1]
        epsilon: baseline lapse rate in [0,1] (anxiety scales it upward)

    Bounds
    - alpha2, theta_conf0, gamma_leak, epsilon in [0,1]
    - beta in [0,10]

    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha2, beta, theta_conf0, gamma_leak, epsilon = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix for MB planning
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Confidence trace and anxiety-modulated leak
    # Higher stai -> higher effective leak -> more weight on recent confidence
    leak_eff = min(1.0, max(0.0, 0.2 + 0.6 * gamma_leak * stai))
    C = 0.5  # initial confidence trace (uninformative mid-point)

    # Anxiety-modulated lapse
    eps_lapse = min(0.5, max(0.0, epsilon * stai))

    for t in range(n_trials):
        # Compute MB first-stage values (using last known q2)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Arbitration weight from confidence trace
        w_mb = min(1.0, max(0.0, theta_conf0 * C))

        # Decision values and softmax for stage 1, with lapse
        q1_dec = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        q1c = q1_dec - np.max(q1_dec)
        probs1_soft = np.exp(beta * q1c)
        probs1_soft /= np.sum(probs1_soft)
        probs1 = (1.0 - eps_lapse) * probs1_soft + eps_lapse * 0.5

        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (softmax with lapse)
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2_soft = np.exp(beta * q2c)
        probs2_soft /= np.sum(probs2_soft)
        probs2 = (1.0 - eps_lapse) * probs2_soft + eps_lapse * 0.5

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2 * pe2

        # MF bootstrapping at stage 1
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha2 * pe1

        # Update confidence trace based on observed stage-2 choice certainty this trial
        # Confidence measure: c_t in [0,1], higher when one action dominates
        c_t = 2.0 * np.max(probs2_soft) - 1.0  # in [0,1]
        C = (1.0 - leak_eff) * C + leak_eff * c_t

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll