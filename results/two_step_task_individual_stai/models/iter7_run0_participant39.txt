def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-gated hybrid control with anxiety-modulated stickiness at both stages.

    The agent learns second-stage values model-free and uses a fixed transition
    model to compute model-based first-stage values. The weight on model-based
    control is dynamically increased by transition surprise (rare transitions),
    with a gain parameter scaled by anxiety. Perseveration (choice stickiness)
    is present at both stages and is modulated by anxiety in opposite directions:
    anxiety increases first-stage perseveration and decreases second-stage
    perseveration.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        alpha: [0,1] learning rate for stage-2 MF values and stage-1 MF bootstrapping
        beta:  [0,10] inverse temperature for softmax at both stages
        rho_surp0: [0,1] gain for surprise-gated increase in planning weight
        kappa_rep0: [0,1] strength of first-stage perseveration (repeat last a1)
        zeta_pers2: [0,1] strength of second-stage perseveration (repeat last a2 in a state)

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, rho_surp0, kappa_rep0, zeta_pers2 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition structure: rows = actions (A,U), cols = states (X,Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2_mf = np.zeros((2, 2))    # second-stage MF values (state x action)

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Dynamic planning weight; baseline from anxiety (lower with higher anxiety)
    # and trial-wise modulation by surprise.
    # Initialize with baseline before any surprise is observed.
    w_mb = max(0.0, min(1.0, 1.0 - stai0))

    # Stickiness trackers
    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    eps = 1e-12
    for t in range(n_trials):

        # Compute MB first-stage values from current second-stage values
        max_q2 = np.max(q2_mf, axis=1)           # best alien per planet
        q1_mb = T @ max_q2                       # expected value per spaceship

        # Combine MF and MB with current weight
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage stickiness (anxiety increases perseveration)
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            kappa_eff = kappa_rep0 * (1.0 + stai0)
            q1_hybrid = q1_hybrid + kappa_eff * stick

        # First-stage policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy with stickiness in the reached state
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        prev_a2 = prev_a2_by_state[s2]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0
            # Anxiety reduces second-stage perseveration (enhanced flexibility under pressure)
            zeta_eff = zeta_pers2 * (1.0 - 0.5 * stai0)
            q2 = q2 + zeta_eff * stick2

        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning at stage-2 (MF)
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Learning at stage-1 (MF bootstrapping toward the obtained second-stage value)
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update dynamic planning weight by transition surprise
        # Surprise = 1 - P(observed state | chosen action)
        p_trans = T[a1, s2]
        surprise = 1.0 - p_trans
        # Anxiety increases sensitivity to surprise (greater shift to MB after rare transitions)
        w_mb = w_mb + rho_surp0 * (1.0 + stai0) * (surprise - (w_mb - (1.0 - stai0)))
        w_mb = max(0.0, min(1.0, w_mb))

        # Update stickiness memory
        prev_a1 = a1
        prev_a2_by_state[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-learning model with anxiety-modulated information bonus and lapse.

    The agent learns both second-stage reward values and first-stage transition
    probabilities online. First-stage choice is model-based using the learned
    transition model, augmented by an information-seeking bonus that favors
    visiting states with more uncertain reward options. Anxiety increases the
    information bonus and lapse (choice noise).

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        lr_val:   [0,1] learning rate for second-stage reward values
        lr_tran:  [0,1] learning rate for transitions P(state | action1)
        beta:     [0,10] inverse temperature for both stages
        inv_u0:   [0,1] baseline weight for information (uncertainty) bonus at stage 1
        lapse0:   [0,1] baseline lapse probability for both stages

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_val, lr_tran, beta, inv_u0, lapse0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transitions T[a, s], start from weak prior (0.5,0.5)
    T = np.ones((2, 2)) * 0.5

    # Second-stage MF values
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    eps = 1e-12
    for t in range(n_trials):
        # Information bonus per first-stage action:
        # We approximate state uncertainty using reward-value dispersion in each state.
        # Compute per-state uncertainty as variance of q2 within state around 0.5
        # (larger when aliens are similar/unknown).
        state_unc = np.zeros(2)
        for s in range(2):
            m = 0.5 * (q2[s, 0] + q2[s, 1])
            v = 0.5 * ((q2[s, 0] - m) ** 2 + (q2[s, 1] - m) ** 2)
            state_unc[s] = v

        # MB value from learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien per state
        q1_mb = T @ max_q2

        # Info bonus: expected state uncertainty under each action
        info_bonus = T @ state_unc

        # Anxiety increases info-seeking and lapse
        inv_u_eff = inv_u0 * (1.0 + stai0)
        lapse_eff = min(1.0, lapse0 * (1.0 + 0.5 * stai0))

        q1_total = q1_mb + inv_u_eff * info_bonus

        # First-stage policy with softmax-lapse mixture
        q1c = q1_total - np.max(q1_total)
        sm = np.exp(beta * q1c)
        sm = sm / (np.sum(sm) + eps)
        probs_1 = (1.0 - lapse_eff) * sm + lapse_eff * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (no info bonus), same lapse mechanism
        s2 = state[t]
        q2c = q2[s2] - np.max(q2[s2])
        sm2 = np.exp(beta * q2c)
        sm2 = sm2 / (np.sum(sm2) + eps)
        probs_2 = (1.0 - lapse_eff) * sm2 + lapse_eff * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learn second-stage rewards
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += lr_val * pe2

        # Learn transitions toward the observed state for the chosen action
        # One-hot target for observed state
        for s in range(2):
            target = 1.0 if s == s2 else 0.0
            T[a1, s] += lr_tran * (target - T[a1, s])

        # Renormalize to keep rows stochastic (for numerical stability)
        row_sum = T[a1, 0] + T[a1, 1]
        if row_sum > 0:
            T[a1, 0] /= row_sum
            T[a1, 1] /= row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-gated learning with forgetting, rare-transition switch bias, and hybrid control.

    Second-stage values are learned with a PE-dependent learning rate that
    increases under anxiety (trait-sensitive volatility gain). Unchosen second-stage
    actions decay toward a neutral value. First-stage choice uses a hybrid of
    model-based (fixed transition model) and model-free values. If the previous
    transition was rare, a switch bias is applied at the first stage; this bias
    is amplified by anxiety.

    Parameters
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage states (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array (1,) or (n_trials,), anxiety trait score in [0,1]
    - model_parameters: iterable of 5 parameters, all used
        eta_base:  [0,1] base learning rate for second-stage values
        beta:      [0,10] inverse temperature for both stages
        chi_decay: [0,1] decay rate of unchosen second-stage actions toward 0.5
        psi_rare:  [0,1] bias magnitude to switch after rare transition at stage 1
        xi_mb:     [0,1] baseline weight on model-based control in first-stage choice

    Returns
    - Negative log-likelihood of observed choices.
    """
    eta_base, beta, chi_decay, psi_rare, xi_mb = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous trial info for rare-transition-induced switching bias
    prev_a1 = None
    prev_rare = False

    eps = 1e-12
    for t in range(n_trials):
        # Compute MB first-stage values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid weight modulated by anxiety (anxiety slightly reduces MB influence)
        w_mb = np.clip(xi_mb * (1.0 - 0.5 * stai0), 0.0, 1.0)
        q1_hyb = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Rare-transition switch bias: if previous transition was rare, bias away from last a1
        if prev_a1 is not None and prev_rare:
            bias = np.zeros(2)
            # Encourage switching by penalizing repeating prev_a1 and favoring the other option
            bias[prev_a1] -= 1.0
            bias[1 - prev_a1] += 1.0
            psi_eff = psi_rare * (1.0 + stai0)  # anxiety amplifies the rare-switch tendency
            q1_hyb = q1_hyb + psi_eff * bias

        # First-stage policy
        q1c = q1_hyb - np.max(q1_hyb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy
        s2 = state[t]
        q2c = q2[s2] - np.max(q2[s2])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # PE-dependent learning rate at stage 2, scaled by anxiety
        pe2 = r - q2[s2, a2]
        eta = np.clip(eta_base * (1.0 + stai0 * abs(pe2)), 0.0, 1.0)
        q2[s2, a2] += eta * pe2

        # Forgetting (decay) for the unchosen action in the visited state toward 0.5
        unchosen_a2 = 1 - a2
        q2[s2, unchosen_a2] += chi_decay * (1.0 + stai0) * (0.5 - q2[s2, unchosen_a2])
        # Optional mild cross-state decay to stabilize values (no anxiety mod here)
        other_state = 1 - s2
        q2[other_state, :] += chi_decay * 0.0 * (0.5 - q2[other_state, :])  # zeroed but uses chi_decay param meaningfully above

        # Update first-stage MF toward obtained second-stage value (bootstrap)
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += eta_base * pe1  # use base rate for MF stage-1 update

        # Determine if current transition was rare to set next-trial bias
        p_trans = T[a1, s2]
        prev_rare = (p_trans < 0.5)
        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll