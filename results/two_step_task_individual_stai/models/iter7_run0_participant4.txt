def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-biased transition learning with model-based planning and stay bias.

    Overview
    - Learns state-2 action values via Rescorlaâ€“Wagner (model-free).
    - Learns state-transition probabilities P(state | action) with learning rate alphaT.
    - First-stage action values are model-based: Q1_MB = P_effective @ max_a2 Q2(state,a2).
    - Anxiety biases transition use toward a prior that favors the common transitions,
      blending the learned transition matrix with the fixed prior as a function of stai and zeta_anx.
    - Includes a stay/perseveration bias at both stages.

    Parameters (all used)
    - alphaQ:     [0,1]   Learning rate for Q updates at stage 2 and for credit assignment to stage 1.
    - beta:       [0,10]  Inverse temperature for softmax at both stages.
    - alphaT:     [0,1]   Learning rate for transition probability updates.
    - zeta_anx:   [0,1]   Weight of anxiety-driven pull toward the prior transition matrix.
    - rho_stay:   [0,1]   Additive stay bias applied to logits for repeating the last action.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alphaQ, beta, alphaT, zeta_anx, rho_stay].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alphaQ, beta, alphaT, zeta_anx, rho_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed prior transitions reflecting task structure (common 0.7)
    prior_T = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Initialize learned transitions to uniform to allow learning
    T = np.ones((2, 2), dtype=float) / 2.0

    # Stage-2 Q-values and stage-1 MF trace (credit assigned via PE2)
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    # For stay bias
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-biased effective transitions: blend learned T with prior
        w_prior = np.clip(zeta_anx * stai, 0.0, 1.0)
        T_eff = (1.0 - w_prior) * T + w_prior * prior_T

        # Model-based first-stage values using current Q2
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_eff @ max_q2

        # Combine MB and MF at stage 1 by simple sum (both on same scale)
        q1 = q1_mb + q1_mf

        # Stay bias logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_stay
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_stay

        # Stage-1 choice probability
        logits1 = beta * (q1 - np.max(q1)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probability
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning: Stage-2 RW
        pe2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * pe2

        # Credit assignment to stage-1 MF via stage-2 PE
        q1_mf[a1] += alphaQ * pe2

        # Transition learning: update row for chosen action toward observed state
        # One-hot target for observed transition
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] = (1.0 - alphaT) * T[a1] + alphaT * target

        # Renormalize to ensure valid probabilities
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive eligibility-trace model with anxiety-modulated loss aversion and stay bias.

    Overview
    - Stage-2 values via RW but with risk-sensitive utility: outcomes are down-weighted when local
      outcome variance is high; anxiety amplifies this aversion.
    - Tracks per-state running variance proxy of rewards to compute utility scaling.
    - Eligibility-trace style credit assignment from stage-2 PE to stage-1 MF value.
    - Stay bias at both stages.

    Parameters (all used)
    - alpha:      [0,1]   Learning rate for Q updates and variance tracking.
    - beta:       [0,10]  Inverse temperature.
    - eta_trace:  [0,1]   Strength of eligibility credit from stage 2 back to stage 1.
    - nu_risk:    [0,1]   Baseline risk sensitivity; amplified by anxiety.
    - rho_rep:    [0,1]   Perseveration strength (stay bias) at both stages.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, eta_trace, nu_risk, rho_rep].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, eta_trace, nu_risk, rho_rep = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q2 = np.zeros((2, 2)) + 0.5
    q1_mf = np.zeros(2)

    # Running statistics per state for risk estimation: mean and mean absolute deviation proxy
    m_state = np.zeros(2) + 0.5
    d_state = np.zeros(2) + 0.1  # deviation proxy (|r - m| averaged)

    # For stay bias
    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Compute risk-adjusted utility based on local deviation and anxiety
        # Effective risk weight increases with anxiety
        risk_w = nu_risk * (1.0 + stai)
        # Utility: down-weight reward toward its local mean depending on deviation magnitude
        # u = r - risk_w * d_state[s] * (r - m_state[s])
        # This pulls outcomes toward mean; for high variance and high anxiety, utility is more conservative.
        u = r - risk_w * d_state[s] * (r - m_state[s])

        # Stay biases
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] += rho_rep
        bias2 = np.zeros(2)
        if prev_a2[s] is not None:
            bias2[prev_a2[s]] += rho_rep

        # Stage-1 softmax over MF q1
        q1 = q1_mf.copy()
        logits1 = beta * (q1 - np.max(q1)) + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s)) + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Stage-2 PE using utility
        pe2 = u - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility-trace style credit to stage-1
        q1_mf[a1] += eta_trace * pe2

        # Update running mean and deviation for risk estimation in this state
        m_state[s] = (1.0 - alpha) * m_state[s] + alpha * r
        d_state[s] = (1.0 - alpha) * d_state[s] + alpha * abs(r - m_state[s])

        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Information-bonus model-based planner with anxiety-dependent lapse and volatility tracking.

    Overview
    - Stage-2 values via RW.
    - Tracks per-state outcome volatility (running abs prediction error of rewards).
    - Adds a directed information bonus at stage-2 state values; stage-1 model-based values
      plan through the fixed transition matrix to prefer actions leading to volatile states.
    - Lapse probability at both stages increases with anxiety (softens choice independently of values).

    Parameters (all used)
    - alpha:        [0,1]   Learning rate for Q updates and volatility tracking.
    - beta:         [0,10]  Inverse temperature for softmax.
    - kappa_info:   [0,1]   Weight of information bonus added to state values.
    - alpha_vol:    [0,1]   Learning rate for volatility (surprise) tracking.
    - xi_lapse_anx: [0,1]   Coefficient controlling anxiety-dependent lapse rate.

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, kappa_info, alpha_vol, xi_lapse_anx].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, kappa_info, alpha_vol, xi_lapse_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition matrix (common 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and volatility trackers
    q2 = np.zeros((2, 2)) + 0.5
    vol = np.zeros(2) + 0.05  # per-state volatility proxy

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Information bonus added to state values (state-level, not action-level)
        v_state = np.max(q2, axis=1) + kappa_info * (1.0 + stai) * vol

        # Stage-1 model-based values by planning through T
        q1 = T @ v_state

        # Anxiety-dependent lapse probabilities
        p_lapse = np.clip(xi_lapse_anx * stai, 0.0, 0.5)  # cap lapse to keep informative choices
        uniform1 = np.array([0.5, 0.5])
        uniform2 = np.array([0.5, 0.5])

        # Stage-1 choice probabilities
        logits1 = beta * (q1 - np.max(q1))
        logits1 = logits1 - np.max(logits1)
        soft1 = np.exp(logits1) / np.sum(np.exp(logits1))
        probs1 = (1.0 - p_lapse) * soft1 + p_lapse * uniform1
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice probabilities
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        logits2 = logits2 - np.max(logits2)
        soft2 = np.exp(logits2) / np.sum(np.exp(logits2))
        probs2 = (1.0 - p_lapse) * soft2 + p_lapse * uniform2
        p_choice_2[t] = probs2[a2]

        # Learning at stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Volatility update from absolute PE (separate rate alpha_vol)
        vol[s] = (1.0 - alpha_vol) * vol[s] + alpha_vol * abs(pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)