def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive model-free SARSA(λ) with anxiety-weighted loss aversion and lapse.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on visited planet).
    reward : array-like of float
        Obtained reward on each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, lambda_e, rho_risk, epsilon)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - lambda_e in [0,1]: eligibility trace from stage-2 TD error onto stage-1 action value.
        - rho_risk in [0,1]: risk sensitivity coefficient; higher increases loss aversion.
        - epsilon in [0,1]: base lapse rate (random choice), amplified by anxiety.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Pure model-free control. Stage-1 values are learned by bootstrapping from stage-2 via an eligibility trace.
    - Anxiety increases loss aversion for negative outcomes and increases lapse.
    - Utility transform: for r >= 0, u = r; for r < 0, u = (1 + rho_risk * stai) * r (amplified losses).
    - Lapse effective rate: eps_eff = min(0.5, epsilon * (0.5 + 0.5 * stai)).
    """
    alpha2, beta, lambda_e, rho_risk, epsilon = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Probabilities recorded for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Model-free Q-values
    Q1 = np.zeros(2)        # first-stage actions
    Q2 = np.zeros((2, 2))   # second-stage (state, action)

    # Anxiety-modulated lapse (capped to avoid degenerate likelihoods)
    eps_eff = min(0.5, max(0.0, epsilon * (0.5 + 0.5 * stai_val)))

    for t in range(n_trials):

        # Stage-1 policy (softmax over model-free Q1)
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)

        # Apply lapse mixture
        probs1 = (1.0 - eps_eff) * probs1 + eps_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over state-conditional Q2)
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)

        # Apply lapse at stage-2
        probs2 = (1.0 - eps_eff) * probs2 + eps_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Risk-sensitive utility shaped by anxiety (amplify losses)
        r = float(reward[t])
        if r >= 0.0:
            u = r
        else:
            u = (1.0 + rho_risk * stai_val) * r

        # TD updates
        # Stage-2 update
        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Stage-1 update with eligibility trace from stage-2 TD error
        Q1[a1] += alpha2 * lambda_e * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with learned transitions and anxiety-damped directed exploration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, tau_T, bonus0, omega0)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta in [0,10]: inverse temperature for softmax.
        - tau_T in [0,1]: learning rate for updating the transition matrix and uncertainty.
        - bonus0 in [0,1]: scale of directed exploration bonus at stage-2.
        - omega0 in [0,1]: baseline arbitration weight for model-based control at stage-1.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Learns transition matrix T online; stage-1 values are a mixture of MB and MF:
        Q1 = w_eff * (T @ max_a Q2[s,a]) + (1 - w_eff) * Q1_MF
      where w_eff = omega0 * (1 - stai).
    - Directed exploration at stage-2: add a bonus proportional to recency-weighted uncertainty,
      dampened by anxiety: bonus = bonus0 * (1 - stai) * sqrt(V[s,a]).
    - Uncertainty V is tracked via a delta-squared update: V <- (1 - tau_T)*V + tau_T*delta2^2.
    """
    alpha2, beta, tau_T, bonus0, omega0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Probabilities recorded for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize learned transition matrix T (rows = actions A/U, cols = states X/Y)
    T = np.full((2, 2), 0.5)

    # Value functions
    Q1_MF = np.zeros(2)      # model-free cached value at stage-1
    Q2 = np.zeros((2, 2))    # second-stage values per state and action

    # Uncertainty tracker for directed exploration at stage-2
    V2 = np.zeros((2, 2))

    # Effective arbitration weight reduced by anxiety
    w_eff_base = omega0 * (1.0 - stai_val)
    w_eff_base = max(0.0, min(1.0, w_eff_base))

    for t in range(n_trials):

        # Model-based evaluation of stage-1
        max_Q2 = np.max(Q2, axis=1)                # per state
        Q1_MB = T @ max_Q2                         # action -> expected value

        # Combine MB and MF with anxiety-damped weight
        Q1 = w_eff_base * Q1_MB + (1.0 - w_eff_base) * Q1_MF

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with directed exploration bonus
        s2 = int(state[t])
        a2 = int(action_2[t])

        # Anxiety-damped directed exploration
        bonus_scale = bonus0 * (1.0 - stai_val)
        bonus_vec = bonus_scale * np.sqrt(np.maximum(V2[s2], 0.0))
        logits2 = beta * (Q2[s2] + bonus_vec)
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = float(reward[t])

        # Update transitions for the chosen first-stage action
        if tau_T > 0.0:
            # Move the chosen row toward a one-hot on the observed state
            T[a1, :] = (1.0 - tau_T) * T[a1, :]
            T[a1, s2] += tau_T
            # Renormalize row
            row_sum = np.sum(T[a1, :])
            if row_sum > 0.0:
                T[a1, :] /= row_sum

        # Stage-2 value update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Update uncertainty estimate for directed exploration
        V2[s2, a2] = (1.0 - tau_T) * V2[s2, a2] + tau_T * (delta2 ** 2)

        # Model-free stage-1 update via bootstrapping from stage-2
        # Use the current greedy value as a bootstrap target
        Q1_MF[a1] += alpha2 * (Q2[s2, a2] - Q1_MF[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Effort-avoidant planner: anxiety- and surprise-gated inverse temperature, with forgetting.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien index on visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta0, phi, rep, decay)
        - alpha2 in [0,1]: learning rate for second-stage Q-values.
        - beta0 in [0,10]: baseline inverse temperature for softmax (both stages).
        - phi in [0,1]: sensitivity of choice stochasticity to surprise × anxiety.
        - rep in [0,1]: repetition bias for stage-1 (tendency to repeat previous spaceship).
        - decay in [0,1]: forgetting rate applied to all second-stage Q-values each trial.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Stage-1 policy is fully model-based with known transitions (A→X, U→Y are common).
    - Surprise is high on rare transitions; beta_eff = beta0 * (1 - phi * surprise * stai).
      Thus, anxiety increases the impact of surprise on reducing choice precision (effort avoidance).
    - A small repetition bias is added to the previously chosen stage-1 action, attenuated by anxiety.
    - Second-stage values undergo anxious forgetting: Q2 ← (1 - decay_eff) * Q2,
      where decay_eff = decay * (0.5 + 0.5 * stai).
    """
    alpha2, beta0, phi, rep, decay = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure (common=0.7, rare=0.3)
    T_known = np.array([[0.7, 0.3],  # action 0 (A): P(X), P(Y)
                        [0.3, 0.7]]) # action 1 (U): P(X), P(Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))  # stage-2 action values

    prev_a1 = None

    # Anxiety-weighted forgetting rate at stage-2
    decay_eff = decay * (0.5 + 0.5 * stai_val)

    for t in range(n_trials):

        # Apply forgetting before decisions
        if decay_eff > 0.0:
            Q2 *= (1.0 - decay_eff)

        # Stage-1 model-based values: expected max Q2 under known transitions
        max_Q2 = np.max(Q2, axis=1)  # per state
        Q1_MB = T_known @ max_Q2

        # Surprise based on whether the observed transition is common or rare
        a1 = int(action_1[t])
        s2 = int(state[t])
        prob_trans = T_known[a1, s2]
        surprise = 1.0 if prob_trans < 0.5 else 0.0

        # Anxiety- and surprise-gated inverse temperature
        beta_eff = beta0 * (1.0 - phi * surprise * stai_val)
        beta_eff = max(0.0, beta_eff)

        # Repetition bias attenuated by anxiety
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias_strength = rep * (1.0 - stai_val)
            bias1[prev_a1] = bias_strength

        # Stage-1 policy
        logits1 = beta_eff * Q1_MB + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy uses the same beta_eff (choice precision spillover)
        a2 = int(action_2[t])
        logits2 = beta_eff * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Reward and learning
        r = float(reward[t])
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll