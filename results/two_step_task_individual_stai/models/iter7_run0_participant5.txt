def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Surprise-gated model-based arbitration with anxiety-modulated lapse and learning.

    Core ideas
    - Stage 1 uses a hybrid of model-based (MB) and model-free (MF) values.
      The MB weight increases on surprising (rare) transitions, and this
      surprise sensitivity is amplified by higher anxiety.
    - Stage 2 values are updated with a single learning rate; Stage 1 MF values
      are updated toward the experienced Stage 2 value.
    - Anxiety increases a lapse component in both stages and slightly reduces
      exploitation (effective beta).

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state reached (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 = alien within planet).
    reward : array-like of float
        Coins received (e.g., 0 or 1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase surprise-gated MB arbitration and lapse,
        and slightly reduce effective inverse temperature.
    model_parameters : array-like of float
        [alpha_s2, beta, arb0, surpr0, lapse0]
        - alpha_s2: learning rate for stage-2 Q updates and scaled stage-1 MF updates.
        - beta: inverse temperature base.
        - arb0: baseline MB weight at stage 1 (0=MF,1=MB) before surprise.
        - surpr0: strength with which transition surprise boosts MB weight.
        - lapse0: baseline lapse rate mixed into softmax; grows with anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_s2, beta_base, arb0, surpr0, lapse0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety-gated parameters
    # Effective beta slightly reduced by anxiety (more randomness)
    beta_eff = np.clip(beta_base * (0.75 + 0.25 * (1.0 - st)), 0.0, 10.0)
    # Lapse increases with anxiety
    lapse_eff = np.clip(lapse0 * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q at stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition surprise based on realized transition probability
        p_common = 0.7 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.3
        surprise = 1.0 - p_common  # 0.3 for common, 0.7 for rare

        # MB weight boosted by surprise; high anxiety amplifies this boost
        w_mb = arb0 + surpr0 * surprise * (0.5 + 0.5 * st)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 choice probability with lapse
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        probs1 = (1.0 - lapse_eff) * probs1 + lapse_eff * 0.5
        p1[t] = probs1[a1]

        # Stage 2 choice probability with same beta, lapse
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        probs2 = (1.0 - lapse_eff) * probs2 + lapse_eff * 0.5
        p2[t] = probs2[a2]

        # Learning
        # Stage 2 PE and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s2 * pe2

        # Stage 1 MF update toward the obtained second-stage value
        # Anxiety slightly increases stage-1 learning when low anxiety (better credit assignment)
        alpha1 = alpha_s2 * (0.6 + 0.4 * (1.0 - st))
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Learned-transition hybrid with anxiety-sensitive exploration and arbitration.

    Core ideas
    - The agent learns the transition matrix T (A->X, U->Y) via a delta rule.
    - Stage 1 uses a hybrid of MB and MF. MB weight decreases when transition
      entropy is high (uncertain), and this reduction is amplified by anxiety.
    - Stage 2 includes an uncertainty-directed exploration bonus that is reduced
      by anxiety (high anxiety â†’ less directed exploration).

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 = alien within planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce directed exploration and MB reliance
        when transition structure is uncertain.
    model_parameters : array-like of float
        [alpha, beta, tau_T, u_bonus, omega_anx]
        - alpha: learning rate for value updates (stage 2) and stage-1 MF bootstrapping.
        - beta: inverse temperature base.
        - tau_T: learning rate for transition probabilities.
        - u_bonus: base directed-exploration bonus at stage 2.
        - omega_anx: strength with which anxiety suppresses MB control under transition uncertainty.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, tau_T, u_bonus, omega_anx = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T[a, s]
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    # Anxiety reduces directed exploration bonus
    bonus_eff = np.clip(u_bonus * (0.6 + 0.4 * (1.0 - st)), 0.0, 1.0)
    beta_eff = float(np.clip(beta, 0.0, 10.0))

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # MB values from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Transition uncertainty (entropy) for chosen action
        p = np.clip(T[a1, 0], 1e-6, 1 - 1e-6)
        ent = -(p * np.log(p) + (1 - p) * np.log(1 - p)) / np.log(2.0)  # in [0,1]
        # MB weight decreases with entropy; anxiety amplifies this reduction
        w_mb = 1.0 - ent * (0.5 + omega_anx * st)
        w_mb = float(np.clip(w_mb, 0.0, 1.0))

        # Hybrid stage-1 values
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 choice
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 with directed exploration bonus reduced by anxiety
        # Uncertainty bonus encourages choosing less-known alien: use proximity to 0.5
        uncert = 1.0 - np.abs(q2[s] - 0.5) * 2.0  # 1 at 0.5, 0 at 0 or 1
        bonus = bonus_eff * uncert
        q2_aug = q2[s] + bonus

        logits2 = beta_eff * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Transition learning: move T[a1] toward the observed state s
        target = np.array([1.0, 0.0]) if s == 0 else np.array([0.0, 1.0])
        T[a1] = (1.0 - tau_T) * T[a1] + tau_T * target
        # Keep rows normalized (should already be)
        T[a1] /= (np.sum(T[a1]) + eps)

        # Value learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping toward obtained stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        # Slightly reduce alpha at stage 1 when anxiety high (poorer credit assignment)
        alpha1 = alpha * (0.7 + 0.3 * (1.0 - st))
        q1_mf[a1] += alpha1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Volatility-sensitive learning with anxiety-gated optimism and perseveration.

    Core ideas
    - Tracks planet-specific volatility from absolute prediction errors; higher
      volatility increases learning rate (adaptive learning).
    - Initial optimism (prior over rewards) is reduced by anxiety.
    - Perseveration (choice stickiness) at both stages increases with anxiety.
    - Stage 1 uses a hybrid of MB (via fixed transitions) and MF; MB weight is
      higher when anxiety is low and when estimated volatility is low.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 = alien within planet).
    reward : array-like of float
        Coins received.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase perseveration, increase volatility
        sensitivity, and reduce optimistic priors and MB reliance.
    model_parameters : array-like of float
        [alpha, beta, k_vol, pers0, prior0]
        - alpha: base learning rate.
        - beta: inverse temperature base.
        - k_vol: volatility gain (how strongly volatility boosts learning).
        - pers0: base perseveration strength added to logits for repeating the last action.
        - prior0: optimistic prior magnitude for initial Q2; reduced by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, k_vol, pers0, prior0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initial optimistic prior reduced by anxiety
    optimism = prior0 * (0.5 + 0.5 * (1.0 - st))
    q2 = 0.5 + optimism * (np.ones((2, 2)) - 0.5)

    q1_mf = np.zeros(2)

    # Planet-specific volatility trackers
    v = np.zeros(2)  # volatility per state

    # Perseveration increases with anxiety
    stick = pers0 * (0.5 + 0.5 * st)

    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)
    eps = 1e-12

    prev_a1 = None
    prev_a2 = [None, None]  # track per state

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # MB weight decreases with anxiety and with mean volatility
        mean_v = float(np.mean(v))
        w_mb = np.clip(0.7 * (1.0 - st) * (1.0 - 0.5 * mean_v), 0.0, 1.0)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 1 stickiness logits
        logits1 = beta * (q1 - np.max(q1))
        if prev_a1 is not None:
            logits1[prev_a1] += stick

        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage 2 stickiness logits (state-dependent)
        logits2 = beta * (q2[s] - np.max(q2[s]))
        if prev_a2[s] is not None:
            logits2[prev_a2[s]] += stick

        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Learning with volatility adaptation
        pe2 = r - q2[s, a2]

        # Update volatility for this state using a simple PE-driven filter
        gamma = 0.3 + 0.7 * np.clip(k_vol * st, 0.0, 1.0)  # stronger update when anxious
        v[s] = (1.0 - gamma) * v[s] + gamma * abs(pe2)
        # Adaptive learning rate boosted by volatility
        alpha_eff = np.clip(alpha * (0.5 + 0.5 * (1.0 - st)) + k_vol * v[s], 0.0, 1.0)

        q2[s, a2] += alpha_eff * pe2

        # Stage-1 MF bootstrapping
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_eff * pe1

        # Update perseveration trackers
        prev_a1 = a1
        prev_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)