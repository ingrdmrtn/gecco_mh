def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB+MF with anxiety-modulated credit assignment after rare transitions and conflict-based arbitration.
    
    Idea
    - Second-stage values are learned model-free.
    - First-stage choice uses a weighted hybrid of model-based (MB) and model-free (MF) values.
    - Arbitration weight increases under action-value conflict; credit assignment to the first-stage MF system is
      attenuated after rare transitions in proportion to anxiety (higher stai -> less credit on rare trials).
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 7 parameters (all used)
        alpha_mf: learning rate for second-stage Q-values (and first-stage MF trace).
        beta: inverse temperature for softmax at both stages [0,10].
        omega_mb: base weight for MB contribution at stage-1 decisions.
        chi_conflict: arbitration boost under conflict (higher when MB action values are close).
        psi_anx_ca: anxiety modulation of credit assignment after rare transitions (attenuates MF credit).
        pers1: perseveration bias to repeat the last first-stage action.
        lambda_elig: eligibility trace from stage-2 PE to first-stage MF values.
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_mf, beta, omega_mb, chi_conflict, psi_anx_ca, pers1, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters to valid bounds
    alpha_mf = min(1.0, max(0.0, alpha_mf))
    beta = min(10.0, max(1e-6, beta))
    omega_mb = min(1.0, max(0.0, omega_mb))
    chi_conflict = min(1.0, max(0.0, chi_conflict))
    psi_anx_ca = min(1.0, max(0.0, psi_anx_ca))
    pers1 = min(1.0, max(0.0, pers1))
    lambda_elig = min(1.0, max(0.0, lambda_elig))

    # Fixed transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],  # A goes to X with 0.7, Y with 0.3
                  [0.3, 0.7]])  # U goes to X with 0.3, Y with 0.7

    # Value structures
    q2 = np.zeros((2, 2))         # second-stage values: state x action
    q1_mf = np.zeros(2)           # first-stage model-free values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):

        # Model-based first-stage values are expectations of best second-stage option
        max_q2 = np.max(q2, axis=1)            # value of best alien on each planet
        q1_mb = T @ max_q2                     # MB value for each spaceship

        # Conflict-based arbitration: more MB weight when MB action values are close (high conflict)
        # conflict in [0,1]: 1 when values equal, ~0 when far apart
        diff = abs(q1_mb[0] - q1_mb[1])
        conflict = 1.0 - np.tanh(diff)         # smooth, in (0,1]
        w = omega_mb + chi_conflict * conflict
        w = min(1.0, max(0.0, w))

        # Combine MB and MF for stage-1 action values
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Perseveration to repeat previous first-stage action
        if last_a1 is not None:
            q1[last_a1] += pers1

        # First-stage choice probability
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage choice probability (softmax)
        s = state[t]
        q2_centered = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage MF update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_mf * pe2

        # Stage-1 MF credit assignment via eligibility trace
        # Anxiety-modulated attenuation after rare transitions
        common_dest = a1  # common planet index for chosen ship
        is_rare = 1.0 if (s != common_dest) else 0.0
        ca_scale = 1.0 - psi_anx_ca * stai * is_rare
        ca_scale = min(1.0, max(0.0, ca_scale))

        q1_mf[a1] += lambda_elig * ca_scale * pe2

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Utility-asymmetric learning with surprise-driven exploration and anxiety-shaped loss sensitivity.
    
    Idea
    - Second-stage values learned with asymmetric learning rates: higher anxiety increases loss learning and
      decreases win learning (negativity bias).
    - Exploration is epsilon-softmax, where epsilon increases with transition surprise (rare transitions produce
      more exploration next choice).
    - First-stage uses MB values from the known transition structure. Q-values at stage-2 slowly leak toward 0
      with anxiety-scaled forgetting.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 7 parameters (all used)
        alpha_r: base learning rate for second-stage rewards.
        beta: inverse temperature for softmax [0,10].
        epsilon0: baseline exploration rate for both stages.
        kappa_surprise: how strongly rare-transition surprise boosts epsilon.
        phi_anx_loss: scales asymmetry in learning rates with anxiety (higher stai -> stronger loss learning).
        tau_leak2: per-trial leak of all second-stage Q-values toward 0, scaled by anxiety.
        stick1: first-stage perseveration to repeat the last chosen ship.
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, epsilon0, kappa_surprise, phi_anx_loss, tau_leak2, stick1 = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters
    alpha_r = min(1.0, max(0.0, alpha_r))
    beta = min(10.0, max(1e-6, beta))
    epsilon0 = min(1.0, max(0.0, epsilon0))
    kappa_surprise = min(1.0, max(0.0, kappa_surprise))
    phi_anx_loss = min(1.0, max(0.0, phi_anx_loss))
    tau_leak2 = min(1.0, max(0.0, tau_leak2))
    stick1 = min(1.0, max(0.0, stick1))

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    def softmax_probs(qvec, beta_):
        x = qvec - np.max(qvec)
        out = np.exp(beta_ * x)
        return out / np.sum(out)

    for t in range(n_trials):

        # Surprise from previous transition affects current epsilon
        # We use the surprise from the MOST RECENT trial if available; else baseline at t=0
        if t == 0:
            eps_t = epsilon0
        else:
            a1_prev = action_1[t - 1]
            s_prev = state[t - 1]
            p_obs = T[a1_prev, s_prev]  # probability of the observed transition
            surprise = 1.0 - p_obs      # 0.3 for common, 0.7 for rare
            eps_t = epsilon0 + kappa_surprise * surprise
            eps_t = min(1.0, max(0.0, eps_t))

        # First-stage model-based values
        q1_mb = T @ np.max(q2, axis=1)
        q1 = q1_mb.copy()
        if last_a1 is not None:
            q1[last_a1] += stick1

        # Epsilon-softmax mixture for stage-1
        probs1_soft = softmax_probs(q1, beta)
        probs1 = (1.0 - eps_t) * probs1_soft + eps_t * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice with same epsilon
        s = state[t]
        probs2_soft = softmax_probs(q2[s], beta)
        probs2 = (1.0 - eps_t) * probs2_soft + eps_t * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and asymmetric learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        # Anxiety-shaped asymmetry: upweight losses (negative PE) with higher stai; downweight wins
        if pe2 >= 0.0:
            alpha_eff = alpha_r * (1.0 - 0.5 * phi_anx_loss * stai)
        else:
            alpha_eff = alpha_r * (1.0 + 0.5 * phi_anx_loss * stai)
        alpha_eff = min(1.0, max(0.0, alpha_eff))

        q2[s, a2] += alpha_eff * pe2

        # Global leak toward 0 (anxiety-scaled)
        leak = tau_leak2 * stai
        if leak > 0.0:
            q2 *= (1.0 - leak)

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Dirichlet transition learning with uncertainty-sensitive arbitration modulated by anxiety.
    
    Idea
    - The agent learns transition probabilities per first-stage action via simple Dirichlet counts.
    - Stage-1 decisions arbitrate between model-based (MB) and model-free (MF) values based on transition
      uncertainty. Higher anxiety increases aversion to uncertain transition models, reducing MB weight when
      the current transition estimate has high entropy.
    - Second-stage includes forgetting of unchosen action values.
    
    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial (e.g., 0/1).
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: 7 parameters (all used)
        alpha_r: reward learning rate for second-stage values.
        beta: inverse temperature for both stages [0,10].
        w0_mb: baseline MB weight at stage-1 (when uncertainty is low).
        kappa_unc: strength of uncertainty aversion in arbitration.
        mu_forget: decay on the unchosen second-stage action at the visited state.
        rep1: perseveration bias to repeat last first-stage choice.
        xi_prior: scales Dirichlet prior strength with anxiety (higher stai -> stronger prior).
    
    Returns
    - Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha_r, beta, w0_mb, kappa_unc, mu_forget, rep1, xi_prior = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters
    alpha_r = min(1.0, max(0.0, alpha_r))
    beta = min(10.0, max(1e-6, beta))
    w0_mb = min(1.0, max(0.0, w0_mb))
    kappa_unc = min(1.0, max(0.0, kappa_unc))
    mu_forget = min(1.0, max(0.0, mu_forget))
    rep1 = min(1.0, max(0.0, rep1))
    xi_prior = min(1.0, max(0.0, xi_prior))

    # Dirichlet prior strength increases with anxiety
    prior_strength = 1.0 + 9.0 * xi_prior * stai  # in [1,10]
    # Initialize Dirichlet counts: for each action, counts over states (X,Y)
    dir_counts = np.array([[prior_strength, prior_strength],
                           [prior_strength, prior_strength]], dtype=float)

    # Values
    q2 = np.zeros((2, 2))
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None

    for t in range(n_trials):

        # Current transition estimates from Dirichlet counts
        T_hat = dir_counts / np.sum(dir_counts, axis=1, keepdims=True)  # rows sum to 1

        # MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_hat @ max_q2

        # Uncertainty (entropy) per action; use entropy of row for last chosen action if available,
        # else average uncertainty across actions for arbitration. Here we combine across both rows
        # by taking the mean entropy.
        ent_rows = []
        for a in (0, 1):
            p = T_hat[a]
            # numerical stability
            p = np.maximum(p, 1e-12)
            ent = -np.sum(p * np.log(p)) / np.log(2.0)  # normalized by log(2) to be in [0,1]
            ent_rows.append(ent)
        mean_entropy = 0.5 * (ent_rows[0] + ent_rows[1])

        # Anxiety-weighted uncertainty aversion: reduce MB weight when entropy is high
        w_eff = w0_mb * (1.0 - kappa_unc * mean_entropy * (1.0 + stai))
        w_eff = min(1.0, max(0.0, w_eff))

        # Combine MB and MF for stage-1 values
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Perseveration
        if last_a1 is not None:
            q1[last_a1] += rep1

        # First-stage policy
        q1_centered = q1 - np.max(q1)
        probs1 = np.exp(beta * q1_centered)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        q2_centered = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2_centered)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and second-stage learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Forgetting of the unchosen action at visited state
        other = 1 - a2
        q2[s, other] *= (1.0 - mu_forget)

        # First-stage MF bootstrap (eligibility via second-stage PE without extra parameter)
        q1_mf[a1] += alpha_r * pe2

        # Update Dirichlet counts with observed transition
        dir_counts[a1, s] += 1.0

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll