def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and learned transitions.
    
    This model blends a model-based (MB) and a model-free (MF) controller at stage 1.
    The MB controller uses a learned transition model that is updated from experience.
    Anxiety modulates both the arbitration weight favoring MB control and the decision
    temperature. Stage-2 reward values are learned via a delta-rule.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to modulate arbitration and temperature.
    model_parameters : list or array
        [alpha_q, beta, mb_bias, anx_mod, trans_lr]
        Bounds:
          alpha_q in [0,1]   : learning rate for Q-values
          beta in [0,10]     : base inverse temperature
          mb_bias in [0,1]   : baseline tendency to favor MB over MF (mapped internally)
          anx_mod in [0,1]   : strength of anxiety influence on arbitration and temperature
          trans_lr in [0,1]  : transition learning rate for MB transition matrix

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_q, beta, mb_bias, anx_mod, trans_lr = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition model T[a, s]
    # Start neutral at 0.5 and learn toward observed transitions
    T = np.full((2, 2), 0.5)

    # Model-free action values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map mb_bias in [0,1] to an internal logit bias around 0 via atanh-like stretching
    # Simple linear center: mb_bias_c in [-1,1]
    mb_bias_c = 2.0 * mb_bias - 1.0
    # Anxiety effect centered around 0, scaled by anx_mod
    anx_centered = (2.0 * stai - 1.0) * (2.0 * anx_mod - 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])

        # Model-based stage-1 Q from learned transition model
        max_q2 = np.max(q2, axis=1)  # best option per second-stage state
        q1_mb = T @ max_q2  # expected value under learned transitions

        # Arbitration weight w in [0,1]: sigmoid of (bias + anxiety)
        # Higher anxiety reduces MB weight if anx_centered > 0 (can also flip depending on sign).
        # Use a smooth mapping via logistic function.
        arb_signal = 2.0 * (mb_bias_c - anx_centered)  # combine bias and anxiety
        w = 1.0 / (1.0 + np.exp(-arb_signal))
        w = np.clip(w, 0.0, 1.0)

        # Combine MB and MF for decision at stage 1
        q1 = w * q1_mb + (1.0 - w) * q1_mf

        # Anxiety-modulated temperature (bounded to be non-negative implicitly by beta bounds)
        beta_eff = beta * (1.0 + 0.5 * (-anx_centered))

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta_eff * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        exp_q2s = np.exp(beta_eff * q2c)
        probs_2 = exp_q2s / np.sum(exp_q2s)
        p_choice_2[t] = probs_2[a2]

        # Learning: Stage-2 Q-learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Learning: Stage-1 MF bootstrapping toward realized second-stage value
        # Targets the current realized second-stage value (after the update).
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_q * td1

        # Update learned transition model row for chosen action using recency-weighted counts
        # Move probability toward observed state; enforce row normalization via two-value update
        T[a1, s] += trans_lr * (1.0 - T[a1, s])
        T[a1, 1 - s] += trans_lr * (0.0 - T[a1, 1 - s])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-modulated loss aversion and rare-transition credit dampening.
    
    Stage-2 values are learned model-free and backed up to stage 1 via an eligibility trace.
    Outcomes are passed through an anxiety-modulated loss-aversion utility. Additionally,
    eligibility backup to stage 1 is dampened on rare transitions, with a strength that
    increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; used to modulate utility and credit assignment.
    model_parameters : list or array
        [alpha, beta, loss_av_base, anx_impact, trace]
        Bounds:
          alpha in [0,1]         : learning rate for Q-values
          beta in [0,10]         : inverse temperature
          loss_av_base in [0,1]  : baseline loss aversion (penalty for zero reward)
          anx_impact in [0,1]    : sensitivity of loss aversion and credit dampening to anxiety
          trace in [0,1]         : eligibility trace for backing up stage-2 to stage-1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, loss_av_base, anx_impact, trace = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Q-values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated loss aversion: penalize zero reward
    loss_av = np.clip(loss_av_base * (1.0 + anx_impact * (2.0 * stai - 1.0)), 0.0, 1.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])

        # Determine whether transition was common for chosen a1
        # Common if (A->X) or (U->Y)
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 0.0 if is_common else 1.0

        # Policies
        q1c = q1 - np.max(q1)
        exp_q1 = np.exp(beta * q1c)
        probs_1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs_1[a1]

        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        exp_q2s = np.exp(beta * q2c)
        probs_2 = exp_q2s / np.sum(exp_q2s)
        p_choice_2[t] = probs_2[a2]

        # Utility transformation with loss aversion for zero reward (reward in {0,1})
        r = reward[t]
        util = r - (1.0 - r) * loss_av

        # Stage-2 MF update
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility backup to stage 1, dampened on rare transitions with anxiety
        damp = 1.0 - anx_impact * stai * is_rare
        lamb_eff = np.clip(trace * damp, 0.0, 1.0)
        q1[a1] += alpha * lamb_eff * pe2

        # Optional direct TD correction of stage-1 toward updated second-stage value
        td1 = q2[s, a2] - q1[a1]
        q1[a1] += alpha * 0.5 * td1  # mild stabilization

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based controller with anxiety- and surprise-adaptive temperature and lapse.
    
    A purely model-based (MB) planner uses the known transition structure (common=0.7).
    Stage-2 values are learned via a delta-rule. The softmax temperature and a lapse
    probability adapt to anxiety and to recent transition surprise (rare vs. common).
    Surprise raises or lowers exploitation depending on kappa_surprise, while anxiety
    reduces temperature and increases lapse.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1; modulates temperature and lapse.
    model_parameters : list or array
        [alpha, beta_base, lapse_base, anx_temp, kappa_surprise]
        Bounds:
          alpha in [0,1]           : learning rate for second-stage Q-values
          beta_base in [0,10]      : baseline inverse temperature
          lapse_base in [0,1]      : baseline lapse probability (uniform mixing)
          anx_temp in [0,1]        : anxiety sensitivity of temperature and lapse
          kappa_surprise in [0,1]  : weight of surprise on temperature and learning

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta_base, lapse_base, anx_temp, kappa_surprise = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed known transition structure
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    q2 = np.zeros((2, 2))  # second-stage Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_surprise = 0.0  # previous-trial surprise (1=rare, 0=common)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])

        # MB stage-1 Qs via Bellman expectation with fixed transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_fixed @ max_q2

        # Adaptive temperature and lapse
        beta_eff = beta_base * (1.0 - anx_temp * stai) + kappa_surprise * prev_surprise * beta_base
        beta_eff = max(1e-8, beta_eff)  # keep positive

        lapse_eff = np.clip(lapse_base * (1.0 + anx_temp * stai), 0.0, 1.0)

        # Stage-1 policy with lapse
        q1c = q1_mb - np.max(q1_mb)
        exp_q1 = np.exp(beta_eff * q1c)
        softmax_1 = exp_q1 / np.sum(exp_q1)
        probs_1 = (1.0 - lapse_eff) * softmax_1 + lapse_eff * 0.5
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with same adaptive parameters
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        exp_q2s = np.exp(beta_eff * q2c)
        softmax_2 = exp_q2s / np.sum(exp_q2s)
        probs_2 = (1.0 - lapse_eff) * softmax_2 + lapse_eff * 0.5
        p_choice_2[t] = probs_2[a2]

        # Learning with optional surprise modulation of learning rate on rare transitions
        r = reward[t]
        # Compute whether current transition is rare
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        is_rare = 0.0 if is_common else 1.0

        alpha_eff = np.clip(alpha * (1.0 + kappa_surprise * (0.5 - stai) * is_rare), 0.0, 1.0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Update surprise for next trial's temperature adaptation
        prev_surprise = is_rare

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll