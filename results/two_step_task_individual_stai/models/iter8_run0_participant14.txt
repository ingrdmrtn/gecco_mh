Below are three new, standalone cognitive models that compute the negative log-likelihood of observed choices for the two-step task. Each function:

- Uses all parameters meaningfully (with bounds: most in [0,1], beta in [0,10])
- Incorporates the participant’s anxiety score (stai) in a distinct, mechanistically interpretable way
- Returns the joint negative log-likelihood across both stages
- Keeps the total number of parameters ≤ 5
- Avoids repeating the exact parameter combinations you’ve tried before

Note: Assume numpy as np is already imported.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Model 1: MB planning with a learned choice-kernel bias modulated by anxiety.
    
    Summary:
    - Stage 1: model-based planning using known transitions (0.7/0.3).
      Adds a choice-kernel bias (recency-driven tendency) learned from previous
      choices independently of reward, with strength amplified by anxiety.
    - Stage 2: model-free Q-learning with a single reward learning rate.
    
    Anxiety use:
    - Increases the effective strength of the choice-kernel bias at stage 1,
      reflecting stronger action inertia under higher anxiety.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate for stage-2 Q-values, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - k_ck: choice-kernel learning rate (how fast the bias updates), in [0,1]
    - g_anx: anxiety gain scaling for the choice-kernel magnitude, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of reached planets (0=X, 1=Y)
    - action_2: array of second-stage choices (0/1) per reached planet
    - reward: array of rewards per trial (float)
    - stai: array-like with single float [0,1] anxiety score
    - model_parameters: array-like of params as above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, k_ck, g_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: rows: spaceship [A(0), U(1)], cols: planet [X(0), Y(1)]
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Q-values for second-stage per planet-alien
    q2 = np.zeros((2, 2))

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Choice kernel over first-stage actions (tendency to repeat independent of reward)
    ck = np.zeros(2)
    # Anxiety-modulated kernel gain (centered around medium anxiety ~0.41; slope by g_anx)
    # Higher anxiety -> stronger effect; clip to keep reasonable range
    ck_gain = 1.0 + g_anx * (stai - 0.41)
    ck_gain = np.clip(ck_gain, 0.0, 2.0)

    for t in range(n_trials):
        # Stage 1 MB values = T @ max_a2 Q2(s,a2)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add kernel bias to logits (scaled by anxiety)
        logits1 = beta * q1_mb + ck_gain * ck
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy: softmax over Q2 at reached state
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)

        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward and updates
        r = reward[t]
        # Stage 2 MF update
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update choice kernel toward the chosen action (recency inertia)
        # One-hot of chosen a1
        oh = np.array([0.0, 0.0])
        oh[a1] = 1.0
        ck += k_ck * (oh - ck)

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model 2: Pearce–Hall surprise-adaptive learning with anxiety-amplified surprise sensitivity.
    
    Summary:
    - Stage 1: model-based planning using known transitions (0.7/0.3).
    - Stage 2: model-free Q-learning with a surprise-adaptive learning rate:
      alpha_eff = alpha0 + k_ph_eff * |RPE|, where k_ph_eff scales with anxiety.
    - Surprise also slightly boosts exploration by adding a small, RPE-dependent
      entropy drive via the same gain (implemented by adjusting logits).
    
    Anxiety use:
    - Increases sensitivity to surprise (|RPE|), which increases the effective
      learning rate and a mild exploration push (akin to attention/arousal).
    
    Parameters (model_parameters):
    - alpha0: base learning rate for Q2 in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - k_ph: base Pearce–Hall gain (how much surprise modulates learning) in [0,1]
    - g_anx: anxiety gain scaling the PH effect in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of reached planets (0=X, 1=Y)
    - action_2: array of second-stage choices (0/1) per reached planet
    - reward: array of rewards per trial (float)
    - stai: array-like with single float [0,1] anxiety score
    - model_parameters: array-like of params as above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha0, beta, k_ph, g_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-scaled PH gain; higher anxiety -> stronger reliance on surprise
    k_ph_eff_scale = 1.0 + g_anx * (stai - 0.31)
    k_ph_eff_scale = np.clip(k_ph_eff_scale, 0.0, 2.0)

    for t in range(n_trials):
        # Stage 1 MB values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        logits1 = beta * q1_mb
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # For stage 2, compute PH-modulated exploration bump from last trial RPE at this state
        # We compute RPE online using the current trial's reward and then use alpha_eff and
        # a small exploration scaling on the logits before logging the probability.
        # First compute softmax probabilities given current q2:
        base_logits2 = beta * q2[s]
        base_logits2 -= np.max(base_logits2)

        # Sample action probability for logging before updating:
        base_probs2 = np.exp(base_logits2)
        base_probs2 /= (np.sum(base_probs2) + 1e-12)

        a2 = action_2[t]
        r = reward[t]
        # Compute RPE on chosen action
        rpe = r - q2[s, a2]
        surprise = abs(rpe)

        # Effective learning rate (capped at 1)
        alpha_eff = alpha0 + (k_ph * k_ph_eff_scale) * surprise
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Mild exploration bump proportional to surprise (entropy drive).
        # Implement as additive zero-mean perturbation to logits:
        # push non-chosen slightly up to encourage exploration when surprise is high.
        # Construct a centered vector v with +0.5 to non-chosen, -0.5 to chosen.
        v = np.array([0.5, 0.5])
        v[a2] = -0.5
        expl_strength = 0.2 * (k_ph * k_ph_eff_scale) * surprise  # bounded by parameters and surprise
        logits2 = base_logits2 + expl_strength * v

        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        p_choice_2[t] = probs2[a2]

        # Q2 update with surprise-adaptive learning rate
        q2[s, a2] += alpha_eff * rpe

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model 3: Transition–outcome interaction bias (MB-like signature) with anxiety scaling and decay.
    
    Summary:
    - Captures the canonical "stay after common+reward / rare+no-reward; switch otherwise"
      tendency without explicitly learning transitions. A decaying memory of the last trial’s
      transition–outcome interaction biases the next stage-1 decision toward staying or switching.
    - Stage 1 policy = softmax over MB values (known transitions) plus this TO bias.
    - Stage 2 uses standard MF Q-learning.
    
    Anxiety use:
    - Anxiety scales the magnitude of the transition–outcome bias (stronger under higher anxiety).
    
    Parameters (model_parameters):
    - alpha: stage-2 reward learning rate in [0,1]
    - beta: inverse temperature for both stages in [0,10]
    - theta0: base strength of transition–outcome bias at stage 1 in [0,1]
    - g_anx: anxiety gain on the bias strength in [0,1]
    - k_decay: decay rate of the bias memory across trials in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0=A, 1=U)
    - state: array of reached planets (0=X, 1=Y)
    - action_2: array of second-stage choices (0/1) per reached planet
    - reward: array of rewards per trial (float)
    - stai: array-like with single float [0,1] anxiety score
    - model_parameters: array-like of params as above
    
    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha, beta, theta0, g_anx, k_decay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Transition–Outcome bias memory m: positive -> repeat last action; negative -> switch
    m = 0.0
    # Anxiety-scaled bias strength
    theta_eff = theta0 * np.clip(1.0 + g_anx * (stai - 0.51), 0.0, 2.0)

    prev_a1 = None
    prev_common = None
    prev_rew = None

    for t in range(n_trials):
        # Compute MB stage-1 values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Build bias vector from memory m relative to last chosen action
        bias = np.zeros(2)
        if prev_a1 is not None:
            # bias toward repeating prev_a1 by +m, and switching by -m
            bias[prev_a1] += theta_eff * m
            bias[1 - prev_a1] -= theta_eff * m

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + 1e-12)

        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        # Stage 2 policy
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]
        # Update Q2
        rpe = r - q2[s, a2]
        q2[s, a2] += alpha * rpe

        # After observing current trial, update the TO memory m for next trial
        # Determine whether the current transition was common or rare
        # Common if (A->X) or (U->Y), rare otherwise
        is_common = int((a1 == 0 and s == 0) or (a1 == 1 and s == 1))

        # Transition–Outcome signal: +1 if (common & rewarded) or (rare & unrewarded); else -1
        to_signal = 1.0 if ((is_common and r > 0.0) or ((1 - is_common) and r <= 0.0)) else -1.0

        # Decay old memory and incorporate new signal
        m = (1.0 - k_decay) * m + k_decay * to_signal

        prev_a1 = a1
        prev_common = is_common
        prev_rew = r

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll

Notes on parameter bounds:
- All rate-like parameters (alpha_r, k_ck, g_anx, alpha0, k_ph, alpha, theta0, k_decay) are intended for [0,1].
- beta is intended for [0,10].

How the participant’s high anxiety (stai=0.5875) is used:
- Model 1: increases recency-driven choice kernel strength (stronger action inertia).
- Model 2: amplifies surprise sensitivity, increasing both learning rate and mild exploration following surprising outcomes.
- Model 3: strengthens the transition–outcome interaction bias, enhancing the tendency to stay after common-reward or rare-no-reward and switch otherwise.

These mechanisms differ from the prior best model (which used learned transitions and stage-1 perseveration) by:
- Model 1: using an explicitly learned, recency-based choice kernel (not simple perseveration) added to MB values.
- Model 2: dynamically adapting learning and exploration to RPE-driven surprise with anxiety scaling (Pearce–Hall).
- Model 3: encoding the classic two-step transition–outcome interaction via a decaying memory signal, anxiety-scaled, without learning transitions.