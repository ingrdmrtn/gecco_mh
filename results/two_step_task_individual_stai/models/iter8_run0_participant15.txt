def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated planning, leaky transition trust, and TD(λ) credit assignment.
    
    Core ideas:
    - Stage-2 values are learned model-free (TD(0)).
    - Stage-1 has both MF values and MB plan values; weight shifts with anxiety:
        w_mb = 1 - stai (higher anxiety => less planning).
    - Anxiety also reduces decisiveness: beta_eff = beta / (1 + c_anx * stai).
    - Imperfect belief in the transition model: a 'leak' blends the true transition with uniform,
      increasing with anxiety (leak_eff = mb_leak * stai).
    - TD(λ): Stage-1 MF receives both the standard bootstrapped update from Q2 and an extra λ-weighted
      update from the immediate Stage-2 prediction error.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for both Stage-1 and Stage-2 value updates.
    - beta: [0,10] Inverse temperature for both stages (modulated by anxiety).
    - lam_e: [0,1] Eligibility trace strength to propagate Stage-2 PE into Stage-1 MF.
    - c_anx: [0,1] Anxiety scaling for temperature softening: beta_eff = beta / (1 + c_anx*stai).
    - mb_leak: [0,1] Degree to leak the transition model toward uniform; effective leak = mb_leak*stai.
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1} (0=X, 1=Y).
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, lam_e, c_anx, mb_leak].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta, beta, lam_e, c_anx, mb_leak = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # True transition structure (A->X common, U->Y common)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    # Anxiety-modulated effective transition belief (leak toward uniform)
    leak_eff = mb_leak * stai
    T_eff = (1.0 - leak_eff) * T_true + leak_eff * 0.5  # row-wise; 0.5 uniform to each state

    # Storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = 0.5 * np.ones((2, 2))  # Stage-2 MF values per planet/state and action
    q1_mf = np.zeros(2)         # Stage-1 MF values (for A and U)

    # Anxiety-modulated arbitration and temperature
    w_mb = max(0.0, min(1.0, 1.0 - stai))  # more anxious => less MB
    beta_eff = beta / (1.0 + c_anx * stai + 1e-12)

    for t in range(n_trials):
        s = state[t]

        # Model-based plan at Stage-1: expected max Q2 under T_eff
        max_q2 = np.max(q2, axis=1)           # value of each planet
        q1_mb = T_eff @ max_q2                # expected value per spaceship

        # Hybrid Stage-1 value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        z1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learn from reward
        r = reward[t]

        # Stage-2 TD(0)
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Stage-1 MF update with TD(λ): bootstrapped target + λ * stage-2 PE
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        lam_eff = lam_e  # can be seen as baseline trace; anxiety already impacts via w_mb and beta
        q1_mf[a1] += eta * (pe1 + lam_eff * pe2)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Utility-transformed learning with uncertainty bonus and anxiety-dampened exploration.
    
    Core ideas:
    - Stage-2 rewards are transformed through a concave utility u(r) = r^(rho_eff),
      where rho_eff increases with anxiety (more anxiety => more concavity => risk-averse).
    - Stage-2 exploration bonus based on value uncertainty: bonus ~ q*(1-q), scaled down by anxiety.
    - Stage-1 uses MB planning from the transformed Q2 values (expected max utility),
      with no explicit MF component to keep parameters parsimonious.
    - Anxiety reduces exploration bonus and slightly softens choice via beta_eff.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Stage-2 value learning.
    - beta: [0,10] Inverse temperature for both stages (modulated by anxiety).
    - rho0: [0,1] Baseline utility curvature; higher => more concave utility.
    - zeta_ent: [0,1] Baseline weight of uncertainty bonus at Stage-2.
    - tau_anx: [0,1] Strength by which anxiety increases risk aversion (utility curvature).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, rho0, zeta_ent, tau_anx].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta, beta, rho0, zeta_ent, tau_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Initialize Stage-2 values
    q2 = 0.5 * np.ones((2, 2))

    # Anxiety effects
    rho_eff = max(0.0, min(1.0, rho0 + tau_anx * stai))      # utility curvature
    zeta_eff = zeta_ent * (1.0 - stai)                       # anxious => less exploration bonus
    beta_eff = beta / (1.0 + 0.5 * stai)                     # modest softening with anxiety

    for t in range(n_trials):
        s = state[t]

        # Stage-2 uncertainty bonus (normalized: max uncertainty for Bernoulli is 0.25 at q=0.5)
        unc = q2[s] * (1.0 - q2[s])           # elementwise; max 0.25
        bonus = zeta_eff * (unc / 0.25)       # in [0, zeta_eff]
        q2_aug = q2[s] + bonus

        # Stage-2 policy
        z2 = beta_eff * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Stage-1 MB: plan over max utility values at each planet
        max_q2 = np.max(q2 + (zeta_eff * ((q2 * (1 - q2)) / 0.25))[:, None] * 0.0, axis=1)
        # Note: Stage-1 uses the base q2 expectations (without adding a Stage-1 bonus),
        # focusing on expected best action utility per planet.
        q1_mb = T @ np.max(q2, axis=1)

        # Stage-1 policy
        z1 = beta_eff * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Learning with utility-transformed reward
        r = reward[t]
        u = (r ** rho_eff) if r >= 0.0 else -((-r) ** rho_eff)  # robust power utility; here r in [0,1]
        pe2 = u - q2[s, a2]
        q2[s, a2] += eta * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learning the transition model with anxiety-weighted 'safety' bias toward predictable spaceships.
    
    Core ideas:
    - The agent learns transition probabilities online (alpha_t) with forgetting (f_forget).
    - Stage-1 uses a hybrid of learned MB values and Stage-1 MF values, with MB weight higher when
      anxiety is low: w_mb = 0.5 + 0.5*(1 - stai).
    - An anxiety-weighted safety bias penalizes actions with higher transition entropy (uncertainty),
      capturing preference for predictable outcomes under anxiety.
    - Stage-2 values are learned model-free.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Stage-2 MF values and Stage-1 MF bootstrapping.
    - beta: [0,10] Inverse temperature for both stages.
    - alpha_t: [0,1] Learning rate for updating the transition matrix rows upon observing transitions.
    - f_forget: [0,1] Per-trial forgetting toward uniform for transition probabilities.
    - omega_safe: [0,1] Strength of safety bias; effective bias scales with stai.
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, alpha_t, f_forget, omega_safe].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta, beta, alpha_t, f_forget, omega_safe = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize learned transition matrix; start near known structure but not exact
    T_learn = np.array([[0.6, 0.4],
                        [0.4, 0.6]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Values
    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    # Anxiety-modulated MB weight and safety bias
    w_mb = 0.5 + 0.5 * (1.0 - stai)          # in [0.5,1.0]; more anxiety => closer to 0.5
    safety_scale = omega_safe * stai          # stronger bias with higher anxiety

    for t in range(n_trials):
        s = state[t]

        # Stage-1 MB plan from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_learn @ max_q2

        # Safety bias: negative entropy of transitions per action (higher entropy => more penalty)
        # Entropy for binary distribution [p, 1-p]: H = -sum p log p (natural log).
        # Normalize by max entropy (ln 2) so bias in [0,1].
        ent = np.zeros(2)
        for a in range(2):
            p = np.clip(T_learn[a, 0], 1e-8, 1.0 - 1e-8)
            q = 1.0 - p
            H = -(p * np.log(p) + q * np.log(q))
            ent[a] = H / np.log(2.0)
        bias = -safety_scale * ent  # penalize uncertainty

        # Hybrid Stage-1 value + safety bias
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + bias

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Reward learning
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta * pe1

        # Transition model forgetting toward uniform
        T_learn = (1.0 - f_forget) * T_learn + f_forget * 0.5

        # Update chosen row of transition matrix toward observed state
        # Move probability mass toward the observed state s
        T_learn[a1, s] += alpha_t * (1.0 - T_learn[a1, s])
        other = 1 - s
        T_learn[a1, other] += alpha_t * (0.0 - T_learn[a1, other])

        # Renormalize rows to sum to 1 and keep probabilities in [eps, 1-eps]
        for a in range(2):
            row = T_learn[a]
            row = np.clip(row, 1e-8, 1.0)
            T_learn[a] = row / np.sum(row)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll