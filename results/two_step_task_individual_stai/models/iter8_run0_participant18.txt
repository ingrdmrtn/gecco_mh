def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-gated arbitration, eligibility, and lapses.

    Description:
    - Stage-1 action values combine model-based (MB) and model-free (MF) estimates.
      The arbitration weight increases with lower anxiety; higher anxiety shifts weight away from MB.
    - Stage-2 policy is purely MF.
    - Reward prediction errors at stage-2 are assigned back to stage-1 via an eligibility parameter
      that is reduced by anxiety (less credit assignment under higher anxiety).
    - A small lapse probability increases with anxiety at both stages.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: inverse temperature for softmax in [0,10]
    - theta_mb: baseline MB weight in [0,1]
    - lambda0: baseline eligibility/credit assignment to stage-1 in [0,1]
    - epsilon: baseline lapse probability in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet (0/1)
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, theta_mb, lambda0, epsilon)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, theta_mb, lambda0, epsilon = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (A->X, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    Q1_mf = np.zeros(2)        # MF stage-1 action values
    Q2 = np.zeros((2, 2))      # Stage-2 state-action values

    # Anxiety-modulated parameters
    w_eff = np.clip((1.0 - st) * theta_mb + st * 0.2, 0.0, 1.0)    # higher anxiety -> less MB
    lam_eff = np.clip(lambda0 * (1.0 - 0.7 * st), 0.0, 1.0)        # higher anxiety -> weaker credit assignment
    eps_lapse = np.clip(epsilon * st, 0.0, 1.0)                    # higher anxiety -> more lapses

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 MB estimate: expected max value at next state
        max_Q2 = np.max(Q2, axis=1)       # max over aliens for each planet
        Q1_mb = T @ max_Q2                # expected value for A and U

        # Arbitration
        Q1 = (1.0 - w_eff) * Q1_mf + w_eff * Q1_mb

        # Stage-1 policy with lapses
        pref1 = Q1
        pref1 -= np.max(pref1)  # stabilize
        exp1 = np.exp(beta * pref1)
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - eps_lapse) * soft1 + eps_lapse * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with lapses (pure MF)
        pref2 = Q2[s]
        pref2 -= np.max(pref2)
        exp2 = np.exp(beta * pref2)
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - eps_lapse) * soft2 + eps_lapse * 0.5
        p_choice_2[t] = probs2[a2]

        # Learning
        # Stage-2 TD(0)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 credit assignment: combine bootstrapping and direct outcome PE
        # Bootstrapped error from second-stage value
        delta1_boot = Q2[s, a2] - Q1_mf[a1]
        # Update MF value at stage-1 with eligibility scaled by anxiety
        Q1_mf[a1] += alpha * lam_eff * (delta1_boot + delta2)

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive temperature with anxiety, stage-1 perseveration, and Q-forgetting.

    Description:
    - Anxiety reduces choice precision via an adaptive inverse temperature:
      beta_eff = beta * (1 - k_temp * stai), bounded below by a small floor.
    - Stage-1 includes a perseveration bias that decays over trials; the bias strength is
      reduced by anxiety.
    - Stage-2 is MF; both stage-2 values and perseveration traces are subject to mild forgetting
      toward zero (recency), controlled by tau_forget.

    Parameters (model_parameters):
    - alpha: reward learning rate in [0,1]
    - beta: base inverse temperature in [0,10]
    - k_temp: anxiety sensitivity of temperature in [0,1] (higher -> more reduction in beta)
    - tau_forget: forgetting/decay parameter in [0,1] applied to Q2 and perseveration traces
    - pers: baseline perseveration strength in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, k_temp, tau_forget, pers)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, k_temp, tau_forget, pers = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q1_mf = np.zeros(2)
    Q2 = np.zeros((2, 2))

    # Perseveration trace for stage-1 actions
    trace1 = np.zeros(2)

    # Anxiety-modulated parameters
    beta_eff = max(1e-3, beta * (1.0 - k_temp * st))  # higher anxiety -> lower precision
    stick_strength = pers * (1.0 - st)                # higher anxiety -> weaker perseveration
    decay = np.clip(tau_forget, 0.0, 1.0)             # used for Q2 and trace forgetting

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1: MB lookahead based on Q2, combined with MF and perseveration bias
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_Q2
        pref1 = 0.5 * Q1_mf + 0.5 * Q1_mb + stick_strength * trace1

        centered1 = pref1 - np.max(pref1)
        exp1 = np.exp(beta_eff * centered1)
        probs1 = exp1 / (np.sum(exp1) + eps)
        p_choice_1[t] = probs1[a1]

        # Stage-2: MF softmax with adaptive temperature
        pref2 = Q2[s]
        centered2 = pref2 - np.max(pref2)
        exp2 = np.exp(beta_eff * centered2)
        probs2 = exp2 / (np.sum(exp2) + eps)
        p_choice_2[t] = probs2[a2]

        # Learning
        # Apply forgetting (toward zero) before updating to emphasize recency
        Q2 *= (1.0 - decay)

        # Stage-2 TD(0)
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update with bootstrapping from Q2
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Perseveration trace update with decay
        trace1 *= (1.0 - decay)
        trace1[a1] += 1.0

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus exploration at stage-2 with anxiety-damped novelty seeking.

    Description:
    - Tracks an exponential moving mean and variance of rewards per second-stage action.
      An exploration bonus proportional to the estimated standard deviation encourages sampling.
    - Anxiety dampens the exploration bonus: higher anxiety -> less novelty seeking.
    - Stage-1 is MB using the bonus-augmented second-stage values to plan.
    - Includes a mild lapse that increases with anxiety.
    - Utility transform: outcomes pass through a concave power utility that becomes more concave
      with higher anxiety (risk/uncertainty aversion).

    Parameters (model_parameters):
    - alpha: reward learning rate for Q-values in [0,1]
    - beta: inverse temperature in [0,10]
    - bonus0: base exploration bonus weight in [0,1]
    - decay_u: learning rate for running moments (mean/variance) in [0,1]
    - xi_lapse: baseline lapse probability in [0,1]

    Inputs:
    - action_1: array-like of ints in {0,1}; chosen spaceship (0=A, 1=U)
    - state: array-like of ints in {0,1}; observed planet (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}; chosen alien on the observed planet
    - reward: array-like of floats in [0,1]; received coins
    - stai: array-like with one float in [0,1]; anxiety score
    - model_parameters: tuple/list (alpha, beta, bonus0, decay_u, xi_lapse)

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, bonus0, decay_u, xi_lapse = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    Q2 = np.zeros((2, 2))
    Q1_mf = np.zeros(2)  # keep a small MF component to capture idiosyncrasies

    # Running moments for uncertainty estimates at stage-2
    m = np.zeros((2, 2))        # mean reward
    m2 = np.zeros((2, 2))       # mean of squared reward

    # Anxiety-modulated components
    bonus_scale = bonus0 * (1.0 - st)             # higher anxiety -> smaller exploration bonus
    lapse_eff = np.clip(xi_lapse * st, 0.0, 1.0)  # higher anxiety -> more lapses
    # Utility exponent: 1 (linear) at low anxiety, more concave as anxiety increases
    util_exp = np.clip(1.0 - 0.5 * st, 0.5, 1.0)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-10

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r_raw = float(reward[t])

        # Compute uncertainty bonus (std estimate)
        var = np.maximum(0.0, m2 - m ** 2)
        std_est = np.sqrt(var + 1e-12)
        bonus = bonus_scale * std_est

        # Stage-2 policy uses bonus-augmented values
        pref2 = Q2[s] + bonus[s]
        pref2 -= np.max(pref2)
        exp2 = np.exp(beta * pref2)
        soft2 = exp2 / (np.sum(exp2) + eps)
        probs2 = (1.0 - lapse_eff) * soft2 + lapse_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Stage-1 uses MB planning from bonus-augmented Q2, with a small MF blend
        max_aug = np.max(Q2 + bonus, axis=1)
        Q1_mb = T @ max_aug
        Q1 = 0.8 * Q1_mb + 0.2 * Q1_mf

        pref1 = Q1
        pref1 -= np.max(pref1)
        exp1 = np.exp(beta * pref1)
        soft1 = exp1 / (np.sum(exp1) + eps)
        probs1 = (1.0 - lapse_eff) * soft1 + lapse_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Utility-transformed outcome
        r = r_raw ** util_exp

        # Update running moments for uncertainty estimates
        m[s, a2] = (1.0 - decay_u) * m[s, a2] + decay_u * r
        m2[s, a2] = (1.0 - decay_u) * m2[s, a2] + decay_u * (r ** 2)

        # TD learning at stage-2
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Update small MF component at stage-1 via bootstrapping
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * 0.5 * delta1
        Q1_mf[a1] += alpha * 0.5 * delta2  # partial direct credit

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll