def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-surprise-gated hybrid control with anxiety-modulated planning and credit assignment.

    Idea:
    - Stage-2 (aliens) values are learned via delta-rule.
    - Stage-1 (spaceships) uses a hybrid of model-based (MB) and model-free (MF) values.
    - Anxiety reduces reliance on planning (MB) and increases MF credit assignment following rare transitions.
    - A bootstrapping parameter controls how strongly stage-2 values propagate to stage-1 MF values.

    Parameters (bounds):
    - model_parameters[0] = a2 (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = w_pln0 (0 to 1): baseline weight on model-based values at stage 1
    - model_parameters[3] = anx_trn (0 to 1): anxiety sensitivity; higher stai reduces MB weight and boosts MF credit after rare transitions
    - model_parameters[4] = chi (0 to 1): bootstrapping strength to update stage-1 MF from stage-2

    Inputs:
    - action_1: array-like ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like floats, received coins per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    a2, beta, w_pln0, anx_trn, chi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure: rows are spaceships (A=0, U=1); columns are planets (X=0, Y=1)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Values
    q2 = np.zeros((2, 2))   # stage-2 values for each planet and alien
    q1_mf = np.zeros(2)     # model-free stage-1 values

    # Choice likelihoods
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated planning weight
    w_eff = np.clip(w_pln0 - anx_trn * stai_val, 0.0, 1.0)

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2 values
        max_q2 = np.max(q2, axis=1)            # best alien per planet
        q1_mb = transition_matrix @ max_q2     # MB spaceship values

        # Hybrid Q for stage 1
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy (on reached planet)
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2_t = action_2[t]
        p_choice_2[t] = probs_2[a2_t]

        # Outcome
        r = reward[t]

        # Update stage-2 value
        delta2 = r - q2[s, a2_t]
        q2[s, a2_t] += a2 * delta2

        # Transition surprise for the experienced transition on this trial
        # P_common given chosen spaceship a1 and planet s
        p_trans = transition_matrix[a1, s]
        # surprise is higher for rare transitions (0.7 for rare, 0.3 for common)
        surprise = 1.0 - p_trans
        # Scale surprise into [0,1] with common->0 and rare->1
        surprise_scaled = (surprise - 0.3) / (0.7 - 0.3)

        # Anxiety boosts MF credit assignment after rare transitions
        chi_eff = chi * (1.0 + anx_trn * surprise_scaled)
        chi_eff = np.clip(chi_eff, 0.0, 1.0)

        # Stage-1 MF bootstrapping
        delta1 = (q2[s, a2_t]) - q1_mf[a1]
        q1_mf[a1] += chi_eff * delta1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Directed uncertainty exploration at stage-2 and anxiety-weighted pessimism at stage-1.

    Idea:
    - Stage-2 values learned with a delta-rule. A directed exploration bonus encourages sampling uncertain aliens.
    - The exploration bonus shrinks with higher anxiety.
    - Stage-1 model-based values are computed from stage-2, but are pessimistically attenuated in proportion to anxiety.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = u_base (0 to 1): baseline magnitude of uncertainty bonus at stage-2
    - model_parameters[3] = anx_unc (0 to 1): anxiety sensitivity for exploration bonus (higher stai reduces bonus)
    - model_parameters[4] = rho_pess (0 to 1): maximum pessimism weight applied to stage-1 values, scaled by stai

    Inputs:
    - action_1: array-like ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like floats, received coins per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, u_base, anx_unc, rho_pess = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))          # stage-2 values
    n_sa = np.zeros((2, 2))        # visit counts for uncertainty estimates

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated exploration magnitude and pessimism
    u_eff = u_base * (1.0 - anx_unc * stai_val)         # less directed exploration with higher anxiety
    u_eff = max(0.0, u_eff)
    rho_eff = np.clip(rho_pess * stai_val, 0.0, 1.0)    # more pessimism with higher anxiety

    for t in range(n_trials):
        # Stage-1 values from MB planning over pessimistically transformed stage-2
        max_q2 = np.max(q2, axis=1)           # best alien per planet
        # Pessimism shrinks values toward 0 (minimum attainable reward)
        v_planet = (1.0 - rho_eff) * max_q2   # min reward assumed 0
        q1_mb = transition_matrix @ v_planet

        # Stage-1 policy
        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy with directed uncertainty bonus
        s = state[t]
        # Uncertainty proxy per alien in current state
        unc = 1.0 / np.sqrt(n_sa[s] + 1.0)    # higher for less-sampled actions
        q2_net = q2[s] + u_eff * unc
        q2c = q2_net - np.max(q2_net)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        a2_t = action_2[t]
        p_choice_2[t] = probs_2[a2_t]

        # Outcome and learning
        r = reward[t]
        delta2 = r - q2[s, a2_t]
        q2[s, a2_t] += alpha * delta2
        n_sa[s, a2_t] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based control with anxiety- and surprise-modulated lapse rate.

    Idea:
    - Stage-2 values learned via delta-rule.
    - Both stages use model-based action values, but choices are corrupted by a lapse process.
    - Lapse probability increases with anxiety and with recent surprise (unsigned reward prediction error magnitude).

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax component at both stages
    - model_parameters[2] = z_base (0 to 1): baseline lapse probability
    - model_parameters[3] = anx_gain (0 to 1): sensitivity of lapse to anxiety (multiplies stai)
    - model_parameters[4] = kappa_pe (0 to 1): sensitivity of lapse to unsigned reward PE from previous trial

    Inputs:
    - action_1: array-like ints in {0,1}, chosen spaceship (0=A, 1=U) per trial
    - state: array-like ints in {0,1}, reached planet (0=X, 1=Y) per trial
    - action_2: array-like ints in {0,1}, chosen alien on reached planet per trial
    - reward: array-like floats, received coins per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified above

    Returns:
    - Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta, z_base, anx_gain, kappa_pe = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # stage-2 values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track previous unsigned PE to modulate lapse
    prev_abs_pe = 0.0

    for t in range(n_trials):
        # Model-based stage-1 values from current stage-2
        max_q2 = np.max(q2, axis=1)
        q1_mb = transition_matrix @ max_q2

        # Compute dynamic lapse probability
        eps_t = z_base + anx_gain * stai_val + kappa_pe * prev_abs_pe
        eps_t = float(np.clip(eps_t, 0.0, 1.0))

        # Stage-1 policy: mixture of softmax and uniform due to lapse
        q1c = q1_mb - np.max(q1_mb)
        sm1 = np.exp(beta * q1c)
        sm1 = sm1 / np.sum(sm1)
        probs_1 = (1.0 - eps_t) * sm1 + eps_t * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: mixture of softmax and uniform due to lapse
        s = state[t]
        q2c = q2[s] - np.max(q2[s])
        sm2 = np.exp(beta * q2c)
        sm2 = sm2 / np.sum(sm2)
        probs_2 = (1.0 - eps_t) * sm2 + eps_t * 0.5
        a2_t = action_2[t]
        p_choice_2[t] = probs_2[a2_t]

        # Outcome and learning at stage-2
        r = reward[t]
        delta2 = r - q2[s, a2_t]
        q2[s, a2_t] += alpha * delta2

        # Update surprise signal for next trial's lapse modulation
        prev_abs_pe = min(1.0, abs(delta2))  # rewards are in [0,1], keep within [0,1]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll