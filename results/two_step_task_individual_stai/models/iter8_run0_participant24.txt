Below are three standalone cognitive models as Python functions. Each returns the negative log-likelihood of the observed choices and uses the participantâ€™s STAI score meaningfully. No imports are included (assume numpy as np is already imported). Each model uses at most five parameters total (including alpha and beta), and the parameter combinations are distinct from those you tried before.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with anxiety-reduced model-based control and MF eligibility.
    
    Summary
    - Stage-1 choice values are a mixture of model-based and model-free values.
    - Second-stage values are learned model-free from rewards.
    - An eligibility-like parameter propagates second-stage value to stage-1 MF values.
    - Anxiety reduces the model-based mixture weight linearly.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (in visited state; X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received rewards per trial (e.g., 0/1).
    stai : array-like of float in [0,1]
        Trait anxiety score; uses stai[0].
    model_parameters : array-like
        [alpha, beta, pi_mb, xi_anx, trace]
        - alpha in [0,1]: learning rate for MF value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - pi_mb in [0,1]: baseline model-based mixture weight.
        - xi_anx in [0,1]: degree to which anxiety reduces model-based control.
        - trace in [0,1]: eligibility-like weight for propagating second-stage value to stage-1 MF.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, pi_mb, xi_anx, trace = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure: A->X common (0.7), U->Y common (0.7)
    T = np.array([[0.7, 0.3],  # from A to (X,Y)
                  [0.3, 0.7]]) # from U to (X,Y)

    # Initialize values
    q1_mf = np.zeros(2)        # stage-1 MF action values
    q2 = np.zeros((2, 2))      # stage-2 MF action values by state
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced model-based weight
    w_mb = pi_mb * (1.0 - xi_anx * s)
    w_mb = min(1.0, max(0.0, w_mb))

    for t in range(n_trials):
        # Model-based Q at stage 1: transition-expected max Q at stage 2
        max_q2 = np.max(q2, axis=1)  # max over actions per state
        q1_mb = T @ max_q2

        # Mixture policy at stage 1
        q1_mix = (1.0 - w_mb) * q1_mf + w_mb * q1_mb
        prefs1 = q1_mix
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in visited state
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Update stage-2 MF values
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Update stage-1 MF with an eligibility-like backup from the realized second-stage value
        backed_value = q2[st, a2]
        pe1 = backed_value - q1_mf[a1]
        q1_mf[a1] += alpha * trace * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive-MF with anxiety-dampened transition-based heuristic bias and volatility-gated learning.
    
    Summary
    - Pure model-free SARSA(0) values at both stages.
    - Learning rate adapts to an online estimate of outcome volatility (EWMA of |PE|).
    - First-stage policy includes a heuristic transition-based bias (stay after reward+common;
      switch after reward+rare), whose strength is reduced by anxiety.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (in visited state).
    reward : array-like of float
        Received rewards per trial (e.g., 0/1).
    stai : array-like of float in [0,1]
        Trait anxiety score; uses stai[0].
    model_parameters : array-like
        [mu0, beta, phi_vol, tau_v, zeta_tr]
        - mu0 in [0,1]: baseline learning rate for MF updates.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - phi_vol in [0,1]: gain on volatility to increase learning rate.
        - tau_v in [0,1]: EWMA smoothing for volatility (higher -> faster tracking).
        - zeta_tr in [0,1]: magnitude of transition-based heuristic bias at stage 1.
          Effective bias is zeta_tr*(1 - stai), so anxiety dampens the heuristic.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    mu0, beta, phi_vol, tau_v, zeta_tr = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Volatility tracker (EWMA of absolute PE at stage 2)
    vol = 0.0

    # For heuristic bias we need previous trial info
    prev_a1 = None
    prev_st = None
    prev_r = None

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # First-stage heuristic transition-based bias
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            # Common if action matches state index under the canonical mapping
            was_common = (prev_a1 == prev_st)  # A->X (0->0), U->Y (1->1) is common
            b = zeta_tr * (1.0 - s)
            if prev_r is not None and prev_r >= 0.5:
                if was_common:
                    # Reward + common: bias to repeat previous action
                    bias1[prev_a1] += b
                else:
                    # Reward + rare: bias to switch (favor the other action)
                    bias1[1 - prev_a1] += b
            else:
                # If not rewarded, apply the opposite (mild) heuristic: avoid previous mapping
                # Here we attenuate by 0.5 to keep it subtle
                b2 = 0.5 * b
                if was_common:
                    bias1[1 - prev_a1] += b2
                else:
                    bias1[prev_a1] += b2

        # Stage 1 policy
        prefs1 = q1 + bias1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in visited state
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning with adaptive learning rate based on volatility
        r = reward[t]
        pe2 = r - q2[st, a2]
        vol = (1.0 - tau_v) * vol + tau_v * abs(pe2)
        eff_alpha = mu0 + phi_vol * vol * (0.5 + 0.5 * s)  # anxiety amplifies volatility gating
        eff_alpha = max(0.0, min(1.0, eff_alpha))

        # Update stage 2
        q2[st, a2] += eff_alpha * pe2

        # Update stage 1 via SARSA-style bootstrap from realized second-stage value
        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += eff_alpha * pe1

        # Carry previous info
        prev_a1 = a1
        prev_st = st
        prev_r = r

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-representation (SR) enhanced planning with anxiety-shortened horizon and MF critic.
    
    Summary
    - Learns second-stage MF values from reward.
    - Learns a first-stage successor map over second-stage states (SR over states given action).
    - Converts SR into action values by multiplying with the current state values (max over actions).
    - Mixes SR-based values with MF first-stage values.
    - Anxiety shortens the effective planning horizon by reducing the discount factor in SR.
    
    Parameters (bounds)
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (in visited state).
    reward : array-like of float
        Received rewards per trial (e.g., 0/1).
    stai : array-like of float in [0,1]
        Trait anxiety score; uses stai[0].
    model_parameters : array-like
        [alpha_v, beta, disc0, chi_stai, kappa_mix]
        - alpha_v in [0,1]: learning rate for both MF critic and SR map.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - disc0 in [0,1]: baseline discount factor used in SR updates.
        - chi_stai in [0,1]: how much anxiety reduces the discount (shorter horizon).
          Effective discount is disc = disc0 * (1 - chi_stai * stai).
        - kappa_mix in [0,1]: mixture weight on SR-derived Q at stage 1
          (1-kappa_mix) is the weight on MF stage-1 Q.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_v, beta, disc0, chi_stai, kappa_mix = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Values
    q1_mf = np.zeros(2)    # MF first-stage values
    q2 = np.zeros((2, 2))  # MF second-stage values

    # SR: mapping from first-stage actions -> discounted occupancy of second-stage states
    # M[a] is a length-2 vector over states [X, Y]
    M = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Effective discount reduced by anxiety
    disc = disc0 * (1.0 - chi_stai * s)
    disc = min(1.0, max(0.0, disc))

    for t in range(n_trials):
        # Compute SR-based Q for stage 1 from current critic of states
        V_states = np.max(q2, axis=1)  # value of each second-stage state as max over its actions
        q1_sr = M @ V_states

        # Mix SR-based with MF values for stage 1
        q1_mix = (1.0 - kappa_mix) * q1_mf + kappa_mix * q1_sr

        # Stage 1 policy
        prefs1 = q1_mix
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy in visited state
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2 (MF critic)
        r = reward[t]
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha_v * pe2

        # Update SR for the chosen stage-1 action toward the observed next-state occupancy
        # Here the SR target is: one-hot(next_state) + disc * 0 (terminal), so effectively learning
        # the (discounted) transition probabilities with discount shaping.
        onehot_st = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        target = onehot_st  # terminal after second stage
        pe_sr = target - M[a1]
        M[a1] += alpha_v * pe_sr  # TD(0) on SR given terminal at second stage

        # Update MF first-stage value toward realized second-stage action value
        backed = q2[st, a2]
        pe1 = backed - q1_mf[a1]
        q1_mf[a1] += alpha_v * pe1

        # Optional: very small discount leakage (if disc<1, encourage sharper mapping)
        # Use discount implicitly by shrinking off-diagonal mass each trial
        if disc < 1.0:
            M[a1] = disc * M[a1] + (1.0 - disc) * target

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll