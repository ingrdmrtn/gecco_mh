def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Counterfactual second-stage learning with anxiety-damped transition confidence.

    Idea
    - First-stage decisions are model-based using a fixed canonical transition (0.7 common),
      but the agent's confidence in this model is imperfect and is modulated by anxiety.
      Lower confidence blends the transition toward an uninformative 0.5/0.5 mapping.
    - Second-stage values are learned model-free with both factual and counterfactual
      (fictive) updates. The counterfactual update strength increases with anxiety.

    Parameters (all floats)
    - alpha: [0,1] learning rate for MF values (both stages)
    - beta:  [0,10] inverse temperature for both stages
    - trans_conf: [0,1] baseline confidence in the canonical transition structure
                  (1.0 = fully trust 0.7/0.3; 0.0 = assume 0.5/0.5)
    - cf_rate: [0,1] baseline rate of counterfactual update to the unchosen second-stage action
    - anx_cf_gain: [0,1] how much anxiety amplifies counterfactual learning and reduces
                   transition confidence

    Inputs
    - action_1: array-like of ints {0,1} (0=A, 1=U)
    - state:    array-like of ints {0,1} (0=X, 1=Y)
    - action_2: array-like of ints {0,1} within reached state
    - reward:   array-like of floats in [0,1]
    - stai:     array-like with one float in [0,1] (anxiety score)
    - model_parameters: iterable of 5 floats as above

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, trans_conf, cf_rate, anx_cf_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Canonical transition: A->X common, U->Y common
    base_T = np.array([[0.7, 0.3],  # A to [X, Y]
                       [0.3, 0.7]])  # U to [X, Y]

    # Anxiety reduces confidence in the canonical transition; blend toward 0.5/0.5
    conf_eff = max(0.0, min(1.0, trans_conf * (1.0 - 0.6 * anx_cf_gain * stai_val)))
    T = 0.5 + conf_eff * (base_T - 0.5)

    # Anxiety amplifies counterfactual learning
    cf_eff = max(0.0, min(1.0, cf_rate * (1.0 + anx_cf_gain * stai_val)))

    # Action values
    q1 = np.zeros(2)         # stage-1 MF cache (bootstrapped from Q2)
    q2 = np.zeros((2, 2))    # stage-2 MF values: state in {X=0,Y=1}, action in {0,1}

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Model-based evaluation for stage 1 using current Q2 and (anxiety-damped) transition
        max_q2 = np.max(q2, axis=1)         # shape (2,)
        q1_mb = T @ max_q2                  # shape (2,)

        # Combine MB evaluation with MF cache (simple average that is implicitly learned through q1)
        # Use purely MB for policy but include MF q1 in learning target below.
        logits1 = beta * (q1_mb - np.max(q1_mb))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]
        # Stage-2 policy
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 learning: factual
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-2 counterfactual learning: unchosen action in same state moves toward 1 - r
        a2_alt = 1 - a2
        pe2_cf = (1.0 - r) - q2[s, a2_alt]
        q2[s, a2_alt] += cf_eff * pe2_cf

        # Stage-1 MF bootstrapping from current second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free first-stage with reward-contingent recency bias gated by anxiety.

    Idea
    - Stage-2 values are learned model-free.
    - Stage-1 choice is primarily model-free (bootstrapped from Q2) but is influenced by a
      reward-contingent recency bias that depends on whether the last transition was common
      or rare. Anxiety strengthens the recency bias and speeds its accumulation.
      This produces stronger "win-stay/lose-switch" after common transitions (and the
      opposite after rare), modulated by anxiety.

    Parameters (all floats)
    - alpha: [0,1] learning rate for MF values (both stages)
    - beta:  [0,10] inverse temperature for both stages
    - recency_rate: [0,1] update/decay rate for the first-stage recency bias
    - common_gain:  [0,1] scales how much a rewarded common transition increases bias
                    relative to rare transitions (signed)
    - anx_recency_gain: [0,1] how strongly anxiety amplifies the recency learning rate
                        and its impact on choice

    Inputs
    - action_1: array-like of ints {0,1} (0=A, 1=U)
    - state:    array-like of ints {0,1} (0=X, 1=Y)
    - action_2: array-like of ints {0,1} within reached state
    - reward:   array-like of floats in [0,1]
    - stai:     array-like with one float in [0,1] (anxiety score)
    - model_parameters: iterable of 5 floats as above

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, recency_rate, common_gain, anx_recency_gain = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Stage values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # First-stage recency bias per action (dynamic)
    b1 = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Effective recency learning rate and bias strength grow with anxiety
    rec_lr = max(0.0, min(1.0, recency_rate * (1.0 + anx_recency_gain * stai_val)))
    bias_strength = (0.5 + 0.5 * anx_recency_gain * stai_val)  # in [0.5,1]

    for t in range(n_trials):
        # Stage-1 policy: MF q1 plus recency bias
        logits1 = q1 + bias_strength * b1
        logits1 = beta * (logits1 - np.max(logits1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Stage-2 policy: MF q2
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Stage-2 MF learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Stage-1 MF bootstrapping from the obtained second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update recency bias: decay then add signed increment based on reward and transition type
        # Determine if the observed transition was common (A->X or U->Y)
        is_common = 1.0 if ((a1 == 0 and s == 0) or (a1 == 1 and s == 1)) else 0.0
        sign = (2.0 * is_common - 1.0)  # +1 for common, -1 for rare
        # Reward-contingent increment: rewarded common -> positive toward chosen; rewarded rare -> negative
        inc = r * sign * (2.0 * common_gain - 1.0)

        # Decay and update bias
        b1 = (1.0 - rec_lr) * b1
        b1[a1] += rec_lr * inc
        # Also apply a small symmetric opposite update to the unchosen action to keep biases centered
        b1[1 - a1] -= rec_lr * inc * 0.5

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Transition-uncertainty-driven exploration at stage 1 with anxiety-suppressed optimism.

    Idea
    - The agent learns second-stage reward values model-free.
    - Simultaneously, it keeps simple Dirichlet-like counts of the transition mapping for
      each first-stage action to estimate p(X|A) and p(X|U).
    - Stage-1 evaluation is model-based using these learned transition probabilities.
      Additionally, an exploration bonus encourages sampling actions whose transition
      probabilities are more uncertain (close to 0.5). Anxiety suppresses this optimism.
    - Stage-2 choice includes a mild choice-trace kernel with its own learning/decay rate.

    Parameters (all floats)
    - alpha: [0,1] learning rate for MF values (both stages)
    - beta:  [0,10] inverse temperature for both stages
    - optimism: [0,1] base strength of the transition-uncertainty exploration bonus at stage 1
    - trace_lr: [0,1] learning/decay rate for the stage-2 choice trace kernel
    - anx_explore: [0,1] how strongly anxiety suppresses the optimism bonus and weakens the trace

    Inputs
    - action_1: array-like of ints {0,1} (0=A, 1=U)
    - state:    array-like of ints {0,1} (0=X, 1=Y)
    - action_2: array-like of ints {0,1} within reached state
    - reward:   array-like of floats in [0,1]
    - stai:     array-like with one float in [0,1] (anxiety score)
    - model_parameters: iterable of 5 floats as above

    Returns
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, optimism, trace_lr, anx_explore = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Second-stage MF values and choice trace
    q2 = np.zeros((2, 2))
    k2 = np.zeros((2, 2))  # choice trace at stage 2

    # First-stage transition counts: for each action a in {0,1}, counts of reaching X and Y
    # Start with symmetric pseudocounts (1,1) for stability
    trans_counts = np.ones((2, 2))  # rows: action (A,U), cols: state (X,Y)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety suppresses optimism and weakens choice trace usage
    opt_eff = max(0.0, optimism * (1.0 - 0.8 * anx_explore * stai_val))
    trace_eff = max(0.0, min(1.0, trace_lr * (1.0 - 0.5 * anx_explore * stai_val)))

    for t in range(n_trials):

        # Estimate transition probabilities from counts
        p_X_given_A = trans_counts[0, 0] / np.sum(trans_counts[0])
        p_X_given_U = trans_counts[1, 0] / np.sum(trans_counts[1])
        pX = np.array([p_X_given_A, p_X_given_U])  # shape (2,)
        pY = 1.0 - pX

        # Model-based Q for stage 1 using learned transitions and current Q2
        max_q2 = np.max(q2, axis=1)  # [Q(X,*).max, Q(Y,*).max]
        q1_mb = pX * max_q2[0] + pY * max_q2[1]

        # Exploration bonus based on transition uncertainty (max at p=0.5)
        # bonus = 1 - |p - 0.5|, in [0.5,1]; center to [0,1] by subtracting 0.5 and scaling
        trans_unc = 1.0 - np.abs(pX - 0.5)               # in [0.5,1]
        bonus = opt_eff * (2.0 * (trans_unc - 0.5))      # in [0, opt_eff]

        logits1 = beta * (q1_mb + bonus - np.max(q1_mb + bonus))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        s = state[t]

        # Stage-2 policy includes a small choice-trace kernel
        logits2 = q2[s] + trace_eff * (2.0 * k2[s] - 1.0)
        logits2 = beta * (logits2 - np.max(logits2))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Update second-stage values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update stage-2 choice trace: decay within state, reinforce chosen action
        k2[s] = (1.0 - trace_eff) * k2[s]
        k2[s, a2] += trace_eff

        # Update transition counts based on observed transition for the chosen first-stage action
        # Increment the count of the reached state for the chosen action
        trans_counts[a1, s] += 1.0

    neg_ll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(neg_ll)