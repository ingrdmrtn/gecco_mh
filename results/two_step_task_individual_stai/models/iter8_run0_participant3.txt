def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated surprise-driven switching with learned transitions (pure model-based at stage-1).

    Mechanism:
    - Stage-2 values Q2(s2, a2) learned with a single learning rate lr_q.
    - Transition model T(a1 -> s2) is learned with learning rate lr_T (row-wise exponential recency-weighting).
    - Stage-1 uses model-based values: Q1_MB(a1) = sum_s2 T[a1, s2] * max_a2 Q2[s2, a2].
    - Perseveration (stay bias) k_stay at both stages; after a rare transition on the previous trial,
      the effective stage-1 stay bias is reduced to encourage switching. This reduction is amplified by anxiety.
    - Stage-2 perseveration is attenuated by anxiety (higher anxiety -> less stickiness).

    Parameters and bounds:
    - model_parameters = (lr_q, beta, k_stay, b_switch, lr_T)
        lr_q    in [0,1]: learning rate for Q2
        beta    in [0,10]: inverse temperature for softmax at both stages
        k_stay  in [0,1]: baseline perseveration (stay) strength
        b_switch in [0,1]: strength by which a previous rare transition reduces stay at stage-1; scaled by anxiety
        lr_T    in [0,1]: learning rate for transition model T

    Inputs:
    - action_1: int array of shape (n_trials,), choices at stage-1 (0=A, 1=U)
    - state:    int array of shape (n_trials,), reached second-stage planet (0=X, 1=Y)
    - action_2: int array of shape (n_trials,), choices at stage-2 (0 or 1)
    - reward:   float array of shape (n_trials,), obtained reward in [0,1]
    - stai:     float array with one element in [0,1], anxiety score
    - model_parameters: tuple/list as defined above

    Returns:
    - Negative log-likelihood of the observed stage-1 and stage-2 choices.
    """
    lr_q, beta, k_stay, b_switch, lr_T = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Initialize values and transition model
    q2 = np.zeros((2, 2), dtype=float)        # Q2[s2, a2]
    T = np.array([[0.7, 0.3],                 # start from instructed/common structure
                  [0.3, 0.7]] , dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = -1
    prev_rare = 0.0  # indicator for whether previous transition was rare under prior T

    for t in range(n_trials):
        # Compute model-based Q at stage-1
        max_q2 = np.max(q2, axis=1)         # value of each second-stage state
        q1_mb = T @ max_q2                  # shape (2,)

        # Stage-1 perseveration with anxiety-modulated surprise-driven switching
        stick1 = np.zeros(2, dtype=float)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # If last transition was rare, reduce stay bias; anxiety amplifies this reduction
        switch_amp = b_switch * (0.5 + 0.5 * anx)   # in [0,1]
        k1_eff = k_stay * (1.0 - switch_amp * prev_rare)

        logits1 = beta * q1_mb + k1_eff * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-attenuated perseveration
        s2 = int(state[t])
        stick2 = np.zeros(2, dtype=float)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k_stay * (1.0 - 0.7 * anx)  # higher anxiety -> less stickiness at stage-2
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = float(reward[t])

        # Determine whether current transition was rare under the pre-update T
        # Rare if probability of observed state given chosen action is less than 0.5
        prev_rare = 1.0 if T[a1, s2] < 0.5 else 0.0

        # Update transition model T for the chosen action row towards the observed state
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        T[a1] = (1.0 - lr_T) * T[a1] + lr_T * onehot_s
        # Renormalize row to guard against numerical drift
        T[a1] = T[a1] / (np.sum(T[a1]) + eps)

        # Update stage-2 Q
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += lr_q * delta2

        # Persist choices for perseveration
        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-inflated epsilon-exploration and risk-sensitive utility.

    Mechanism:
    - Stage-2 utility uses a concave power transform u(r) = r^gamma, with gamma decreasing as anxiety rises.
    - Stage-2 Q2(s2, a2) is learned via TD on utility u(r).
    - Stage-1 combines model-based (MB) values from a fixed transition structure (0.7/0.3) and
      model-free (MF) values Q1_MF updated by bootstrapping from the reached second-stage value.
    - Mixture weight omega is reduced by anxiety (higher anxiety -> less MB).
    - Policies at both stages use softmax with inverse temperature beta, blended with epsilon-greedy;
      epsilon increases with anxiety, promoting more random exploration.

    Parameters and bounds:
    - model_parameters = (lr_q, beta, omega_base, nu_u, eps0)
        lr_q       in [0,1]: learning rate for Q2 and Q1_MF
        beta       in [0,10]: inverse temperature
        omega_base in [0,1]: baseline MB weight at stage-1
        nu_u       in [0,1]: controls utility concavity; gamma = 0.5 + 0.5*nu_u*(1 - stai)
        eps0       in [0,1]: baseline epsilon; epsilon_eff = eps0 * (0.5 + 0.5*stai)

    Inputs:
    - action_1, state, action_2, reward: arrays per trial (see task)
    - stai: array with one element in [0,1]
    - model_parameters: tuple/list as defined above

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_q, beta, omega_base, nu_u, eps0 = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Fixed transition structure as instructed/common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2), dtype=float)  # Q2[s2, a2]
    q1_mf = np.zeros(2, dtype=float)    # model-free Q at stage-1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Anxiety-modulated parameters
    omega = float(np.clip(omega_base * (1.0 - 0.5 * anx), 0.0, 1.0))
    epsilon_eff = float(np.clip(eps0 * (0.5 + 0.5 * anx), 0.0, 1.0))
    gamma_util = 0.5 + 0.5 * nu_u * (1.0 - anx)   # in [0.5, 1.0]

    for t in range(n_trials):
        # MB component at stage-1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        q1 = omega * q1_mb + (1.0 - omega) * q1_mf

        # Stage-1 policy: epsilon-softmax
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        soft1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        probs1 = (1.0 - epsilon_eff) * soft1 + 0.5 * epsilon_eff
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: epsilon-softmax on Q2
        s2 = int(state[t])
        logits2 = beta * q2[s2]
        logits2 -= np.max(logits2)
        soft2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        probs2 = (1.0 - epsilon_eff) * soft2 + 0.5 * epsilon_eff
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome with risk-sensitive utility
        r = float(reward[t])
        u = (r + eps) ** gamma_util

        # Update Q2
        delta2 = u - q2[s2, a2]
        q2[s2, a2] += lr_q * delta2

        # Update Q1_MF toward the reached state's value (bootstrapping on max Q2)
        target1 = np.max(q2[s2])
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += lr_q * delta1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-like first-stage representation with anxiety-modulated discount and forgetting.

    Mechanism:
    - Maintain an action-conditioned state-occupancy map M[a1, s2] (successor-like kernel).
      For this two-step task, M approximates the transition structure but is allowed to decay/forget.
    - Stage-1 model-based values use M to project the best available second-stage values: Q1(a1) = sum_s M[a1,s]*V[s],
      where V[s] = max_a Q2[s,a].
    - Stage-2 Q2(s2, a2) learned by TD with a single learning rate.
    - Anxiety shortens the effective planning horizon via a lower discount (gamma) and increases forgetting.
    - Perseveration (stickiness) at both stages; stage-2 stickiness is reduced by anxiety.

    Parameters and bounds:
    - model_parameters = (lr_q, beta, gamma0, f0, k_stick)
        lr_q    in [0,1]: learning rate for Q2 and for updating M toward observed occupancy
        beta    in [0,10]: inverse temperature
        gamma0  in [0,1]: baseline discount controlling how sharply M focuses on immediate outcomes
        f0      in [0,1]: baseline forgetting; higher means faster decay of M each trial
        k_stick in [0,1]: perseveration strength (applied to both stages; attenuated at stage-2 by anxiety)

    Inputs:
    - action_1, state, action_2, reward: arrays per trial (see task)
    - stai: array with one element in [0,1]
    - model_parameters: tuple/list as defined above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_q, beta, gamma0, f0, k_stick = model_parameters
    n_trials = len(action_1)
    anx = float(stai[0])

    # Initialize successor-like kernel M and Q2
    # Start from instructed/common structure
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)
    q2 = np.zeros((2, 2), dtype=float)

    # Anxiety-modulated planning discount and forgetting
    gamma_eff = float(np.clip(gamma0 * (1.0 - 0.5 * anx), 0.0, 1.0))
    forget_eff = float(np.clip(f0 * (0.5 + 0.5 * anx), 0.0, 1.0))

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Value projection using M
        V = np.max(q2, axis=1)         # V[s2]
        q1 = M @ V                     # stage-1 values

        # Stage-1 perseveration
        stick1 = np.zeros(2, dtype=float)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0
        logits1 = beta * q1 + k_stick * stick1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with anxiety-attenuated perseveration
        s2 = int(state[t])
        stick2 = np.zeros(2, dtype=float)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k_stick * (1.0 - 0.7 * anx)
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = float(reward[t])

        # Update Q2
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += lr_q * delta2

        # Update successor-like kernel M for the chosen action:
        # 1) Global forgetting toward zero (blunting old associations), stronger with anxiety
        M *= (1.0 - forget_eff)

        # 2) Move the chosen action's row toward the observed state occupancy,
        #    with an immediacy emphasis set by (1 - gamma_eff).
        onehot_s = np.array([1.0 if i == s2 else 0.0 for i in range(2)], dtype=float)
        target_row = (1.0 - gamma_eff) * onehot_s + gamma_eff * M[a1]  # retain some of prior structure
        M[a1] = (1.0 - lr_q) * M[a1] + lr_q * target_row

        # Renormalize rows to keep a proper weighting over states (avoid collapse)
        row_sums = np.sum(M, axis=1, keepdims=True) + eps
        M = M / row_sums

        prev_a1 = a1
        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)