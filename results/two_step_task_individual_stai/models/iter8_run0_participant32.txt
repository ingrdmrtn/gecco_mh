def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Hybrid MBâ€“MF with anxiety-modulated transition uncertainty and eligibility-trace credit assignment.

    Core mechanisms
    - Second-stage model-free Q-learning with learning rate eta_q.
    - First-stage hybrid policy: convex combination of learned model-based values (from a learned transition model)
      and model-free Q1 values. Anxiety reduces MB reliance and increases transition uncertainty.
    - Transition model is learned online and simultaneously "blurred" toward 0.5 by anxiety (uncertainty inflation).
    - Eligibility-trace credit assignment from stage 2 to stage 1 controlled by lambda_elig.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=A, 1=U) per trial.
    state : array-like of int {0,1}
        Observed second-stage state (0=X, 1=Y) per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1 for alien) per trial.
    reward : array-like of float
        Reward per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1]-like range (as provided).
    model_parameters : list or array-like of float
        [eta_q, beta, zeta_mb, xi_anxTrans, lambda_elig]
        Bounds:
        - eta_q: [0,1] learning rate for Q-updates.
        - beta: [0,10] inverse temperature for both stages.
        - zeta_mb: [0,1] baseline weight on model-based valuation at stage 1.
        - xi_anxTrans: [0,1] anxiety-sensitivity for transition uncertainty/learning.
        - lambda_elig: [0,1] eligibility-trace strength for credit assignment to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    eta_q, beta, zeta_mb, xi_anxTrans, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Initialize value functions
    q1_mf = np.zeros(2)          # model-free first-stage Q
    q2 = np.zeros((2, 2))        # second-stage Q: state x action

    # Initialize transition model T(a, s') with nominal common transitions (A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated parameters
    # Effective MB weight decreases with anxiety (less reliance on planning under higher anxiety)
    w_mb_eff = np.clip(zeta_mb * (1.0 - 0.6 * stai), 0.0, 1.0)
    # Anxiety increases uncertainty inflation of the learned transition model toward 0.5
    blur_strength = np.clip(xi_anxTrans * (0.3 + 0.7 * stai), 0.0, 1.0)
    # Transition learning rate also scales with anxiety (learn faster from observed transitions)
    tau_T = np.clip(0.2 + 0.6 * xi_anxTrans * stai, 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 policy (hybrid MB + MF)
        max_q2 = np.max(q2, axis=1)           # value of each second-stage state
        q1_mb = T @ max_q2                    # model-based action values
        q1_hyb = (1.0 - w_mb_eff) * q1_mf + w_mb_eff * q1_mb

        # Softmax for stage 1
        logits1 = beta * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        q2_s = q2[s].copy()
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Second-stage TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_q * pe2

        # First-stage credit assignment with eligibility trace:
        # Combine direct backpropagation of stage-2 PE and alignment toward the best value in the reached state.
        target1 = (lambda_elig * r) + ((1.0 - lambda_elig) * np.max(q2[s]))
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_q * pe1

        # Transition model update toward observed state
        # Move chosen action's transition row toward one-hot on observed state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] = (1.0 - tau_T) * T[a1, s_idx] + tau_T * target

        # Anxiety-driven uncertainty inflation (blurring rows toward 0.5)
        T[a1, :] = (1.0 - blur_strength) * T[a1, :] + blur_strength * 0.5
        # Renormalize to guard against drift
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Successor-representation (SR) stage-1 valuation with anxiety-modulated SR reliance and lapse.

    Core mechanisms
    - Learn a one-step SR row for each first-stage action (effectively the transition distribution),
      and value actions by SR dot second-stage state values (max Q2).
    - Combine SR-based values with model-free Q1 via a weight that decreases with anxiety.
    - Anxiety increases a small choice lapse (stimulus-independent noise).
    - Second-stage choices are standard softmax on Q2.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward obtained each trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [mu, beta, theta_sr, z_anxShift, eps_lapse]
        Bounds:
        - mu: [0,1] learning rate for Q and SR updates.
        - beta: [0,10] inverse temperature for both stages.
        - theta_sr: [0,1] baseline weight on SR-based valuation at stage 1.
        - z_anxShift: [0,1] scales how much anxiety reduces SR reliance.
        - eps_lapse: [0,1] baseline lapse; scaled up by anxiety.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    mu, beta, theta_sr, z_anxShift, eps_lapse = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    # SR over next-state occupancy for each action (rows sum to 1 initially)
    # Initialize with nominal common transitions
    M = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    w_sr = np.clip(theta_sr * (1.0 - z_anxShift * (0.5 + 0.5 * stai)), 0.0, 1.0)
    lapse = np.clip(eps_lapse * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Stage 1 SR-based value
        v_state = np.max(q2, axis=1)     # value of each second-stage state
        q1_sr = M @ v_state
        q1_hyb = (1.0 - w_sr) * q1_mf + w_sr * q1_sr

        # Softmax with lapse at stage 1
        logits1 = beta * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        probs1 = (1.0 - lapse) * probs1 + lapse * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (with the same lapse for parsimony)
        s = state[t]
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        probs2 = (1.0 - lapse) * probs2 + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += mu * pe2

        # First-stage MF learning toward the realized second-stage action value
        target1 = q2[s, a2]
        q1_mf[a1] += mu * (target1 - q1_mf[a1])

        # SR update for the chosen action towards the observed state one-hot
        e = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        M[a1, :] = (1.0 - mu) * M[a1, :] + mu * e
        # Renormalize for numerical stability
        M[a1, :] = M[a1, :] / np.sum(M[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Volatility-adaptive learning with anxiety-modulated exploration and MB mixing.

    Core mechanisms
    - Learning rates adapt online to surprise: unsigned second-stage PE and transition surprise increase alpha_t.
    - Anxiety reduces decisiveness (temperature scaling) and interacts with surprise sensitivity.
    - First-stage policy is a hybrid of MF Q1 and MB values using a fixed mixing parameter.
    - Transition model is learned and contributes to transition surprise.

    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial.
    state : array-like of int {0,1}
        Reached second-stage state per trial.
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Reward per trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha0, beta, k_vol, w_mb, psi_tempAnx]
        Bounds:
        - alpha0: [0,1] baseline learning rate.
        - beta: [0,10] base inverse temperature.
        - k_vol: [0,1] sensitivity of learning rate to surprise/volatility.
        - w_mb: [0,1] weight of model-based valuation at stage 1.
        - psi_tempAnx: [0,1] how strongly anxiety reduces inverse temperature.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, beta, k_vol, w_mb, psi_tempAnx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values and transitions
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-reduced decisiveness
    beta_eff = beta * np.clip(1.0 - psi_tempAnx * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        # Model-based action values
        v_state = np.max(q2, axis=1)
        q1_mb = T @ v_state
        q1_hyb = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage 1 choice
        logits1 = beta_eff * (q1_hyb - np.max(q1_hyb))
        probs1 = np.exp(logits1)
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 choice
        s = state[t]
        logits2 = beta_eff * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Surprise computations
        # Transition surprise: 1 - predicted probability of observed state
        trans_surprise = 1.0 - T[a1, s]
        # Reward PE (unsigned)
        pe2_unsigned = abs(r - q2[s, a2])

        # Volatility-adaptive learning rates (per trial)
        alpha2_t = np.clip(alpha0 + k_vol * (0.5 * pe2_unsigned + 0.5 * trans_surprise) * (0.5 + 0.5 * stai), 0.0, 1.0)
        alpha1_t = np.clip(0.5 * alpha0 + k_vol * abs(np.max(q2[s]) - q1_mf[a1]) * (0.5 + 0.5 * stai), 0.0, 1.0)

        # Update Q2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha2_t * pe2

        # Update Q1 toward the reached-state value
        target1 = q2[s, a2]
        q1_mf[a1] += alpha1_t * (target1 - q1_mf[a1])

        # Update transition model for chosen action
        tau_T = np.clip(0.1 + 0.8 * k_vol * (0.5 + 0.5 * stai), 0.0, 1.0)
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] = (1.0 - tau_T) * T[a1, s_idx] + tau_T * target
        # Normalize
        T[a1, :] = T[a1, :] / np.sum(T[a1, :])

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik