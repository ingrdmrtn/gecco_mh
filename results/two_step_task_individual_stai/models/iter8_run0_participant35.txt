def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """MF-MB hybrid with anxiety-damped directed exploration via value-uncertainty (UCB-like).

    This model learns mean and uncertainty (variance proxy) for each second-stage action and
    uses an uncertainty bonus to drive exploration at both stages. Anxiety reduces the directed
    exploration bonus, encouraging exploitation. First-stage choice values mix model-based
    and model-free components. Second-stage values are mean reward plus an exploration bonus.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_r: reward learning rate for Q2 in [0,1]
        beta: inverse temperature for both stages in [0,10]
        theta_mb: base MB weight for stage-1 mixing in [0,1]
        nu_var: process noise for uncertainty dynamics in [0,1] (bigger -> more uncertainty)
        xi_dir: base exploration bonus coefficient in [0,1]

    Bounds
    - alpha_r, theta_mb, nu_var, xi_dir in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Directed exploration bonus is reduced by anxiety:
        xi_eff = xi_dir * (1 - stai)
      Higher anxiety lowers uncertainty-directed exploration.
    - Model-based weight is modestly reduced by anxiety:
        w_mb = clip(theta_mb * (1 - 0.3*stai), 0, 1)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_r, beta, theta_mb, nu_var, xi_dir = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Fixed environment transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions A/U; cols: states X/Y

    # Value and uncertainty trackers
    q1_mf = np.zeros(2)           # MF first-stage cached values for A/U
    q2_mean = np.zeros((2, 2))    # mean value for each state-action
    q2_var = np.ones((2, 2)) * 0.25  # initialize moderate uncertainty

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated coefficients
    xi_eff = max(0.0, min(1.0, xi_dir * (1.0 - stai)))
    w_mb = max(0.0, min(1.0, theta_mb * (1.0 - 0.3 * stai)))

    for t in range(n_trials):
        # Construct exploration bonus at second stage
        bonus2 = xi_eff * np.sqrt(np.maximum(q2_var, 1e-12))  # state-action specific

        # Model-based forward value uses max over Q2 + bonus
        max_q2_bonus = np.max(q2_mean + bonus2, axis=1)  # size 2 for states X/Y
        q1_mb = T @ max_q2_bonus

        # Stage-1 decision values as MB/MF mix
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 softmax
        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with exploration bonus
        s = int(state[t])
        q2_dec = q2_mean[s] + bonus2[s]
        c_q2 = q2_dec - np.max(q2_dec)
        probs2 = np.exp(beta * c_q2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage mean with simple delta rule
        pe2 = r - q2_mean[s, a2]
        q2_mean[s, a2] += alpha_r * pe2

        # Update uncertainty: shrink when sampled, inflate globally with process noise
        # Simple variance proxy update: sampled arm uncertainty contracts, all arms diffuse
        # Contract sampled: (1 - alpha_r) factor; Diffuse: + nu_var * 0.05
        q2_var *= (1.0 + 0.0 * 0)  # no-op to emphasize array op
        q2_var[s, a2] = (1.0 - alpha_r) * q2_var[s, a2] + nu_var * 0.05
        # Unchosen action at the same state diffuses slightly
        other = 1 - a2
        q2_var[s, other] = q2_var[s, other] + nu_var * 0.02
        # Keep bounds
        q2_var = np.clip(q2_var, 1e-6, 1.0)

        # MF backup to stage-1 (bootstrapping from realized state-action value)
        target1 = q2_mean[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Learning transitions with anxiety-amplified rare-event sensitivity and MB/MF arbitration.

    This model learns the transition matrix online and uses it to compute model-based (MB)
    first-stage values, which are mixed with model-free (MF) values. When a rare transition
    is observed, anxiety increases the learning rate for that transition, making high-anxiety
    participants update their beliefs more strongly on surprising events.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: learning rate for Q-values in [0,1]
        beta: inverse temperature for both stages in [0,10]
        tau_T: base learning rate for transition probabilities in [0,1]
        delta_rare: extra learning-rate gain on rare transitions in [0,1]
        w_mix: base MB mixture weight in [0,1]

    Bounds
    - alpha_q, tau_T, delta_rare, w_mix in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Rare transition update boost:
        lr_T = tau_T * (1 + delta_rare * stai) if rare else tau_T
      Rarity defined relative to the canonical structure (A->X common, U->Y common).
    - MB weight reduced with anxiety: w_eff = clip(w_mix * (1 - 0.4*stai), 0, 1)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_q, beta, tau_T, delta_rare, w_mix = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Canonical environment (for rarity detection only)
    T_true = np.array([[0.7, 0.3],
                       [0.3, 0.7]], dtype=float)

    # Learned transition matrix: initialize neutral (0.5/0.5)
    T_hat = np.array([[0.5, 0.5],
                      [0.5, 0.5]], dtype=float)

    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    w_eff = max(0.0, min(1.0, w_mix * (1.0 - 0.4 * stai)))

    for t in range(n_trials):
        # MB stage-1 values via learned transitions and current q2
        max_q2 = np.max(q2, axis=1)         # best alien per planet
        q1_mb = T_hat @ max_q2              # propagate through learned transitions

        # Mix MB and MF for stage-1 choice
        q1 = w_eff * q1_mb + (1.0 - w_eff) * q1_mf

        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax over q2
        s = int(state[t])
        c_q2 = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * c_q2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Reward update at second stage
        r = reward[t]
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # MF backup to stage-1 from realized state-action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Transition learning update for chosen action row of T_hat
        # Determine if observed transition was rare under canonical structure
        is_rare = ((a1 == 0 and s == 1) or (a1 == 1 and s == 0))
        lr_T = tau_T * (1.0 + (delta_rare * stai if is_rare else 0.0))
        lr_T = max(0.0, min(1.0, lr_T))

        # For two states, keep row normalized by updating prob-to-X and setting prob-to-Y=1-p
        p_to_X = T_hat[a1, 0]
        target = 1.0 if s == 0 else 0.0
        p_to_X = (1.0 - lr_T) * p_to_X + lr_T * target
        T_hat[a1, 0] = p_to_X
        T_hat[a1, 1] = 1.0 - p_to_X

        # Slight inertia for the unchosen action row (no update)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with anxiety-enhanced WSLS at stage 2 and value leakage.

    This model uses:
    - Risk-sensitive utility at stage 2 with loss aversion that increases with anxiety.
    - A mixture of softmax and win-stay/lose-shift (WSLS) at stage 2; anxiety increases
      the WSLS weight, promoting heuristic repetition/switching.
    - A leak/forgetting term on Q-values to capture drift and reduced confidence.
    - Stage-1 values are MF bootstrapped from Q2; softmax inverse temperature decreases
      with anxiety (noisier choices).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U)
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y)
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1)
    - reward: array-like of floats in [0,1], reward outcome
    - stai: array-like length-1, scalar anxiety score in [0,1]
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: base learning rate for Q updates in [0,1]
        beta_base: base inverse temperature in [0,10]
        phi_leak: value leak/forgetting rate in [0,1] (applied each trial)
        zeta_wsls: base WSLS mixture weight in [0,1]
        nu_loss: base loss aversion coefficient in [0,1] (utility for negative outcomes)

    Bounds
    - alpha_q, phi_leak, zeta_wsls, nu_loss in [0,1]
    - beta_base in [0,10]

    Anxiety usage
    - Loss aversion increases with anxiety: nu_eff = nu_loss * (1 + stai)
    - WSLS weight increases with anxiety: w_wsls = clip(zeta_wsls * (0.5 + 0.5*stai), 0, 1)
    - Inverse temperature decreases with anxiety: beta = beta_base * (1 - 0.5*stai)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_q, beta_base, phi_leak, zeta_wsls, nu_loss = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective parameters after anxiety modulation
    beta = max(0.0, min(10.0, beta_base * (1.0 - 0.5 * stai)))
    w_wsls = max(0.0, min(1.0, zeta_wsls * (0.5 + 0.5 * stai)))
    nu_eff = max(0.0, min(2.0, nu_loss * (1.0 + stai)))  # cap at 2 to keep utilities reasonable
    leak = max(0.0, min(1.0, phi_leak))

    # Value functions
    q1 = np.zeros(2)         # MF first-stage
    q2 = np.zeros((2, 2))    # second-stage

    # For WSLS at stage 2, track previous action and reward sign per state
    prev_a2 = np.zeros(2, dtype=int)  # last chosen action for each state
    prev_sign = np.zeros(2)           # last reward sign (+1 / -1) for each state
    has_prev = np.zeros(2, dtype=bool)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Stage-1 softmax over MF q1
        c_q1 = q1 - np.max(q1)
        probs1 = np.exp(beta * c_q1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: mixture softmax(Q2) and WSLS heuristic within observed state
        s = int(state[t])
        c_q2 = q2[s] - np.max(q2[s])
        probs2_soft = np.exp(beta * c_q2)
        probs2_soft /= np.sum(probs2_soft)

        # WSLS policy vector for current state
        if has_prev[s]:
            if prev_sign[s] >= 0.0:
                # Win-stay: pick previous action
                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[prev_a2[s]] = 1.0
            else:
                # Lose-shift: choose the other action
                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[1 - prev_a2[s]] = 1.0
        else:
            wsls_probs = np.array([0.5, 0.5])

        probs2 = (1.0 - w_wsls) * probs2_soft + w_wsls * wsls_probs
        probs2 /= np.sum(probs2)

        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Outcome and risk-sensitive utility
        r = float(reward[t])
        util = r if r >= 0.0 else -nu_eff * (-r)

        # Apply leak (forgetting) before update
        q2 *= (1.0 - leak)
        q1 *= (1.0 - leak)

        # Update Q2 with utility-based PE
        pe2 = util - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Backup to Q1 from realized state-action (MF TD(1)-like)
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha_q * pe1

        # Update WSLS memory
        prev_a2[s] = a2
        prev_sign[s] = 1.0 if r >= 0.0 else -1.0
        has_prev[s] = True

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll