def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric-valence learning with anxiety-modulated eligibility and learned transitions (MB confidence-weighted hybrid).

    The agent:
    - Learns second-stage (planet-alien) values model-free with asymmetric learning rates for reward vs. no-reward.
    - Learns the first-stage transition model T online with its own learning rate.
    - Computes model-based first-stage values from the learned T and max second-stage values.
    - Maintains a model-free first-stage value updated via an eligibility trace from stage-2 prediction errors.
    - Blends MB and MF first-stage values per action by a confidence weight derived from each row of learned T,
      scaled down by anxiety (higher stai -> less planning).
    - Anxiety (stai) also reduces the eligibility trace and increases learning from negative outcomes.

    Parameter bounds:
    - alpha_pos: [0,1] learning rate for positive outcomes at stage-2
    - alpha_neg: [0,1] learning rate for negative outcomes at stage-2
    - beta: [0,10] inverse temperature used at both stages
    - lambda_et: [0,1] eligibility trace strength for stage-1 MF update
    - tau_T: [0,1] learning rate for transition model T

    Inputs:
    - action_1: np.array (n_trials,), first-stage actions (0=spaceship A, 1=spaceship U)
    - state:    np.array (n_trials,), second-stage state (0=planet X, 1=planet Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens within the visited planet)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array with one scalar in [0,1], trait anxiety score
    - model_parameters: iterable of 5 parameters [alpha_pos, alpha_neg, beta, lambda_et, tau_T]

    Returns:
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_pos, alpha_neg, beta, lambda_et, tau_T = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Initialize learned transition model; start neutral (0.5/0.5 per action) to express initial uncertainty
    T = np.array([[0.5, 0.5],
                  [0.5, 0.5]], dtype=float)

    # Model-free values
    q1_mf = np.zeros(2, dtype=float)         # first-stage MF values for spaceships
    q2_mf = np.zeros((2, 2), dtype=float)    # second-stage MF values per planet x alien

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        # Compute max second-stage values per planet (for MB planning)
        max_q2 = np.max(q2_mf, axis=1)                 # shape (2,)

        # Model-based first-stage values from learned transitions
        q1_mb = T @ max_q2                              # shape (2,)

        # Per-action MB confidence weight: higher when a row of T is far from uniform
        # conf_a in [0,1], then anxiety reduces reliance on MB
        conf = np.abs(T - 0.5)
        conf_per_row = np.sum(conf, axis=1) / np.sum(np.abs(np.array([0.5, 0.5]) - 0.5))  # denominator = 0 -> guard below
        # The denominator above is zero; so do a safe computation:
        # For a 2-prob row, sum |p-0.5| ranges from 0 (0.5,0.5) to 1 (1,0). Normalize by 1.
        conf_per_row = np.sum(np.abs(T - 0.5), axis=1)  # already in [0,1]
        w_mb_vec = conf_per_row * (1.0 - stai0)         # element-wise per action weight in [0,1]

        # Blend MF and MB action values per action
        q1_hybrid = w_mb_vec * q1_mb + (1.0 - w_mb_vec) * q1_mf

        # First-stage policy
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second-stage policy (pure MF values)
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 update with asymmetric learning; anxiety attenuates positive learning and enhances negative learning
        if r > q2_mf[s2, a2]:
            alpha2 = alpha_pos * (1.0 - 0.3 * stai0)
        else:
            alpha2 = alpha_neg * (1.0 + 0.3 * stai0)
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha2 * pe2

        # Stage-1 MF update via eligibility trace from stage-2 PE
        lambda_eff = lambda_et * (1.0 - 0.5 * stai0)
        q1_mf[a1] += alpha2 * lambda_eff * pe2

        # Update learned transition model T row for the chosen action toward observed state
        # Anxiety reduces confidence in transition learning slightly (slower updates at high stai)
        tau_eff = tau_T * (1.0 - 0.3 * stai0)
        target_row = np.array([0.0, 0.0], dtype=float)
        target_row[s2] = 1.0
        T[a1, :] = (1.0 - tau_eff) * T[a1, :] + tau_eff * target_row

        # Keep rows normalized
        row_sum = np.sum(T[a1, :]) + eps
        T[a1, :] = T[a1, :] / row_sum

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Information-bonus exploration with anxiety-modulated temperature, lapse, and first-stage bias.

    The agent:
    - Uses model-based evaluation at stage 1 with the fixed transition matrix (A->X, U->Y common).
    - Learns second-stage values model-free.
    - Adds an uncertainty-driven information bonus at stage 2 inversely proportional to visit counts.
    - Applies a soft lapse that blends the softmax policy with a uniform policy; lapse increases with anxiety.
    - Adds a first-stage bias toward commonly transitioning ships, strengthened by anxiety.

    Parameter bounds:
    - alpha: [0,1] learning rate for stage-2 MF and stage-1 MF bootstrapping
    - beta_base: [0,10] base inverse temperature (reduced by anxiety)
    - lapse_base: [0,1] base lapse rate (increased by anxiety)
    - omega_info: [0,1] information bonus strength at stage 2
    - kappa_bias: [0,1] strength of first-stage bias toward common transitions

    Inputs:
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1) within state
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array with one scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters [alpha, beta_base, lapse_base, omega_info, kappa_bias]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta_base, lapse_base, omega_info, kappa_bias = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Fixed transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and counts
    q1_mf = np.zeros(2, dtype=float)
    q2_mf = np.zeros((2, 2), dtype=float)
    visit_counts = np.ones((2, 2), dtype=float)  # start at 1 to avoid div-by-zero; state x alien

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Anxiety-modulated parameters
    beta = beta_base * (1.0 - 0.5 * stai0)              # higher anxiety -> lower temperature (more noise)
    lapse = min(1.0, lapse_base * (0.5 + stai0))        # higher anxiety -> more lapses
    omega_eff = omega_info * (1.0 - stai0)              # higher anxiety -> less info seeking
    bias_mag = kappa_bias * (0.5 + stai0)               # higher anxiety -> stronger bias

    for t in range(n_trials):
        # Compute MB first-stage values
        max_q2 = np.max(q2_mf, axis=1)                 # best alien per planet
        q1_mb = T @ max_q2

        # Add small MF leak scaled by anxiety (higher anxiety relies slightly more on MF shortcut)
        q1_val = q1_mb + stai0 * q1_mf

        # Add first-stage bias toward commonly transitioning ships
        # Common: A->X, U->Y; implement as [+1, +1] toward their respective common outcomes.
        # With two actions, assign bias +bias_mag to both in proportion to their common transition prob minus 0.5.
        common_bias = np.array([T[0, 0] - 0.5, T[1, 1] - 0.5]) * (2.0 * bias_mag)  # scales to ~[0, bias_mag]
        q1_val = q1_val + common_bias

        # Stage-1 policy with lapse
        q1c = q1_val - np.max(q1_val)
        pi1_soft = np.exp(beta * q1c)
        pi1_soft = pi1_soft / (np.sum(pi1_soft) + eps)
        pi1 = (1.0 - lapse) * pi1_soft + lapse * 0.5  # uniform over two actions
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy with info bonus
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        # Uncertainty bonus: inverse sqrt of visits
        bonus = omega_eff / np.sqrt(visit_counts[s2] + 0.0)
        q2_bonus = q2 + bonus
        q2c = q2_bonus - np.max(q2_bonus)
        pi2_soft = np.exp(beta * q2c)
        pi2_soft = pi2_soft / (np.sum(pi2_soft) + eps)
        pi2 = (1.0 - lapse) * pi2_soft + lapse * 0.5
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        # Outcome
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Update counts
        visit_counts[s2, a2] += 1.0

        # Stage-1 MF bootstrapping from realized second-stage value
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor representation (SR) with anxiety-modulated discount, forgetting, and stickiness.

    The agent:
    - Learns second-stage MF values.
    - Learns a first-stage successor representation M (actions -> expected discounted occupancy of planets).
      Since the environment has a single transition step, M approximates the transition mapping scaled by a discount.
    - Computes first-stage values as q1 = M @ max(Q2), i.e., feature expectation times learned state features.
    - Applies forgetting to M to allow adaptation to nonstationarities; both discount and forgetting depend on anxiety.
    - Includes choice stickiness at both stages, reduced by anxiety at stage 2 but increased at stage 1.

    Parameter bounds:
    - alpha:   [0,1] learning rate for value and SR updates
    - beta:    [0,10] inverse temperature at both stages
    - gamma0:  [0,1] base discount used in SR updates
    - forget0: [0,1] base forgetting rate for SR
    - stick0:  [0,1] base stickiness strength

    Inputs:
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1) within state
    - reward:   np.array (n_trials,), outcomes (0/1)
    - stai:     np.array with one scalar in [0,1], trait anxiety
    - model_parameters: iterable of 5 parameters [alpha, beta, gamma0, forget0, stick0]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, gamma0, forget0, stick0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Successor representation from actions to planets: M[a, s]
    M = np.zeros((2, 2), dtype=float)

    # MF values for second-stage
    q2_mf = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    # Anxiety-modulated SR parameters
    gamma_eff = gamma0 * (1.0 - 0.5 * stai0)          # higher anxiety -> shorter planning horizon
    forget_eff = forget0 * (0.5 + stai0)              # higher anxiety -> more forgetting
    kappa1 = stick0 * (0.5 + stai0)                   # anxiety increases first-stage stickiness
    kappa2 = stick0 * (1.0 - 0.5 * stai0)             # anxiety decreases second-stage stickiness

    for t in range(n_trials):
        # First-stage value via SR features
        features = np.max(q2_mf, axis=1)             # state features from MF values
        q1_sr = M @ features

        # Add first-stage stickiness
        if prev_a1 is not None:
            stick_vec = np.zeros(2, dtype=float)
            stick_vec[prev_a1] = 1.0
            q1_sr = q1_sr + kappa1 * stick_vec

        # Stage-1 policy
        q1c = q1_sr - np.max(q1_sr)
        pi1 = np.exp(beta * q1c)
        pi1 = pi1 / (np.sum(pi1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy with stickiness within state
        s2 = state[t]
        q2 = q2_mf[s2].copy()
        prev_a2 = prev_a2_by_state[s2]
        if prev_a2 is not None:
            stick2 = np.zeros(2, dtype=float)
            stick2[prev_a2] = 1.0
            q2 = q2 + kappa2 * stick2

        q2c = q2 - np.max(q2)
        pi2 = np.exp(beta * q2c)
        pi2 = pi2 / (np.sum(pi2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = pi2[a2]

        # Outcome and learning
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # SR update for the chosen action toward observed state occupancy
        # One-step environment: target successor for chosen action is gamma * onehot(s2)
        target = gamma_eff * (np.array([0.0, 0.0], dtype=float))
        target[s2] = gamma_eff
        # Forgetting (leaky integration) then TD-like update
        M[a1, :] = (1.0 - forget_eff) * M[a1, :] + alpha * (target - M[a1, :])

        # Bookkeeping for stickiness
        prev_a1 = a1
        prev_a2_by_state[s2] = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll