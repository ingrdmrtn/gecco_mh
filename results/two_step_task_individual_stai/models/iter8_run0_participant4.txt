Below are three standalone cognitive models tailored for the two-step task. Each function:

- Takes action_1, state, action_2, reward, stai, model_parameters as input.
- Uses all parameters meaningfully (including stai).
- Respects parameter bounds: [0,1] for most, [0,10] for beta.
- Returns the negative log-likelihood of the observed choices.
- Uses distinct mechanisms and parameter combinations from those you've already explored.

Note: Assume numpy as np is already available in the environment, and do not add any imports.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-amplified directed exploration via count-based bonuses.

    Overview
    - Stage 2 learns model-free Q-values (Rescorla-Wagner).
    - First-stage action values are a hybrid of model-based planning and model-free value.
      MB uses the known transition structure (common 0.7 / rare 0.3).
    - Adds directed exploration bonuses derived from action-visit counts:
        b2[s,a] = k_eff / sqrt(count[s,a] + 1), where k_eff increases with anxiety (stai).
      First-stage bonuses equal the expected downstream bonus under the transition matrix.
    - Anxiety (stai) increases the exploration bonus scale: k_eff = kappa_ent * (1 + xi_anx * stai).

    Parameters (all used)
    - alpha:      [0,1]   Learning rate for Q2 and MF credit to Q1.
    - beta:       [0,10]  Inverse temperature for softmax at both stages.
    - kappa_ent:  [0,1]   Baseline directed exploration bonus weight.
    - xi_anx:     [0,1]   Scales exploration bonus by anxiety (multiplicative).
    - omega_mf:   [0,1]   Weight of model-free Q1 in the hybrid (0=MB only, 1=MF only).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta, kappa_ent, xi_anx, omega_mf].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta, kappa_ent, xi_anx, omega_mf = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Value functions
    q2 = np.zeros((2, 2), dtype=float) + 0.5  # stage-2 Q-values
    q1_mf = np.zeros(2, dtype=float)          # stage-1 MF values

    # Count-based exploration (for directed exploration bonuses)
    counts2 = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Anxiety-amplified exploration scale
        k_eff = kappa_ent * (1.0 + xi_anx * stai)

        # Compute per-action directed exploration bonus at stage 2
        b2 = k_eff / np.sqrt(counts2 + 1.0)  # shape (2,2)

        # Stage-1 MB values: expect max Q2 + bonus under known transitions
        max_q2_plus_bonus = np.max(q2 + b2, axis=1)  # shape (2,)
        q1_mb = T_known @ max_q2_plus_bonus          # shape (2,)

        # Hybrid Q1
        q1 = (1.0 - omega_mf) * q1_mb + omega_mf * q1_mf

        # Policy for stage 1
        logits1 = beta * (q1 - np.max(q1))
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Policy for stage 2 in reached state s, with exploration bonus
        q2_s_eff = q2[s] + b2[s]
        logits2 = beta * (q2_s_eff - np.max(q2_s_eff))
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning updates
        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Model-free credit to stage-1 value for chosen a1
        # Use the same PE2 as a credit signal (eligibility style)
        q1_mf[a1] += alpha * pe2

        # Update counts for directed exploration
        counts2[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based planner with anxiety-inflated noise after rare transitions and perseveration.

    Overview
    - Stage 2 learns Q-values via Rescorla-Wagner (model-free).
    - Stage 1 uses purely model-based planning using the known transition matrix.
    - Anxious surprise effect: effective inverse temperature decreases after rare transitions,
      scaled by stai and eta_surp. This increases stochasticity when a rare transition occurs.
      Applied to both stages on that trial.
    - Includes a perseveration (stay) bias applied to both stages, parameterized by kappa_pers.

    Parameters (all used)
    - alpha:       [0,1]   Learning rate for stage-2 Q-values.
    - beta_base:   [0,10]  Baseline inverse temperature.
    - eta_surp:    [0,1]   Magnitude by which surprise (rare transition) reduces beta.
    - kappa_pers:  [0,1]   Additive bias to repeat previous action at each stage.
                           Implemented as an additive term in the logits.
    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha, beta_base, eta_surp, kappa_pers].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha, beta_base, eta_surp, kappa_pers = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Values
    q2 = np.zeros((2, 2), dtype=float) + 0.5

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = None
    prev_a2 = np.array([None, None], dtype=object)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Determine if observed transition was rare
        # A (0) commonly -> X (0); U (1) commonly -> Y (1)
        is_rare = (a1 == 0 and s == 1) or (a1 == 1 and s == 0)

        # Anxiety-inflated surprise reduces beta on rare transitions
        beta_eff = beta_base * (1.0 - eta_surp * stai * (1.0 if is_rare else 0.0))
        beta_eff = max(1e-6, beta_eff)

        # Stage-1 MB values: expected max Q2 under known transitions
        max_q2 = np.max(q2, axis=1)  # per state
        q1_mb = T_known @ max_q2

        # Perseveration biases
        bias1 = np.zeros(2, dtype=float)
        if prev_a1 is not None:
            bias1[prev_a1] += kappa_pers
        bias2 = np.zeros(2, dtype=float)
        if prev_a2[s] is not None:
            bias2[int(prev_a2[s])] += kappa_pers

        # Policy for stage 1
        logits1 = beta_eff * (q1_mb - np.max(q1_mb)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Policy for stage 2
        q2_s = q2[s]
        logits2 = beta_eff * (q2_s - np.max(q2_s)) + bias2
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning stage-2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update previous choices
        prev_a1 = a1
        prev_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """MF with anxiety-gated eligibility credit, forgetting of unchosen options, and prior bias for ship A.

    Overview
    - Stage 2 learns Q-values via Rescorla-Wagner.
    - Stage 1 uses purely model-free Q1 updated from the stage-2 prediction error via an
      anxiety-gated eligibility credit. High anxiety increases the credit assigned from
      second-stage outcomes back to the first-stage choice.
    - Adds forgetting on the unchosen second-stage action within the visited state.
    - Adds a prior choice bias favoring spaceship A (status-quo bias) that scales with anxiety.

    Parameters (all used)
    - alpha_base:  [0,1]   Baseline learning rate for Q2 and Q1 credit.
    - beta:        [0,10]  Inverse temperature for softmax at both stages.
    - phi_forget:  [0,1]   Forgetting factor applied to the unchosen second-stage action.
                           q2[s, 1-a2] <- (1 - phi_forget) * q2[s, 1-a2].
    - theta_anx:   [0,1]   Scales eligibility credit to Q1 as theta_eff = theta_anx * stai.
    - omega_prior: [0,1]   Prior bias toward choosing ship A at stage 1, scaled by stai
                           (logit bonus added to A only).

    Inputs
    - action_1: int array (n_trials,), chosen spaceship per trial (0=A, 1=U).
    - state:    int array (n_trials,), reached planet per trial (0=X, 1=Y).
    - action_2: int array (n_trials,), chosen alien per trial (0/1).
    - reward:   float array (n_trials,), coins obtained per trial in [0,1].
    - stai:     float array with one element in [0,1], anxiety score.
    - model_parameters: iterable/list/array with [alpha_base, beta, phi_forget, theta_anx, omega_prior].

    Returns
    - Negative log-likelihood (float) of observed stage-1 and stage-2 choices.
    """
    alpha_base, beta, phi_forget, theta_anx, omega_prior = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Values
    q2 = np.zeros((2, 2), dtype=float) + 0.5
    q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety-gated eligibility credit strength
    theta_eff = theta_anx * stai

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 policy (pure MF) with prior bias toward A scaled by anxiety
        bias1 = np.array([omega_prior * stai, 0.0], dtype=float)  # add to logits
        logits1 = beta * (q1_mf - np.max(q1_mf)) + bias1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1) / np.sum(np.exp(logits1))
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2) / np.sum(np.exp(logits2))
        p_choice_2[t] = probs2[a2]

        # Learning at stage 2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_base * pe2

        # Forgetting of the unchosen action in the visited state
        unchosen = 1 - a2
        q2[s, unchosen] = (1.0 - phi_forget) * q2[s, unchosen]

        # Anxiety-gated eligibility credit from stage 2 back to Q1 for chosen a1
        q1_mf[a1] += alpha_base * theta_eff * pe2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)