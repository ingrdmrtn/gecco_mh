def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """MF-MB arbitration via confidence; anxiety shifts arbitration toward MF.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha, beta, arb0, xi)
        - alpha in [0,1]: learning rate for stage-2 TD and MF credit to stage-1.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - arb0 in [0,1]: baseline arbitration bias toward MB (transformed to [-2,2] internally).
        - xi in [0,1]: confidence gain; how strongly choice confidence pushes toward MB.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Stage-1 action values are a convex combination of MB and MF values:
        Q1 = w * Q1_MB + (1 - w) * Q1_MF
      where w is determined by a logistic transform of (baseline + confidence - anxiety).
    - Confidence (from previous trial) is 1 - normalized entropy of the stage-1 policy,
      with 0 meaning uncertain and 1 meaning certain.
    - Anxiety reduces the MB weight directly and reduces eligibility credit to stage-1:
      lambda = 1 - stai.
    - Transitions are assumed known and stationary (A->X 0.7, U->Y 0.7).
    """
    alpha, beta, arb0, xi = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Probabilities for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))   # stage-2 values
    Q1_MF = np.zeros(2)     # stage-1 model-free values

    # Previous-trial policy information for confidence
    prev_logits1 = None
    log2 = np.log(2.0)

    # Transform parameters to useful ranges
    # arb0 in [0,1] -> baseline in [-2, 2] to allow strong prior toward MB
    baseline = 4.0 * (arb0 - 0.5)
    xi_eff = xi  # already [0,1]
    lam = 1.0 - stai_val  # eligibility trace to stage-1 dampened by anxiety

    for t in range(n_trials):
        # Compute MB values for stage-1 from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Confidence from previous trial's stage-1 policy
        if prev_logits1 is None:
            conf = 0.0  # no confidence on first trial
        else:
            logits1_prev = prev_logits1 - np.max(prev_logits1)
            probs_prev = np.exp(logits1_prev)
            probs_prev /= np.sum(probs_prev)
            # entropy normalized to [0,1]
            H = -np.sum(probs_prev * (np.log(probs_prev + 1e-12)))
            conf = 1.0 - (H / log2)

        # Arbitration weight toward MB
        z = baseline + xi_eff * conf - stai_val
        w = 1.0 / (1.0 + np.exp(-z))  # in (0,1)

        # Combined stage-1 values
        Q1 = w * Q1_MB + (1.0 - w) * Q1_MF

        # Stage-1 policy and likelihood
        a1 = int(action_1[t])
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy and likelihood
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]
        # Stage-2 TD
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Model-free credit to chosen first-stage action via eligibility
        Q1_MF[a1] += alpha * lam * delta2

        # Store for next-trial confidence
        prev_logits1 = beta * Q1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Model-based control with learned transitions and anxiety-modulated utility curvature.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Observed reward each trial.
    stai : array-like of float
        Trait anxiety in [0,1]; stai[0] used.
    model_parameters : tuple/list
        (alpha2, beta, eta_T, rho0)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - eta_T in [0,1]: transition learning rate for updating P(state|action).
        - rho0 in [0,1]: baseline utility curvature; anxiety increases concavity.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - The agent learns the transition matrix T (rows = actions, cols = states) online.
    - Utility transformation: u(r) = sign(r) * |r|^rho_eff,
      where rho_eff = 0.5 + 0.5 * (1 - stai) * rho0, making outcomes more concave
      (risk-averse) as anxiety increases.
    - Transition learning is mildly dampened by anxiety: eta_eff = eta_T * (1 - 0.5*stai).
    - Stage-1 policy is fully model-based using learned T and current stage-2 values.
    """
    alpha2, beta, eta_T, rho0 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize learned transitions to agnostic 0.5
    T = np.full((2, 2), 0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))

    # Anxiety-modulated parameters
    eta_eff = eta_T * (1.0 - 0.5 * stai_val)
    rho_eff = 0.5 + 0.5 * (1.0 - stai_val) * rho0  # in [0.5, 1]

    for t in range(n_trials):
        # Model-based stage-1 values from current T and Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Observed reward -> utility
        r = reward[t]
        # utility transformation with curvature
        abs_r = np.abs(r)
        u = np.sign(r) * (abs_r ** rho_eff)

        # Update transitions for the chosen first-stage action
        if eta_eff > 0.0:
            a_row = a1
            # Decay row and add mass to observed state, then renormalize
            T[a_row, :] = (1.0 - eta_eff) * T[a_row, :]
            T[a_row, s2] += eta_eff
            row_sum = np.sum(T[a_row, :])
            if row_sum > 0:
                T[a_row, :] /= row_sum

        # TD update at stage-2 on utility
        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Surprise-weighted credit assignment with anxiety-amplified misassignment and perseveration.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial.
    reward : array-like of float
        Observed rewards.
    stai : array-like of float
        Trait anxiety in [0,1]; stai[0] is used.
    model_parameters : tuple/list
        (alpha2, beta, surpr, pi_rep)
        - alpha2 in [0,1]: learning rate for stage-2 Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - surpr in [0,1]: strength of surprise-driven misassignment after rare transitions.
        - pi_rep in [0,1]: perseveration strength added to repeating previous first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    
    Notes
    -----
    - Stage-1 action values are a hybrid: MB values plus MF values carried via eligibility.
    - After rare transitions (given fixed transition structure 0.7/0.3), a portion of
      credit is misassigned to the unchosen first-stage action. This misassignment
      scales with anxiety (higher anxiety -> stronger misassignment).
    - Perseveration bias is added to logits for repeating the previous first-stage choice.
    """
    alpha2, beta, surpr, pi_rep = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure to define "rare" vs "common"
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q2 = np.zeros((2, 2))
    Q1_MF = np.zeros(2)

    prev_a1 = None

    # Eligibility to chosen action is reduced by anxiety; misassignment enhanced by anxiety
    lam_chosen = 1.0 - stai_val
    lam_unchosen_base = surpr  # base scale
    lam_unchosen = lam_unchosen_base * stai_val  # only active proportionally to anxiety

    for t in range(n_trials):
        # Model-based Q1 from fixed transitions and current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T_fixed @ max_Q2

        # Combine MB and MF for stage-1 values
        Q1 = 0.5 * Q1_MB + 0.5 * Q1_MF

        # Add perseveration bias to logits
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = pi_rep

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1 + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Learning at stage-2
        r = reward[t]
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Determine whether transition was "common" or "rare"
        prob_seen = T_fixed[a1, s2]
        is_rare = 1 if prob_seen < 0.5 else 0  # here rare = 0.3

        # Credit assignment to stage-1 MF
        # Always credit chosen action
        Q1_MF[a1] += alpha2 * lam_chosen * delta2

        # If rare transition, misassign some credit to the unchosen action; anxiety scales it
        if is_rare:
            a1_other = 1 - a1
            Q1_MF[a1_other] += alpha2 * lam_unchosen * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll