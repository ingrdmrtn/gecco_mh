def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Surprise-gated model-based control with learned transitions and stage-specific learning.

    Core ideas:
    - Learn the transition matrix from experience (rather than assuming it).
    - Stage-1 action values are a hybrid of model-based (via learned transitions)
      and model-free; the MB weight is increased when recent transition surprise is high,
      especially for participants with lower anxiety.
    - Separate learning rates for stage 1 and stage 2 allow differential updating.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within planet (0/1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values dampen the surprise-to-MB gating.
    model_parameters : array-like of float
        [alpha_s1, alpha_s2, beta, wmb_base, t_alpha]
        - alpha_s1 in [0,1]: learning rate for stage-1 MF values.
        - alpha_s2 in [0,1]: learning rate for stage-2 values.
        - beta in [0,10]: inverse temperature for both stages.
        - wmb_base in [0,1]: baseline weight on MB at stage 1.
        - t_alpha in [0,1]: learning rate for transition probabilities.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_s1, alpha_s2, beta, wmb_base, t_alpha = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition matrix T[a, s]; start near-common but not deterministic
    T = np.array([[0.6, 0.4],
                  [0.4, 0.6]], dtype=float)

    # Stage-2 values per state-action
    q2 = 0.5 * np.ones((2, 2), dtype=float)
    # Stage-1 model-free values
    q1_mf = np.zeros(2, dtype=float)

    p1 = np.zeros(n_trials, dtype=float)
    p2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Running estimate of transition surprise (absolute prediction error on transitions)
    surpr = 0.0

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based Q at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)  # best value at each planet
        q1_mb = T @ max_q2

        # Surprise-gated MB weight: more MB when surprise is high and anxiety is low
        w_mb = np.clip(wmb_base + (1.0 - st) * surpr, 0.0, 1.0)

        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Policy stage 1
        logits1 = beta * (q1 - np.max(q1))
        prob1 = np.exp(logits1)
        prob1 /= (np.sum(prob1) + eps)
        p1[t] = prob1[a1]

        # Policy stage 2
        logits2 = beta * (q2[s] - np.max(q2[s]))
        prob2 = np.exp(logits2)
        prob2 /= (np.sum(prob2) + eps)
        p2[t] = prob2[a2]

        # Update transition matrix from observed transition a1 -> s
        # Prediction error on transition for chosen action
        pe_T = 1.0 - T[a1, s]
        # Update chosen state's probability toward 1 and the other toward 0
        T[a1, s] += t_alpha * pe_T
        T[a1, 1 - s] += t_alpha * (-T[a1, 1 - s])
        # Keep rows normalized (numerical safety)
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum
        # Update running surprise (EWMA of absolute transition PE)
        surpr = 0.8 * surpr + 0.2 * abs(pe_T)

        # Stage-2 value update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_s2 * pe2

        # Stage-1 MF update towards the realized second-stage value (SARSA-style backup)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_s1 * pe1

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Anxiety-gated exploration with perseveration and hybrid MB/MF valuation.

    Core ideas:
    - Stage 1 combines model-based and model-free values using a fixed blend.
    - Anxiety decreases effective beta (more exploration) and increases both lapse
      probability and perseveration strength (tendency to repeat).
    - Perseveration biases both stages toward repeating previous actions.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within planet (0/1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce beta and increase lapse/perseveration.
    model_parameters : array-like of float
        [alpha, beta_base, pers0, lapse_base]
        - alpha in [0,1]: learning rate for both stages.
        - beta_base in [0,10]: baseline inverse temperature.
        - pers0 in [0,1]: baseline perseveration strength.
        - lapse_base in [0,1]: baseline lapse probability (choice noise).

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha, beta_base, pers0, lapse_base = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition matrix (task structure)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values
    q1_mf = np.zeros(2, dtype=float)
    q2 = 0.5 * np.ones((2, 2), dtype=float)

    # Anxiety-gated parameters
    beta_eff = np.clip(beta_base * (0.6 + 0.4 * (1.0 - st)), 0.0, 10.0)
    lapse = np.clip(lapse_base * (0.5 + 0.5 * st), 0.0, 1.0)
    pers = np.clip(pers0 * (0.5 + 0.5 * st), 0.0, 1.0)

    # MB/MF blend fixed
    w_mb = 0.5

    # Perseveration traces (last actions), initialized to 0 (no bias)
    last_a1 = None
    last_a2 = [None, None]  # separate per second-stage state

    p1 = np.zeros(n_trials, dtype=float)
    p2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Perseveration biases as additive logits
        bias1 = np.zeros(2, dtype=float)
        if last_a1 is not None:
            bias1[last_a1] += pers

        bias2 = np.zeros(2, dtype=float)
        if last_a2[s] is not None:
            bias2[last_a2[s]] += pers

        # Stage 1 choice with lapse
        logits1 = beta_eff * (q1 - np.max(q1)) + bias1
        soft1 = np.exp(logits1)
        soft1 /= (np.sum(soft1) + eps)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        p1[t] = probs1[a1]

        # Stage 2 choice with lapse
        logits2 = beta_eff * (q2[s] - np.max(q2[s])) + bias2
        soft2 = np.exp(logits2)
        soft2 /= (np.sum(soft2) + eps)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        p2[t] = probs2[a2]

        # Update values
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update perseveration memory
        last_a1 = a1
        last_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-asymmetric learning with anxiety-tuned UCB exploration and common-transition bias.

    Core ideas:
    - Stage-2 learning uses separate learning rates for positive vs. negative prediction errors.
    - Exploration at stage 2 uses an uncertainty bonus (UCB) whose strength increases with anxiety,
      modeling anxiety-driven information seeking about uncertain aliens.
    - Stage-1 policy is model-based from the known transition structure, plus a bias favoring the
      more common transition option; this bias increases with anxiety.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices within planet (0/1).
    reward : array-like of float
        Coins received on each trial.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase UCB bonus and the common-transition bias,
        and mildly reduce beta.
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, ucb_base, common_bias0]
        - alpha_pos in [0,1]: learning rate when PE > 0.
        - alpha_neg in [0,1]: learning rate when PE < 0.
        - beta in [0,10]: inverse temperature (both stages).
        - ucb_base in [0,1]: baseline uncertainty bonus weight at stage 2.
        - common_bias0 in [0,1]: baseline bias toward the more common spaceship at stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_pos, alpha_neg, beta, ucb_base, common_bias0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Known transition matrix (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Stage-2 values and counts to quantify uncertainty (1/sqrt(N))
    q2 = 0.5 * np.ones((2, 2), dtype=float)
    n2 = np.ones((2, 2), dtype=float)  # start at 1 to avoid div-by-zero

    # Stage-1 value is purely model-based here; no MF component to keep parameter budget
    p1 = np.zeros(n_trials, dtype=float)
    p2 = np.zeros(n_trials, dtype=float)
    eps = 1e-12

    # Anxiety effects
    beta_eff = np.clip(beta * (0.7 + 0.3 * (1.0 - st)), 0.0, 10.0)
    ucb_w = np.clip(ucb_base * (0.5 + 0.5 * st), 0.0, 1.0)
    bias_common = np.clip(common_bias0 * (0.5 + 0.5 * st), 0.0, 1.0)

    for t in range(n_trials):
        a1 = int(action_1[t])
        s = int(state[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Stage-1 model-based values: expected max Q2 under transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Add common-transition bias: favor A over U by +b/-b (A is more common to X)
        # This captures anxious reliance on habitual/common routes.
        bias_vec = np.array([bias_common, -bias_common], dtype=float)
        logits1 = beta_eff * (q1_mb - np.max(q1_mb)) + bias_vec
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Stage-2 policy with UCB exploration bonus based on visit uncertainty
        # Uncertainty proxy: 1/sqrt(N)
        ucb_bonus = ucb_w * (1.0 / np.sqrt(n2[s] + eps))
        q2_aug = q2[s] + ucb_bonus
        logits2 = beta_eff * (q2_aug - np.max(q2_aug))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Update counts
        n2[s, a2] += 1.0

        # Stage-2 asymmetric learning
        pe2 = r - q2[s, a2]
        lr2 = alpha_pos if pe2 >= 0.0 else alpha_neg
        q2[s, a2] += lr2 * pe2

        # No separate MF at stage 1; values for stage 1 are recomputed from updated q2 via MB

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)