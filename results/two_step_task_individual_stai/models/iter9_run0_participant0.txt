Below are three standalone Python functions implementing distinct cognitive models for the two-step task. Each uses all inputs, keeps parameters within the required bounds, and incorporates the participantâ€™s STAI score in a meaningful way. All three functions return the negative log-likelihood of the observed choices.

def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Volatility-scaled reward learning, anxiety-weighted rare-transition correction, and second-stage choice-kernel.
    
    Rationale
    - Second-stage reward learning rate increases with unsigned prediction error and anxiety (volatility-scaled).
    - After rare transitions, an anxiety-weighted bias shifts first-stage values toward the ship whose common destination matches the last visited planet (reward-independent).
    - A second-stage, state-specific choice-kernel captures short-term repetition at the alien level.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: array-like (n_trials,), first-stage choices (0=A, 1=U).
    - state: array-like (n_trials,), reached planet index (0=X, 1=Y).
    - action_2: array-like (n_trials,), second-stage choices (0 or 1).
    - reward: array-like (n_trials,), reward per trial.
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: length-6
        alpha_r: base learning rate for second-stage Q-values.
        beta: inverse temperature used at both stages [0,10].
        nu_vol: volatility gain on learning rate via |PE| and anxiety.
        kappa_choice2: kernel learning rate for second-stage choice kernel (strength of updating toward chosen).
        decay_k2: decay of second-stage choice kernel toward 0 each trial.
        anx_common: magnitude of anxiety-weighted bias after rare transitions at stage 1.

    Returns
    - Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_r, beta, nu_vol, kappa_choice2, decay_k2, anx_common = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters to bounds
    beta = max(1e-6, min(10.0, beta))
    alpha_r = min(1.0, max(0.0, alpha_r))
    nu_vol = min(1.0, max(0.0, nu_vol))
    kappa_choice2 = min(1.0, max(0.0, kappa_choice2))
    decay_k2 = min(1.0, max(0.0, decay_k2))
    anx_common = min(1.0, max(0.0, anx_common))

    # Fixed transition structure (common A->X, U->Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Value tables
    q2 = np.zeros((2, 2))       # second-stage Q-values for planets X(0), Y(1), aliens (0/1)
    K2 = np.zeros((2, 2))       # second-stage choice kernel (state-dependent)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a1 = None
    last_state = None
    last_transition_rare = 0  # for biasing next first-stage choice

    for t in range(n_trials):
        # Model-based first-stage values from current second-stage values
        v2 = np.max(q2, axis=1)  # best alien value on each planet
        q1_mb = T @ v2
        q1 = q1_mb.copy()

        # Anxiety-weighted rare-transition correction (reward-independent)
        # If the previous transition was rare, bias toward the ship whose common destination equals last_state.
        if last_state is not None and last_transition_rare == 1:
            # ship 0 commonly -> state 0; ship 1 commonly -> state 1
            target_ship = last_state  # 0 if last_state==X, 1 if Y
            bias = anx_common * stai
            q1[target_ship] += bias
            q1[1 - target_ship] -= bias

        # Softmax for first-stage choice
        a1 = action_1[t]
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Second stage policy with choice-kernel bias
        s = state[t]
        a2 = action_2[t]
        q2_biased = q2[s] + K2[s]
        exp_q2 = np.exp(beta * (q2_biased - np.max(q2_biased)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Second-stage learning with volatility scaling driven by |PE| and anxiety
        pe2 = r - q2[s, a2]
        alpha_eff = alpha_r * (1.0 + nu_vol * stai * abs(pe2))
        alpha_eff = min(1.0, max(0.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        # Update second-stage choice kernel: decay then strengthen chosen
        K2[s] *= (1.0 - decay_k2)
        # move chosen action kernel toward +1 and the other toward -1 with rate kappa_choice2
        K2[s, a2] += kappa_choice2 * (1.0 - K2[s, a2])
        K2[s, 1 - a2] += kappa_choice2 * (-1.0 - K2[s, 1 - a2])

        # Determine whether transition was rare on this trial for next-trial bias
        common_dest = a1  # 0->X, 1->Y
        last_transition_rare = int(s != common_dest)
        last_state = s
        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Arbitrated MB/MF stage-1 control via surprise and anxiety, with global first-stage choice trace and anxiety-scaled temperature.
    
    Rationale
    - Stage-1 action values combine model-based (MB) and model-free (MF) components.
    - Arbitration weight increases with transition surprise (rare) and decreases with anxiety.
    - A decaying first-stage choice trace captures ship-level perseveration.
    - Anxiety also reduces effective inverse temperature (more stochastic) at both stages.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: (n_trials,) first-stage choices (0=A, 1=U).
    - state: (n_trials,) reached planet (0=X, 1=Y).
    - action_2: (n_trials,) second-stage choices (0 or 1).
    - reward: (n_trials,) reward per trial.
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: length-6
        alpha_r: learning rate for second-stage Q-values and MF first-stage backup.
        beta: base inverse temperature [0,10].
        psi_mix: baseline logit for MB weight at stage 1 (higher -> more MB).
        chi_surprise: sensitivity of MB weight to rare-transition surprise (rare -> more MB).
        rep1_decay: strength of first-stage choice trace update/decay.
        anx_beta_scale: scales how much anxiety reduces beta (exploration under anxiety).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_r, beta, psi_mix, chi_surprise, rep1_decay, anx_beta_scale = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Clip parameters
    beta = max(1e-6, min(10.0, beta))
    alpha_r = min(1.0, max(0.0, alpha_r))
    psi_mix = min(1.0, max(0.0, psi_mix))     # keep in [0,1], then map to logit below
    chi_surprise = min(1.0, max(0.0, chi_surprise))
    rep1_decay = min(1.0, max(0.0, rep1_decay))
    anx_beta_scale = min(1.0, max(0.0, anx_beta_scale))

    # Fixed transition matrix
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Q-values
    q2 = np.zeros((2, 2))      # second-stage
    q1_mf = np.zeros(2)        # stage-1 model-free values
    choice_trace1 = np.zeros(2)  # first-stage choice trace

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Map psi_mix in [0,1] to a logit center around 0 for flexibility
    # logit(x) with stabilization:
    x = max(1e-6, min(1 - 1e-6, psi_mix))
    psi_logit = np.log(x) - np.log(1 - x)

    last_a1 = None

    for t in range(n_trials):
        # Model-based Q1
        v2 = np.max(q2, axis=1)
        q1_mb = T @ v2

        # Surprise (1 if rare transition on previous trial, else 0)
        if t == 0:
            surprise_prev = 0.0
        else:
            a1_prev = action_1[t - 1]
            s_prev = state[t - 1]
            common_prev = a1_prev
            surprise_prev = 1.0 if (s_prev != common_prev) else 0.0

        # Arbitration weight w_mb via logistic mapping
        # Base logit + surprise gain - anxiety penalty
        logit_w = psi_logit + chi_surprise * surprise_prev - (stai * psi_logit)  # anxiety reduces MB weight proportional to baseline
        w_mb = 1.0 / (1.0 + np.exp(-logit_w))
        w_mb = min(1.0, max(0.0, w_mb))

        # Combine MB and MF for stage-1 Q
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf + choice_trace1

        # Anxiety-scaled temperature (higher anxiety -> lower beta_eff)
        beta_eff = beta * (1.0 - anx_beta_scale * stai)
        beta_eff = max(1e-6, min(10.0, beta_eff))

        # First-stage policy
        a1 = action_1[t]
        exp_q1 = np.exp(beta_eff * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy
        s = state[t]
        a2 = action_2[t]
        exp_q2 = np.exp(beta_eff * (q2[s] - np.max(q2[s])))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Second-stage update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # One-step MF backup to stage-1 for chosen a1 using current second-stage state value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Update first-stage choice trace: decay overall, then increment chosen
        choice_trace1 *= (1.0 - rep1_decay)
        choice_trace1[a1] += rep1_decay

        last_a1 = a1

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric PE learning modulated by anxiety, transition relearning with surprise-driven reset, and planet preference bias.
    
    Rationale
    - Second-stage learning uses an anxiety-scaled asymmetry: negative PEs learn faster when anxiety is high.
    - Transition matrix is learned online; after rare transitions it partially resets toward uncertainty, scaled by anxiety.
    - First-stage values include an anxiety-modulated planet preference projected through the learned transition.
    - A global second-stage stickiness (across planets) biases repeating the last alien choice.

    Parameters (all in [0,1] except beta in [0,10])
    - action_1: (n_trials,) first-stage choices (0=A, 1=U).
    - state: (n_trials,) reached planet (0=X, 1=Y).
    - action_2: (n_trials,) second-stage choices (0 or 1).
    - reward: (n_trials,) reward per trial.
    - stai: array-like (1,), anxiety score in [0,1].
    - model_parameters: length-7
        alpha0: base learning rate for second-stage Q-values.
        beta: inverse temperature used at both stages [0,10].
        kappa_loss_av: scales anxiety-driven boost for negative PE learning (loss aversion in learning).
        alpha_t: base learning rate for transition matrix rows for the chosen ship.
        reset_t: surprise-driven reset toward uniform transitions after rare transitions (scaled by anxiety).
        phi_planet: baseline preference for planet X over Y; expressed via first-stage expected reach (MB term).
        stick2g: global second-stage stickiness to repeat last alien (across states).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha0, beta, kappa_loss_av, alpha_t, reset_t, phi_planet, stick2g = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Bound parameters
    beta = max(1e-6, min(10.0, beta))
    alpha0 = min(1.0, max(0.0, alpha0))
    kappa_loss_av = min(1.0, max(0.0, kappa_loss_av))
    alpha_t = min(1.0, max(0.0, alpha_t))
    reset_t = min(1.0, max(0.0, reset_t))
    phi_planet = min(1.0, max(0.0, phi_planet))
    stick2g = min(1.0, max(0.0, stick2g))

    # Initialize learned transitions near common structure but learnable
    T_hat = np.array([[0.65, 0.35],
                      [0.35, 0.65]], dtype=float)

    q2 = np.zeros((2, 2))  # second-stage Q-values
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    last_a2 = None  # for global second-stage stickiness

    for t in range(n_trials):
        # Planet preference projected to actions via current T_hat:
        # preference vector over planets: pref[X]=+b, pref[Y]=-b, with anxiety sign/scale
        b = phi_planet * (2.0 * stai - 1.0)  # high anxiety -> closer to +phi or -phi depending on stai>0.5
        pref_planet = np.array([b, -b])
        pref_action = T_hat @ pref_planet

        # Model-based stage-1 values from learned transitions + planet preference
        v2 = np.max(q2, axis=1)
        q1_mb = T_hat @ v2
        q1 = q1_mb + pref_action

        # First-stage policy
        a1 = action_1[t]
        exp_q1 = np.exp(beta * (q1 - np.max(q1)))
        probs1 = exp_q1 / np.sum(exp_q1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy with global stickiness
        s = state[t]
        a2 = action_2[t]
        bias2 = np.zeros(2)
        if last_a2 is not None:
            bias2[last_a2] += stick2g
            bias2[1 - last_a2] -= stick2g
        exp_q2 = np.exp(beta * ((q2[s] + bias2) - np.max(q2[s] + bias2)))
        probs2 = exp_q2 / np.sum(exp_q2)
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Asymmetric, anxiety-modulated second-stage learning
        pe2 = r - q2[s, a2]
        neg = 1.0 if pe2 < 0.0 else 0.0
        pos = 1.0 - neg
        # negative PE gets boosted learning rate with anxiety; positive PE slightly discounted to keep overall in [0,1]
        alpha_eff = alpha0 * (pos * (1.0 - 0.5 * kappa_loss_av * stai) + neg * (1.0 + kappa_loss_av * stai))
        alpha_eff = min(1.0, max(0.0, alpha_eff))
        q2[s, a2] += alpha_eff * pe2

        # Update learned transition row for chosen a1 toward observed state s
        row = T_hat[a1].copy()
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            row[ss] = row[ss] + alpha_t * (target - row[ss])

        # Normalize and optionally reset toward uncertainty after rare transition
        # Rare if s != common destination (a1)
        common_dest = a1
        is_rare = int(s != common_dest)
        if is_rare == 1:
            # mix with uniform according to reset scaled by anxiety
            lam = reset_t * stai
            row = (1.0 - lam) * row + lam * np.array([0.5, 0.5])

        row_sum = np.sum(row)
        if row_sum <= 0.0:
            row = np.array([0.5, 0.5])
        else:
            row = row / row_sum
        T_hat[a1] = row

        last_a2 = a2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll