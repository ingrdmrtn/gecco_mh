def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated arbitration and learned transitions.
    
    This model blends a model-free (MF) and model-based (MB) controller at stage 1.
    The transition matrix T(s|a) is learned online and row-normalized via a soft update.
    Anxiety (stai) modulates:
      - the effective MB/MF arbitration weight (w_eff)
      - the transition learning rate (alpha_T)
    An eligibility trace backs up second-stage prediction errors to the chosen first-stage action.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Used to modulate arbitration and transition learning.
    model_parameters : list or array
        [alpha_v, beta, w_hyb, anx_trans, elig]
        Bounds:
          alpha_v   in [0,1]  : MF learning rate for second-stage values
          beta      in [0,10] : inverse temperature for both stages
          w_hyb     in [0,1]  : baseline MB weight at stage 1 (before anxiety modulation)
          anx_trans in [0,1]  : strength of anxiety's effect on arbitration and transition learning
          elig      in [0,1]  : eligibility trace for backing up stage-2 PE to stage-1 MF value
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_v, beta, w_hyb, anx_trans, elig = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition model T(a, s)
    T = np.ones((2, 2)) * 0.5  # start uninformative, rows sum to 1

    # Model-free values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety modulation term in [-1, 1]
    mod = 2.0 * (stai - 0.5)

    # Effective MB weight: move w_hyb toward MB (increase) when mod>0 and anx_trans large
    w_eff_base_shift = anx_trans * mod
    w_eff = np.clip(w_hyb + w_eff_base_shift * (1.0 - w_hyb), 0.0, 1.0)

    # Transition learning rate increases with anxiety if mod>0
    alpha_T = np.clip(0.5 + 0.5 * anx_trans * mod, 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Model-based Q at stage 1 from current transition beliefs and MF q2
        max_q2 = np.max(q2, axis=1)         # best alien on each planet
        q1_mb = T @ max_q2                  # expected value per spaceship via learned transitions

        # Hybrid stage-1 value
        q1 = (1.0 - w_eff) * q1_mf + w_eff * q1_mb

        # Softmax for stage 1
        q1c = q1 - np.max(q1)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy: softmax over planet-conditional q2
        q2s = q2[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # MF learning at stage 2
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha_v * pe2

        # Back up to stage 1 MF with eligibility
        q1_mf[a1] += elig * alpha_v * pe2

        # Additional temporal-difference consistency update at stage 1 MF
        td1 = (q2[s, a2] - q1_mf[a1])
        q1_mf[a1] += (1.0 - elig) * alpha_v * td1

        # Update transition beliefs for the chosen spaceship: soft row update keeps row summing to 1
        # Move probability mass toward the observed state s with rate alpha_T
        T[a1, :] = (1.0 - alpha_T) * T[a1, :]
        T[a1, s] += alpha_T

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-based with anxiety-modulated utility curvature and forgetting.
    
    The agent plans at stage 1 using the known common-rare transition structure (0.7/0.3).
    At stage 2, it learns expected utility values, where instantaneous utility is u(r) = r**gamma_eff.
    Anxiety increases curvature (risk aversion) and forgetting of unchosen actions.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Modulates utility curvature and forgetting.
    model_parameters : list or array
        [alpha_u, beta, phi_curv, anx_gain, kappa_forget]
        Bounds:
          alpha_u      in [0,1]  : learning rate for expected utility at stage 2
          beta         in [0,10] : inverse temperature for both stages
          phi_curv     in [0,1]  : baseline utility curvature (gamma baseline)
          anx_gain     in [0,1]  : strength of anxiety's influence on curvature
          kappa_forget in [0,1]  : baseline forgetting rate for unchosen second-stage actions
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_u, beta, phi_curv, anx_gain, kappa_forget = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Stage-2 expected utility values
    q2u = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-modulated curvature: gamma_eff in (0,1], more concave with higher stai
    # Move phi_curv toward stronger concavity as stai and anx_gain increase.
    gamma_eff = np.clip(phi_curv * (1.0 - anx_gain * stai) + (1.0 - anx_gain * stai) * 0.0 + anx_gain * (1.0 - stai) * 0.0 + 1e-9, 1e-6, 1.0)
    # Simplify: gamma_eff = phi_curv * (1 - anx_gain*stai), clipped to (0,1]

    # Anxiety-modulated forgetting: stronger with higher stai
    forget_eff = np.clip(kappa_forget * (0.5 + 0.5 * stai), 0.0, 1.0)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Stage 1: MB evaluation using expected utility values
        max_q2 = np.max(q2u, axis=1)
        q1_mb = T_known @ max_q2

        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2: softmax over expected utility on visited planet
        q2s = q2u[s].copy()
        q2c = q2s - np.max(q2s)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # Utility of observed reward
        u = reward[t] ** gamma_eff

        # Update chosen action toward utility
        q2u[s, a2] += alpha_u * (u - q2u[s, a2])

        # Forget unchosen action on the visited planet
        other = 1 - a2
        q2u[s, other] = (1.0 - forget_eff) * q2u[s, other]

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-bonus exploration with anxiety-scaled bonus and temperature collapse.
    
    Second-stage action values are learned model-free. An exploration bonus inversely
    proportional to visit count is added to second-stage values; anxiety scales this bonus.
    Stage-1 is model-based using the known transition structure and bonus-augmented second-stage values.
    Inverse temperature collapses with anxiety (more noise for higher STAI).
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states per trial (0=planet X, 1=planet Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices per trial (0=first alien on that planet, 1=second alien).
    reward : array-like of float (0 or 1)
        Reward outcomes per trial.
    stai : array-like of float
        Anxiety score array of length 1. Modulates exploration bonus and decision temperature.
    model_parameters : list or array
        [alpha_r, beta0, xi_bonus, anx_explore, tau_decay]
        Bounds:
          alpha_r     in [0,1]  : learning rate for second-stage rewards
          beta0       in [0,10] : baseline inverse temperature
          xi_bonus    in [0,1]  : base weight of uncertainty bonus
          anx_explore in [0,1]  : how strongly STAI scales the bonus
          tau_decay   in [0,1]  : fraction by which STAI reduces temperature (higher = more collapse)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_r, beta0, xi_bonus, anx_explore, tau_decay = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # MF values at stage 2 and visit counts for uncertainty bonus
    q2 = np.zeros((2, 2))
    visits = np.ones((2, 2))  # start at 1 to avoid division by zero

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety-scaled temperature: higher STAI -> lower beta_eff
    beta_eff = beta0 * (1.0 - tau_decay * stai)
    beta_eff = max(beta_eff, 1e-6)

    # Anxiety-scaled exploration bonus factor
    bonus_scale = xi_bonus * (1.0 + anx_explore * stai)

    for t in range(n_trials):
        s = state[t]
        a1 = action_1[t]
        a2 = action_2[t]

        # Compute uncertainty bonus for both planets
        bonus = bonus_scale / np.sqrt(visits + 1e-9)
        # Bonus-augmented second-stage values
        q2_plus = q2 + bonus

        # Stage 1 MB using bonus-augmented values
        max_q2_plus = np.max(q2_plus, axis=1)
        q1_mb = T_known @ max_q2_plus

        q1c = q1_mb - np.max(q1_mb)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / np.sum(probs_1)
        p_choice_1[t] = probs_1[a1]

        # Stage 2 policy on visited planet with bonus
        q2s_plus = q2_plus[s].copy()
        q2c = q2s_plus - np.max(q2s_plus)
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / np.sum(probs_2)
        p_choice_2[t] = probs_2[a2]

        # MF learning at stage 2
        pe2 = reward[t] - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update visit counts
        visits[s, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll