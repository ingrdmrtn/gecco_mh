def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety- and surprise-adaptive arbitration.
    
    Mechanism:
    - Stage 2 values are learned via MF Q-learning.
    - Stage 1 action values combine model-based (MB) planning with a model-free
      (MF) cache. The arbitration weight toward MB is dynamically modulated by:
        (a) participant anxiety (stai), and
        (b) the magnitude of the previous trial's reward prediction error
            ("surprise"), pushing toward MF when surprises are large.
    - Transitions are assumed known: A->X common, U->Y common (0.7/0.3).
    
    Parameters (model_parameters):
    - alpha_b: base learning rate for rewards (Q2) and MF cache updates, in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - mix0: base MB mixing weight at stage 1, in [0,1]
    - anx_mix: anxiety modulation of MB weight (positive increases MB reliance
               with higher anxiety; negative decreases), in [0,1] effectively
               scaled by centered stai
    - k_pe: arbitration sensitivity to previous unsigned prediction error
            (larger shifts weight toward MF), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (e.g., 0/1)
    - stai: array-like with a single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_b, beta, mix0, anx_mix, k_pe = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure (rows: spaceships A/U, cols: planets X/Y)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q2 = np.zeros((2, 2))   # per-planet, per-alien
    q1_mf = np.zeros(2)     # MF cache for stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_abs_pe = 0.0

    # Anxiety-centered scaling around the medium/high boundary (0.51)
    anx_centered = stai - 0.51

    for t in range(n_trials):

        # Model-based projection of stage-1 values
        max_q2 = np.max(q2, axis=1)      # best alien per planet
        q1_mb = T @ max_q2               # MB value for spaceships

        # Arbitration weight toward MB considering anxiety and surprise
        mix = mix0 + anx_mix * anx_centered - k_pe * prev_abs_pe
        mix = np.clip(mix, 0.0, 1.0)

        q1 = mix * q1_mb + (1.0 - mix) * q1_mf

        # Stage 1 policy
        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning
        r = reward[t]

        # Stage 2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_b * pe2

        # Stage 1 MF cache update toward realized second-stage value
        # (bootstrapping MF cache from the experienced branch)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_b * pe1

        # Update previous unsigned prediction error for arbitration
        prev_abs_pe = min(1.0, abs(pe2))

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid planning with anxiety-reduced planning weight and stage-2 choice kernel.
    
    Mechanism:
    - Stage 1 uses a hybrid of model-based (MB) planning and model-free (MF)
      cache, with the planning weight reduced by higher anxiety.
    - Stage 2 uses MF Q-learning combined with a per-planet choice kernel that
      captures perseveration/exploitative inertia; kernel strength increases
      with anxiety.
    - Transitions are assumed known: A->X, U->Y with 0.7 common probability.
    
    Parameters (model_parameters):
    - alpha_r: reward learning rate (Q2 and MF cache), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - plan0: baseline weight for MB at stage 1, in [0,1]
    - anx_shift: how much anxiety shifts MB weight (reduces plan with higher stai)
                 and increases the stage-2 kernel strength, in [0,1]
    - k2: choice-kernel learning/decay rate at stage 2, in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (e.g., 0/1)
    - stai: array-like with a single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha_r, beta, plan0, anx_shift, k2 = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Known transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    q2 = np.zeros((2, 2))   # second-stage MF Q-values
    q1_mf = np.zeros(2)     # stage-1 MF cache
    kernel2 = np.zeros((2, 2))  # per-planet choice kernel for stage 2

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Anxiety effects
    anx_centered = stai - 0.51
    # Higher anxiety reduces planning and increases perseveration kernel
    plan_eff = np.clip(plan0 - anx_shift * anx_centered, 0.0, 1.0)
    theta2 = k2 * (1.0 + np.clip(stai, 0.0, 1.0) * anx_shift)  # kernel strength

    for t in range(n_trials):

        # Stage 1 hybrid values
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2
        q1 = plan_eff * q1_mb + (1.0 - plan_eff) * q1_mf

        logits1 = beta * q1
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy with choice kernel
        s = state[t]
        logits2 = beta * q2[s] + theta2 * kernel2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Q2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Stage-1 MF cache update toward realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

        # Update stage-2 choice kernel: decay + reinforce chosen
        kernel2[s] *= (1.0 - k2)
        kernel2[s, a2] += k2

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Learned transitions with anxiety-modulated valence asymmetry and rare-transition bias.
    
    Mechanism:
    - Learns the transition matrix from experience.
    - Stage 1 uses purely model-based planning from the learned transitions,
      but includes a bias term based on whether the previous transition was
      common vs rare: rare transitions bias switching; common transitions bias
      repeating. The magnitude of this bias grows with anxiety.
    - Stage 2 MF Q-learning uses an anxiety-modulated valence asymmetry:
      higher anxiety increases learning from negative outcomes (and/or reduces
      learning from positive), controlled by 'valence'.
    
    Parameters (model_parameters):
    - alpha: base learning rate for rewards (Q2), in [0,1]
    - beta: inverse temperature for both stages, in [0,10]
    - valence: strength of anxiety-driven valence asymmetry in Q2 learning, in [0,1]
    - trans_sens: sensitivity to previous trial's transition type in stage-1 bias, in [0,1]
    - eta_t: transition learning rate (for learned transition matrix), in [0,1]
    
    Inputs:
    - action_1: array of first-stage choices (0: A, 1: U)
    - state: array of reached planets (0: X, 1: Y)
    - action_2: array of second-stage choices (0/1)
    - reward: array of rewards per trial (e.g., 0/1)
    - stai: array-like with a single float (0-1) anxiety score
    - model_parameters: array-like of parameters as above
    
    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta, valence, trans_sens, eta_t = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition matrix with weak prior toward common mapping
    # Rows: A(0), U(1); Cols: X(0), Y(1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    q2 = np.zeros((2, 2))   # second-stage MF Q-values

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    prev_a1 = None
    prev_s = None

    for t in range(n_trials):

        # Model-based Q at stage 1 from learned transitions
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Bias from previous transition type:
        # - If previous transition was rare, bias to switch.
        # - If previous transition was common, bias to repeat.
        bias = np.zeros(2)
        if prev_a1 is not None and prev_s is not None:
            was_common = (prev_s == prev_a1)  # A->X and U->Y are "common"
            # Anxiety scales the magnitude of the bias
            anx_gain = (0.5 + 0.5 * np.clip(stai, 0.0, 1.0))
            mag = trans_sens * anx_gain
            if was_common:
                bias[prev_a1] += mag  # repeat after common
            else:
                bias[1 - prev_a1] += mag  # switch after rare

        logits1 = beta * q1_mb + bias
        logits1 -= np.max(logits1)
        exp1 = np.exp(logits1)
        probs1 = exp1 / (np.sum(exp1) + 1e-12)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage 2 policy (pure MF at second stage)
        s = state[t]
        logits2 = beta * q2[s]
        logits2 -= np.max(logits2)
        exp2 = np.exp(logits2)
        probs2 = exp2 / (np.sum(exp2) + 1e-12)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Valence- and anxiety-modulated learning rate
        # Positive outcomes (r>0.5): amplify learning less with higher anxiety if valence<0,
        # or more if valence>0. Negative outcomes (r<=0.5) amplified with higher anxiety when valence>0.
        if r > 0.5:
            alpha_eff = alpha * (1.0 + valence * (stai - 0.5))
        else:
            alpha_eff = alpha * (1.0 - valence * (stai - 0.5))
        alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

        # Q2 update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_eff * pe2

        # Learn the transition matrix row for chosen spaceship toward observed planet
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += eta_t * (target - T[a1])
        # Normalize to keep valid probabilities
        T[a1] = np.clip(T[a1], 1e-8, 1.0)
        T[a1] /= np.sum(T[a1])

        prev_a1 = a1
        prev_s = s

    eps = 1e-10
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll