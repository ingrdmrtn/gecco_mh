def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-modulated eligibility trace with choice perseveration (pure model-free).
    
    Core ideas:
    - Two-stage model-free Q-learning with an eligibility trace λ that increases when anxiety is low.
    - Perseveration (stickiness) bias to repeat previous actions at both stages; bias grows with anxiety.
    - One inverse temperature beta for both stages.
    
    Parameters (all used; total=5):
    - eta: [0,1] Learning rate for Q-value updates.
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - lam0: [0,1] Baseline eligibility trace parameter.
    - kappa_p: [0,1] Base magnitude of perseveration bias.
    - anx_gain: [0,1] Modulates λ with anxiety: λ_eff = lam0 + anx_gain*(1 - stai).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [eta, beta, lam0, kappa_p, anx_gain].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    eta, beta, lam0, kappa_p, anx_gain = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Effective eligibility and perseveration magnitudes
    lam = max(0.0, min(1.0, lam0 + anx_gain * (1.0 - stai)))  # higher when anxiety is lower
    stick_mag = kappa_p * (0.5 + 0.5 * stai)                  # stronger stickiness with higher anxiety

    # Q-values
    q1 = np.zeros(2)           # stage-1 MF values over ships
    q2 = 0.5 * np.ones((2, 2)) # stage-2 MF values over aliens per planet

    # Choice probabilities storage
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Perseveration traces (last choices), initialized to 0 (no bias on first trial)
    last_a1 = None
    last_a2 = [None, None]  # separate memory per planet

    for t in range(n_trials):
        s = int(state[t])

        # Perseveration features
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += stick_mag

        bias2 = np.zeros(2)
        la2 = last_a2[s]
        if la2 is not None:
            bias2[la2] += stick_mag

        # Stage-1 policy (softmax over q1 + bias)
        prefs1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(prefs1 - np.max(prefs1))
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (softmax over q2[s] + bias)
        prefs2 = beta * (q2[s] - np.max(q2[s])) + bias2
        probs2 = np.exp(prefs2 - np.max(prefs2))
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Learning
        r = float(reward[t])

        # Save q2 value before updating for lambda bootstrapping
        q2_old_sa = q2[s, a2]

        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta * pe2

        # Stage-1 TD(λ) update: mix of bootstrapping via q2_old and outcome r
        # pe1_lambda = (1 - λ) * (q2_old - q1[a1]) + λ * (r - q1[a1])
        pe1 = (1.0 - lam) * (q2_old_sa - q1[a1]) + lam * (r - q1[a1])
        q1[a1] += eta * pe1

        # Update perseveration memories
        last_a1 = a1
        last_a2[s] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Count-based exploration at stage-2 with anxiety-dampened novelty and transition-aware credit at stage-1.
    
    Core ideas:
    - Stage-2 uses a count-based bonus (novelty seeking), scaled down by anxiety.
    - Stage-1 Q is model-free but its credit assignment depends on transition commonality,
      with stronger preference to credit common transitions (and to discount rare) as anxiety increases.
    - Hybrid action values at stage-1: blend of MF Q1 and a simple model-based estimate via known transitions.
    
    Parameters (all used; total=5):
    - alpha_r: [0,1] Learning rate for reward prediction errors (applied to both stages).
    - beta: [0,10] Inverse temperature for softmax at both stages.
    - xi0: [0,1] Base magnitude of count-based exploration bonus at stage-2.
    - kappa_anx: [0,1] Scales how much anxiety suppresses exploration: xi_eff = xi0*(1 - kappa_anx*stai).
    - omega_trans: [0,1] Strength of common-vs-rare credit asymmetry at stage-1 (stronger with higher anxiety).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [alpha_r, beta, xi0, kappa_anx, omega_trans].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    alpha_r, beta, xi0, kappa_anx, omega_trans = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition structure: A->X common, U->Y common
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Effective exploration bonus magnitude decreases with anxiety
    xi_eff = xi0 * max(0.0, 1.0 - kappa_anx * stai)

    # Q-values
    q1_mf = np.zeros(2)           # stage-1 MF values
    q2 = 0.5 * np.ones((2, 2))    # stage-2 values per planet/alien

    # Count table for exploration bonus
    N = np.ones((2, 2), dtype=float)  # start at 1 to avoid division by zero

    # Choice probabilities
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # MB-MF arbitration weight depends on anxiety (less anxious -> more MB)
    w_mb = 0.5 + 0.5 * (1.0 - stai)

    for t in range(n_trials):
        s = int(state[t])

        # Stage-2 policy with count-based exploration bonus
        bonus = xi_eff / np.sqrt(N[s] + 1e-8)
        q2_bonus = q2[s] + bonus

        z2 = beta * (q2_bonus - np.max(q2_bonus))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Compute model-based proxy for stage-1: expected best value at next state
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Observe reward and update
        r = float(reward[t])

        # Stage-2 learning
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2
        N[s, a2] += 1.0

        # Transition commonality
        is_common = ((a1 == 0 and s == 0) or (a1 == 1 and s == 1))
        # Weight common credits more, rare credits less; effect larger with higher anxiety and omega_trans
        base = 0.5
        ampl = 0.5 * omega_trans * (0.5 + 0.5 * stai)  # in [0,0.5]
        w_common = base + ampl
        w_rare = base - ampl
        w_credit = w_common if is_common else w_rare

        # Stage-1 MF credit toward the realized second-stage value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += (alpha_r * w_credit) * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with asymmetric learning and anxiety-modulated MB arbitration by reliability.
    
    Core ideas:
    - Utility transformation u(r) = r^(theta) to capture risk sensitivity.
    - Asymmetric learning rates for positive vs. negative prediction errors at stage-2.
    - Model-based (MB) values from the known transition matrix; arbitration weight scales with
      an online reliability signal (low reward variance) and is reduced by anxiety.
    
    Parameters (all used; total=5):
    - lr_pos: [0,1] Learning rate when the stage-2 prediction error is positive.
    - lr_neg: [0,1] Learning rate when the stage-2 prediction error is negative.
    - beta: [0,10] Inverse temperature for both stages.
    - theta_risk: [0,1] Risk sensitivity; theta<1 concave utility (risk-averse), theta=1 linear.
    - zeta_anx: [0,1] Anxiety penalty on MB arbitration: w_mb *= (1 - zeta_anx*stai).
    
    Inputs:
    - action_1: int array of shape (T,), first-stage choices in {0,1} (0=A, 1=U).
    - state: int array of shape (T,), second-stage planet in {0,1}.
    - action_2: int array of shape (T,), second-stage choices in {0,1}.
    - reward: float array of shape (T,), rewards in [0,1].
    - stai: array-like with a single float in [0,1], anxiety score.
    - model_parameters: iterable [lr_pos, lr_neg, beta, theta_risk, zeta_anx].
    
    Returns:
    - Negative log-likelihood of observed Stage-1 and Stage-2 choices.
    """
    lr_pos, lr_neg, beta, theta_risk, zeta_anx = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transition model for MB
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Q-values over utility (not raw reward) at stage-2
    q2 = 0.5 * np.ones((2, 2))
    q1_mf = np.zeros(2)

    # Reliability tracker from reward stream: running mean and variance
    # Decay depends on anxiety (more anxious -> faster updating, less smoothing)
    gamma = 0.9 - 0.4 * stai   # in [0.5, 0.9]
    m = 0.5
    v = 0.25  # initial variance ~ Bernoulli(0.5)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])

        # Compute MB value using current q2 (utility domain)
        max_q2 = np.max(q2, axis=1)
        q1_mb = T_known @ max_q2

        # Reliability from variance (lower variance => higher reliability)
        v = max(1e-6, v)  # avoid zero
        reliability = 1.0 - min(1.0, v / (m * (1.0 - m) + 1e-6)) if 0.0 < m < 1.0 else 0.0
        reliability = np.clip(reliability, 0.0, 1.0)

        # Anxiety reduces reliance on MB via multiplicative penalty
        w_mb = reliability * max(0.0, 1.0 - zeta_anx * stai)
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage-1 policy
        z1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(z1)
        probs1 /= np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (over utility values)
        z2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(z2)
        probs2 /= np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and compute utility
        r = float(reward[t])
        u = r ** max(1e-6, theta_risk)

        # Stage-2 learning with asymmetric rates on utility PE
        pe2 = u - q2[s, a2]
        lr = lr_pos if pe2 >= 0.0 else lr_neg
        q2[s, a2] += lr * pe2

        # Stage-1 MF bootstrapping toward the realized second-stage value (utility domain)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += 0.5 * (lr_pos + lr_neg) * pe1  # use average LR to keep within [0,1]

        # Update reliability trackers (mean and variance of raw reward stream)
        m = gamma * m + (1.0 - gamma) * r
        # Exponential moving variance update
        v = gamma * v + (1.0 - gamma) * (r - m) ** 2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll