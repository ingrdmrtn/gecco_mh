def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Optimistic-uncertainty model with anxiety-modulated optimism and adaptive subjective transitions.

    Core ideas:
    - Stage-2 Q-values are learned via incremental prediction errors.
    - An uncertainty bonus encourages exploration early; its magnitude is modulated by anxiety.
      Higher anxiety reduces optimism/exploration.
    - Stage-1 planning uses a learned subjective transition matrix that adapts to experienced transitions.

    Parameters (bounds):
    - model_parameters[0] = alpha (0 to 1): learning rate for stage-2 Q-values
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = opt0 (0 to 1): baseline optimism/uncertainty bonus strength
    - model_parameters[3] = anx_opt (0 to 1): strength of anxiety modulation of optimism
        optimism_eff = clip(opt0 + (0.5 - stai) * anx_opt, 0, 1); higher stai lowers optimism if anx_opt>0
    - model_parameters[4] = kappaT (0 to 1): learning rate for updating subjective transition matrix

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial on the reached planet
    - reward: array of floats in [0,1], coins received per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified

    Returns:
    - Negative log-likelihood of the observed sequence of choices (stage 1 and stage 2).
    """
    alpha, beta, opt0, anx_opt, kappaT = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Initialize canonical transition structure (A->X common, U->Y common)
    T0 = np.array([[0.7, 0.3],
                   [0.3, 0.7]], dtype=float)
    # Subjective transition model starts at canonical but adapts with kappaT
    T_hat = T0.copy()

    # Stage-2 values and simple visitation counts for uncertainty
    q2 = np.zeros((2, 2), dtype=float)
    n_sa = np.zeros((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    # Anxiety-modulated optimism/uncertainty bonus
    optimism_eff = np.clip(opt0 + (0.5 - stai_val) * anx_opt, 0.0, 1.0)

    for t in range(n_trials):
        # Compute uncertainty bonus for each state-action
        bonus = optimism_eff / np.sqrt(n_sa + 1.0)

        # Stage-1: model-based planning using subjective transitions and optimistic values
        max_q2_bonus = np.max(q2 + bonus, axis=1)   # shape (2,)
        q1_mb = T_hat @ max_q2_bonus               # shape (2,)
        q1c = q1_mb - np.max(q1_mb)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2: softmax on optimistic values in the actually reached state
        s = int(state[t])
        q2_net = q2[s] + bonus[s]
        q2c = q2_net - np.max(q2_net)
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Learning at stage 2
        n_sa[s, a2] += 1.0
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        # Update subjective transition model for the chosen stage-1 action toward observed state
        # Move the chosen row toward a one-hot vector indicating the observed state
        # This preserves row stochasticity.
        oh = np.array([0.0, 0.0])
        oh[s] = 1.0
        T_hat[a1] = (1.0 - kappaT) * T_hat[a1] + kappaT * oh
        # Ensure numerical issues don't break stochasticity
        T_hat[a1] = T_hat[a1] / np.sum(T_hat[a1])

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Asymmetric model-free learning with eligibility trace and anxiety-driven lapse.

    Core ideas:
    - Stage-2 and Stage-1 model-free values are updated using asymmetric learning rates
      for positive vs negative prediction errors.
    - An eligibility trace propagates Stage-2 outcomes to Stage-1 values on the chosen action.
    - Choice policy includes an anxiety-driven lapse component: higher anxiety -> more random choice.

    Parameters (bounds):
    - model_parameters[0] = a_pos (0 to 1): learning rate for positive prediction errors
    - model_parameters[1] = a_neg (0 to 1): learning rate for negative prediction errors
    - model_parameters[2] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[3] = lapse0 (0 to 1): baseline lapse mixing weight
        Effective lapse = lapse0 * stai (higher anxiety increases random responding)
    - model_parameters[4] = lam (0 to 1): eligibility trace strength from stage 2 to stage 1

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial on the reached planet
    - reward: array of floats in [0,1], coins received per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified

    Returns:
    - Negative log-likelihood of the observed sequence of choices (stage 1 and stage 2).
    """
    a_pos, a_neg, beta, lapse0, lam = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Effective lapse increases with anxiety
    lapse = np.clip(lapse0 * stai_val, 0.0, 1.0)

    # Model-free values
    q1 = np.zeros(2, dtype=float)        # stage-1 action values (MF)
    q2 = np.zeros((2, 2), dtype=float)   # stage-2 values for each state

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Stage-1 policy (with lapse)
        q1c = q1 - np.max(q1)
        soft1 = np.exp(beta * q1c)
        soft1 = soft1 / np.sum(soft1)
        probs1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (with lapse)
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        soft2 = np.exp(beta * q2c)
        soft2 = soft2 / np.sum(soft2)
        probs2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        r = float(reward[t])

        # Stage-2 learning with asymmetric rates
        delta2 = r - q2[s, a2]
        alpha2 = a_pos if delta2 >= 0.0 else a_neg
        q2[s, a2] += alpha2 * delta2

        # Stage-1 MF update via eligibility trace from Stage-2 PE (asymmetric)
        alpha1 = a_pos if delta2 >= 0.0 else a_neg
        q1[a1] += (alpha1 * lam) * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Risk-sensitive utility with volatility-adaptive learning and anxiety-modulated risk aversion.

    Core ideas:
    - Rewards are transformed by a power utility u(r) = r^(rho_eff),
      where rho_eff decreases with anxiety (more risk-averse with higher anxiety).
    - The learning rate adapts to recent outcome volatility estimated from absolute prediction errors.
    - Stage-1 uses model-based planning via fixed known transitions.

    Parameters (bounds):
    - model_parameters[0] = lr0 (0 to 1): base learning rate
    - model_parameters[1] = beta (0 to 10): inverse temperature for softmax at both stages
    - model_parameters[2] = rho0 (0 to 1): baseline risk sensitivity exponent
    - model_parameters[3] = anx_rho (0 to 1): anxiety modulation of risk sensitivity
        rho_eff = clip(rho0 - anx_rho * stai, 0, 1); higher stai -> smaller rho -> more concave utility
    - model_parameters[4] = tau_v (0 to 1): volatility smoothing rate for absolute PEs

    Inputs:
    - action_1: array of ints in {0,1}, chosen spaceship per trial (0=A, 1=U)
    - state: array of ints in {0,1}, reached planet per trial (0=X, 1=Y)
    - action_2: array of ints in {0,1}, chosen alien per trial on the reached planet
    - reward: array of floats in [0,1], coins received per trial
    - stai: array-like with one float in [0,1], participant anxiety score
    - model_parameters: list/array of 5 parameters as specified

    Returns:
    - Negative log-likelihood of the observed sequence of choices (stage 1 and stage 2).
    """
    lr0, beta, rho0, anx_rho, tau_v = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure (known to the participant)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated risk sensitivity
    rho_eff = np.clip(rho0 - anx_rho * stai_val, 0.0, 1.0)

    # Values and volatility tracker
    q2 = np.zeros((2, 2), dtype=float)
    vol = 0.0  # running estimate of absolute PE magnitude in [0,1]

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    for t in range(n_trials):
        # Stage-1 model-based planning using current stage-2 values
        max_q2 = np.max(q2, axis=1)         # best action per state
        q1_mb = T @ max_q2
        q1c = q1_mb - np.max(q1_mb)
        probs1 = np.exp(beta * q1c)
        probs1 = probs1 / np.sum(probs1)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 choice
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        probs2 = np.exp(beta * q2c)
        probs2 = probs2 / np.sum(probs2)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Risk-sensitive utility of reward
        r = float(reward[t])
        # Avoid 0^0 by adding tiny epsilon inside power when needed
        u = (r + 1e-12) ** rho_eff

        # Prediction error and volatility update
        delta2 = u - q2[s, a2]
        vol = (1.0 - tau_v) * vol + tau_v * abs(delta2)
        # Volatility-adaptive learning rate: larger vol -> larger effective lr, bounded in [0, lr0]
        alpha_eff = lr0 * (0.5 + 0.5 * np.clip(vol, 0.0, 1.0))

        # Update stage-2 value
        q2[s, a2] += alpha_eff * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)