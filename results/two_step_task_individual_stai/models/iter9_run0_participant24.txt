def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with anxiety-modulated model-based reliance and exploration bonus.

    This model mixes a model-free (MF) learner with a model-based (MB) planner. It adds an
    uncertainty-driven exploration bonus at the second stage that is stronger with higher anxiety.
    Anxiety also shifts the MB/MF mixing weight toward more model-based control.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, psi_mb0, chi_bonus0, chi_anx]
        - alpha in [0,1]: learning rate for MF values at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - psi_mb0 in [0,1]: baseline weight on MB control (before anxiety modulation).
        - chi_bonus0 in [0,1]: baseline strength of second-stage exploration bonus.
        - chi_anx in [0,1]: how much anxiety increases the exploration bonus.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, psi_mb0, chi_bonus0, chi_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Fixed transition structure (A->X common, U->Y common)
    T_fixed = np.array([[0.7, 0.3],  # A to [X, Y]
                        [0.3, 0.7]])  # U to [X, Y]

    # Values
    q1_mf = np.zeros(2)        # MF at stage 1
    q2_mf = np.zeros((2, 2))   # MF at stage 2 (per state, per alien)

    # Visit counts for exploration bonus
    n_visits = np.ones((2, 2))  # start at 1 to keep bonus finite

    # Anxiety-modulated MB weight and bonus strength
    # Higher anxiety pushes weight toward 1 (more MB).
    omega = psi_mb0 * (1 - s) + (1 - psi_mb0) * s
    # Bonus strength increases with anxiety
    bonus_gain = chi_bonus0 + chi_anx * s

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute MB Q-values at stage 1 using fixed transitions and current stage-2 MF values + bonus
        bonus_per_state = bonus_gain / np.sqrt(1.0 + np.sum(n_visits, axis=1))  # smaller bonus as state gets visited
        # Add per-action bonus at stage 2
        bonus_actions = bonus_gain / np.sqrt(n_visits)  # shape (2,2)
        max_q2_with_bonus = np.max(q2_mf + bonus_actions, axis=1)  # shape (2,)
        q1_mb = T_fixed @ max_q2_with_bonus

        # Mixed stage-1 values
        q1_mix = omega * q1_mb + (1 - omega) * q1_mf

        # Stage-1 policy
        prefs1 = q1_mix
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in the visited state, with exploration bonus
        st = state[t]
        prefs2 = q2_mf[st] + bonus_actions[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Learning updates (MF)
        r = reward[t]

        # Stage-2 MF update
        pe2 = r - q2_mf[st, a2]
        q2_mf[st, a2] += alpha * pe2

        # Stage-1 MF update bootstrapping on stage-2 value (post-update)
        pe1 = q2_mf[st, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update visit counts
        n_visits[st, a2] += 1.0

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Pure model-based with learned transitions, anxiety-modulated surprise weighting, and lapse.

    This model learns:
      - the transition model T(a->s) online,
      - second-stage values for aliens,
    and chooses using a model-based plan. Anxiety increases sensitivity to surprising (rare) transitions
    when updating T, and increases a lapse rate that mixes choices with uniform randomness.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [nu_q, beta, k_T, eps0, eps_anx]
        - nu_q in [0,1]: learning rate for second-stage alien values.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - k_T in [0,1]: baseline learning rate for transition probabilities.
        - eps0 in [0,1]: baseline lapse rate mixing with uniform choice.
        - eps_anx in [0,1]: how much anxiety increases the lapse rate.

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    nu_q, beta, k_T, eps0, eps_anx = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # Initialize transition model close to symmetric (uninformative)
    T = np.ones((2, 2)) * 0.5  # rows sum to 1
    q2 = np.zeros((2, 2))      # second-stage values

    # Common transitions given task structure (for defining "surprise")
    T_common = np.array([[0.7, 0.3],
                         [0.3, 0.7]])

    # Anxiety-modulated lapse rate
    eps = min(1.0, eps0 + eps_anx * s)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # MB action values at stage 1: expected value across next states
        max_q2 = np.max(q2, axis=1)      # value of best alien per state
        q1_mb = T @ max_q2               # shape (2,)

        # Stage-1 softmax with lapse
        prefs1 = q1_mb
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        soft1 = exp1 / np.sum(exp1)
        probs1 = (1 - eps) * soft1 + eps * 0.5
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 softmax with lapse (within visited state)
        st = state[t]
        prefs2 = q2[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        soft2 = exp2 / np.sum(exp2)
        probs2 = (1 - eps) * soft2 + eps * 0.5
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome
        r = reward[t]

        # Update second-stage values (MB learns expected rewards at aliens)
        pe2 = r - q2[st, a2]
        q2[st, a2] += nu_q * pe2

        # Update transition model T based on surprise (rare transitions get larger updates with higher anxiety)
        # Identify whether the observed transition was "common" or "rare" under the known structure.
        observed_common_prob = T_common[a1, st]
        was_rare = 1.0 if observed_common_prob < 0.5 else 0.0

        # Surprise-scaled learning rate: increase for rare transitions proportionally to anxiety
        k_eff = k_T * (1.0 + s * 0.75 * was_rare)
        k_eff = min(1.0, max(0.0, k_eff))

        # Move T[a1] toward the one-hot of the observed state
        target = np.array([1.0 if st == 0 else 0.0, 1.0 if st == 1 else 0.0])
        T[a1] = (1 - k_eff) * T[a1] + k_eff * target
        # Ensure row normalization
        T[a1] = T[a1] / np.sum(T[a1])

    eps_num = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps_num)) + np.sum(np.log(p_choice_2 + eps_num)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with anxiety-driven uncertainty aversion and forgetting.

    This model is purely model-free but includes two anxiety-dependent mechanisms:
      - Uncertainty aversion at the second stage: preference subtracts a penalty proportional
        to the current reward uncertainty of each alien; anxiety increases this penalty.
      - Value forgetting: Q-values decay toward 0.5 each trial; forgetting increases with anxiety.

    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices (0=A, 1=U).
    state : array-like of int (0 or 1)
        Second-stage states visited (0=X, 1=Y).
    action_2 : array-like of int (0 or 1)
        Second-stage choices within the visited state (X: 0=W,1=S; Y: 0=P,1=H).
    reward : array-like of float
        Received reward on each trial (e.g., 0/1).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used.
    model_parameters : array-like
        [alpha, beta, kappa0, kappa_anx, phi0]
        - alpha in [0,1]: learning rate for MF updates at both stages.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - kappa0 in [0,1]: baseline penalty weight for uncertainty at stage 2.
        - kappa_anx in [0,1]: how much anxiety increases uncertainty aversion.
        - phi0 in [0,1]: baseline forgetting rate toward 0.5 per trial (scaled by anxiety).

    Returns
    -------
    float
        Negative log-likelihood of the observed first- and second-stage choices.
    """
    alpha, beta, kappa0, kappa_anx, phi0 = model_parameters
    n_trials = len(action_1)
    s = float(stai[0])

    # MF values
    q1 = np.zeros(2)
    q2 = np.zeros((2, 2))

    # Track counts and success counts per alien to estimate uncertainty
    # Beta-Bernoulli posterior with Beta(1,1) prior
    n_counts = np.zeros((2, 2))
    n_success = np.zeros((2, 2))

    # Anxiety-modulated parameters
    kappa = kappa0 + kappa_anx * s                 # uncertainty penalty weight
    phi = min(1.0, phi0 * (0.5 + s))               # forgetting toward 0.5, larger with anxiety

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute uncertainty estimates per alien: posterior variance of Beta(a,b)
        a_post = 1.0 + n_success
        b_post = 1.0 + n_counts - n_success
        p_post = a_post / (a_post + b_post)
        var_post = (a_post * b_post) / (((a_post + b_post) ** 2) * (a_post + b_post + 1.0))

        # Stage-1 policy (MF)
        prefs1 = q1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        probs1 = exp1 / np.sum(exp1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy in visited state: subtract uncertainty penalty
        st = state[t]
        prefs2 = q2[st] - kappa * var_post[st]
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        probs2 = exp2 / np.sum(exp2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # Update counts for uncertainty estimates
        n_counts[st, a2] += 1.0
        n_success[st, a2] += r

        # MF learning updates
        pe2 = r - q2[st, a2]
        q2[st, a2] += alpha * pe2

        # Bootstrapped update to stage-1 MF value using the chosen alien's value (post-update)
        pe1 = q2[st, a2] - q1[a1]
        q1[a1] += alpha * pe1

        # Forgetting toward 0.5 baseline, scaled by anxiety
        q2 = (1 - phi) * q2 + phi * 0.5
        q1 = (1 - phi) * q1 + phi * 0.5

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll