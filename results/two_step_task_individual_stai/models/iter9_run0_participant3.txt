def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF model with anxiety-tuned reward curvature and transition mistrust.

    Mechanism overview:
    - Stage-2 values Q2(s2, a2) are learned with a single learning rate.
    - Stage-1 action values are a hybrid of model-based planning (through an effective
      transition matrix) and model-free cached values.
    - High anxiety reduces trust in the known common transitions by blending them toward
      an uncertain (uniform) transition matrix; it also reduces choice precision.
    - Rewards are distorted by a curvature parameter that is amplified by anxiety.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices (A=0, U=1)
    - state:    int array (n_trials,) in {0,1}; reached second-stage planet (X=0, Y=1)
    - action_2: int array (n_trials,) in {0,1}; second-stage alien choice (W/S on X; P/H on Y)
    - reward:   float array (n_trials,) in [0,1]; coins received
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha_q   in [0,1]: learning rate for Q2 and MF backup to Q1
        beta      in [0,10]: inverse temperature for softmax at both stages
        omega_mix in [0,1]: weight on model-based component at stage-1
        phi_shape in [0,1]: reward curvature control (anxiety-amplified)
        zeta_trans in [0,1]: anxiety-amplified mistrust of transitions (blend toward uniform)

    Returns:
    - Negative log-likelihood of observed stage-1 and stage-2 choices.
    """
    alpha_q, beta, omega_mix, phi_shape, zeta_trans = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Known common transitions (A->X, U->Y with prob 0.7)
    T_nom = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Value tables
    Q2 = np.zeros((2, 2), dtype=float)   # Q2[s2, a2]
    Q1_mf = np.zeros(2, dtype=float)     # cached model-free values at stage-1

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    eps = 1e-12

    for t in range(n_trials):
        # Anxiety-tuned effective transition: blend nominal with uniform as anxiety increases
        k_mistrust = zeta_trans * s_anx
        T_eff = (1.0 - k_mistrust) * T_nom + k_mistrust * 0.5

        # Planning with current Q2
        max_q2 = np.max(Q2, axis=1)              # value of second-stage states
        Q1_mb = T_eff @ max_q2                   # model-based action values

        # Anxiety-tuned softmax precision (higher anxiety -> more noise)
        beta_eff = beta * (1.2 - 0.4 * s_anx)

        # Stage-1 policy
        Q1 = omega_mix * Q1_mb + (1.0 - omega_mix) * Q1_mf
        logits1 = beta_eff * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy
        s2 = int(state[t])
        logits2 = beta_eff * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Anxiety-amplified reward curvature (phi_eff > 0)
        # phi_shape in [0,1] -> center at 1, modulated by anxiety
        phi_eff = 1.0 + (phi_shape - 0.5) * (0.5 + s_anx)
        phi_eff = float(max(0.1, phi_eff))
        r = float(reward[t])
        r_tilde = r ** phi_eff

        # Updates
        # Stage-2 value learning
        delta2 = r_tilde - Q2[s2, a2]
        Q2[s2, a2] += alpha_q * delta2

        # Stage-1 model-free update: bootstrap toward current Q2 and propagate immediate delta
        target1 = Q2[s2, a2]
        delta1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha_q * delta1
        # additional eligibility-like propagation of reward prediction error
        Q1_mf[a1] += alpha_q * delta2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """MB Stage-1 with anxiety-shaped habit and uncertainty-sensitive exploration at Stage-2.

    Mechanism overview:
    - Learns both transition probabilities and second-stage values.
    - Stage-1 choices are model-based, augmented by a habit term (perseveration) whose
      strength scales with anxiety (higher anxiety -> stronger habit).
    - Stage-2 choices use an uncertainty-sensitive bonus (UCB-style). Anxiety flips the
      sign of the bonus: low anxiety seeks uncertainty; high anxiety avoids it.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices
    - state:    int array (n_trials,) in {0,1}; reached second-stage state
    - action_2: int array (n_trials,) in {0,1}; second-stage choice
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        alpha_v  in [0,1]: learning rate for Q2
        beta     in [0,10]: inverse temperature at both stages
        c_u      in [0,1]: magnitude of uncertainty bonus at stage-2
        omega_h  in [0,1]: habit strength at stage-1 (anxiety-scaled)
        tau_T    in [0,1]: learning rate for transition probabilities

    Returns:
    - Negative log-likelihood of observed choices (stage-1 and stage-2).
    """
    alpha_v, beta, c_u, omega_h, tau_T = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Initialize learned transitions (start agnostic)
    T = np.ones((2, 2), dtype=float) * 0.5

    # Second-stage values and reward-counts for uncertainty estimate
    Q2 = np.zeros((2, 2), dtype=float)
    # Beta-Binomial conjugate counts for each (state, action): prior a=b=1
    a_succ = np.ones((2, 2), dtype=float)
    b_fail = np.ones((2, 2), dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a1 = -1
    eps = 1e-12

    for t in range(n_trials):
        # Model-based value for each first-stage action
        max_q2 = np.max(Q2, axis=1)
        Q1_mb = T @ max_q2

        # Habit term scaled by anxiety
        habit = np.zeros(2, dtype=float)
        if prev_a1 in (0, 1):
            habit[prev_a1] = 1.0
        habit_weight = omega_h * (0.5 + 0.5 * s_anx)

        # Stage-1 softmax
        logits1 = beta * Q1_mb + habit_weight * habit
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 uncertainty-sensitive softmax (UCB-style bonus)
        s2 = int(state[t])

        # Posterior mean/variance for reward probability
        # var = (a*b)/((a+b)^2 * (a+b+1))
        a_post = a_succ[s2]
        b_post = b_fail[s2]
        var = (a_post * b_post) / (((a_post + b_post) ** 2) * (a_post + b_post + 1.0) + eps)

        # Anxiety flips exploration direction: low anxiety -> seek (positive bonus),
        # high anxiety -> avoid (negative bonus).
        sign = 1.0 - 2.0 * s_anx  # +1 at stai=0, 0 at .5, -1 at 1
        bonus = sign * c_u * np.sqrt(var)

        q2_eff = Q2[s2] + bonus
        logits2 = beta * q2_eff
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward and update
        r = float(reward[t])

        # Transition learning: move chosen row toward observed state
        T[a1, :] = (1.0 - tau_T) * T[a1, :]
        T[a1, s2] += tau_T
        # ensure normalization (small numerical guard)
        T[a1, :] = T[a1, :] / (np.sum(T[a1, :]) + eps)

        # Update Q2
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha_v * delta2

        # Update Beta-Binomial counts for uncertainty tracking
        a_succ[s2, a2] += r
        b_fail[s2, a2] += (1.0 - r)

        prev_a1 = a1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive associability (Pearce-Hall) with anxiety-weighted uncertainty avoidance and stickiness.

    Mechanism overview:
    - Stage-2 values learn with an adaptive, option-specific learning rate (associability)
      that increases with recent surprise. Anxiety amplifies effective learning.
    - Stage-1 action values combine model-based planning with a model-free cache.
    - Anxiety drives avoidance of uncertain states: the MB evaluation subtracts an
      expected uncertainty penalty proportional to the associability of target states.
    - Stage-2 includes action stickiness that grows with anxiety.

    Parameters and bounds:
    - action_1: int array (n_trials,) in {0,1}; first-stage choices
    - state:    int array (n_trials,) in {0,1}; reached second-stage state
    - action_2: int array (n_trials,) in {0,1}; second-stage choice
    - reward:   float array (n_trials,) in [0,1]
    - stai:     float array with single element in [0,1]; anxiety score
    - model_parameters: tuple/list with five params:
        kappa0     in [0,1]: associability update rate (toward recent |PE|)
        beta       in [0,10]: inverse temperature at both stages
        omega_plan in [0,1]: weight on MB vs MF at stage-1
        psi_anx    in [0,1]: strength of anxiety effects (on associability and penalty)
        stick2     in [0,1]: stage-2 perseveration strength (anxiety-scaled)

    Returns:
    - Negative log-likelihood of observed choices (stage-1 and stage-2).
    """
    kappa0, beta, omega_plan, psi_anx, stick2 = model_parameters
    n_trials = len(action_1)
    s_anx = float(stai[0])

    # Fixed nominal transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Values and associabilities
    Q2 = np.zeros((2, 2), dtype=float)
    A2 = np.ones((2, 2), dtype=float) * 0.5  # associability per (state, action) in [0,1]
    Q1_mf = np.zeros(2, dtype=float)

    p_choice_1 = np.zeros(n_trials, dtype=float)
    p_choice_2 = np.zeros(n_trials, dtype=float)

    prev_a2 = -1
    eps = 1e-12

    for t in range(n_trials):
        # Expected uncertainty per state as mean associability
        unc_state = np.mean(A2, axis=1)  # shape (2,)

        # Model-based evaluation with anxiety-weighted uncertainty penalty
        max_q2 = np.max(Q2, axis=1)
        # Expected uncertainty of successor states for each a1
        exp_unc = T @ unc_state
        penalty = s_anx * psi_anx * exp_unc
        Q1_mb = (T @ max_q2) - penalty

        # Hybrid with model-free cache
        Q1 = omega_plan * Q1_mb + (1.0 - omega_plan) * Q1_mf

        # Stage-1 policy
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1) / (np.sum(np.exp(logits1)) + eps)
        a1 = int(action_1[t])
        p_choice_1[t] = probs1[a1]

        # Stage-2 stickiness with anxiety scaling
        s2 = int(state[t])
        stick_vec = np.zeros(2, dtype=float)
        if prev_a2 in (0, 1):
            stick_vec[prev_a2] = 1.0
        stick2_eff = stick2 * (0.5 + 0.5 * s_anx)

        logits2 = beta * Q2[s2] + stick2_eff * stick_vec
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2) / (np.sum(np.exp(logits2)) + eps)
        a2 = int(action_2[t])
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = float(reward[t])

        # Adaptive learning at stage-2 (Pearce-Hall associability)
        pe2 = r - Q2[s2, a2]
        # Update associability toward |PE|
        A2[s2, a2] = (1.0 - kappa0) * A2[s2, a2] + kappa0 * abs(pe2)
        # Effective learning rate amplified by anxiety
        lr_eff = A2[s2, a2] * (0.5 + 0.5 * psi_anx * s_anx)
        lr_eff = float(np.clip(lr_eff, 0.0, 1.0))
        Q2[s2, a2] += lr_eff * pe2

        # Stage-1 model-free cache: bootstrap toward current Q2 and propagate pe2
        target1 = Q2[s2, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += lr_eff * pe1
        Q1_mf[a1] += lr_eff * pe2

        prev_a2 = a2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return float(nll)