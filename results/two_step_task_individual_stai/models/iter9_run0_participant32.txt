def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Kalman-TD with anxiety-modulated observation noise and eligibility trace.
    
    Core idea:
    - Second-stage (aliens) values are learned with a Kalman-like adaptive learning rate that depends on estimated uncertainty.
    - Observation noise increases with anxiety, reducing the Kalman gain (i.e., slower learning under higher anxiety).
    - First-stage values are updated via a TD backup with an eligibility-like gate that scales with anxiety.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state reached per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage action (alien) per trial.
    reward : array-like of float
        Reward outcome per trial (e.g., 0 or 1).
    stai : array-like of float
        Anxiety score array; uses stai[0] in [0,1+].
    model_parameters : list or array-like of float
        [v0_init, beta, eta_obs, lambda_trace, chi_anxGain]
        Bounds:
        - v0_init: [0,1] initial uncertainty scale for second-stage Q-values.
        - beta: [0,10] inverse temperature shared across stages.
        - eta_obs: [0,1] baseline observation noise (higher -> smaller learning rates).
        - lambda_trace: [0,1] baseline trace gating for first-stage TD update.
        - chi_anxGain: [0,1] scales how much anxiety inflates observation noise.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    v0_init, beta, eta_obs, lambda_trace, chi_anxGain = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Action values
    q1 = np.zeros(2)           # first-stage actions A/U
    q2 = np.zeros((2, 2))      # second-stage states X/Y by aliens

    # Uncertainty (variance) for second-stage Q-values (Kalman filter variance per state-action)
    V2 = np.ones((2, 2)) * (1e-4 + 0.99 * v0_init)  # small positive to start

    # Process noise (how quickly contingencies drift) tied to initial uncertainty
    process_noise = 0.05 * v0_init

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # POLICY first stage: purely model-free (from q1)
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # POLICY second stage: from q2 of reached state
        s = state[t]
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # OUTCOME
        r = reward[t]

        # Kalman-like update at second stage
        # Observation noise increases with anxiety (reducing gain)
        obs_noise = eta_obs * (1.0 + chi_anxGain * stai) + 1e-8
        # Prior variance increases due to process noise
        V_prior = V2[s, a2] + process_noise
        # Kalman gain
        K = V_prior / (V_prior + obs_noise)
        pe2 = r - q2[s, a2]
        q2[s, a2] += K * pe2
        # Posterior variance
        V2[s, a2] = (1.0 - K) * V_prior

        # First-stage TD update with eligibility-like gate scaled by anxiety
        pe1 = q2[s, a2] - q1[a1]
        gate = lambda_trace * (0.5 + 0.5 * stai)  # stronger trace with higher anxiety
        lr1 = np.clip(gate * K, 0.0, 1.0)        # learning rate inherits adaptivity from K
        q1[a1] += lr1 * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Learnable transitions with anxiety-modulated model-based reliance under surprise.
    
    Core idea:
    - The agent learns second-stage rewards (model-free) and first-stage transition probabilities.
    - First-stage policy blends model-based and model-free values.
    - Anxiety reduces reliance on the model-based plan when a surprising transition occurs.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Reached second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage choices per trial.
    reward : array-like of float
        Trial-wise reward.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha_r, beta, omega_plan, nu_trans, kappa_anxMB]
        Bounds:
        - alpha_r: [0,1] learning rate for reward values (Q2) and first-stage TD values (Q1-MF).
        - beta: [0,10] inverse temperature for both stages.
        - omega_plan: [0,1] baseline weight on model-based values at the first stage.
        - nu_trans: [0,1] learning rate for the transition model.
        - kappa_anxMB: [0,1] scales how strongly anxiety reduces model-based weight under surprising transitions.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_r, beta, omega_plan, nu_trans, kappa_anxMB = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Initialize learned transition model T[a, s]
    # Start with weak prior centered at 0.5 (uncertain)
    T = np.ones((2, 2)) * 0.5  # rows: actions A/U, cols: states X/Y

    # Model-free Q-values
    q1_mf = np.zeros(2)
    q2 = np.zeros((2, 2))

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based first-stage values from learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien in each state
        q1_mb = T @ max_q2

        # Surprise based on the probability of the observed state given chosen action
        a1 = action_1[t]
        s = state[t]
        p_obs = T[a1, s]
        surprise = 1.0 - p_obs  # higher if transition was unlikely under current model

        # Anxiety reduces MB reliance under surprise
        w = omega_plan - kappa_anxMB * stai * surprise
        w = np.clip(w, 0.0, 1.0)

        # First-stage policy: mixture of MB and MF
        q1_mix = w * q1_mb + (1.0 - w) * q1_mf
        logits1 = beta * (q1_mix - np.max(q1_mix))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Second-stage policy: softmax over q2 in reached state
        q2_s = q2[s]
        logits2 = beta * (q2_s - np.max(q2_s))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning
        r = reward[t]

        # Update transition model T for chosen action
        # Move the row toward a one-hot on the observed state
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s else 0.0
            T[a1, s_idx] = (1.0 - nu_trans) * T[a1, s_idx] + nu_trans * target
        # Renormalize row to ensure valid probabilities
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] = T[a1] / row_sum

        # Update second-stage value (model-free)
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * pe2

        # Update first-stage model-free value with TD from the realized second-stage value
        pe1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha_r * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Risk-sensitive exploitation with anxiety-amplified uncertainty and exploration bonus.
    
    Core idea:
    - Maintain running estimates of both mean and variance of each alien's payouts.
    - Second-stage choice uses an effective value: mean - risk_penalty*variance + exploration_bonus*sqrt(variance).
      Anxiety increases risk penalty and also boosts the uncertainty bonus (restless exploration).
    - First-stage policy is model-based from a fixed transition structure combined with a TD model-free update.
    
    Parameters
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices per trial (0=A, 1=U).
    state : array-like of int {0,1}
        Second-stage state per trial (0=X, 1=Y).
    action_2 : array-like of int {0,1}
        Second-stage actions per trial.
    reward : array-like of float
        Rewards per trial.
    stai : array-like of float
        Anxiety score array; uses stai[0].
    model_parameters : list or array-like of float
        [alpha_rw, beta, tau_risk, z_bon, psi_anxU]
        Bounds:
        - alpha_rw: [0,1] learning rate for mean and variance estimates; also used for first-stage TD.
        - beta: [0,10] inverse temperature.
        - tau_risk: [0,1] baseline weight on variance penalty (risk aversion).
        - z_bon: [0,1] baseline exploration bonus weight on sqrt(variance).
        - psi_anxU: [0,1] scales how anxiety amplifies both risk penalty and uncertainty bonus.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_rw, beta, tau_risk, z_bon, psi_anxU = model_parameters
    n_trials = len(action_1)
    stai = stai[0]

    # Fixed transition structure (common vs. rare)
    # A -> X common, U -> Y common
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Track mean and variance for each state-action
    m = np.zeros((2, 2))      # running mean of rewards
    m2 = np.zeros((2, 2))     # running second moment for variance estimation

    # First-stage model-free values
    q1_mf = np.zeros(2)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute variance from moments
        var = np.maximum(m2 - m * m, 0.0)

        # Anxiety-modulated weights
        risk_w = tau_risk * (0.5 + 0.5 * stai)
        bon_w = z_bon * (1.0 + psi_anxU * stai)

        # Second-stage effective Q for policy
        q2_eff = m - risk_w * var + bon_w * np.sqrt(var + 1e-12)

        # First-stage model-based value from fixed transitions using effective Q
        max_q2_eff = np.max(q2_eff, axis=1)
        q1_mb = T_fixed @ max_q2_eff

        # Blend MB with MF first-stage values equally weighted by anxiety-dependent confidence
        # Higher anxiety slightly down-weights MF in favor of MB planning here
        w_mb = np.clip(0.5 + 0.3 * stai, 0.0, 1.0)
        q1_mix = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage policy
        logits1 = beta * (q1_mix - np.max(q1_mix))
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / np.sum(probs1)
        a1 = action_1[t]
        p_choice_1[t] = probs1[a1]

        # Second-stage policy at reached state using q2_eff
        s = state[t]
        q2_s_eff = q2_eff[s]
        logits2 = beta * (q2_s_eff - np.max(q2_s_eff))
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / np.sum(probs2)
        a2 = action_2[t]
        p_choice_2[t] = probs2[a2]

        # Outcome and learning of moments
        r = reward[t]
        # Update moments for the selected state-action
        m_sa = m[s, a2]
        m[s, a2] = (1.0 - alpha_rw) * m[s, a2] + alpha_rw * r
        m2[s, a2] = (1.0 - alpha_rw) * m2[s, a2] + alpha_rw * (r * r)

        # First-stage TD update from realized effective value (using current q2_eff)
        target = q2_eff[s, a2]
        pe1 = target - q1_mf[a1]
        q1_mf[a1] += alpha_rw * pe1

    eps = 1e-12
    neg_log_lik = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return neg_log_lik