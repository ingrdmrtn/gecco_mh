def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid model-based/model-free with eligibility traces and anxiety-weighted arbitration.

    Core ideas
    - Stage-1 action values are a convex combination of model-free (bootstrapped) and
      model-based (transition-structured) values.
    - Stage-2 values are updated via TD learning.
    - An eligibility trace propagates the stage-2 prediction error back to the chosen
      stage-1 action.
    - Anxiety reduces planning reliance (model-based weight) and increases trace strength,
      and also reduces choice precision.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions (0=A, 1=U).
    - state: array-like of ints in {0,1}, reached second-stage state (0=X, 1=Y).
    - action_2: array-like of ints in {0,1}, second-stage actions (0/1).
    - reward: array-like of floats in [0,1], reward outcome per trial.
    - stai: array-like length-1, scalar anxiety score in [0,1].
    - model_parameters: tuple/list with 4 parameters:
        alpha_mf: base learning rate for Q updates in [0,1]
        beta: base inverse temperature for softmax in [0,10]
        w0_hybrid: base weight on model-based control in [0,1]
        lambda_elig: base eligibility trace strength (backprop to stage-1) in [0,1]

    Bounds
    - alpha_mf, w0_hybrid, lambda_elig in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Effective model-based weight: w_mb = clip(w0_hybrid * (1 - 0.7*stai), 0, 1)
      Higher anxiety reduces reliance on planning.
    - Effective eligibility trace: lam = clip(lambda_elig * (0.5 + 0.5*stai), 0, 1)
      Higher anxiety increases credit assignment to the chosen stage-1 action.
    - Effective inverse temperature: beta_eff = clip(beta * (1 - 0.3*stai), 0, 10)
      Higher anxiety yields noisier choices.

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_mf, beta_base, w0_hybrid, lambda_elig = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Transition structure: A->X common, U->Y common
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])  # rows: action A/U, cols: state X/Y

    # Anxiety-modulated parameters
    beta_eff = max(0.0, min(10.0, beta_base * (1.0 - 0.3 * stai)))
    w_mb = max(0.0, min(1.0, w0_hybrid * (1.0 - 0.7 * stai)))
    lam = max(0.0, min(1.0, lambda_elig * (0.5 + 0.5 * stai)))
    alpha = max(0.0, min(1.0, alpha_mf))

    # Initialize values
    q1_mf = np.zeros(2)         # model-free stage-1 Q
    q2 = np.zeros((2, 2))       # stage-2 Q for states X,Y and actions 0,1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Compute model-based stage-1 values from current q2
        max_q2 = np.max(q2, axis=1)        # value of each second-stage state
        q1_mb = T @ max_q2                 # expected value under known transitions

        # Hybrid action values for stage 1
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta_eff * q1c)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy for the reached state
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        pi2 = np.exp(beta_eff * q2c)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Outcomes and updates
        r = float(reward[t])

        # Stage-2 TD error and update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Eligibility trace back to chosen stage-1 action (bootstrapped by reached Q2)
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += lam * alpha * pe1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Adaptive learning rate from surprise with anxiety and perseveration bias at stage 1.

    Core ideas
    - Learning rates adapt on each trial as a function of recent surprise (|prediction error|).
    - Anxiety amplifies the sensitivity to surprise, increasing learning rate when outcomes are
      unexpected, and reduces choice precision.
    - A perseveration (stickiness) bias at stage 1 is strengthened by anxiety.
    - Stage-1 values are model-based (computed via the transition matrix over current stage-2 Q).

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions.
    - state: array-like of ints in {0,1}, reached second-stage state.
    - action_2: array-like of ints in {0,1}, second-stage actions.
    - reward: array-like of floats in [0,1], reward outcome per trial.
    - stai: array-like length-1, scalar anxiety score in [0,1].
    - model_parameters: tuple/list with 4 parameters:
        alpha_base: baseline learning rate in [0,1]
        beta: base inverse temperature in [0,10]
        k_vol: surprise-to-learning multiplier in [0,1]
        psi_stay: perseveration weight added to previous first-stage action in [0,1]

    Bounds
    - alpha_base, k_vol, psi_stay in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Surprise sensitivity: k_eff = k_vol * (1 + 0.8*stai)
    - Inverse temperature: beta_eff = beta * (1 - 0.35*stai), clipped to [0,10]
    - Perseveration: psi_eff = psi_stay * (0.5 + 0.5*stai)

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_base, beta_base, k_vol, psi_stay = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    beta_eff = max(0.0, min(10.0, beta_base * (1.0 - 0.35 * stai)))
    k_eff = max(0.0, min(1.0, k_vol * (1.0 + 0.8 * stai)))
    psi_eff = max(0.0, min(1.0, psi_stay * (0.5 + 0.5 * stai)))
    alpha0 = max(0.0, min(1.0, alpha_base))

    q2 = np.zeros((2, 2))  # stage-2 values
    q1_prev_action = None  # for perseveration

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Track last PE magnitude to adjust next-step learning rate
    last_abs_pe2 = 0.0

    for t in range(n_trials):
        # Model-based stage-1 values from current q2
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Perseveration bias on stage-1 logits
        bias = np.zeros(2)
        if q1_prev_action is not None:
            bias[int(q1_prev_action)] = psi_eff

        # Stage-1 policy
        logits1 = q1_mb + bias
        logits1 -= np.max(logits1)
        pi1 = np.exp(beta_eff * logits1)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy
        s = int(state[t])
        q2c = q2[s] - np.max(q2[s])
        pi2 = np.exp(beta_eff * q2c)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Outcome and adaptive learning rate
        r = float(reward[t])
        alpha_t = alpha0 + k_eff * last_abs_pe2
        alpha_t = max(0.0, min(1.0, alpha_t))

        # Update Q2
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha_t * pe2

        # No separate q1 table; stage-1 is purely model-based here

        # Book-keeping
        last_abs_pe2 = abs(pe2)
        q1_prev_action = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Successor-structured stage-1 values with anxiety-modulated structure reliance,
    information bonus at stage 2, and forgetting.

    Core ideas
    - Stage-1 values are computed from a successor-like expectation over next states:
      Q1[a] = sum_s M[a,s] * max(Q2[s]), where M blends the known transitions with
      a uniform prior; anxiety reduces reliance on structure.
    - Stage-2 choices include an exploration bonus proportional to uncertainty
      (running variance), with anxiety increasing the bonus weight.
    - Both Q2 and uncertainty statistics undergo forgetting to capture nonstationarity.

    Parameters
    - action_1: array-like of ints in {0,1}, first-stage actions.
    - state: array-like of ints in {0,1}, reached second-stage state.
    - action_2: array-like of ints in {0,1}, second-stage actions.
    - reward: array-like of floats in [0,1], reward outcome per trial.
    - stai: array-like length-1, scalar anxiety score in [0,1].
    - model_parameters: tuple/list with 5 parameters:
        alpha_q: stage-2 learning rate in [0,1]
        beta: base inverse temperature in [0,10]
        gamma_sr: base reliance on transition structure in [0,1]
        kappa_bonus: base information bonus weight in [0,1]
        tau_forget: forgetting/decay factor per trial in [0,1]

    Bounds
    - alpha_q, gamma_sr, kappa_bonus, tau_forget in [0,1]
    - beta in [0,10]

    Anxiety usage
    - Structure reliance: gamma_eff = clip(gamma_sr * (1 - 0.5*stai), 0, 1)
      Higher anxiety reduces use of the transition model at stage 1.
    - Exploration bonus: kappa_eff = clip(kappa_bonus * (0.5 + 0.5*stai), 0, 1)
      Higher anxiety increases information seeking/avoidance of uncertainty.
    - Inverse temperature: beta_eff = clip(beta * (1 - 0.3*stai), 0, 10)
    - Forgetting: forget = clip(tau_forget * (0.5 + 0.5*stai), 0, 1)
      Higher anxiety increases value and memory decay.

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_q, beta_base, gamma_sr, kappa_bonus, tau_forget = model_parameters
    n_trials = len(action_1)
    stai = float(stai[0])

    # Known transitions (A->X common, U->Y common)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])
    U = np.ones((2, 2)) / 2.0  # uniform prior over next states

    beta_eff = max(0.0, min(10.0, beta_base * (1.0 - 0.3 * stai)))
    gamma_eff = max(0.0, min(1.0, gamma_sr * (1.0 - 0.5 * stai)))
    kappa_eff = max(0.0, min(1.0, kappa_bonus * (0.5 + 0.5 * stai)))
    forget = max(0.0, min(1.0, tau_forget * (0.5 + 0.5 * stai)))
    alpha = max(0.0, min(1.0, alpha_q))

    # Stage-2 value and uncertainty trackers
    q2 = np.zeros((2, 2))
    m = np.zeros((2, 2))    # running mean of rewards per (state, action)
    m2 = np.zeros((2, 2))   # running mean of squared rewards per (state, action)
    n = np.ones((2, 2)) * 1e-6  # pseudo-counts to stabilize early variance

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    for t in range(n_trials):
        # Successor-like mixing of structure and uniform
        M = gamma_eff * T + (1.0 - gamma_eff) * U

        # Stage-1 values from successor-like expectation over next states
        max_q2 = np.max(q2, axis=1)
        q1 = M @ max_q2

        # Stage-1 policy
        q1c = q1 - np.max(q1)
        pi1 = np.exp(beta_eff * q1c)
        pi1 /= np.sum(pi1)
        a1 = int(action_1[t])
        p_choice_1[t] = pi1[a1]

        # Stage-2 policy with information bonus
        s = int(state[t])
        # Compute running variance estimate for state s actions
        var_sa = np.maximum(0.0, m2[s] - m[s] ** 2)
        bonus = kappa_eff * np.sqrt(var_sa)
        pref2 = q2[s] + bonus
        pref2c = pref2 - np.max(pref2)
        pi2 = np.exp(beta_eff * pref2c)
        pi2 /= np.sum(pi2)
        a2 = int(action_2[t])
        p_choice_2[t] = pi2[a2]

        # Outcome
        r = float(reward[t])

        # Forgetting/decay
        q2 *= (1.0 - forget)
        m *= (1.0 - forget)
        m2 *= (1.0 - forget)
        n *= (1.0 - 0.5 * forget)  # keep some inertia in counts

        # Update Q2 with TD error
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Update uncertainty statistics (EWMA via pseudo-counts)
        n[s, a2] += 1.0
        w = 1.0 / n[s, a2]
        m[s, a2] = (1 - w) * m[s, a2] + w * r
        m2[s, a2] = (1 - w) * m2[s, a2] + w * (r * r)

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll