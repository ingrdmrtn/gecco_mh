def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Anxiety-weighted aversion to rare transitions with volatility-adaptive temperature.

    Summary:
    - Model-free learning of second-stage values; stage-1 values are MF bootstraps of stage-2.
    - Aversion to rare transitions at the first stage: the spaceship that just produced a rare
      transition gets a temporary penalty, scaled by anxiety (higher anxiety, stronger avoidance).
    - Softmax temperature adapts to recent volatility (running absolute PE) so that higher
      volatility lowers effective beta (more exploration). Volatility sensitivity increases
      with anxiety.
    - Constant side bias at stage 1 depends on the participantâ€™s anxiety (bias toward A when
      anxiety is lower, toward U when anxiety is higher).

    Parameters (all used; bounds in brackets):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), observed second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1; aliens within state)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha       [0,1]: learning rate for stage-2 MF values and stage-1 bootstrapping
        beta0       [0,10]: base inverse temperature for both stages
        chi_rare    [0,1]: penalty magnitude for the spaceship that produced a rare transition
        phi_vol     [0,1]: volatility sensitivity (scales beta reduction with volatility)
        bias0       [0,1]: maximum side bias magnitude at stage 1

    Returns:
    - Negative log-likelihood of observed choices across both stages.
    """
    alpha, beta0, chi_rare, phi_vol, bias0 = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition structure
    T = np.array([[0.7, 0.3],  # P(X|A), P(Y|A)
                  [0.3, 0.7]]) # P(X|U), P(Y|U)

    # Model-free values
    q1_mf = np.zeros(2)          # first-stage MF action values
    q2_mf = np.zeros((2, 2))     # second-stage MF action values

    # Likelihood containers
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Rare-transition aversion: penalty to the chosen spaceship if last transition was rare
    rare_penalty = np.zeros(2)   # added to q1 when choosing
    # Volatility tracker (running abs prediction error at stage 2)
    vol = 0.0
    vol_alpha = 0.2 + 0.6 * stai0  # higher anxiety -> track volatility more aggressively

    eps = 1e-12
    for t in range(n_trials):
        # Compute model-based estimate for stage-1 from current q2_mf
        max_q2 = np.max(q2_mf, axis=1)      # per-state best alien value
        q1_mb = T @ max_q2                  # expected value per spaceship

        # Hybrid policy: here we rely on MF for learning but allow MB to shape policy via expectation
        # We combine both sources additively (simple sum) and add penalties/biases.
        # Side bias: bias toward A when stai low, toward U when stai high
        side_bias = np.array([+1.0, -1.0]) * bias0 * (0.5 - stai0)

        # Volatility-adaptive inverse temperature (higher volatility -> lower effective beta)
        beta_eff = beta0 / (1.0 + phi_vol * (1.0 + stai0) * vol)

        # Combine values with rare-transition aversion
        q1_eff = 0.5 * q1_mf + 0.5 * q1_mb + rare_penalty + side_bias
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta_eff * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Second stage policy (pure MF at the encountered state)
        s2 = state[t]
        q2c = q2_mf[s2] - np.max(q2_mf[s2])
        probs_2 = np.exp(beta_eff * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Update second stage MF values
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        # Bootstrap first stage MF from realized second-stage value (SARSA(0)-like)
        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update volatility (running absolute PE magnitude)
        vol = (1.0 - vol_alpha) * vol + vol_alpha * abs(pe2)

        # Update rare-transition aversion for next trial
        # Identify whether the realized transition was common or rare
        p_trans = T[a1, s2]
        is_rare = 1.0 - p_trans  # 0.3 -> rare weight 0.7, 0.7 -> rare weight 0.3
        rare_penalty = np.zeros(2)
        # Apply a penalty to the chosen spaceship if the transition was rare (scaled by anxiety)
        rare_penalty[a1] = -chi_rare * (stai0) * is_rare

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Reliability-based arbitration between model-based and model-free control with anxiety bias.

    Summary:
    - Stage-2 values are learned model-free.
    - A meta-controller arbitrates at stage 1 between model-based (MB) and model-free (MF)
      values using their relative reliabilities. MB reliability decreases with recent
      transition surprise; MF reliability decreases with recent reward PE variance.
    - Anxiety biases arbitration toward MF control (higher anxiety -> more MF).
    - MF values include a small decay to model forgetting.

    Parameters (all used; bounds in brackets):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), observed second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha       [0,1]: learning rate for stage-2 MF values and stage-1 bootstrapping
        beta        [0,10]: inverse temperature for both stages
        theta_rel   [0,1]: sensitivity of the arbitration to reliability differences
        decay_mf    [0,1]: forgetting/decay applied to MF values each trial
        xi_anx      [0,1]: strength of anxiety-driven shift toward MF control

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, theta_rel, decay_mf, xi_anx = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Values
    q1_mf = np.zeros(2)
    q2_mf = np.zeros((2, 2))

    # Running uncertainty trackers
    # Reward PE variance proxy for MF reliability; transition surprise proxy for MB reliability
    mf_var = 0.0
    mb_var = 0.0
    eta_track = 0.2  # tracker smoothing

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    for t in range(n_trials):
        # Decay MF values
        q1_mf *= (1.0 - 0.5 * decay_mf)
        q2_mf *= (1.0 - decay_mf)

        # Model-based Q for stage 1 from current MF stage-2 values
        max_q2 = np.max(q2_mf, axis=1)
        q1_mb = T @ max_q2

        # Compute reliabilities as inverse of uncertainty proxies
        rel_mb = 1.0 / (1.0 + mb_var)
        rel_mf = 1.0 / (1.0 + mf_var)

        # Arbitration weight on MB versus MF
        # anxiety shifts weight toward MF by subtracting from the MB-MF difference
        # w_mb in [0,1] via a smooth mapping
        diff_rel = rel_mb - rel_mf - xi_anx * stai0
        w_mb = 1.0 / (1.0 + np.exp(-10.0 * theta_rel * diff_rel))  # squashing function

        q1_eff = w_mb * q1_mb + (1.0 - w_mb) * q1_mf
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Stage-2 policy
        s2 = state[t]
        q2c = q2_mf[s2] - np.max(q2_mf[s2])
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        r = reward[t]

        # Learning updates
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Update uncertainty trackers
        # MF uncertainty from reward PE magnitude; MB uncertainty from transition surprise
        mf_var = (1.0 - eta_track) * mf_var + eta_track * (pe2 ** 2)

        p_trans = T[a1, s2]
        surprise = 1.0 - p_trans  # larger for rare transitions
        # Anxiety increases the impact of surprise on MB uncertainty
        mb_var = (1.0 - eta_track) * mb_var + eta_track * ((1.0 + stai0) * surprise) ** 2

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Uncertainty-driven exploration (UCB) with anxiety-dampened bonuses and value decay.

    Summary:
    - Second-stage values are learned via incremental averaging (MF) with a decay term.
    - The policy incorporates an Upper Confidence Bound (UCB) bonus at stage 2 that promotes
      exploration of less-sampled aliens; the bonus is propagated to stage 1 via the known
      transition model.
    - Anxiety reduces the effective exploration bonus (more anxious -> less uncertainty seeking).
    - A separate softmax temperature controls stochasticity.

    Parameters (all used; bounds in brackets):
    - action_1: np.array (n_trials,), first-stage actions (0=A, 1=U)
    - state:    np.array (n_trials,), observed second-stage state (0=X, 1=Y)
    - action_2: np.array (n_trials,), second-stage actions (0/1)
    - reward:   np.array (n_trials,), outcomes (0/1 coins)
    - stai:     np.array (1,) or (n_trials,), anxiety score in [0,1]
    - model_parameters: iterable of 5 parameters
        alpha       [0,1]: learning rate for reward value updates (MF)
        beta        [0,10]: inverse temperature for both stages
        kappa_ucb   [0,1]: base weight of uncertainty bonus
        tau_unc     [0,1]: anxiety sensitivity scaling that suppresses bonus
        decay       [0,1]: value decay applied each trial to encourage continual exploration

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, kappa_ucb, tau_unc, decay = model_parameters
    n_trials = len(action_1)
    stai0 = float(stai[0])

    # Known transition model
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Model-free values and counts for UCB
    q2 = np.zeros((2, 2))
    n2 = np.zeros((2, 2))  # visit counts per (state, action)
    q1_mf = np.zeros(2)    # MF bootstrap at stage 1

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)
    eps = 1e-12

    # Anxiety-dampened exploration bonus weight
    kappa_eff = kappa_ucb * (1.0 - tau_unc * stai0)

    for t in range(n_trials):
        # Apply decay
        q2 *= (1.0 - decay)
        q1_mf *= (1.0 - 0.5 * decay)

        # UCB bonus for each state-action
        bonus = np.zeros((2, 2))
        for s in (0, 1):
            for a in (0, 1):
                bonus[s, a] = kappa_eff / np.sqrt(n2[s, a] + 1.0)

        # Stage-2 policy uses Q + bonus at the encountered state
        s2 = state[t]
        q2_bonus = q2[s2] + bonus[s2]
        q2c = q2_bonus - np.max(q2_bonus)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = action_2[t]
        p_choice_2[t] = probs_2[a2]

        # Stage-1 policy uses MB expectation over Q+bonus and combines with MF bootstrap
        max_q2_bonus = np.max(q2 + bonus, axis=1)
        q1_mb = T @ max_q2_bonus
        q1_eff = 0.5 * q1_mb + 0.5 * q1_mf
        q1c = q1_eff - np.max(q1_eff)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = action_1[t]
        p_choice_1[t] = probs_1[a1]

        # Observe reward and update values/counts
        r = reward[t]
        n2[s2, a2] += 1.0
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Bootstrap stage-1 MF toward realized second-stage value (without bonus)
        td_target1 = q2[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll