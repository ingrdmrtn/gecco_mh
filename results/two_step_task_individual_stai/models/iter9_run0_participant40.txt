def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """Hybrid MB/MF with volatility-gated arbitration; anxiety reduces reliance on MB and inflates perceived volatility.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used. Higher anxiety reduces MB weighting
        and amplifies volatility gating.
    model_parameters : tuple/list
        (alpha2, beta, mb_weight, vol_lr, kappa2)
        - alpha2 in [0,1]: learning rate for second-stage Q-values and MF backup to stage 1.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - mb_weight in [0,1]: baseline arbitration weight on model-based values.
        - vol_lr in [0,1]: learning rate for estimating transition volatility from surprise.
        - kappa2 in [0,1]: second-stage choice stickiness within each planet.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Model-based (MB) uses a fixed known transition structure: A->X, U->Y common (0.7).
    - A latent volatility v_t is learned from transition surprise (rare=1, common=0).
      The effective MB weight is omega = clip(mb_weight * (1 - stai) * (1 - v_t), 0, 1).
    - Model-free (MF) first-stage values are updated by backing up second-stage values with
      an eligibility trace lambda = 0.5 * (1 - stai).
    - Second-stage policy includes a within-planet stickiness bias kappa2 to repeat the last alien.
    """
    alpha2, beta, mb_weight, vol_lr, kappa2 = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # Prob of each observed choice for likelihood
    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Value functions
    Q2 = np.zeros((2, 2))      # second-stage Q-values per planet/state
    Q1_mf = np.zeros(2)        # first-stage model-free Q-values

    # Second-stage stickiness: last chosen alien per state (initialize to -1 meaning none)
    last_a2_by_state = np.array([-1, -1], dtype=int)

    # Volatility estimate (starts neutral)
    v = 0.5

    # Eligibility trace for MF credit assignment (anxiety reduces λ)
    lam = 0.5 * (1.0 - stai_val)

    for t in range(n_trials):
        a1 = int(action_1[t])

        # Model-based first-stage values from current Q2
        max_Q2 = np.max(Q2, axis=1)   # best alien on each planet
        Q1_mb = T_fixed @ max_Q2

        # Arbitration weight gated by volatility and anxiety
        omega = mb_weight * (1.0 - stai_val) * (1.0 - v)
        if omega < 0.0:
            omega = 0.0
        elif omega > 1.0:
            omega = 1.0

        Q1 = omega * Q1_mb + (1.0 - omega) * Q1_mf

        # Stage-1 policy (softmax)
        logits1 = beta * Q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy with within-planet stickiness
        s2 = int(state[t])
        a2 = int(action_2[t])

        bias2 = np.zeros(2)
        if last_a2_by_state[s2] != -1:
            bias2[last_a2_by_state[s2]] = kappa2

        logits2 = beta * Q2[s2] + bias2
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        r = reward[t]

        # Learn Q2 (TD)
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # MF backup to stage-1 with eligibility λ (uses realized Q2 after update)
        backup_value = Q2[s2, a2]
        delta1 = backup_value - Q1_mf[a1]
        Q1_mf[a1] += alpha2 * lam * delta1

        # Update volatility estimate from transition surprise (rare=1, common=0)
        is_common = int((a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1))
        surprise = 1.0 - float(is_common)  # 0 for common, 1 for rare
        v = (1.0 - vol_lr) * v + vol_lr * surprise

        # Update stickiness memory
        last_a2_by_state[s2] = a2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """Purely model-based control with risk-sensitive, loss-averse utility; anxiety increases loss aversion.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used. Higher anxiety increases effective loss aversion.
    model_parameters : tuple/list
        (alpha2, beta, xi_risk, lambda_loss, kappa1_rep)
        - alpha2 in [0,1]: learning rate for second-stage utility Q-values.
        - beta in [0,10]: inverse temperature for softmax (both stages).
        - xi_risk in [0,1]: utility curvature (risk sensitivity); lower => more concave/convex.
        - lambda_loss in [0,1]: baseline loss aversion multiplier for negative outcomes.
        - kappa1_rep in [0,1]: perseveration bias to repeat the previous first-stage action.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Rewards are transformed into a prospect-style utility: u(r) = r^xi for gains,
      and u(r) = -lambda_eff * |r|^xi for losses, with lambda_eff = lambda_loss * (1 + stai).
    - First-stage policy is fully model-based using the fixed transition matrix and learned
      second-stage expected utilities. A first-stage perseveration bias is included.
    """
    alpha2, beta, xi_risk, lambda_loss, kappa1_rep = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition matrix: rows = actions (A=0, U=1), cols = states (X=0, Y=1)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    # Second-stage utility Q-values
    Q2 = np.zeros((2, 2))

    # First-stage perseveration memory
    prev_a1 = -1

    # Anxiety-modulated loss aversion and risk curvature (kept within bounds)
    lambda_eff = lambda_loss * (1.0 + stai_val)
    if lambda_eff < 0.0:
        lambda_eff = 0.0
    xi_eff = xi_risk * (1.0 - 0.5 * stai_val)  # anxiety mildly increases concavity/convexity
    if xi_eff < 1e-6:
        xi_eff = 1e-6

    def utility(r):
        if r >= 0.0:
            return (r ** xi_eff)
        else:
            return -lambda_eff * ((-r) ** xi_eff)

    for t in range(n_trials):
        # Compute MB first-stage action values from current Q2
        max_Q2 = np.max(Q2, axis=1)
        Q1_MB = T @ max_Q2

        # Perseveration bias
        bias1 = np.zeros(2)
        if prev_a1 != -1:
            bias1[prev_a1] = kappa1_rep

        # Stage-1 policy
        a1 = int(action_1[t])
        logits1 = beta * Q1_MB + bias1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy (on-planet)
        s2 = int(state[t])
        a2 = int(action_2[t])
        logits2 = beta * Q2[s2]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        p_choice_2[t] = probs2[a2]

        # Observe outcome and learn utility Q2
        r = reward[t]
        u = utility(r)

        delta2 = u - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        prev_a1 = a1

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """Model-free with uncertainty-driven lapses; anxiety increases noise when uncertain and weakens rare-transition credit assignment.
    
    Parameters
    ----------
    action_1 : array-like of int (0 or 1)
        First-stage choices each trial: 0 = spaceship A, 1 = spaceship U.
    state : array-like of int (0 or 1)
        Second-stage state each trial: 0 = planet X, 1 = planet Y.
    action_2 : array-like of int (0 or 1)
        Second-stage choices each trial (alien on the visited planet).
    reward : array-like of float
        Obtained reward each trial (gold coins; can be negative, zero, or positive).
    stai : array-like of float
        Trait anxiety score in [0,1]; stai[0] is used. Higher anxiety increases lapse
        probability under uncertainty and reduces credit assignment after rare transitions.
    model_parameters : tuple/list
        (alpha2, beta0, eps_base, zeta_unc)
        - alpha2 in [0,1]: learning rate for Q-values (both stages).
        - beta0 in [0,10]: inverse temperature for softmax before lapse mixing.
        - eps_base in [0,1]: baseline lapse rate mixed with softmax (uniform exploration).
        - zeta_unc in [0,1]: sensitivity of lapse to on-planet uncertainty (entropy).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    
    Notes
    -----
    - Policies at both stages are a mixture: P = (1 - eps_eff)*softmax(beta0*Q) + eps_eff*uniform.
    - The effective lapse eps_eff increases with the entropy of the current second-stage policy
      and with anxiety: eps_eff = clip(eps_base + zeta_unc * H * (0.5 + stai), 0, 0.5).
    - First-stage values are purely MF and updated using the second-stage TD error, with
      anxiety-dampened credit assignment after rare transitions.
    - Small forgetting proportional to anxiety is applied to both Q1 and Q2 to reflect instability.
    """
    alpha2, beta0, eps_base, zeta_unc = model_parameters
    n_trials = len(action_1)
    stai_val = float(stai[0])

    # Fixed transition structure to detect rare vs common
    def is_common_transition(a1, s2):
        return (a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1)

    p_choice_1 = np.zeros(n_trials)
    p_choice_2 = np.zeros(n_trials)

    Q1 = np.zeros(2)        # first-stage MF values
    Q2 = np.zeros((2, 2))   # second-stage MF values

    # Forgetting proportional to anxiety (bounded)
    forget = 0.1 * stai_val
    if forget < 0.0:
        forget = 0.0
    if forget > 0.2:
        forget = 0.2

    for t in range(n_trials):
        a1 = int(action_1[t])
        s2 = int(state[t])
        a2 = int(action_2[t])

        # Compute uncertainty (entropy) of current second-stage state policy (softmax over Q2[s2])
        logits2_view = beta0 * Q2[s2]
        logits2_view -= np.max(logits2_view)
        probs2_view = np.exp(logits2_view)
        probs2_view /= np.sum(probs2_view)
        # Entropy in nats for two actions (max = ln 2)
        H = -np.sum(probs2_view * (np.log(probs2_view + 1e-12)))

        # Effective lapse increases with uncertainty and anxiety
        eps_eff = eps_base + zeta_unc * H * (0.5 + stai_val)
        if eps_eff < 0.0:
            eps_eff = 0.0
        if eps_eff > 0.5:
            eps_eff = 0.5

        # Stage-1 policy: softmax over Q1, then mix with uniform via eps_eff
        logits1 = beta0 * Q1
        logits1 -= np.max(logits1)
        probs1_sm = np.exp(logits1)
        probs1_sm /= np.sum(probs1_sm)
        probs1 = (1.0 - eps_eff) * probs1_sm + eps_eff * 0.5
        p_choice_1[t] = probs1[a1]

        # Stage-2 policy: softmax over Q2[s2], then mix with uniform via same eps_eff
        logits2 = beta0 * Q2[s2]
        logits2 -= np.max(logits2)
        probs2_sm = np.exp(logits2)
        probs2_sm /= np.sum(probs2_sm)
        probs2 = (1.0 - eps_eff) * probs2_sm + eps_eff * 0.5
        p_choice_2[t] = probs2[a2]

        # Observe reward
        r = reward[t]

        # TD learning at stage 2 with forgetting
        Q2 *= (1.0 - forget)
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha2 * delta2

        # Credit assignment to stage 1 depends on transition type and anxiety
        common = 1.0 if is_common_transition(a1, s2) else 0.0
        credit = common + (1.0 - common) * (1.0 - stai_val)  # reduced for rare when anxious

        # MF update for stage 1 with forgetting and anxiety-weighted credit
        Q1 *= (1.0 - forget)
        Q1[a1] += alpha2 * credit * delta2

    eps = 1e-12
    nll = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
    return nll