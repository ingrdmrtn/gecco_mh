def cognitive_model1(action_1, state, action_2, reward, stai, model_parameters):
    """
    Uncertainty-arbitrated MB/MF with anxiety-shaped utility and learned transitions.

    Idea:
    - Learn the transition matrix online.
    - Compute a model-based (MB) value using the learned transitions and current stage-2 values.
    - Compute a model-free (MF) value learned from experienced rewards.
    - Arbitrates MB vs MF using the uncertainty (entropy) of the learned transition model.
      Anxiety increases MF reliance (reduces MB weight).
    - Anxiety also shapes outcome utility by making unrewarded outcomes feel worse.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state reached (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices on visited planet (0/1=alien).
    reward : array-like of float
        Obtained coins (typically 0 or 1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values reduce MB arbitration weight and increase
        the negativity of unrewarded outcomes.
    model_parameters : array-like of float
        [alpha_q, beta, t_learn, rho_u]
        - alpha_q in [0,1]: learning rate for Q-value updates (both stages).
        - beta in [0,10]: inverse temperature for softmax choice at both stages.
        - t_learn in [0,1]: learning rate for updating the transition matrix.
        - rho_u in [0,1]: utility resilience; higher reduces anxiety-driven negativity.

    Returns
    -------
    float
        Negative log-likelihood of observed first- and second-stage choices.
    """
    alpha_q, beta, t_learn, rho_u = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Initialize learned transition model (rows sum to 1)
    T = 0.5 * np.ones((2, 2))  # start agnostic; will learn from experience

    # Stage-1 MF and stage-2 values
    q1_mf = np.zeros(2)
    q2 = 0.5 * np.ones((2, 2))

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    ln2 = np.log(2.0)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Model-based action values at stage 1 using learned transitions
        max_q2 = np.max(q2, axis=1)  # best alien per planet
        q1_mb = T @ max_q2

        # Arbitration weight based on transition uncertainty (entropy)
        # Lower entropy -> higher MB weight; higher anxiety -> lower MB weight.
        H_rows = -np.sum(np.clip(T, eps, 1.0) * np.log(np.clip(T, eps, 1.0)), axis=1)  # entropy per action
        H_norm = np.mean(H_rows) / ln2  # normalize to [0,1]
        w_mb = np.clip((1.0 - H_norm) * (1.0 - 0.6 * st), 0.0, 1.0)

        # Hybrid Q at stage 1
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Choice probabilities stage 1
        logits1 = beta * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        # Choice probabilities stage 2
        logits2 = beta * (q2[s] - np.max(q2[s]))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Anxiety-shaped utility: unrewarded outcomes feel worse when anxious,
        # mitigated by rho_u. r in {0,1} -> r_eff in [-st*(1-rho_u), 1]
        r_eff = r if r >= 1.0 - eps else -st * (1.0 - rho_u)

        # Q-learning updates
        # Stage 2
        pe2 = r_eff - q2[s, a2]
        q2[s, a2] += alpha_q * pe2

        # Stage 1 MF bootstraps from realized stage-2 action value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += alpha_q * pe1

        # Learn transition model from observed (a1 -> s)
        # Move T[a1] toward one-hot for observed state
        for ss in (0, 1):
            target = 1.0 if ss == s else 0.0
            T[a1, ss] += t_learn * (target - T[a1, ss])
        # Ensure normalization and numerical stability
        row_sum = np.sum(T[a1])
        if row_sum > 0:
            T[a1] /= row_sum
        T[a1] = np.clip(T[a1], eps, 1.0)
        T[a1] /= np.sum(T[a1])

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model2(action_1, state, action_2, reward, stai, model_parameters):
    """
    Valence-asymmetric MF with anxiety-gated eligibility and perseveration.

    Idea:
    - Pure model-free learner with separate learning rates for positive vs negative
      prediction errors at stage 2.
    - Stage-1 values receive additional credit via an eligibility trace from stage 2.
    - Anxiety increases perseveration (choice stickiness) and increases sensitivity
      to negative outcomes (higher effective alpha for negative PEs).
    - Fixed environment transitions are not used for planning (MF-only policy).

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within the visited planet).
    reward : array-like of float
        Coins received (0/1).
    stai : array-like of float in [0,1]
        Anxiety score; higher values:
          - increase perseveration strength,
          - increase reliance on negative learning,
          - reduce eligibility trace (shallower credit assignment).
    model_parameters : array-like of float
        [alpha_pos, alpha_neg, beta, stick0, lam0]
        - alpha_pos in [0,1]: learning rate for positive PEs at stage 2.
        - alpha_neg in [0,1]: learning rate for negative PEs at stage 2.
        - beta in [0,10]: inverse temperature for choices at both stages.
        - stick0 in [0,1]: baseline perseveration strength.
        - lam0 in [0,1]: baseline eligibility trace from stage 2 to stage 1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    alpha_pos, alpha_neg, beta, stick0, lam0 = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Values
    q1 = np.zeros(2)            # stage-1 MF values
    q2 = 0.5 * np.ones((2, 2))  # stage-2 MF values

    # Perseveration states
    prev_a1 = 0
    prev_a2 = np.zeros(2, dtype=int)  # per state

    # Anxiety-gated components
    kappa = 2.0 * stick0  # scale stickiness
    kappa_eff = kappa * (0.5 + 0.5 * st)  # more stickiness with anxiety
    lam_eff = np.clip(lam0 * (1.0 - 0.6 * st), 0.0, 1.0)

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Softmax with perseveration biases
        bias1 = np.array([0.0, 0.0])
        bias1[prev_a1] += kappa_eff
        logits1 = beta * (q1 - np.max(q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        bias2 = np.array([0.0, 0.0])
        bias2[prev_a2[s]] += kappa_eff
        logits2 = beta * (q2[s] - np.max(q2[s])) + bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Stage 2 update with valence asymmetry and anxiety skew
        pe2 = r - q2[s, a2]
        if pe2 >= 0:
            alpha2 = alpha_pos * (1.0 - 0.3 * st)  # anxious -> slightly reduced positive learning
        else:
            alpha2 = alpha_neg * (0.7 + 0.3 * st)  # anxious -> heightened negative learning
        q2[s, a2] += alpha2 * pe2

        # Stage 1: standard bootstrapping to realized stage-2 value
        target1 = q2[s, a2]
        pe1 = target1 - q1[a1]
        # Base update
        q1[a1] += 0.5 * (alpha_pos + alpha_neg) * pe1
        # Eligibility trace credit from stage-2 PE
        q1[a1] += lam_eff * ((alpha_pos + alpha_neg) * 0.5) * pe2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2[s] = a2

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)


def cognitive_model3(action_1, state, action_2, reward, stai, model_parameters):
    """
    Volatility-sensitive hybrid with anxiety-driven optimism prior and exploration bonus.

    Idea:
    - Stage 2 learns rewards with a constant learning rate, but we track running
      outcome volatility from absolute prediction errors.
    - Anxiety inflates perceived volatility, which lowers exploitation (reduces beta)
      and shifts arbitration toward MF (less reliance on MB).
    - Optimism prior on alien values is stronger when anxiety is low.
    - Add an uncertainty/exploration bonus at stage 2 that diminishes with visit count
      and grows with perceived volatility and optimism.

    Parameters (all in [0,1] except beta in [0,10])
    ----------
    action_1 : array-like of int {0,1}
        First-stage choices (0=spaceship A, 1=spaceship U).
    state : array-like of int {0,1}
        Reached second-stage state (0=planet X, 1=planet Y).
    action_2 : array-like of int {0,1}
        Second-stage choices (0/1=alien within planet).
    reward : array-like of float
        Coins obtained.
    stai : array-like of float in [0,1]
        Anxiety score; higher values increase perceived volatility (more exploration,
        less MB arbitration) and reduce optimism prior.
    model_parameters : array-like of float
        [eta_r, beta, phi_vol, opt_bias]
        - eta_r in [0,1]: learning rate for stage-2 Q-value updates.
        - beta in [0,10]: baseline inverse temperature for both stages.
        - phi_vol in [0,1]: sensitivity for volatility update and its impact.
        - opt_bias in [0,1]: optimism prior and exploration gain base.

    Returns
    -------
    float
        Negative log-likelihood of observed choices at both stages.
    """
    eta_r, beta, phi_vol, opt_bias = model_parameters
    n_trials = len(action_1)
    st = float(stai[0])

    # Fixed transition structure (common transitions)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Optimism prior shaped by anxiety: low anxiety -> higher prior mean
    prior_mean = 0.5 + (1.0 - st) * (opt_bias - 0.5)
    q2 = prior_mean * np.ones((2, 2))

    # Stage-1 MF values
    q1_mf = np.zeros(2)

    # Visit counts for exploration bonus
    counts = np.ones((2, 2))  # start at 1 to avoid div-by-zero in bonus

    # Volatility estimate
    vol = 0.0

    eps = 1e-12
    p1 = np.zeros(n_trials)
    p2 = np.zeros(n_trials)

    for t in range(n_trials):
        s = int(state[t])
        a1 = int(action_1[t])
        a2 = int(action_2[t])
        r = float(reward[t])

        # Perceived volatility inflated by anxiety
        vol_eff = np.clip(vol + st * phi_vol, 0.0, 1.0)

        # Arbitration and temperature controlled by volatility and anxiety
        w_mb = np.clip((1.0 - vol_eff) * (1.0 - 0.5 * st), 0.0, 1.0)
        beta_eff = np.clip(beta * (1.0 - 0.5 * vol_eff), 0.0, 10.0)

        # Model-based value for stage 1
        max_q2 = np.max(q2, axis=1)
        q1_mb = T @ max_q2

        # Hybrid value
        q1 = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # Stage 2 exploration bonus: larger with perceived volatility and optimism;
        # diminishes with visits to the specific (state,action).
        bonus_gain = vol_eff * opt_bias / (counts[s, :] + 0.0)
        q2_biased = q2[s] + bonus_gain

        # Choice probabilities
        logits1 = beta_eff * (q1 - np.max(q1))
        probs1 = np.exp(logits1)
        probs1 /= (np.sum(probs1) + eps)
        p1[t] = probs1[a1]

        logits2 = beta_eff * (q2_biased - np.max(q2_biased))
        probs2 = np.exp(logits2)
        probs2 /= (np.sum(probs2) + eps)
        p2[t] = probs2[a2]

        # Learning updates
        pe2 = r - q2[s, a2]
        q2[s, a2] += eta_r * pe2

        # Update MF stage-1 toward realized second-stage chosen value
        target1 = q2[s, a2]
        pe1 = target1 - q1_mf[a1]
        q1_mf[a1] += eta_r * pe1

        # Volatility update from absolute PE (running exponential average)
        vol = (1.0 - phi_vol) * vol + phi_vol * abs(pe2)
        vol = np.clip(vol, 0.0, 1.0)

        # Update visit counts for exploration bonus
        counts[s, a2] += 1.0

    neg_ll = -(np.sum(np.log(p1 + eps)) + np.sum(np.log(p2 + eps)))
    return float(neg_ll)