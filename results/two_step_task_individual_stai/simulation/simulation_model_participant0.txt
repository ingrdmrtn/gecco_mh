import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the transition-dependent model-free model
    with anxiety-driven spillover and ship-A bias (matches cognitive_model3 logic).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple): [alpha, spill, rare_bias, beta, biasA, stai]
            - alpha: learning rate for all TD updates (0..1)
            - spill: baseline fraction of reward that generalizes across states (0..1)
            - rare_bias: baseline fraction of TD misassignment after rare transitions (0..1)
            - beta: inverse temperature for both stages (0..10)
            - biasA: additive bias toward first-stage action 0 (ship A) (0..1)
            - stai: trait anxiety in [0,1]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for
            the two second-stage states and two actions:
            - state 0: actions 0,1 -> drift1[t], drift2[t]
            - state 1: actions 0,1 -> drift3[t], drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    alpha, spill, rare_bias, beta, biasA, stai = parameters

    rng = np.random.default_rng()
    T_known = np.array([[0.7, 0.3],
                        [0.3, 0.7]])

    # MB/MF weight depends on anxiety: more anxiety -> more MF
    w = np.clip(1.0 - float(stai), 0.0, 1.0)

    # Anxiety-amplified spillover and rare miscrediting
    spill_eff = spill * (0.5 + 0.5 * float(stai))
    rare_eff = rare_bias * (0.5 + 0.5 * float(stai))

    # Value functions
    q1_mf = np.zeros(2)      # stage-1 MF
    q2 = np.zeros((2, 2))    # stage-2 MF for each state and action

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Trial-specific reward probabilities
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]], dtype=float)

        # Compute stage-1 action values (MB + MF mixture)
        max_q2 = np.max(q2, axis=1)           # best action in each second-stage state
        q1_mb = T_known @ max_q2              # MB backup using known transitions
        q1 = w * q1_mb + (1.0 - w) * q1_mf    # mixture

        # Add ship-A bias to action 0
        logits1 = q1.copy()
        logits1[0] += biasA

        # Softmax for stage-1 choice
        l1 = beta * (logits1 - np.max(logits1))
        p1 = np.exp(l1)
        p1 /= p1.sum()
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state
        s = rng.choice([0, 1], p=T_known[a1])

        # Stage-2 choice softmax
        logits2 = q2[s].copy()
        l2 = beta * (logits2 - np.max(logits2))
        p2 = np.exp(l2)
        p2 /= p2.sum()
        a2 = rng.choice([0, 1], p=p2)

        # Reward sampling
        r = int(rng.random() < reward_probs[s, a2])

        # Learning updates
        # Stage-2 TD update
        pe2 = r - q2[s, a2]
        q2[s, a2] += alpha * pe2

        # Spillover to unvisited state's same action
        other_s = 1 - s
        q2[other_s, a2] += alpha * spill_eff * (r - q2[other_s, a2])

        # Stage-1 MF TD backup
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        # Transition-dependent MF miscrediting after rare transitions
        is_common = (a1 == 0 and s == 0) or (a1 == 1 and s == 1)
        if not is_common:
            a1_other = 1 - a1
            q1_mf[a1_other] += alpha * rare_eff * (q2[s, a2] - q1_mf[a1_other])

        # Store outcomes
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward