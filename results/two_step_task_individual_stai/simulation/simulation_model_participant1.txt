def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using Model 1: learned transitions with
    anxiety-modulated surprise bonus and choice stickiness. Second-stage policy
    is model-free Q-learning; first-stage policy is model-based using the learned
    transition matrix plus stickiness and an anxiety-scaled surprise bonus.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or array):
            [alpha, beta, alpha_t, kappa_stick, phi_anx, stai]
            - alpha in [0,1]: learning rate for second-stage Q-value updates.
            - beta in [0,10]: inverse temperature at both stages.
            - alpha_t in [0,1]: transition learning rate for updating the transition matrix.
            - kappa_stick in [0,1]: strength of first-stage choice perseveration.
            - phi_anx in [0,1]: scales surprise-to-bonus mapping as a function of anxiety.
            - stai in [0,1]: anxiety level used to scale surprise bonus and stickiness.
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two second-stage states and two actions:
            - state 0: [drift1, drift2]
            - state 1: [drift3, drift4]

    Returns:
        stage1_choice (np.ndarray, int): First-stage choices (0 or 1).
        state2 (np.ndarray, int): Second-stage states (0 or 1).
        stage2_choice (np.ndarray, int): Second-stage choices (0 or 1).
        reward (np.ndarray, int): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, alpha_t, kappa_stick, phi_anx, stai = parameters
    rng = np.random.default_rng()

    # Initialize learned transition matrix and second-stage Q-values
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)  # rows: actions (0,1), cols: states (0,1)
    q2 = np.zeros((2, 2), dtype=float)       # q2[state, action]

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    prev_a1 = None

    for t in range(n_trials):
        # Compute model-based first-stage values from current transition matrix and q2
        max_q2 = np.max(q2, axis=1)      # best second-stage action per state
        q1_mb = T @ max_q2               # shape (2,)

        # Surprise bonus based on previous trial's transition surprise.
        # We apply the bonus to the previous first-stage action (augmenting perseveration)
        # scaled by anxiety, which is a plausible generative counterpart to the NLL term.
        bonus = np.zeros(2, dtype=float)
        if t > 0:
            a1_prev = stage1_choice[t - 1]
            s2_prev = state2[t - 1]
            p_obs = T[a1_prev, s2_prev]
            surprise = 1.0 - p_obs
            bonus[a1_prev] += phi_anx * stai * surprise

        # Stickiness term toward previous choice, scaled by anxiety
        stickiness = np.zeros(2, dtype=float)
        if prev_a1 is not None:
            stickiness[prev_a1] = kappa_stick * (1.0 + stai)

        # First-stage softmax policy
        q1_eff = q1_mb + bonus + stickiness
        q1_eff = q1_eff - np.max(q1_eff)  # numerical stability
        exp_q1 = np.exp(beta * q1_eff)
        probs_1 = exp_q1 / np.sum(exp_q1)

        a1 = rng.choice([0, 1], p=probs_1)

        # Transition to second-stage state using current learned T
        s2 = rng.choice([0, 1], p=T[a1])

        # Second-stage softmax policy (same beta as first-stage per fitting code)
        q2_s = q2[s2].copy()
        q2_s = q2_s - np.max(q2_s)
        exp_q2 = np.exp(beta * q2_s)
        probs_2 = exp_q2 / np.sum(exp_q2)

        a2 = rng.choice([0, 1], p=probs_2)

        # Generate reward from provided drifting probabilities
        # Map state and action to the appropriate drift
        if s2 == 0:
            p_r = drift1[t] if a2 == 0 else drift2[t]
        else:
            p_r = drift3[t] if a2 == 0 else drift4[t]
        r = int(rng.random() < p_r)

        # Second-stage Q-learning update
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        # Update transition matrix row for chosen first-stage action via delta rule
        a1_row = T[a1].copy()
        for s_idx in (0, 1):
            target = 1.0 if s_idx == s2 else 0.0
            a1_row[s_idx] += alpha_t * (target - a1_row[s_idx])
        # Renormalize to ensure row sums to 1
        T[a1] = a1_row / np.sum(a1_row)

        # Record and carry over
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r
        prev_a1 = a1

    return stage1_choice, state2, stage2_choice, reward