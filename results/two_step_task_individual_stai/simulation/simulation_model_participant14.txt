def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the learned-transition model-based RL
    with anxiety-modulated perseveration at stage 1 and MF Q-learning at stage 2.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple): [alpha_r, alpha_t, beta, stick, gamma_anx, stai]
            - alpha_r: reward learning rate for Q2 updates, in [0,1]
            - alpha_t: transition learning rate, in [0,1]
            - beta: inverse temperature for both stages
            - stick: base perseveration strength added to stage-1 logits
            - gamma_anx: anxiety gain on perseveration
            - stai: anxiety score (0-1)
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two second-stage states and two actions:
            reward_probs = [[drift1[t], drift2[t]],
                            [drift3[t], drift4[t]]]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha_r, alpha_t, beta, stick, gamma_anx, stai = parameters

    # True environment transition (fixed, as in standard two-step task)
    true_T = np.array([[0.7, 0.3],
                       [0.3, 0.7]])

    # Agent's learned transition starts uninformative and is updated via alpha_t
    T = np.ones((2, 2)) * 0.5  # rows: stage-1 actions [0,1], cols: states [0,1]
    q2 = np.zeros((2, 2))      # second-stage Q-values per state (rows) and action (cols)

    rng = np.random.default_rng()

    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    prev_a1 = None
    # Anxiety-modulated perseveration strength
    stick_eff = stick * np.clip(1.0 + gamma_anx * (stai - 0.31), 0.0, 2.0)

    for t in range(n_trials):
        # Trial-specific reward probabilities for state-action pairs
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]])

        # Model-based stage-1 values from learned transition and current q2
        max_q2 = np.max(q2, axis=1)  # best action per state
        q1_mb = T @ max_q2           # expected value of each stage-1 action

        # Perseveration bias toward repeating previous stage-1 action
        bias = np.zeros(2)
        if prev_a1 is not None:
            bias[prev_a1] = stick_eff

        # Stage 1 choice via softmax over MB values plus bias
        logits1 = beta * q1_mb + bias
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (probs1.sum() + 1e-12)
        a1 = rng.choice([0, 1], p=probs1)

        # Transition to stage-2 state using true environment transition
        s = rng.choice([0, 1], p=true_T[a1])

        # Stage 2 choice via softmax over q2 for reached state
        logits2 = beta * q2[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (probs2.sum() + 1e-12)
        a2 = rng.choice([0, 1], p=probs2)

        # Generate reward from drifting Bernoulli probs
        r = int(rng.random() < reward_probs[s, a2])

        # Update MF stage-2 values
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha_r * delta2

        # Update learned transition toward observed state
        target = np.array([0.0, 0.0])
        target[s] = 1.0
        T[a1] += alpha_t * (target - T[a1])
        T[a1] = np.clip(T[a1], 1e-6, 1.0)
        T[a1] = T[a1] / T[a1].sum()

        # Record and carry over perseveration
        prev_a1 = a1
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward