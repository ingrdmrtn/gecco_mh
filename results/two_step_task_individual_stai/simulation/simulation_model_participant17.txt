def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the cognitive_model1 logic:
    - Hybrid MB/MF at stage 1 with anxiety-shaped MB weighting
    - Single beta for both stages
    - Anxiety-scaled forgetting/decay of Q-values
    - State-conditional perseveration at stage 2 that scales with anxiety

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alpha, beta, w_slope, rho_forget, tau_stay, stai]
            - alpha in [0,1]
            - beta in [0,10]
            - w_slope in [0,1]
            - rho_forget in [0,1]
            - tau_stay in [0,1]
            - stai in [0,1]
        drift1..drift4 (np.ndarray): Trial-wise reward probabilities for second-stage
            actions per state:
            reward_probs = [[drift1[t], drift2[t]],   # state 0, actions 0/1
                            [drift3[t], drift4[t]]]   # state 1, actions 0/1

    Returns:
        stage1_choice (np.ndarray, int): First-stage choices (0 or 1).
        state2 (np.ndarray, int): Second-stage states (0 or 1).
        stage2_choice (np.ndarray, int): Second-stage choices (0 or 1).
        reward (np.ndarray, int): Obtained rewards (0 or 1).
    """
    import numpy as np
    rng = np.random.default_rng()

    alpha, beta, w_slope, rho_forget, tau_stay, stai = parameters
    stai = float(stai)

    # Transition matrix: common transitions = 0.7
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Anxiety-shaped MB weight and forgetting decay
    w_mb = (1.0 - stai) * w_slope + stai * (1.0 - w_slope)
    w_mb = 0.0 if w_mb < 0.0 else (1.0 if w_mb > 1.0 else w_mb)

    decay = 1.0 - rho_forget * stai
    if decay < 0.0:
        decay = 0.0
    if decay > 1.0:
        decay = 1.0

    # Initialize values
    q1_mf = np.zeros(2)        # stage-1 model-free values
    q2 = np.zeros((2, 2))      # stage-2 values for each state and action

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # State-conditional perseveration memory (previous a2 within same state)
    prev_a2 = None
    prev_s = None

    for t in range(n_trials):
        # Reward probabilities for this trial by state and action
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute MB estimate at stage 1 from current stage-2 values
        max_q2 = np.max(q2, axis=1)    # best action per state
        q1_mb = T @ max_q2             # expected value via transition model

        # Hybrid value for stage 1
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # Stage-1 softmax
        z1 = q1 - np.max(q1)
        exp1 = np.exp(beta * z1)
        p1 = exp1 / np.sum(exp1)
        a1 = rng.choice([0, 1], p=p1)

        # Transition to stage-2 state
        s = rng.choice([0, 1], p=T[a1])

        # Stage-2 choice with state-conditional perseveration bias
        bias2 = np.zeros(2)
        if prev_a2 is not None and prev_s == s:
            bias2[prev_a2] += tau_stay * stai

        q2_s = q2[s].copy()
        z2 = (q2_s + bias2) - np.max(q2_s + bias2)
        exp2 = np.exp(beta * z2)
        p2 = exp2 / np.sum(exp2)
        a2 = rng.choice([0, 1], p=p2)

        # Generate reward from drifting probabilities
        r = int(rng.random() < reward_probs[s][a2])

        # Forgetting/decay before updates
        q1_mf *= decay
        q2 *= decay

        # TD updates
        delta2 = r - q2[s, a2]
        q2[s, a2] += alpha * delta2

        delta1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * delta1

        # Update perseveration memory
        prev_a2 = a2
        prev_s = s

        # Record trial
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward