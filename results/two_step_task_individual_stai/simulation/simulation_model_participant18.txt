def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the adaptive-temperature, stage-1 perseveration,
    and Q-forgetting model (matching cognitive_model2).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alpha, beta, k_temp, tau_forget, pers, stai]
            - alpha: reward learning rate [0,1]
            - beta: base inverse temperature [0,10]
            - k_temp: anxiety sensitivity of temperature [0,1]
            - tau_forget: forgetting decay applied to Q2 and perseveration traces [0,1]
            - pers: baseline perseveration strength [0,1]
            - stai: anxiety score [0,1]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for:
            - state 0, action 0 -> drift1
            - state 0, action 1 -> drift2
            - state 1, action 0 -> drift3
            - state 1, action 1 -> drift4

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, k_temp, tau_forget, pers, stai = parameters
    n_trials = int(n_trials)

    rng = np.random.default_rng()

    # Transition structure
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Initialize value functions and perseveration trace
    Q1_mf = np.zeros(2)        # Stage-1 MF values for actions {0,1}
    Q2 = np.zeros((2, 2))      # Stage-2 MF values per state {0,1} and action {0,1}
    trace1 = np.zeros(2)       # Stage-1 perseveration trace

    # Anxiety-adapted parameters
    st = float(stai)
    beta_eff = max(1e-3, beta * (1.0 - k_temp * st))   # higher anxiety -> lower precision
    stick_strength = pers * (1.0 - st)                 # higher anxiety -> weaker perseveration
    decay = np.clip(tau_forget, 0.0, 1.0)              # forgetting for Q2 and trace

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Current reward probability matrix by state,action
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]], dtype=float)

        # Compute model-based stage-1 values via one-step lookahead using current Q2
        max_Q2 = np.max(Q2, axis=1)       # best action per state
        Q1_mb = T @ max_Q2                # expected value per stage-1 action

        # Combine MF and MB with equal weight and add perseveration bias
        pref1 = 0.5 * Q1_mf + 0.5 * Q1_mb + stick_strength * trace1

        # Softmax for stage-1
        centered1 = pref1 - np.max(pref1)
        exp1 = np.exp(beta_eff * centered1)
        probs1 = exp1 / np.sum(exp1)
        a1 = rng.choice([0, 1], p=probs1)

        # Transition to second-stage state
        s = rng.choice([0, 1], p=T[a1])

        # Stage-2 softmax over Q2 at observed state
        pref2 = Q2[s]
        centered2 = pref2 - np.max(pref2)
        exp2 = np.exp(beta_eff * centered2)
        probs2 = exp2 / np.sum(exp2)
        a2 = rng.choice([0, 1], p=probs2)

        # Generate reward from drifting probabilities
        r = int(rng.random() < reward_probs[s, a2])

        # Forgetting/recency toward zero on Q2
        Q2 *= (1.0 - decay)

        # Stage-2 MF update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update toward updated Q2 value
        delta1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Update perseveration trace with decay
        trace1 *= (1.0 - decay)
        trace1[a1] += 1.0

        # Log trial outcomes
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward