def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the pessimistic lookahead model with
    anxiety-modulated loss aversion and forgetting (converted from cognitive_model1).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple): [alpha, beta, lambda_loss, kappa_anx, phi_forget, stai]
            - alpha: learning rate for Q-value updates at both stages, in [0,1]
            - beta: inverse temperature for both stages, in [0,10]
            - lambda_loss: base loss-aversion coefficient, in [0,1]
            - kappa_anx: strength of anxiety-driven pessimism in model-based lookahead, in [0,1]
            - phi_forget: forgetting rate for unchosen actions (value decay), in [0,1]
            - stai: anxiety score in [0,1]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the 2x2 second-stage actions:
              state 0: [drift1[t], drift2[t]]
              state 1: [drift3[t], drift4[t]]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, lambda_loss, kappa_anx, phi_forget, stai = parameters
    stai = float(stai)

    rng = np.random.default_rng()

    # Transition structure (common: 0->state0 with 0.7, 1->state1 with 0.7)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize value functions
    Q1_mf = np.zeros(2)      # model-free values for first-stage actions
    Q2 = np.zeros((2, 2))    # second-stage state-action values

    # Precompute anxiety-modulated terms (constant across trials here)
    lambda_eff = lambda_loss * (1.0 + stai)                 # effective loss aversion (unused if rewards are non-negative)
    xi = np.clip(1.0 - kappa_anx * stai, 0.0, 1.0)          # pessimism weight: xi=1 is optimistic (max), xi=0 is pessimistic (min)
    forget = np.clip(phi_forget * (0.5 + stai), 0.0, 1.0)   # forgetting rate for unchosen actions

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    eps = 1e-12

    for t in range(n_trials):
        # Pessimistic model-based lookahead from current Q2
        vmax = np.max(Q2, axis=1)   # per state
        vmin = np.min(Q2, axis=1)   # per state
        V_state = xi * vmax + (1.0 - xi) * vmin

        Q1_mb_pess = T @ V_state

        # Anxiety-dependent arbitration weight (as in fitting code)
        w_eff = np.clip(0.5 + 0.5 * stai * (1.0 - lambda_loss), 0.0, 1.0)

        # Net first-stage values
        Q1 = w_eff * Q1_mb_pess + (1.0 - w_eff) * Q1_mf

        # Softmax choice at stage 1
        q1c = Q1 - np.max(Q1)
        p1_unnorm = np.exp(beta * q1c)
        p1 = p1_unnorm / (np.sum(p1_unnorm) + eps)
        a1 = int(rng.choice([0, 1], p=p1))

        # Transition to second stage
        s = int(rng.choice([0, 1], p=T[a1]))

        # Softmax choice at stage 2
        q2s = Q2[s, :]
        q2c = q2s - np.max(q2s)
        p2_unnorm = np.exp(beta * q2c)
        p2 = p2_unnorm / (np.sum(p2_unnorm) + eps)
        a2 = int(rng.choice([0, 1], p=p2))

        # Generate reward from drifting Bernoulli environment
        # Map per-trial probabilities to states/actions
        if s == 0:
            p_r = float(drift1[t]) if a2 == 0 else float(drift2[t])
        else:
            p_r = float(drift3[t]) if a2 == 0 else float(drift4[t])

        r = int(rng.random() < p_r)

        # Utility with loss aversion (if rewards could be negative; with 0/1, u=r)
        if r >= 0:
            u = r
        else:
            u = - (1.0 + lambda_eff) * (-r)

        # Learning updates
        # Second stage: chosen action update and unchosen forgetting
        delta2 = u - Q2[s, a2]
        Q2[s, a2] += alpha * delta2
        Q2[s, 1 - a2] *= (1.0 - forget)

        # First stage: bootstrap from updated second-stage value, with forgetting for unchosen first-stage action
        td1 = Q2[s, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * td1
        Q1_mf[1 - a1] *= (1.0 - forget)

        # Record trial outcomes
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward