import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using a pure model-free SARSA(0) model with
    anxiety-modulated learning asymmetry and choice stickiness.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple):
            [alpha_pos, alpha_neg, beta, kappa0, kappa_stai, stai]
            - alpha_pos in [0,1]: base learning rate for positive PEs.
            - alpha_neg in [0,1]: base learning rate for negative PEs.
            - beta in [0,10]: inverse temperature for softmax at both stages.
            - kappa0 in [0,1]: baseline perseveration bias on last chosen action.
            - kappa_stai in [0,1]: how much anxiety increases stickiness.
            - stai in [0,1]: trait anxiety level.
        drift1, drift2, drift3, drift4 (array-like): Trial-wise reward probabilities for
            second-stage actions:
            - State 0: action 0 -> drift1[t], action 1 -> drift2[t]
            - State 1: action 0 -> drift3[t], action 1 -> drift4[t]

    Returns:
        stage1_choice (np.ndarray, int): First-stage choices (0 or 1).
        state2 (np.ndarray, int): Second-stage states (0 or 1).
        stage2_choice (np.ndarray, int): Second-stage choices (0 or 1).
        reward (np.ndarray, int): Rewards (0 or 1).
    """

    alpha_pos, alpha_neg, beta, kappa0, kappa_stai, stai = parameters
    s = float(stai)

    # Transition structure: action 0 commonly -> state 0, action 1 commonly -> state 1
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # MF Q-values
    q1 = np.zeros(2)          # stage-1 MF values
    q2 = np.zeros((2, 2))     # stage-2 MF values

    # Stickiness tracking
    last_a1 = None
    last_a2_by_state = {0: None, 1: None}

    # Anxiety-modulated stickiness
    kappa = kappa0 + kappa_stai * s

    # Preallocate outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # Ensure arrays
    drift1 = np.asarray(drift1)
    drift2 = np.asarray(drift2)
    drift3 = np.asarray(drift3)
    drift4 = np.asarray(drift4)

    rng = np.random.default_rng()

    for t in range(n_trials):
        # Build reward probs for this trial
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Bias (stickiness) vectors
        bias1 = np.zeros(2)
        if last_a1 is not None:
            bias1[last_a1] += kappa

        # Stage-1 choice
        prefs1 = q1 + bias1
        exp1 = np.exp(beta * (prefs1 - np.max(prefs1)))
        p1 = exp1 / np.sum(exp1)
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage-2 bias depends on last choice in this state
        bias2 = np.zeros(2)
        if last_a2_by_state[s2] is not None:
            bias2[last_a2_by_state[s2]] += kappa

        # Stage-2 choice
        prefs2 = q2[s2] + bias2
        exp2 = np.exp(beta * (prefs2 - np.max(prefs2)))
        p2 = exp2 / np.sum(exp2)
        a2 = rng.choice([0, 1], p=p2)

        # Reward sampling
        r = int(rng.random() < reward_probs[s2][a2])

        # Anxiety-modulated asymmetric learning rates
        eff_alpha_pos = (1 - s) * alpha_pos + s * alpha_neg
        eff_alpha_neg = (1 - s) * alpha_neg + s * alpha_pos

        # SARSA(0) updates: stage-2 then stage-1
        pe2 = r - q2[s2, a2]
        a2_lr = eff_alpha_pos if pe2 >= 0 else eff_alpha_neg
        q2[s2, a2] += a2_lr * pe2

        pe1 = q2[s2, a2] - q1[a1]
        a1_lr = eff_alpha_pos if pe1 >= 0 else eff_alpha_neg
        q1[a1] += a1_lr * pe1

        # Update stickiness memory
        last_a1 = a1
        last_a2_by_state[s2] = a2

        # Record
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward