def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the anxiety-sensitive exploration model
    with rare-transition down-weighting, Pavlovian safety bias, and value forgetting.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [alpha, beta, mb_weight, bias_safe, forget, stai]
            - alpha in [0,1]: learning rate for Q updates.
            - beta in [0,10]: base inverse temperature.
            - mb_weight in [0,1]: baseline model-based mixing weight at stage 1.
            - bias_safe in [0,1]: strength of Pavlovian bias favoring action 0 under uncertainty.
            - forget in [0,1]: forgetting rate pulling Qs toward 0.5 each trial.
            - stai in [0,1]: trait/state anxiety scaling the effects.
        drift1..drift4 (np.ndarray): Trial-wise reward probabilities for each state-action:
            state 0: actions [drift1, drift2]
            state 1: actions [drift3, drift4]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np
    rng = np.random.default_rng()

    alpha, beta, mb_weight, bias_safe, forget, stai = parameters
    T_fixed = np.array([[0.7, 0.3],
                        [0.3, 0.7]], dtype=float)

    # Initialize values
    Q1_mf = np.zeros(2, dtype=float)       # model-free values at stage 1
    Q2 = 0.5 * np.ones((2, 2), dtype=float)  # stage-2 state-action values

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Trial-specific reward probabilities by state and action
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute effective parameters for choice
        # Model-based plan from current Q2 (before forgetting/learning on this trial)
        max_Q2 = np.max(Q2, axis=1)
        Q1_mb = T_fixed @ max_Q2

        # Expected down-weighting of MB before transition is known
        # E[mb_w_eff] = 0.7*mb_weight + 0.3*mb_weight*(1 - 0.7*stai)
        mb_w_exp = mb_weight * (1.0 - 0.21 * stai)

        # Blend MB and MF values
        Q1 = mb_w_exp * Q1_mb + (1.0 - mb_w_exp) * Q1_mf

        # Anxiety-reduced inverse temperature
        beta_eff = beta * (1.0 - 0.5 * stai)

        # Stage 1 choice
        logits1 = beta_eff * Q1
        logits1 = logits1 - np.max(logits1)
        p_stage1 = np.exp(logits1)
        p_stage1 /= np.sum(p_stage1)
        a1 = int(rng.choice([0, 1], p=p_stage1))

        # Transition to stage-2 state
        s2 = int(rng.choice([0, 1], p=T_fixed[a1]))

        # Stage 2 choice with Pavlovian safety bias toward action 0 under uncertainty
        uncertainty = 1.0 - abs(Q2[s2, 0] - Q2[s2, 1])  # in [0,1]
        bias_vec = np.array([bias_safe * stai * uncertainty, 0.0], dtype=float)

        logits2 = beta_eff * Q2[s2] + bias_vec
        logits2 = logits2 - np.max(logits2)
        p_stage2 = np.exp(logits2)
        p_stage2 /= np.sum(p_stage2)
        a2 = int(rng.choice([0, 1], p=p_stage2))

        # Outcome
        r = int(rng.random() < reward_probs[s2][a2])

        # Forgetting (toward 0.5 for Q2; toward 0 for Q1_mf baseline as in fitting code)
        Q2 = (1.0 - forget) * Q2 + forget * 0.5
        Q1_mf = (1.0 - forget) * Q1_mf + forget * 0.0

        # Learning
        # Stage-2 update
        delta2 = r - Q2[s2, a2]
        Q2[s2, a2] += alpha * delta2

        # Stage-1 MF update (uses updated Q2[s2, a2] and mixes delta1 and delta2)
        delta1 = Q2[s2, a2] - Q1_mf[a1]
        Q1_mf[a1] += alpha * (0.5 * delta1 + 0.5 * delta2)

        # Log trial
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward