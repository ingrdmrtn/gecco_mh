def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the cognitive_model3:
      - Stage-2 model-free learning with risk-curvature utility
      - Anxiety-shaped learning-rate asymmetry
      - Forgetting toward a neutral prior (0.5)
      - Single inverse temperature for both stages, softened by anxiety

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [alpha, beta, c_asym, forget, curv, stai]
            - alpha: base learning rate [0,1]
            - beta: inverse temperature baseline [0,10]
            - c_asym: scales asymmetry via anxiety
            - forget: forgetting rate toward 0.5 for all Qs each trial [0,1]
            - curv: utility curvature in [0,1], u = r**curv
            - stai: anxiety score in [0,1]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the two second-stage states (rows) and two actions (cols):
            [[drift1, drift2],
             [drift3, drift4]]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, c_asym, forget, curv, stai = parameters
    stai = float(stai)

    # Transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Effective temperature reduced by anxiety (match fitting code)
    beta_eff = beta * (1.0 - 0.5 * (stai - 0.5))
    beta_eff = max(beta_eff, 1e-6)

    # Initialize Q-values
    Q1 = np.zeros(2)        # Stage-1 values
    Q2 = np.zeros((2, 2))   # Stage-2 values for states X(0) and Y(1)

    # Precompute asymmetric learning rates
    alpha_plus = alpha * (1.0 + c_asym * stai)
    alpha_minus = alpha * (1.0 - c_asym * stai)
    alpha_plus = min(alpha_plus, 1.0)
    alpha_minus = max(alpha_minus, 0.0)

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    rng = np.random.default_rng()
    eps = 1e-12

    for t in range(n_trials):
        # Forgetting toward neutral prior 0.5
        Q1 = (1.0 - forget) * Q1 + forget * 0.5
        Q2 = (1.0 - forget) * Q2 + forget * 0.5

        # Stage-1 choice (softmax over Q1)
        q1c = Q1 - np.max(Q1)
        ps1_unnorm = np.exp(beta_eff * q1c)
        ps1 = ps1_unnorm / (np.sum(ps1_unnorm) + eps)
        a1 = rng.choice([0, 1], p=ps1)

        # State transition given a1
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage-2 choice (softmax over Q2 in current state)
        q2c = Q2[s2] - np.max(Q2[s2])
        ps2_unnorm = np.exp(beta_eff * q2c)
        ps2 = ps2_unnorm / (np.sum(ps2_unnorm) + eps)
        a2 = rng.choice([0, 1], p=ps2)

        # Reward probability from drifting bandits
        p_r = float([[drift1[t], drift2[t]], [drift3[t], drift4[t]]][s2][a2])
        r = int(rng.random() < p_r)

        # Utility transformation
        u = r ** curv  # with Bernoulli rewards, equals r (kept for consistency)

        # Learning updates (asymmetric by sign of PE; same as fitting)
        pe2 = u - Q2[s2, a2]
        a2_lr = alpha_plus if pe2 >= 0.0 else alpha_minus
        Q2[s2, a2] += a2_lr * pe2

        pe1 = Q2[s2, a2] - Q1[a1]
        a1_lr = alpha_plus if pe1 >= 0.0 else alpha_minus
        Q1[a1] += a1_lr * pe1

        # Record trial data
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward