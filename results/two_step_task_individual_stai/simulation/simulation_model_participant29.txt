def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the model-free with uncertainty aversion
    and an anxiety-modulated choice kernel (matches cognitive_model2 logic).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple, length 6):
            alpha         — learning rate for MF values              [0,1]
            beta          — inverse temperature (both stages)        [0,10]
            kernel_lr     — learning/decay rate of choice kernels    [0,1]
            risk_penalty  — base penalty weight for uncertainty      [0,1]
            anx_risk_gain — amplifies uncertainty aversion by anxiety [0,1]
            stai          — anxiety score in [0,1]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the four second-stage options:
              state 0: actions (0-> drift1[t], 1-> drift2[t])
              state 1: actions (0-> drift3[t], 1-> drift4[t])

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2        (np.ndarray): Second-stage state (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward        (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, kernel_lr, risk_penalty, anx_risk_gain, stai = parameters
    stai = float(stai)

    # Transition structure: action 0 commonly -> state 0; action 1 commonly -> state 1
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize value functions and choice kernels
    q1 = np.zeros(2)         # first-stage MF values
    q2 = np.zeros((2, 2))    # second-stage MF values for each state-action pair

    k1 = np.zeros(2)         # first-stage choice kernel
    k2 = np.zeros((2, 2))    # second-stage choice kernels per state

    # Anxiety-modulated parameters
    lam = risk_penalty * (1.0 + anx_risk_gain * stai)           # uncertainty aversion
    kernel_strength = kernel_lr * (1.0 + 0.5 * anx_risk_gain * stai)

    # Storage
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    rng = np.random.default_rng()
    eps = 1e-12

    for t in range(n_trials):
        # Trial-wise reward probabilities by state and action
        reward_probs = [[float(drift1[t]), float(drift2[t])],
                        [float(drift3[t]), float(drift4[t])]]

        # Estimated uncertainty from current Q2 (p*(1-p))
        var2 = q2 * (1.0 - q2)
        unc2 = np.sqrt(np.clip(var2, 0.0, 0.25))  # in [0, 0.5]

        # Stage 1 choice
        logits1 = q1 + kernel_strength * (2.0 * k1 - 1.0)
        logits1 = beta * (logits1 - np.max(logits1))
        p1_unnorm = np.exp(logits1)
        p1 = p1_unnorm / (np.sum(p1_unnorm) + eps)
        a1 = int(rng.choice([0, 1], p=p1))

        # Transition to second-stage state
        s2 = int(rng.choice([0, 1], p=transition_matrix[a1]))

        # Stage 2 choice (uncertainty penalized in decision values)
        logits2 = (q2[s2] - lam * unc2[s2]) + kernel_strength * (2.0 * k2[s2] - 1.0)
        logits2 = beta * (logits2 - np.max(logits2))
        p2_unnorm = np.exp(logits2)
        p2 = p2_unnorm / (np.sum(p2_unnorm) + eps)
        a2 = int(rng.choice([0, 1], p=p2))

        # Outcome
        pr = reward_probs[s2][a2]
        r = int(rng.random() < pr)

        # Learning with uncertainty-penalized subjective reward
        var_chosen = q2[s2, a2] * (1.0 - q2[s2, a2])
        unc_chosen = np.sqrt(min(0.25, max(0.0, var_chosen)))
        r_subj = r - lam * unc_chosen

        pe2 = r_subj - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        target1 = q2[s2, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Update choice kernels (decay + reinforce chosen)
        k1 = (1.0 - kernel_lr) * k1
        k1[a1] += kernel_lr

        k2[s2] = (1.0 - kernel_lr) * k2[s2]
        k2[s2, a2] += kernel_lr

        # Log data
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward