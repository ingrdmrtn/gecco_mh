def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the uncertainty-gated model-based control
    with anxiety-amplified arbitration and stickiness (matching cognitive_model1).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [rho_v, beta, k1, omega0, xi_unc, stai]
            - rho_v   in [0,1]: learning rate for Q2 and MF backup to Q1
            - beta    in [0,10]: inverse temperature for softmax at both stages
            - k1      in [0,1]: perseveration strength (shared)
            - omega0  in [0,1]: baseline weight on model-based control at stage-1
            - xi_unc  in [0,1]: strength of uncertainty-driven boost to MB weight
            - stai    in [0,1]: anxiety score
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for the four second-stage state-action combinations:
            - state 0: actions 0->drift1[t], 1->drift2[t]
            - state 1: actions 0->drift3[t], 1->drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    rho_v, beta, k1, omega0, xi_unc, stai = parameters
    s_anx = float(stai)

    # True environment transition matrix (common 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]], dtype=float)

    rng = np.random.default_rng()

    # Initialize value functions
    q2 = np.zeros((2, 2), dtype=float)   # Q2[s2, a2]
    q1_mf = np.zeros(2, dtype=float)     # model-free Q at stage-1

    # Transition posterior (Dirichlet counts), symmetric prior -> starts at 0.5/0.5
    trans_counts = np.ones((2, 2), dtype=float)
    eps = 1e-12

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # Perseveration tracking
    prev_a1 = -1
    prev_a2 = -1

    for t in range(n_trials):
        # Build trial-wise reward probability matrix for state-action pairs
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]], dtype=float)

        # Current transition belief T from counts
        T = trans_counts / (np.sum(trans_counts, axis=1, keepdims=True) + eps)

        # Uncertainty as average normalized entropy of each row of T (base-2)
        ent = -np.sum(T * (np.log(T + eps)), axis=1) / np.log(2 + eps)
        unc = 0.5 * (ent[0] + ent[1])  # scalar in [0,1]

        # Model-based Q at stage-1: expected max Q2 under T
        max_q2 = np.max(q2, axis=1)        # value of each second-stage state
        q1_mb = T @ max_q2                 # MB value for each first-stage action

        # Arbitration weight with anxiety-amplified uncertainty term
        omega_eff = omega0 + xi_unc * unc * (0.5 + 0.5 * s_anx)
        omega_eff = float(np.clip(omega_eff, 0.0, 1.0))

        # Hybrid stage-1 value
        q1 = omega_eff * q1_mb + (1.0 - omega_eff) * q1_mf

        # Stage-1 stickiness
        stick1 = np.zeros(2)
        if prev_a1 in (0, 1):
            stick1[prev_a1] = 1.0

        # Stage-1 choice
        logits1 = beta * q1 + k1 * stick1
        logits1 = logits1 - np.max(logits1)
        p1 = np.exp(logits1)
        p1 = p1 / (np.sum(p1) + eps)
        a1 = int(rng.choice([0, 1], p=p1))

        # Transition to second-stage state using true environment transitions
        s2 = int(rng.choice([0, 1], p=transition_matrix[a1]))

        # Stage-2 stickiness with anxiety-attenuated strength
        stick2 = np.zeros(2)
        if prev_a2 in (0, 1):
            stick2[prev_a2] = 1.0
        k2_eff = k1 * (1.0 - 0.7 * s_anx)

        # Stage-2 choice
        logits2 = beta * q2[s2] + k2_eff * stick2
        logits2 = logits2 - np.max(logits2)
        p2 = np.exp(logits2)
        p2 = p2 / (np.sum(p2) + eps)
        a2 = int(rng.choice([0, 1], p=p2))

        # Reward sampling from drifting probabilities
        r = int(rng.random() < reward_probs[s2, a2])

        # Learning updates

        # Update transition posterior with observed transition
        trans_counts[a1, s2] += 1.0

        # Stage-2 TD update
        delta2 = r - q2[s2, a2]
        q2[s2, a2] += rho_v * delta2

        # Stage-1 MF: backup toward current Q2 value of taken branch
        target1 = q2[s2, a2]
        delta1 = target1 - q1_mf[a1]
        q1_mf[a1] += rho_v * delta1

        # Additional eligibility-like backup by the reward prediction error
        q1_mf[a1] += rho_v * delta2

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

        # Log trial outcomes
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward