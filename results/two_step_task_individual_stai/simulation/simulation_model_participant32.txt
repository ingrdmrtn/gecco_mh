def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the asymmetric MF learner with
    anxiety-modulated forgetting and choice persistence (stickiness).

    Model matches cognitive_model2:
    - Separate learning rates for positive vs. negative PEs at both stages.
    - Single inverse temperature beta for both stages.
    - Forgetting toward neutral prior (0.5) scaled by anxiety.
    - Choice persistence at both stages, scaled by anxiety.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or array-like):
            [mu_win, mu_loss, beta, z_forget, psi_persist, stai]
            where:
            - mu_win in [0,1]: learning rate for positive PE.
            - mu_loss in [0,1]: learning rate for negative PE.
            - beta in [0,10]: inverse temperature (both stages).
            - z_forget in [0,1]: baseline forgetting strength per trial.
            - psi_persist in [0,1]: baseline stickiness strength.
            - stai in [0,1] or broader: anxiety score (scales forgetting and stickiness).
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for state 0 actions (drift1, drift2) and state 1 actions (drift3, drift4).

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np
    rng = np.random.default_rng()

    mu_win, mu_loss, beta, z_forget, psi_persist, stai = parameters

    # Two-step transition structure (common 0.7 / rare 0.3)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Initialize Q-values
    q1_mf = np.zeros(2)      # first-stage MF values
    q2 = np.zeros((2, 2))    # second-stage values per state x action

    # Neutral priors for forgetting
    prior_q1 = 0.5
    prior_q2 = 0.5

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # Persistence tracking
    prev_a1 = None
    prev_a2_state = {0: None, 1: None}

    for t in range(n_trials):
        # Stickiness/persistence and forgetting scales with anxiety
        stick = psi_persist * (0.5 + 0.5 * stai)
        f = np.clip(z_forget * (0.5 + 0.5 * stai), 0.0, 1.0)

        # Stage 1 choice
        bias1 = np.zeros(2)
        if prev_a1 is not None:
            bias1[prev_a1] = 1.0
        logits1 = beta * (q1_mf - np.max(q1_mf)) + stick * bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 = probs1 / probs1.sum()
        a1 = rng.choice([0, 1], p=probs1)

        # Transition to second-stage state
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage 2 choice (within reached state)
        q2_s = q2[s2].copy()
        bias2 = np.zeros(2)
        if prev_a2_state[s2] is not None:
            bias2[prev_a2_state[s2]] = 1.0
        logits2 = beta * (q2_s - np.max(q2_s)) + stick * bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 = probs2 / probs2.sum()
        a2 = rng.choice([0, 1], p=probs2)

        # Reward from drifting probabilities
        p_r = [[drift1[t], drift2[t]], [drift3[t], drift4[t]]][s2][a2]
        r = int(rng.random() < p_r)

        # Second-stage update (asymmetric learning)
        pe2 = r - q2[s2, a2]
        lr2 = mu_win if pe2 >= 0.0 else mu_loss
        q2[s2, a2] += lr2 * pe2

        # First-stage MF update toward updated second-stage value (asymmetric)
        pe1 = q2[s2, a2] - q1_mf[a1]
        lr1 = mu_win if pe1 >= 0.0 else mu_loss
        q1_mf[a1] += lr1 * pe1

        # Anxiety-modulated forgetting toward neutral priors
        q1_mf = (1.0 - f) * q1_mf + f * prior_q1
        q2 = (1.0 - f) * q2 + f * prior_q2

        # Persistency bookkeeping
        prev_a1 = a1
        prev_a2_state[s2] = a2

        # Save trial data
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward