def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the hybrid model with anxiety-biased transition learning
    and first-stage perseveration (matching cognitive_model3 logic).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate, stai]
            - alphaQ in [0,1]: MF learning rate (both stages).
            - beta in [0,10]: inverse temperature for both stages.
            - tau_T in [0,1]: base learning rate for transition probabilities (agent's belief).
            - k_anx_trans_bias in [0,1]: scales anxiety-dependent modulation of transition learning.
            - psi_perseverate in [0,1]: base perseveration weight at stage 1.
            - stai in [0,1]: anxiety level used to modulate transition learning and perseveration.
        drift1..drift4 (np.ndarray): Trial-wise reward probabilities for:
            state 0, action 0 -> drift1[t]
            state 0, action 1 -> drift2[t]
            state 1, action 0 -> drift3[t]
            state 1, action 1 -> drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np
    rng = np.random.default_rng()

    alphaQ, beta, tau_T, k_anx_trans_bias, psi_perseverate, stai = parameters
    stai_val = float(stai)

    # True environment transitions (used to generate states)
    trans_true = np.array([[0.7, 0.3],
                           [0.3, 0.7]])

    # Agent's internal transition belief (learned online)
    T = np.full((2, 2), 0.5)

    # Model-free Q-values
    q1_mf = np.zeros(2)       # first-stage MF values
    q2 = np.zeros((2, 2))     # second-stage values per state/action

    # Data containers
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    prev_a1 = None

    for t in range(n_trials):
        # Trial-wise reward probability matrix for the environment
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]])

        # Model-based first-stage values from current transition belief and second-stage values
        max_q2 = np.max(q2, axis=1)     # best action value in each second-stage state
        q1_mb = T @ max_q2              # expected value under learned T

        # First-stage perseveration bias scaled by anxiety
        stick_vec = np.zeros(2)
        if prev_a1 is not None:
            stick_vec[prev_a1] = 1.0
        perseveration_bias = psi_perseverate * stai_val * stick_vec

        # Hybrid value (fixed 0.5/0.5 mixture)
        q1_comb = 0.5 * q1_mb + 0.5 * q1_mf

        # First-stage choice via softmax (with perseveration bias)
        logits1 = q1_comb + perseveration_bias
        logits1 = logits1 - np.max(logits1)
        p1 = np.exp(beta * logits1)
        p1 = p1 / p1.sum()
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state via true environment transitions
        s = rng.choice([0, 1], p=trans_true[a1])

        # Second-stage choice via softmax on q2[s]
        logits2 = q2[s] - np.max(q2[s])
        p2 = np.exp(beta * logits2)
        p2 = p2 / p2.sum()
        a2 = rng.choice([0, 1], p=p2)

        # Reward draw from trial-wise probabilities
        r = int(rng.random() < reward_probs[s, a2])

        # Update learned transition belief for the chosen first-stage action
        p_obs = T[a1, s]
        is_rare = p_obs < 0.5
        if is_rare:
            tau_eff = tau_T * (1.0 + k_anx_trans_bias * stai_val)
        else:
            tau_eff = tau_T * (1.0 - k_anx_trans_bias * stai_val)
        tau_eff = min(max(tau_eff, 0.0), 1.0)

        T[a1, s] += tau_eff * (1.0 - T[a1, s])
        other = 1 - s
        T[a1, other] = 1.0 - T[a1, s]

        # Model-free Q-updates
        # Second stage
        delta2 = r - q2[s, a2]
        q2[s, a2] += alphaQ * delta2

        # First stage bootstraps toward the value of the taken second-stage action
        boot = q2[s, a2]
        delta1 = boot - q1_mf[a1]
        q1_mf[a1] += alphaQ * delta1

        # Store and update perseveration tracker
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r
        prev_a1 = a1

    return stage1_choice, state2, stage2_choice, reward