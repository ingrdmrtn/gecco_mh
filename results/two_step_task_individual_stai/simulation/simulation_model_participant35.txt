def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the anxiety-modulated, risk-sensitive utility model
    with WSLS mixture at stage 2 and value leakage (cognitive_model3 simulation).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alpha_q, beta_base, phi_leak, zeta_wsls, nu_loss, stai]
            - alpha_q in [0,1]
            - beta_base in [0,10]
            - phi_leak in [0,1]
            - zeta_wsls in [0,1]
            - nu_loss in [0,1] (not impactful with binary rewards, but kept for parity)
            - stai in [0,1]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities
            for second-stage actions:
              state X: action0 -> drift1[t], action1 -> drift2[t]
              state Y: action0 -> drift3[t], action1 -> drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    # Unpack parameters
    alpha_q, beta_base, phi_leak, zeta_wsls, nu_loss, stai = parameters
    stai = float(stai)

    # Anxiety-modulated parameters (match fitting code)
    beta = max(0.0, min(10.0, beta_base * (1.0 - 0.5 * stai)))
    w_wsls = max(0.0, min(1.0, zeta_wsls * (0.5 + 0.5 * stai)))
    nu_eff = max(0.0, min(2.0, nu_loss * (1.0 + stai)))  # not impactful with 0/1 rewards
    leak = max(0.0, min(1.0, phi_leak))

    # Two-step transitions (common 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    rng = np.random.default_rng()

    # Initialize value functions
    q1 = np.zeros(2)         # First-stage MF
    q2 = np.zeros((2, 2))    # Second-stage MF by state

    # WSLS bookkeeping per state
    prev_a2 = np.zeros(2, dtype=int)
    prev_sign = np.zeros(2)             # +1 or -1 based on reward sign (here 0/1 -> non-negative)
    has_prev = np.zeros(2, dtype=bool)

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Build reward probability matrix for this trial
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]])

        # Stage 1 choice: softmax over q1 with temperature beta
        c_q1 = q1 - np.max(q1)
        logits1 = beta * c_q1
        # stable softmax
        exp1 = np.exp(logits1 - np.max(logits1))
        p_stage1 = exp1 / np.sum(exp1)
        a1 = rng.choice([0, 1], p=p_stage1)

        # Transition to second-stage state
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage 2 policy: mixture of softmax over q2[s2] and WSLS heuristic
        c_q2 = q2[s2] - np.max(q2[s2])
        logits2 = beta * c_q2
        exp2 = np.exp(logits2 - np.max(logits2))
        probs2_soft = exp2 / np.sum(exp2)

        if has_prev[s2]:
            if prev_sign[s2] >= 0.0:
                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[prev_a2[s2]] = 1.0
            else:
                wsls_probs = np.array([0.0, 0.0])
                wsls_probs[1 - prev_a2[s2]] = 1.0
        else:
            wsls_probs = np.array([0.5, 0.5])

        probs2 = (1.0 - w_wsls) * probs2_soft + w_wsls * wsls_probs
        probs2 /= np.sum(probs2)
        a2 = rng.choice([0, 1], p=probs2)

        # Outcome
        r = int(rng.random() < reward_probs[s2, a2])

        # Utility (matches fitting code; with binary rewards this equals r)
        util = r if r >= 0.0 else -nu_eff * (-r)

        # Value leakage
        q2 *= (1.0 - leak)
        q1 *= (1.0 - leak)

        # Stage 2 update
        pe2 = util - q2[s2, a2]
        q2[s2, a2] += alpha_q * pe2

        # Stage 1 bootstrapped update from Q2 (MF from the chosen state-action)
        target1 = q2[s2, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha_q * pe1

        # Update WSLS memory
        prev_a2[s2] = a2
        prev_sign[s2] = 1.0 if r >= 0.0 else -1.0
        has_prev[s2] = True

        # Log trial
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward