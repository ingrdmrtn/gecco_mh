def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the volatility-adaptive hybrid MB/MF model
    with anxiety-modulated exploration and perseveration (matching cognitive_model1).

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or array):
            [alpha, beta_base, k_vol, w_MB, stickiness, stai(optional)]
            - alpha: learning rate for both stages.
            - beta_base: base inverse temperature for both stages.
            - k_vol: volatility learning rate for adapting temperature via PE variance.
            - w_MB: weight on model-based values at stage 1 (0..1).
            - stickiness: perseveration bias strength added to the previously chosen action.
            - stai (optional): anxiety in [0,1]; higher reduces effective temperature.
              If omitted, defaults to 0.0.
        drift1, drift2, drift3, drift4 (np.ndarray):
            Trial-wise reward probabilities for each second-stage state-action:
              - state 0 (planet X): actions 0/1 -> drift1[t], drift2[t]
              - state 1 (planet Y): actions 0/1 -> drift3[t], drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    # Unpack parameters; allow optional stai (defaults to 0.0 if not provided)
    if len(parameters) == 5:
        alpha, beta_base, k_vol, w_MB, stickiness = parameters
        stai = 0.0
    else:
        alpha, beta_base, k_vol, w_MB, stickiness, stai = parameters

    rng = np.random.default_rng()

    # Fixed transition model (common=0.7, rare=0.3)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Initialize values
    Q2 = np.zeros((2, 2))   # second-stage state-action values
    Q1_mf = np.zeros(2)     # model-free first-stage values

    # Volatility estimate (tracks squared PE)
    v = 0.0

    # Perseveration memory (previous actions), start at 0 to mirror fitting code
    prev_a1 = 0
    prev_a2 = 0

    # Anxiety-modulated base inverse temperature
    beta_base_eff = beta_base * (1.0 - 0.5 * float(stai))

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # Current reward probabilities for the two second-stage states
        # state 0: [drift1, drift2], state 1: [drift3, drift4]
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]], dtype=float)

        # Model-based evaluation via transition model and current Q2
        max_Q2 = np.max(Q2, axis=1)          # max over actions for each second-stage state
        Q1_mb = T @ max_Q2                   # expected value for each first-stage action

        # Hybrid first-stage value
        Q1 = w_MB * Q1_mb + (1.0 - w_MB) * Q1_mf

        # Volatility-adaptive temperature
        beta_t = beta_base_eff / (1.0 + v)

        # Stage 1 softmax with perseveration bias
        bias1 = np.zeros(2)
        bias1[prev_a1] += stickiness
        logits1 = beta_t * (Q1 - np.max(Q1)) + bias1
        probs1 = np.exp(logits1 - np.max(logits1))
        probs1 /= probs1.sum()
        a1 = int(rng.choice([0, 1], p=probs1))

        # Transition to second-stage state
        s = int(rng.choice([0, 1], p=T[a1]))

        # Stage 2 softmax with perseveration bias
        bias2 = np.zeros(2)
        bias2[prev_a2] += stickiness
        q2_s = Q2[s]
        logits2 = beta_t * (q2_s - np.max(q2_s)) + bias2
        probs2 = np.exp(logits2 - np.max(logits2))
        probs2 /= probs2.sum()
        a2 = int(rng.choice([0, 1], p=probs2))

        # Reward sampling from current drift probabilities
        r_prob = float(reward_probs[s, a2])
        r = int(rng.random() < r_prob)

        # Learning updates
        # Second-stage TD update and volatility tracking
        pe2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * pe2
        v = (1.0 - k_vol) * v + k_vol * (pe2 * pe2)

        # First-stage model-free backup from realized second-stage value
        target1 = Q2[s, a2]
        pe1 = target1 - Q1_mf[a1]
        Q1_mf[a1] += alpha * pe1

        # Update perseveration memory
        prev_a1 = a1
        prev_a2 = a2

        # Log trial outcomes
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward