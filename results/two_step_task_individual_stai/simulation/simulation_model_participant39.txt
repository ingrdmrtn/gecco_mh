def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the surprise-gated hybrid control model
    with anxiety-modulated perseveration at both stages.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple): [alpha, beta, rho_surp0, kappa_rep0, zeta_pers2, stai]
            - alpha: learning rate for MF values (stage-2 and stage-1 bootstrapping)
            - beta: inverse temperature for both stages
            - rho_surp0: gain for surprise-gated increase in planning weight
            - kappa_rep0: base strength of first-stage perseveration
            - zeta_pers2: base strength of second-stage perseveration (state-specific)
            - stai: anxiety trait in [0, 1], scales w_mb dynamics and perseveration
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for:
            - state 0, action 0 -> drift1
            - state 0, action 1 -> drift2
            - state 1, action 0 -> drift3
            - state 1, action 1 -> drift4

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, rho_surp0, kappa_rep0, zeta_pers2, stai = parameters
    stai = float(stai)

    rng = np.random.default_rng()

    # Transition matrix (rows: first-stage action; cols: second-stage state)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]])

    # Value functions
    q1_mf = np.zeros(2)         # model-free first-stage values
    q2_mf = np.zeros((2, 2))    # second-stage MF values (state x action)

    # Dynamic model-based weight (initialized as in fitting code)
    w_mb = max(0.0, min(1.0, 1.0 - stai))

    # Perseveration trackers
    prev_a1 = None
    prev_a2_by_state = {0: None, 1: None}

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    eps = 1e-12

    for t in range(n_trials):
        # Drift reward probabilities for this trial organized by state, action
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute model-based first-stage values from second-stage MF values
        max_q2 = np.max(q2_mf, axis=1)   # best alien per planet
        q1_mb = T @ max_q2               # expected value per spaceship

        # Hybrid first-stage values
        q1_hybrid = w_mb * q1_mb + (1.0 - w_mb) * q1_mf

        # First-stage perseveration modulated by anxiety
        if prev_a1 is not None:
            stick = np.zeros(2)
            stick[prev_a1] = 1.0
            kappa_eff = kappa_rep0 * (1.0 + stai)
            q1_hybrid = q1_hybrid + kappa_eff * stick

        # Softmax for first-stage choice
        q1c = q1_hybrid - np.max(q1_hybrid)
        probs_1 = np.exp(beta * q1c)
        probs_1 = probs_1 / (np.sum(probs_1) + eps)
        a1 = rng.choice([0, 1], p=probs_1)

        # Transition to second-stage state based on chosen spaceship
        s2 = rng.choice([0, 1], p=T[a1])

        # Second-stage values with state-specific perseveration
        q2 = q2_mf[s2].copy()
        prev_a2 = prev_a2_by_state[s2]
        if prev_a2 is not None:
            stick2 = np.zeros(2)
            stick2[prev_a2] = 1.0
            zeta_eff = zeta_pers2 * (1.0 - 0.5 * stai)
            q2 = q2 + zeta_eff * stick2

        # Softmax for second-stage choice
        q2c = q2 - np.max(q2)
        probs_2 = np.exp(beta * q2c)
        probs_2 = probs_2 / (np.sum(probs_2) + eps)
        a2 = rng.choice([0, 1], p=probs_2)

        # Generate reward based on drifting probabilities for the reached state and chosen action
        r = int(rng.random() < reward_probs[s2][a2])

        # Learning updates
        pe2 = r - q2_mf[s2, a2]
        q2_mf[s2, a2] += alpha * pe2

        td_target1 = q2_mf[s2, a2]
        pe1 = td_target1 - q1_mf[a1]
        q1_mf[a1] += alpha * pe1

        # Surprise-gated update of model-based weight
        p_trans = T[a1, s2]
        surprise = 1.0 - p_trans
        w_mb = w_mb + rho_surp0 * (1.0 + stai) * (surprise - (w_mb - (1.0 - stai)))
        w_mb = max(0.0, min(1.0, w_mb))

        # Update perseveration memory
        prev_a1 = a1
        prev_a2_by_state[s2] = a2

        # Log trial outcomes
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward