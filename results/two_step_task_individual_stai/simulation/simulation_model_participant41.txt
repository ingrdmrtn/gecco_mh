def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards using the provided hybrid MB/MF model with
    anxiety-modulated arbitration and perseveration.

    Model details (matching cognitive_model1):
    - First-stage Q is a hybrid of model-free (MF) and model-based (MB) values.
    - MB value is computed from MF second-stage values via transition matrix and max over actions.
    - Single inverse temperature beta is used at both stages.
    - Eligibility trace propagates second-stage TD error to first-stage MF values.
    - Perseveration adds an additive bias on the previously chosen first-stage action.
    - Anxiety (stai) reduces the MB weight and increases perseveration.

    Parameters
    ----------
    n_trials : int
        Number of trials to simulate.
    parameters : list or array-like
        [alpha, lambda_, beta, w_base, pers_base, stai]
        - alpha in [0,1]: learning rate for both stages.
        - lambda_ in [0,1]: eligibility trace for first-stage update from second-stage TD error.
        - beta in [0,10]: inverse temperature for softmax at both stages.
        - w_base in [0,1]: baseline MB weight before anxiety modulation.
        - pers_base in [0,1]: baseline perseveration magnitude (additive bias).
        - stai in [0,1]: anxiety level; decreases MB weight and increases perseveration.
    drift1, drift2, drift3, drift4 : np.ndarray
        Trial-wise reward probabilities for second-stage actions:
        - State 0: action 0 -> drift1[t], action 1 -> drift2[t]
        - State 1: action 0 -> drift3[t], action 1 -> drift4[t]

    Returns
    -------
    stage1_choice : np.ndarray of int (0 or 1)
    state2 : np.ndarray of int (0 or 1)
    stage2_choice : np.ndarray of int (0 or 1)
    reward : np.ndarray of int (0 or 1)
    """
    import numpy as np

    alpha, lambda_, beta, w_base, pers_base, stai = parameters

    rng = np.random.default_rng()

    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # MF values
    q_stage1_mf = np.zeros(2)        # for first-stage actions (A=0, U=1)
    q_stage2_mf = np.zeros((2, 2))   # for second-stage actions per state

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # Anxiety-modulated parameters (fixed across trials as in fitting code)
    w_eff_scale = 1.0 - 0.5 * stai
    w_eff_base = w_base * w_eff_scale
    pers_eff = pers_base * (0.5 + 0.5 * stai)

    prev_a1 = -1  # for perseveration bias

    for t in range(n_trials):
        # Reward probabilities for this trial
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]])

        # Model-based first-stage values: expected max second-stage value under transitions
        max_q_stage2 = np.max(q_stage2_mf, axis=1)       # shape (2,)
        q_stage1_mb = transition_matrix @ max_q_stage2   # shape (2,)

        # Arbitration weight (constant across trials per fitting code)
        w_eff = w_eff_base

        # Combine MB and MF at stage 1
        q1_combined = w_eff * q_stage1_mb + (1.0 - w_eff) * q_stage1_mf

        # Perseveration bias
        bias = np.zeros(2)
        if prev_a1 >= 0:
            bias[prev_a1] = pers_eff

        # Stage 1 policy and choice
        q1_policy_vals = q1_combined + bias
        q1_shift = q1_policy_vals - np.max(q1_policy_vals)
        p1 = np.exp(beta * q1_shift)
        p1 = p1 / p1.sum()
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state
        s = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage 2 policy and choice (MF only; same beta)
        q2 = q_stage2_mf[s, :]
        q2_shift = q2 - np.max(q2)
        p2 = np.exp(beta * q2_shift)
        p2 = p2 / p2.sum()
        a2 = rng.choice([0, 1], p=p2)

        # Outcome
        r = int(rng.random() < reward_probs[s, a2])

        # Learning updates
        # Second-stage TD update
        delta2 = r - q_stage2_mf[s, a2]
        q_stage2_mf[s, a2] += alpha * delta2

        # First-stage bootstrapped update
        delta1_boot = q_stage2_mf[s, a2] - q_stage1_mf[a1]
        q_stage1_mf[a1] += alpha * delta1_boot

        # Eligibility trace from second-stage TD error
        q_stage1_mf[a1] += alpha * lambda_ * delta2

        # Store and update perseveration memory
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r
        prev_a1 = a1

    return stage1_choice, state2, stage2_choice, reward