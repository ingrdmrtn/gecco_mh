def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the Asymmetric model-free SARSA with
    anxiety-modulated lapse and endogenous eligibility.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or tuple):
            [alpha_gain, alpha_loss, beta1, beta2, lapse_base, stai]
            - alpha_gain in [0,1]: learning rate when second-stage PE > 0
            - alpha_loss in [0,1]: learning rate when second-stage PE <= 0
            - beta1 in [0,10]: inverse temperature for first-stage softmax
            - beta2 in [0,10]: inverse temperature for second-stage softmax
            - lapse_base in [0,1]: base lapse; effective lapse = min(0.5, lapse_base * stai)
            - stai in [0,1]: anxiety level; eligibility lambda = 1 - stai
        drift1, drift2, drift3, drift4 (np.ndarray):
            Trial-wise reward probabilities for second-stage options:
            - State 0: actions 0 -> drift1[t], 1 -> drift2[t]
            - State 1: actions 0 -> drift3[t], 1 -> drift4[t]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha_g, alpha_l, beta1, beta2, lapse_base, stai = parameters

    # Anxiety-modulated eligibility and lapse
    lam = float(np.clip(1.0 - float(stai), 0.0, 1.0))
    lapse = float(np.clip(lapse_base * float(stai), 0.0, 0.5))

    # Two-step transition structure (common = 0.7)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    rng = np.random.default_rng()

    # Initialize Q-values
    q1 = np.zeros(2)        # first-stage Q-values for actions [0,1]
    q2 = np.zeros((2, 2))   # second-stage Q-values: states x actions

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # -------- Stage 1 choice --------
        pref1 = beta1 * q1
        # numerically-stable softmax
        z1 = np.max(pref1)
        exp1 = np.exp(pref1 - z1)
        soft1 = exp1 / np.sum(exp1)
        probs_1 = (1.0 - lapse) * soft1 + lapse * 0.5
        a1 = rng.choice([0, 1], p=probs_1)

        # -------- Transition to Stage 2 state --------
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # -------- Stage 2 choice --------
        pref2 = beta2 * q2[s2]
        z2 = np.max(pref2)
        exp2 = np.exp(pref2 - z2)
        soft2 = exp2 / np.sum(exp2)
        probs_2 = (1.0 - lapse) * soft2 + lapse * 0.5
        a2 = rng.choice([0, 1], p=probs_2)

        # -------- Reward sampling from drifting Bernoulli --------
        # State 0 -> [drift1, drift2]; State 1 -> [drift3, drift4]
        if s2 == 0:
            p_r = drift1[t] if a2 == 0 else drift2[t]
        else:
            p_r = drift3[t] if a2 == 0 else drift4[t]
        r = int(rng.random() < p_r)

        # -------- Learning updates (asymmetric SARSA with endogenous eligibility) --------
        # Second-stage TD error and asymmetric learning rate
        pe2 = r - q2[s2, a2]
        alpha2 = alpha_g if pe2 > 0.0 else alpha_l
        q2[s2, a2] += alpha2 * pe2

        # First-stage SARSA-style backup with anxiety-modulated eligibility
        pe1 = q2[s2, a2] - q1[a1]
        q1[a1] += alpha2 * (pe1 + lam * pe2)

        # -------- Store trial data --------
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward