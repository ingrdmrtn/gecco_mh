def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards for a risk-sensitive model-free learner
    with anxiety-modulated risk aversion and value decay.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list or array):
            [alpha, beta, rho_base, k_anx_rho, decay, stai]
              - alpha (0..1): EW update rate for mean and second moment
              - beta (0..10): inverse temperature at both stages
              - rho_base (0..1): baseline risk aversion
              - k_anx_rho (0..1): slope of STAI effect on risk aversion
              - decay (0..1): per-trial decay applied to unchosen values
              - stai (0..1): trait anxiety scaler for risk aversion
        drift1, drift2, drift3, drift4 (np.ndarray):
            Trial-wise reward probabilities for the four second-stage options arranged as:
              state 0: [drift1[t], drift2[t]]
              state 1: [drift3[t], drift4[t]]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, rho_base, k_anx_rho, decay, stai_val = parameters
    stai_val = float(stai_val)
    rho = rho_base + k_anx_rho * stai_val
    rho = min(max(rho, 0.0), 1.0)

    rng = np.random.default_rng()

    # Transition matrix (standard two-step task)
    transition_matrix = np.array([[0.7, 0.3],
                                  [0.3, 0.7]])

    # Stage-2 EW statistics: mean (m) and mean of squared reward (m2)
    m = np.zeros((2, 2))
    m2 = np.zeros((2, 2))

    # Stage-1 action values
    q1 = np.zeros(2)

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    eps = 1e-12

    for t in range(n_trials):
        # Current reward probabilities per state-action
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute current risk-sensitive utilities at stage-2
        var = np.maximum(m2 - m**2, 0.0)
        std = np.sqrt(var + 1e-12)
        u = m - rho * std  # utility matrix [2 x 2]

        # Stage-1 softmax over q1
        logits1 = beta * q1
        logits1 = logits1 - np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 = probs1 / (np.sum(probs1) + eps)
        a1 = rng.choice([0, 1], p=probs1)

        # Transition to second-stage state
        s = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage-2 softmax over utilities in reached state
        logits2 = beta * u[s]
        logits2 = logits2 - np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 = probs2 / (np.sum(probs2) + eps)
        a2 = rng.choice([0, 1], p=probs2)

        # Outcome
        r = int(rng.random() < reward_probs[s][a2])

        # Apply decay to unchosen stage-2 entries (as in fitting code)
        other_a2 = 1 - a2
        m[s, other_a2] *= (1.0 - decay)
        m2[s, other_a2] *= (1.0 - decay)
        other_state = 1 - s
        m[other_state, 0] *= (1.0 - decay)
        m[other_state, 1] *= (1.0 - decay)
        m2[other_state, 0] *= (1.0 - decay)
        m2[other_state, 1] *= (1.0 - decay)

        # Update EW mean and second moment for chosen state-action
        m[s, a2] = m[s, a2] + alpha * (r - m[s, a2])
        m2[s, a2] = m2[s, a2] + alpha * (r * r - m2[s, a2])

        # Compute updated utility for the experienced pair
        var_sa = max(m2[s, a2] - m[s, a2] ** 2, 0.0)
        u_sa = m[s, a2] - rho * np.sqrt(var_sa + 1e-12)

        # Stage-1 value decay for unchosen action and TD update toward u_sa
        q1[1 - a1] *= (1.0 - decay)
        delta1 = u_sa - q1[a1]
        q1[a1] = q1[a1] + alpha * delta1

        # Log trial
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward