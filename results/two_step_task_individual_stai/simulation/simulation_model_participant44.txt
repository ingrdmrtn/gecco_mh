def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the anxiety-modulated transition-learning hybrid MF/MB model
    with forgetting and temperature shift.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list): [alpha, beta, phi_trans, eta_forget, xi_anxTemp, stai]
            - alpha in [0,1]: learning rate for Q updates
            - beta in [0,10]: base inverse temperature
            - phi_trans in [0,1]: base transition learning rate
            - eta_forget in [0,1]: base forgetting rate
            - xi_anxTemp in [0,1]: strength of anxiety-driven temperature reduction
            - stai in [0,1]: trait/state anxiety scalar used by the model
        drift1..drift4 (np.ndarray): Trial-wise reward probabilities for the 2x2 second-stage bandits.

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, phi_trans, eta_forget, xi_anxTemp, stai = parameters

    rng = np.random.default_rng()

    # True environment transition matrix (fixed)
    env_T = np.array([[0.7, 0.3],
                      [0.3, 0.7]], dtype=float)

    # Agent's internal transition estimate (starts from canonical 0.7/0.3 as in fitting code)
    T = np.array([[0.7, 0.3],
                  [0.3, 0.7]], dtype=float)

    # Anxiety-modulated effective parameters
    beta_eff = max(1e-3, beta * (1.0 - xi_anxTemp * stai))               # higher anxiety -> lower beta
    phi_eff = np.clip(phi_trans * (1.0 + 0.5 * stai), 0.0, 1.0)          # higher anxiety -> faster transition learning
    forget_eff = np.clip(eta_forget * stai, 0.0, 1.0)                    # higher anxiety -> more forgetting
    lambda_anx = np.clip(stai, 0.0, 1.0)                                  # eligibility trace equals anxiety

    # Value functions
    q1_mf = np.zeros(2)          # MF Q for first-stage actions (0/1)
    q2 = np.zeros((2, 2))        # Q values for second-stage actions (state 0/1, action 0/1)

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    for t in range(n_trials):
        # 1) Compute MB first-stage values from current internal transition estimate
        max_q2 = np.max(q2, axis=1)       # best value on each second-stage state
        q1_mb = T @ max_q2                # MB Q for first-stage actions

        # 2) Compute arbitration weight w_mb based on transition entropy and anxiety
        row_ent = []
        for r in range(2):
            p = T[r]
            # Binary entropy in bits
            h = 0.0
            for x in p:
                if x > 0:
                    h -= x * (np.log(x) / np.log(2))
            row_ent.append(h)  # in [0,1] for binary
        ent_mean = 0.5 * (row_ent[0] + row_ent[1])  # 0=certainty, 1=max uncertainty
        w_mb = np.clip((1.0 - stai) * (1.0 - ent_mean), 0.0, 1.0)

        # 3) Arbitration to form first-stage action values
        q1 = (1.0 - w_mb) * q1_mf + w_mb * q1_mb

        # 4) First-stage choice via softmax with beta_eff
        logits1 = beta_eff * q1
        logits1 -= np.max(logits1)
        probs1 = np.exp(logits1)
        probs1 /= np.sum(probs1)
        a1 = rng.choice([0, 1], p=probs1)

        # 5) Transition to second-stage state via true environment transition
        s = rng.choice([0, 1], p=env_T[a1])

        # 6) Second-stage choice via softmax with beta_eff on q2[s]
        logits2 = beta_eff * q2[s, :]
        logits2 -= np.max(logits2)
        probs2 = np.exp(logits2)
        probs2 /= np.sum(probs2)
        a2 = rng.choice([0, 1], p=probs2)

        # 7) Generate reward from drifting Bernoulli probabilities
        reward_probs = [[drift1[t], drift2[t]], [drift3[t], drift4[t]]]
        r = int(rng.random() < reward_probs[s][a2])

        # 8) Forgetting
        q1_mf = (1.0 - forget_eff) * q1_mf
        q2 = (1.0 - forget_eff) * q2

        # 9) MF TD updates and eligibility trace from stage 2 to stage 1
        td1 = q2[s, a2] - q1_mf[a1]
        q1_mf[a1] += alpha * td1

        td2 = r - q2[s, a2]
        q2[s, a2] += alpha * td2

        q1_mf[a1] += alpha * lambda_anx * td2

        # 10) Transition learning: update internal T based on observed state
        obs = np.array([1.0 if i == s else 0.0 for i in range(2)], dtype=float)
        T[a1, :] = (1.0 - phi_eff) * T[a1, :] + phi_eff * obs
        # Renormalize for safety
        T[a1, :] = np.clip(T[a1, :], 1e-6, 1.0)
        T[a1, :] /= np.sum(T[a1, :])

        # 11) Save trial data
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward