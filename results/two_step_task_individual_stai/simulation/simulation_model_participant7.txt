import numpy as np

def simulate_model(
    n_trials,
    parameters,
    drift1,
    drift2,
    drift3,
    drift4):
    """
    Simulates choices and rewards using the Hybrid MB/MF model with
    anxiety-modulated surprise-seeking intrinsic motivation.

    Model (matches cognitive_model2):
    - Stage 2: TD(0) learning on rewards (single learning rate alpha).
    - Stage 1: mixture of model-based and model-free values.
    - Surprise trace S per first-stage action updated from -log P(state|action).
    - Surprise bonus added to stage-1 values with strength z_eff = clip(z0 + k_z * stai, 0, 1).
    - Single inverse temperature beta shared across stages.

    Parameters:
        n_trials (int): Number of trials to simulate.
        parameters (list/tuple): [alpha, beta, w, z0, k_z, stai]
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for the 2x2 second-stage actions.
            reward_probs = [[drift1[t], drift2[t]], [drift3[t], drift4[t]]]

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    alpha, beta, w, z0, k_z, stai = parameters
    stai = float(stai)

    # Transition structure
    p_common = 0.7
    T = np.array([[p_common, 1 - p_common],
                  [1 - p_common, p_common]])

    # Initialize values and surprise traces
    Q2 = np.zeros((2, 2))   # stage-2 Q-values: state x action
    Q1_mf = np.zeros(2)     # stage-1 model-free values per action
    S = np.zeros(2)         # surprise trace per first-stage action

    # Effective surprise scaling
    z_eff = z0 + k_z * stai
    z_eff = max(0.0, min(1.0, z_eff))

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    rng = np.random.default_rng()

    for t in range(n_trials):
        # Trial-specific reward probabilities for second-stage actions
        reward_probs = [[drift1[t], drift2[t]],
                        [drift3[t], drift4[t]]]

        # Compute model-based first-stage values from current Q2
        max_Q2 = np.max(Q2, axis=1)   # best action value in each second-stage state
        Q1_mb = T @ max_Q2            # expected value of first-stage actions via transitions

        # Combine MB/MF and surprise bonus
        Q1 = w * Q1_mb + (1.0 - w) * Q1_mf + z_eff * S

        # Stage 1 choice via softmax
        q1c = Q1 - np.max(Q1)
        p1 = np.exp(beta * q1c)
        p1 /= p1.sum()
        a1 = rng.choice([0, 1], p=p1)

        # Transition to second-stage state
        s = rng.choice([0, 1], p=T[a1])

        # Stage 2 choice via softmax on Q2[s]
        q2c = Q2[s] - np.max(Q2[s])
        p2 = np.exp(beta * q2c)
        p2 /= p2.sum()
        a2 = rng.choice([0, 1], p=p2)

        # Generate reward from drifting bandits
        r = int(rng.random() < reward_probs[s][a2])

        # Stage-2 TD(0) update
        delta2 = r - Q2[s, a2]
        Q2[s, a2] += alpha * delta2

        # Stage-1 MF update bootstrapped from updated Q2
        boot = Q2[s, a2]
        delta1 = boot - Q1_mf[a1]
        Q1_mf[a1] += alpha * delta1

        # Surprise update for chosen first-stage action
        p_s_given_a = T[a1, s]
        p_s_given_a = max(p_s_given_a, 1e-8)
        surprise = -np.log(p_s_given_a)
        S[a1] += alpha * (surprise - S[a1])

        # Save trial data
        stage1_choice[t] = a1
        state2[t] = s
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward