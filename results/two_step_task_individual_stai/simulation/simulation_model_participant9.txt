def simulate_model(n_trials, parameters, drift1, drift2, drift3, drift4):
    """
    Simulates choices and rewards for the model-free controller with anxiety-modulated
    forgetting and WSLS bias depending on reward and transition type.

    Parameters:
        n_trials (int): Number of trials.
        parameters (list or tuple): [alpha, beta, decay, wsls_gain, xi, stai]
            - alpha: learning rate (0..1) for both stages
            - beta: inverse temperature (0..10) shared across stages
            - decay: baseline forgetting rate (0..1)
            - wsls_gain: magnitude of WSLS bias on first-stage logits (0..1)
            - xi: mixes reward vs transition contributions to WSLS (0..1)
            - stai: trait anxiety in [0,1]; higher -> more forgetting and specific lose-shift
        drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities for:
            state X action0, state X action1, state Y action0, state Y action1

    Returns:
        stage1_choice (np.ndarray): First-stage choices (0 or 1).
        state2 (np.ndarray): Second-stage states (0 or 1).
        stage2_choice (np.ndarray): Second-stage choices (0 or 1).
        reward (np.ndarray): Rewards (0 or 1).
    """
    import numpy as np

    alpha, beta, decay, wsls_gain, xi, stai = parameters
    rng = np.random.default_rng()

    # Transition dynamics: common 0.7, rare 0.3
    transition_matrix = np.array([[0.7, 0.3],  # If choose A (0): X with 0.7, Y with 0.3
                                  [0.3, 0.7]]) # If choose U (1): X with 0.3, Y with 0.7

    # Q-values
    q1 = np.zeros(2)        # first-stage MF values
    q2 = np.zeros((2, 2))   # second-stage MF values for states X(0), Y(1)

    # Outputs
    stage1_choice = np.zeros(n_trials, dtype=int)
    state2 = np.zeros(n_trials, dtype=int)
    stage2_choice = np.zeros(n_trials, dtype=int)
    reward = np.zeros(n_trials, dtype=int)

    # For WSLS bias
    last_a1 = None
    last_reward = 0.0
    last_is_common = 0  # 1 common, 0 rare

    # Precompute drift reward matrix per trial for convenience
    # reward_probs[s][a]

    for t in range(n_trials):
        reward_probs = np.array([[drift1[t], drift2[t]],
                                 [drift3[t], drift4[t]]])

        # Trial-wise forgetting (decay) modulated by anxiety
        decay_eff = decay * (0.5 + 0.5 * float(stai))
        q1 *= (1.0 - decay_eff)
        q2 *= (1.0 - decay_eff)

        # First-stage WSLS bias based on previous outcome and transition type
        bias1 = np.zeros(2)
        if last_a1 is not None:
            reward_term = (2.0 * np.clip(last_reward, 0.0, 1.0)) - 1.0  # +1 if reward=1, -1 if reward=0
            trans_term = 1.0 if last_is_common == 1 else -1.0

            base_signal = xi * reward_term + (1.0 - xi) * trans_term
            anxiety_gain = (0.5 + 0.5 * float(stai))
            signed_signal = base_signal * (1.0 - float(stai)) - reward_term * (anxiety_gain - (1.0 - float(stai)))

            bias_strength = wsls_gain * signed_signal
            bias1[last_a1] += bias_strength

        # Stage 1 choice
        logits1 = beta * (q1 - np.max(q1)) + bias1
        z1 = logits1 - np.max(logits1)
        p_stage1 = np.exp(z1) / np.sum(np.exp(z1))
        a1 = rng.choice([0, 1], p=p_stage1)

        # Transition to state 2
        s2 = rng.choice([0, 1], p=transition_matrix[a1])

        # Stage 2 choice
        logits2 = beta * (q2[s2] - np.max(q2[s2]))
        z2 = logits2 - np.max(logits2)
        p_stage2 = np.exp(z2) / np.sum(np.exp(z2))
        a2 = rng.choice([0, 1], p=p_stage2)

        # Reward
        r = int(rng.random() < reward_probs[s2, a2])

        # Determine whether the observed transition was common or rare
        is_common = int((a1 == 0 and s2 == 0) or (a1 == 1 and s2 == 1))

        # Q-learning updates
        pe2 = r - q2[s2, a2]
        q2[s2, a2] += alpha * pe2

        target1 = q2[s2, a2]
        pe1 = target1 - q1[a1]
        q1[a1] += alpha * pe1

        # Store for next trial's WSLS bias
        last_a1 = a1
        last_reward = r
        last_is_common = is_common

        # Log trial
        stage1_choice[t] = a1
        state2[t] = s2
        stage2_choice[t] = a2
        reward[t] = r

    return stage1_choice, state2, stage2_choice, reward